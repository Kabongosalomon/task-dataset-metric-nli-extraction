<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END AUDIO STRIKES BACK: BOOSTING AUGMENTATIONS TOWARDS AN EFFICIENT AUDIO CLASSIFICATION NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Gazneli</surname></persName>
							<email>avi.g@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Zimerman</surname></persName>
							<email>gadi.zimerman@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
							<email>tal.ridnik@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
							<email>gilad.sharir@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<email>asaf.noy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END AUDIO STRIKES BACK: BOOSTING AUGMENTATIONS TOWARDS AN EFFICIENT AUDIO CLASSIFICATION NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While efficient architectures and a plethora of augmentations for end-to-end image classification tasks have been suggested and heavily investigated, state-of-the-art techniques for audio classifications still rely on numerous representations of the audio signal together with large architectures, fine-tuned from large datasets. By utilizing the inherited lightweight nature of audio and novel audio augmentations, we were able to present an efficient end-to-end (e2e) network with strong generalization ability. Experiments on a variety of sound classification sets demonstrate the effectiveness and robustness of our approach, by achieving state-of-the-art results in various settings. Public code is available at: https://github.com/Alibaba-MIIL/AudioClassification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ERANN-1-3 <ref type="bibr" target="#b0">[1]</ref> EAT-S PANN <ref type="bibr" target="#b0">[1]</ref> ERANN-2-5 <ref type="bibr" target="#b0">[1]</ref> EAT-S EAT-M AemNet <ref type="bibr" target="#b1">[2]</ref> AST <ref type="bibr" target="#b2">[3]</ref> PaSST-S <ref type="bibr" target="#b3">[4]</ref> HTS-AT <ref type="bibr" target="#b4">[5]</ref> #Parameters[?10 6 ]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy[%]</head><p>Accuracy-Parameters Trade-off on ESC-50 From scratch AudioSet ImageNet+AudioSet <ref type="figure">Figure 1</ref>: Comparison of our EAT architecture. Achieving SotA on 'From scratch' training (+3% from <ref type="bibr" target="#b5">[6]</ref>) and on AudioSet pretrain (+3.8% from efficient e2e method <ref type="bibr" target="#b1">[2]</ref>)</p><p>In signal processing, sound pattern recognition plays a crucial role with a wide range of applications. Recognition can be modeled as a classification task, whether single-label or multilabel, where the algorithm outputs predictions for class labels. A typical audio signal consists of speech, music, and other environmental sounds. Environment sound refers to a wide range of classes spanning from sea waves through engines, etc. An audio signal is typically handled by converting it to a known time-frequency (T-F) representation, usually with the help of the spectrogram and its compressed form, known as the mel-spectrogram. The former is obtained by applying Short-Time Fourier Transform (STFT) on the waveform and taking the magnitude, while the latter requires an additional stage in which mel filter-banks are applied for squashing the frequency axis to mel bins with logarithmic spacing. However, the use of mel-spectrogram comes at the cost of having to carefully adjust the parameters for time-frequency resolution and compression rate, which might vary for different classes. By nature, sound samples for distinct classes possess different characteristics manifested mainly in duration and frequency spectrum. Hence, finding one set of parameters suitable for all is implausible. For instance, the duration of the mouse-click event lasts several milliseconds, necessitating a shorter window size, compared to the cow mooing event, which lasts a few seconds, as depicted in <ref type="figure">Fig. 2</ref>.</p><p>In addition, the usage of frequency compression with logarithmic binning can also degrade the signal. For instance, chirping bird sounds naturally occupy a high-frequency band, where the transformation assigns coarsely spaced bins, as depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>, which was already handled by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>.  <ref type="figure">Figure 2</ref>: Impact of window length on time-frequency resolution: Observing a slow event, cow mooing (a), compared to a fast event, mouse click (b) through 3 typical window lengths, with 75% overlap.  In audio processing, the representation issue remains an active topic. The trade-off travels from domain knowledge incorporation in signal representations at the expense of complex architectures requiring large data to fit, through end-to-end systems which maintain performance gap, mainly on limited data scenarios <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>. In this work we prefer the end-to-end strategy. The absence of pre-processing streamlines the system, by requiring fewer parameters to tune, and facilitates shifting between tasks with distinct signal content. Furthermore, usage of raw signal allows us to apply wide set of augmentations, including two novel schemes, which significantly decrease the gap reported in the early works. On top of data manipulation schemes, we propose a neural network, designed to handle raw audio signal characteristics. The resulted solution is simple with a low memory footprint and short inference time, as well as robust for distinct audio contents. The proposed system was evaluated on several public datasets, such as ESC-50 <ref type="bibr" target="#b9">[10]</ref>, UrbanSound8K <ref type="bibr" target="#b10">[11]</ref>, AudioSet <ref type="bibr" target="#b11">[12]</ref> and SpeechCommands <ref type="bibr" target="#b12">[13]</ref>, achieving state of art results on several scenarios. The contribution of the paper can be summarized as follows:</p><p>? Introducing two novel and effective augmentations for audio signals ? Designing an efficient deep learning architecture ? Demonstrating the potential of end-to-end methods, and their superiority in several audio benchmarks 2 Related Work Audio pattern recognition systems mainly rely on transforming the raw-audio signal to time-frequency representation, mostly to a mel-spectrogram representation, and then using deep neural networks to output the class prediction. Empirically, they outperform early models which used e2e audio networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="bibr" target="#b14">[15]</ref> performed a comprehensive comparison among numerous representations given the network architecture, deducing the superiority of the melspectrogram compared to others. The common modus operandi in deep learning is to use transfer learning from pretrained networks. <ref type="bibr" target="#b15">[16]</ref> demonstrated the benefit of using architectures such as DenseNet <ref type="bibr" target="#b16">[17]</ref>, ResNet <ref type="bibr" target="#b17">[18]</ref> and Inception <ref type="bibr" target="#b18">[19]</ref> pre-trained on ImageNet <ref type="bibr" target="#b19">[20]</ref> when applied on mel-spectrograms for the audio classification tasks. To adapt the input type to mentioned architectures, they deduced that incorporating different time-frequency resolution maps is beneficial over simple replication across the channels. The authors of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> preferred the log-spectrogram and wavelet-based representations for their ResNet50-based network. The diversity in input type may indicate on lack of robustness across tasks and require carefully adjusting the pre-defined parameters. Several works focused on e2e architectures, albeit, there was a performance gap mainly on limited data scenarios <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>. Improved performance obtained by incorporating some domain knowledge with initialization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, complex architectures <ref type="bibr" target="#b0">[1]</ref>, even using additional self supervised training phase <ref type="bibr" target="#b22">[23]</ref>. Lately, the emergence of transformers <ref type="bibr" target="#b23">[24]</ref> infiltrated to audio processing domain with an invigorating effect. For instance, <ref type="bibr" target="#b2">[3]</ref> applied the transformer on mel-spectrogram patches with impressive results across several datasets. In order to relax the training complexity and enabling variable size inference, <ref type="bibr" target="#b3">[4]</ref> suggested dedicated regularization scheme during the training process together with the disentanglement of positional encoding to time and frequency axis. However, the state-of-the-art (SotA) results rely on complex models, posing hard constraints on deployment and inference. <ref type="bibr" target="#b24">[25]</ref> introduced a set of augmentations and became ubiquitous for spectrogram-based systems. The list involves time warping and masking the time/frequency axis. An additional widely used <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref> strategy is to mix pairs of samples in amplitude <ref type="bibr" target="#b8">[9]</ref>, similar to <ref type="bibr" target="#b27">[28]</ref> on image pixels, except the mixing ratio normalized by sample gain. Our solution expands the augmentation portfolio by suggesting two novel mixing strategies, by scrambling the pairs of signals in frequency and phase, in addition to a neural network architecture dedicated to processing the signal efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this chapter, we describe our approach to audio classification. In general, the method involves augmenting data distribution and better integrating sound characteristics into architecture design. First, we will describe our architecture for audio classification, then we will present novel ways of augmenting sound signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EAT Architecture</head><p>The proposed audio classification network is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. During this stage, the primary focus was to build a neural network that has a large receptive field, while keeping complexity low. One can decompose the network into two main blocks, a 1D convolution stack, and a transformer encoder block. The former downsamples along the time axis with a convolution layer coupled to a fixed low-pass filter <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, followed by intermittent residual blocks <ref type="bibr" target="#b17">[18]</ref>. The residual blocks are modified according to <ref type="bibr" target="#b30">[31]</ref>, consisting of depth-wise convolution with a large kernel operating on the time axis, and f (x) is convolution with kernel size equal to 1 operating across channels. At this point, the signal is decimated using a sequence of factors d i by an overall factor of d = d i . For instance, signal with a 5-second duration the downsampling sequence is equal to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4]</ref>, performing a reduction by a factor of 256. This can be to some extent linked to downsampling performed during spectrogram operation 1 . The following building blocks perform additional reduction, with each followed by a stack of dilated residual blocks <ref type="bibr" target="#b31">[32]</ref>. This refinement enables to increase in the receptive field per frame, hence being more robust to variable duration events among the classes in environmental sound scenarios. Gathering feature maps across frames was implemented using a transformer encoder block, which followed by fully connected layer to project the embedding vector to class space. For complexity analysis and details about EAT-S and EAT-M models refer to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation</head><p>Data augmentation is a ubiquitous step during the training phase for deep learning networks, particularly in the case of limited data. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> already pointed out the inferiority of e2e audio-based systems in case of limited data. This can be mitigated to some extent by enriching the list of augmentations. We noticed that specifically, augmentations involving label mixing were beneficial for generalization. We suggest mixing frequency bins and phases and naming them as FreqMix and PhaseMix. In FreqMix, alg. <ref type="bibr" target="#b0">[1]</ref>, given pair of samples with corresponding labels, we perform the mix by choosing low-frequency bins from one sample and concatenating with high frequency with another sample, or vice versa. The operation is equivalent to applying ideal filters in frequency domain and adding their results. In addition to the mixup <ref type="bibr" target="#b27">[28]</ref> operation, FreqMix contributes to enlarging the size of in-between samples, by introducing a convex combination of filtered versions of the pair, Equation 1. The filtering erases a small amount of information in the frequency domain from the original sample, and fills the filtered spectrum portion from its counterpart. Sound signals are rarely narrow-band, such as a pure sine wave, thus removing a segment of contiguous frequency is unlikely to erase the entire data. The mixing, in addition spans the linear behaviour for in-between samples, to larger set than the original mixup.</p><p>MixUp:</p><formula xml:id="formula_0">x mix [n] = ? ? x 1 [n] + (1 ? ?) ? x 2 [n] FreqMix: x mix [n] = x 1 [n] * h 1 [n; ?] + x 2 [n] * h 2 [n; ?]<label>(1)</label></formula><p>where, h 1 [n; ?], h 2 [n; ?] are low-pass and high-pass filters parametrized by ?, controlling the cut-of frequency. Raw audio maintains an additional signal characteristic, the phase. To demonstrate that phase contains some amount of discriminative information, we conducted an experiment, as described in Appendix A, in which we synthesized waveforms from the phase of the signal and trained our neural network. The classification result was significantly higher than the random guess, as detailed in <ref type="table" target="#tab_8">Table 7</ref>. As a consequence, we suggest adding more robustness by mixing the phase among the samples, and name it PhaseMix. In PhaseMix, alg. <ref type="bibr" target="#b1">[2]</ref>, the amplitude of the original signal remains, while phase is mixed. The level of mixing is dictated according to the mixing ratio, which is randomly drawn in each iteration. These two mixing strategies come on top of applying modified mixup <ref type="bibr" target="#b8">[9]</ref> and cutmix <ref type="bibr" target="#b32">[33]</ref> that we adopted for the 1D case, with results summarized in <ref type="table" target="#tab_7">Table 6</ref>.</p><p>In addition to the transformations involving the mixing of labels, we used transforms that preserved labels, such as amplitude manipulation, re-sampling, filtering, time-shifting, and a variety of noises. It is worth mentioning that working with raw signals enables incorporating larger set of transformations with mathematical interpretations. Shifting the signal in time, for example, is reflected in the frequency domain by adding linear phase. This has no affect when working with spectrogram, where the magnitude step by definition discards phase. Furthermore, mimicking the time shift by shifting the spectrogram along the time axis, is not an equivalent operation, since time shift and absolute value are not interchangeable. With the proposed augmentation scheme, the early reported gap for limited data scenarios <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> was eliminated. The whole set of transforms detailed in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we will provide the experiments conducted on public classification benchmarks, such as ESC-50 <ref type="bibr" target="#b9">[10]</ref>, AudioSet <ref type="bibr" target="#b11">[12]</ref> UrbanSound8K <ref type="bibr" target="#b10">[11]</ref>. In addition to ESC scenarios, we examined the system on SpeechCommands <ref type="bibr" target="#b12">[13]</ref> dataset, containing spoken words in English, to show some robustness to audio signal type. During the training process, we used AdamW <ref type="bibr" target="#b34">[35]</ref> optimizer with maximal learning rate of 5 ? 10 ?4 and one cycle strategy <ref type="bibr" target="#b35">[36]</ref>. In addition, we use Algorithm 1 FreqMix 1: let (x 1 , y 1 ), (x 2 , y 2 ) be samples from dataset X 2:</p><formula xml:id="formula_1">X 1 = ST F T (x 1 ), 3: X 2 = ST F T (x 2 ) 4: ? ? U [0.5, 1], p ? U [0, 1] 5: k c = int(? ? n f f t ) 6: X mix = X 1 [n f f t ? k c :, :] ? X 2 [: k c , :] p ? 0.5 X 1 [: k c , :] ? X 2 [n f f t ? k c :, :] p &gt; 0.5 7: x mix = IST F T (X mix ) 8: y mix = ? ? y 1 + (1 ? ?) ? y 2</formula><p>Algorithm 2 PhaseMix 1: let (x 1 , y 1 ), (x 2 , y 2 ) be samples from dataset X 2:</p><formula xml:id="formula_2">X 1 = ST F T (x 1 ) 3: X 2 = ST F T (x 2 ) 4: ? 1 [k, l] = ?X 1 [k, l] 5: ? 2 [k, l] = ?X 2 [k, l] 6: ? ? U [0, 1] 7: ? y = 0.5 ? ? + 0.5 8: ? mix = ? ? ? 1 + (1 ? ?) ? ? 2 9: X mix [k, l] = |X 1 [k, l]| ? e j?mix[k,l] 10: x mix = IST F T (X mix ) 11: y mix = ? y ? y 1 + (1 ? ? y ) ? y 2</formula><p>weight decay with 10 ?5 , EMA <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> with decay rate of 0.995 and SKD <ref type="bibr" target="#b38">[39]</ref>. The loss is label-smoothing with a noise parameter set to 0.1 for single-label classification tasks, and binary cross-entropy for the multi-label classification case. When applying mixing augmentations we use multi-label objective and use binary cross-entropy, as suggested by <ref type="bibr" target="#b39">[40]</ref>. To handle distinct sample lengths across datasets we adjust the parameter controlling the downsample of the network. The set of augmentations used is detailed 1, with one noise type and one mixing strategy being randomly selected in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ESC-50</head><p>The ESC-50 set <ref type="bibr" target="#b9">[10]</ref> consists of 2000 samples of environmental sounds for 50 classes. Each sample has a length of 5 seconds and is sampled at 44.1KHz. The set has an official split into 5 folds. We resampled the samples to 22.05KHz for being compliant with the majority of other works, and followed the standard 5-fold cross-validation to evaluate our model. Each experiment repeated three times and averaged to final score.  It is evident from the results that our method is more effective than others under the same settings. In absence of external data, the next in line in accuracy <ref type="bibr" target="#b5">[6]</ref> possess ?2.6 more parameters, while similar model size network <ref type="bibr" target="#b1">[2]</ref> has a 10% gap in accuracy. In the AudioSet fine-tuned case, we manage achieve SotA while being 33% lighter than <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UrbanSound8K</head><p>UrbanSound8K is an audio dataset containing 8732 labeled sound samples, belonging to 10 class labels, split to 10 folds. The samples last up to 4 seconds and the sampling rate varies from 16KHz-48KHz. The classes are drawn from the urban sound taxonomy and all excerpts are taken from field recordings 2 . The experiment was conducted on the official 10 fold split, with samples resampled to 22.05KHz and zero-padding the short samples to 4 seconds. The results on the UrbanSound8K dataset, detailed in <ref type="table" target="#tab_4">Table 3</ref>, follow the same pattern as on the ESC-50, <ref type="table" target="#tab_3">Table 2</ref>, by outperforming previous approaches in the limited data scenario, while being competitive in the fine-tune mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SpeechCommands</head><p>Speech Commands V2 <ref type="bibr" target="#b12">[13]</ref> is a dataset consisting of ? 106K recordings for 35 words with a 1-second duration, with a sampling rate equal to 16KHz. The set has an official train, validation, and test split with ? 84K, ? 10K, and ? 11K samples, respectively. Our experiment involves the 35-class classification task.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we see that our approach achieves SotA results even without using external data, while being at least ?6 lighter than other methods. Furthermore, demonstrating that our method is robust to additional content, such as speech. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">AudioSet</head><p>AudioSet <ref type="bibr" target="#b11">[12]</ref> is a collection of over 2 million 10-second audio clips excised from YouTube videos with a class ontology of 527 labels covering a wide range of everyday sounds, from human and animal sounds, to natural and environmental sounds, to musical and miscellaneous sounds. The set consist of two subsets, named balanced with 20K samples and unbalanced training with 2M samples, with evaluation set with 20K samples. The noise augmentations were excluded during training due to the presence of noise, and pink noise in class labels.  <ref type="table" target="#tab_6">Table 5</ref>, evidently demonstrates the advantage of training a large model, for fitting large sets. Yet, our method can be a considered a good balance in terms of accuracy vs efficiency, without apparent affect on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency and edge deployment</head><p>In this section the focus will be on model complexity. Complexity translates to model size and inference time, which can induce costs and inflexibility for platforms and applications. Our EAT-S model has 5.3M parameters, which resembles the proportion of MobileNet-V2 architecture [41] in both size and inference time, as detailed in Appendix B. This makes EAT-S a candidate for deploying audio classification capabilities in low-memory edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation study</head><p>In this section we explore the impact of our suggestions for augmentations and architecture. The experiments were conducted on the ESC-50 dataset. For Tables 6a and 6b , in each experiment, we incrementally add to the baseline and report the relative result. For the augmentation ablation study, <ref type="table" target="#tab_7">Table 6a</ref>, the baseline refers to not applying any mixing augmentations, while in the architecture ablation study, <ref type="table" target="#tab_7">Table 6b</ref>, the baseline refers to our architecture without the suggested modifications -(a) modified residual block and dilated residual blocks vs. common residual block and (b) transformer vs. convolution layer with global average pooling. We can see from Tables 6a and 6b that in both cases, the increments significantly improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented new audio augmentations and a novel, simple and efficient architecture for sound classification. We were able to show, through analysis and experiments, that end-to-end audio systems can no longer be considered inferior, especially in low data scenarios. The suggested scheme achieves state of the art results in several datasets both in 'from-scratch' and AudioSet-pretraining setups, all while being exceptionally light-weight and robust. Future work can elaborate this work to solve additional tasks and contents, such as sound event detection, localization or speech and speaker recognition.</p><p>[41] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Phase waveform synthesis</head><p>We assumed that the audio signal's phase component may contain discriminative information. To confirm this assumption, we synthesized waveforms from the phase of the signal. Spectrogram based representations inherently ignore the phase, by taking only the magnitude. Mathematically this is equivalent to filtering the original signal with a unit amplitude filter with the opposite phase, as described at equations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>: </p><formula xml:id="formula_3">X</formula><p>The "phase" waveform was extracted according to equation 3. For this experiment, the classifier was trained (a) on the original signal, (b) signals based on phase, (c) signals based on the magnitude, and (d) on both (b)+(c), concatenated to produce a 2-dimensional input signal. The experiment was conducted on the ESC-50 dataset, with noise and mixing augmentations being disabled 4 . From <ref type="table" target="#tab_8">Table 7</ref> we can see two outcomes. At first, the phase signal resulted in significantly higher accuracy than a random guess. Second, seems that this information is complementary to the magnitude signal. These observations lead us to augment the phase domain during the training process, by adding phase noise and mixing the phases among pairs of signals.   <ref type="table" target="#tab_11">Table 9</ref> details the configuration for our models, EAT-S/M. "Channels" refers to number of filter at the first stage of the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Inference time details</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>spectrogram and mel-spectrogram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Impact of mel scale compression: Comparing linear frequency spacing (left) against logarithmic mel scale (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The proposed EAT architecture, CNNstyle backbone followed by a transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[k, l] = ST F T (x[n]) X[k, l] = |X[k, l]| ? e j?[k,l] |X[k, l]| = X[k, l] ? e ?j?[k,l](2)Which can rephrased as filtering operation -|X[k, l]| = X[k, l] ? H[k, l] h[n] = IST F T (H[k, l])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>List of label preserving and label mixing augmentations</figDesc><table><row><cell>Name</cell><cell>Description</cell></row><row><cell>amplitude</cell><cell>random amplitude to whole or fragment of sample</cell></row><row><cell>noise</cell><cell>white, blue, pink, violet, red, uniform, phase noise</cell></row><row><cell>time shift</cell><cell>linear/cyclic with integer and fractional delay</cell></row><row><cell>filtering</cell><cell>low/high pass filter with randomized cutoff frequency</cell></row><row><cell>invert polarity</cell><cell>multiply by -1</cell></row><row><cell>time masking</cell><cell>similar to cutout [34], masking fragment of sample</cell></row><row><cell>quantization</cell><cell>quantize sample using ? law or linear regime</cell></row><row><cell>mixup</cell><cell>mixing amplitude [28], [9]</cell></row><row><cell>timemix</cell><cell>mixing in time axis, similar to cutmix [33]</cell></row><row><cell>freqmix</cell><cell>alg.[1]</cell></row><row><cell>phasemix</cell><cell>alg.[2]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>ESC-50, accuracy with model size and inference time measured on P-100 machine.</figDesc><table><row><cell>Model</cell><cell>e2e</cell><cell>Pretrained</cell><cell cols="3">Accuracy[%] #Parameters[?10 6 ] time[msec]</cell></row><row><cell>ESResNet-Att [7]</cell><cell></cell><cell>none</cell><cell>83.15</cell><cell>32.6</cell><cell>11.3</cell></row><row><cell>ERANN-1-3 [6]</cell><cell></cell><cell>none</cell><cell>89.2</cell><cell>13.6</cell><cell>-</cell></row><row><cell>EnvNet-v2 [9]</cell><cell></cell><cell>none</cell><cell>84.9</cell><cell>101</cell><cell>2.7</cell></row><row><cell>AemNet WM1.0 [2]</cell><cell></cell><cell>none</cell><cell>81.5</cell><cell>5</cell><cell>-</cell></row><row><cell>EAT-S</cell><cell></cell><cell>none</cell><cell>92.15</cell><cell>5.3</cell><cell>8.3</cell></row><row><cell>PANN [1]</cell><cell></cell><cell>AudioSet</cell><cell>94.7</cell><cell>81</cell><cell>-</cell></row><row><cell>ERANN-2-5 [6]</cell><cell></cell><cell>AudioSet</cell><cell>96.1</cell><cell>37.9</cell><cell>-</cell></row><row><cell>AemNet-DW WM1.0 [2]</cell><cell></cell><cell>AudioSet</cell><cell>92.32</cell><cell>1.2</cell><cell>-</cell></row><row><cell>EAT-S</cell><cell></cell><cell>AudioSet</cell><cell>95.25</cell><cell>5.3</cell><cell>8.3</cell></row><row><cell>EAT-M</cell><cell></cell><cell>AudioSet</cell><cell>96.3</cell><cell>25.5</cell><cell>9.6</cell></row><row><cell>AST [3]</cell><cell></cell><cell>ImageNet+AudioSet</cell><cell>95.6</cell><cell>88.1</cell><cell>26.7</cell></row><row><cell>PaSST-S [4]</cell><cell></cell><cell>ImageNet+AudioSet</cell><cell>96.8</cell><cell>85.4</cell><cell>25.4</cell></row><row><cell>HTS-AT [5]</cell><cell></cell><cell>ImageNet+AudioSet</cell><cell>97</cell><cell>31</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>UrbanSound8K, accuracy with model size and inference time measured on P-100 machine</figDesc><table><row><cell>Model</cell><cell>e2e</cell><cell>Pretrained</cell><cell cols="3">Accuracy[%] #Parameters[?10 6 ] time[msec]</cell></row><row><cell>ESResnet-Att [7]</cell><cell></cell><cell>none</cell><cell>82.76</cell><cell>32.6</cell><cell>11</cell></row><row><cell>AemNet WM1.0 [2]</cell><cell></cell><cell>none</cell><cell>81.5</cell><cell>5</cell><cell>-</cell></row><row><cell>ERANN-1-4 [6]</cell><cell></cell><cell>none</cell><cell>83.5</cell><cell>24.1</cell><cell>-</cell></row><row><cell>EAT-S</cell><cell></cell><cell>none</cell><cell>85.5</cell><cell>5.3</cell><cell>8.5</cell></row><row><cell>ERANN-2-6 [6]</cell><cell></cell><cell>AudioSet</cell><cell>90.8</cell><cell>54.5</cell><cell>-</cell></row><row><cell>EAT-S</cell><cell></cell><cell>AudioSet</cell><cell>88.1</cell><cell>5.3</cell><cell>8.5</cell></row><row><cell>EAT-M</cell><cell></cell><cell>AudioSet</cell><cell>90</cell><cell>25.5</cell><cell>9.6</cell></row><row><cell>ESResNeXt-fbsp [21]</cell><cell></cell><cell>ImageNet+AudioSet</cell><cell>89.14</cell><cell>25</cell><cell>18.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Speech Commands V2 (35 classes), accuracy with model size and inference time measured on P-100 machine Model e2e Pretrained type Result[%] #Parameters[?10 6 ] time[msec]</figDesc><table><row><cell>AST [3]</cell><cell>ImageNet</cell><cell>98.11</cell><cell>87.3</cell><cell>11</cell></row><row><cell>HTS-AT [5]</cell><cell>AudioSet</cell><cell>98.0</cell><cell>31.0</cell><cell>-</cell></row><row><cell>EAT-S</cell><cell>none</cell><cell>98.15</cell><cell>5.3</cell><cell>7.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">: Audioset, mAP with model size and inference time measured on P-100, w/o external data</cell></row><row><cell>Model</cell><cell cols="4">e2e Pretrained mAP[%] #Parameters[?10 6 ] time[msec]</cell></row><row><cell>AST [3]</cell><cell>none</cell><cell>36.6</cell><cell>88.1</cell><cell>63.5</cell></row><row><cell>ERANN-1-6 [6]</cell><cell>none</cell><cell>45.6</cell><cell>54.5</cell><cell>14 3</cell></row><row><cell>AemNet WM1.0 [2]</cell><cell>none</cell><cell>33.16</cell><cell>5</cell><cell>-</cell></row><row><cell>HTS-AT [5]</cell><cell>none</cell><cell>45.3</cell><cell>31</cell><cell>-</cell></row><row><cell>EAT-S</cell><cell>none</cell><cell>40.5</cell><cell>5.3</cell><cell>8.4</cell></row><row><cell>EAT-M</cell><cell>none</cell><cell>42.6</cell><cell>25.5</cell><cell>14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablations -Classification results conducted on ESC-50 (incremental improvements over baselines)</figDesc><table><row><cell cols="2">(a) Baseline 83% (without any mix)</cell><cell cols="2">(b) Baseline 80.5% (without architecture modification)</cell></row><row><cell>Model</cell><cell>Relative accuracy</cell><cell>Block</cell><cell>Relative accuracy</cell></row><row><cell></cell><cell>to baseline[%]</cell><cell></cell><cell>to baseline[%]</cell></row><row><cell>+mixup</cell><cell>+3.5</cell><cell>+modified residual blocks</cell><cell>+1.8</cell></row><row><cell>+cutmix</cell><cell>+0.5</cell><cell>+dilated residual blocks</cell><cell>+6.5</cell></row><row><cell>+freqmix</cell><cell>+3.1</cell><cell>+transformer</cell><cell>+2</cell></row><row><cell>+phasemix</cell><cell>+0.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>ESC-50, accuracy vs input content</figDesc><table><row><cell>Mode</cell><cell>Accuracy[%]</cell></row><row><cell>phase</cell><cell>60</cell></row><row><cell>magnitude</cell><cell>78.5</cell></row><row><cell>phase+magnitude</cell><cell>80.5</cell></row><row><cell>baseline</cell><cell>81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>details the inference time for various network configurations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Inference time measured on V-100 machine, and Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz on EAT-S model sample length[s] gpu-time[ms] cpu-time[ms]</figDesc><table><row><cell>1</cell><cell>5.3</cell><cell>38.3</cell></row><row><cell>5</cell><cell>5.5</cell><cell>67</cell></row><row><cell>10</cell><cell>5.6</cell><cell>145</cell></row></table><note>4 Due to the noisy nature of the phase signal C EAT Models -Architectures details</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Architecture details Model Channels Transformer layers/heads embedding dimension #Parameters[?10 6 ]</figDesc><table><row><cell>EAT-S(mall)</cell><cell>16</cell><cell>4/8</cell><cell>128</cell><cell>5.3</cell></row><row><cell>EAT-M(edium)</cell><cell>32</cell><cell>6/16</cell><cell>256</cell><cell>25.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Depending on the sampling rate and STFT parameters, for example, the typical choice for 22.05KHz can be window size equal to 1024 with hopping of 256, which effectively decimates by 256 the time axis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Can be found at www.freesound.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Measured on V-100 machine<ref type="bibr" target="#b5">[6]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient end-to-end audio embeddings generation for audio classification on target applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Lopez-Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">A</forename><surname>Del Hoyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Ontiveros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="601" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ast: Audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01778</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient training of audio transformers with patchout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05069</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00874</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eranns: Efficient residual audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Verbitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Berikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viacheslav</forename><surname>Vyshegorodtsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01621</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Esresnet: Environmental sound classification based on visual domain models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4933" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">End-to-end learning for music audio tagging at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Prockup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02520</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10282</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongpil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhyoung</forename><forename type="middle">Luke</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01789</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Comparison of time-frequency representations for environmental sound classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Huzaifah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07156</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking cnn models for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalesh</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11154</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Esresne(x)t-fbsp: Learning robust time-frequency transformation of audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">J?rn Hees, and Andreas Dengel</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Utilizing domain knowledge in end-to-end audio processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tycho Max Sylvester</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Luis Diez Antich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Purwins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00254</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06508</idno>
		<title level="m">Multi-format contrastive learning of audio representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for audio signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Purwins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo-Yiin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3292" to="3306" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Axial residual networks for cyclegan-based voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyuhyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalhyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsu</forename><surname>Chae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08075</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Melgan: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Zhen</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>De Br?bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
