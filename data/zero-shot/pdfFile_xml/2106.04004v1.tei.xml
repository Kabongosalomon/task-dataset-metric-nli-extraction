<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Task-Generic Hierarchical Human Motion Prior using VAEs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaman</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sounthern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
								<address>
									<addrLine>3 Adobe Research 4 Pinscreen 5 UC</addrLine>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfei</forename><surname>Kuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sounthern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
								<address>
									<addrLine>3 Adobe Research 4 Pinscreen 5 UC</addrLine>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
								<address>
									<addrLine>3 Adobe Research 4 Pinscreen 5 UC</addrLine>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Task-Generic Hierarchical Human Motion Prior using VAEs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Our general purpose motion prior consists of a latent space of human motions and is learned using a hierarchical motion variational autoencoder (HM-VAE). Our approach is task-generic and can be directly adopted to a wide range of applications. Left: Motion interpolation and completion can be accomplished by traversing the latent space. Right: Noisy pose estimation can be refined by projecting noisy inputs into our latent space and decoding back. And a latent vector in the learned latent space is corresponding to a valid motion sequence.</p><p>Abstract A deep generative model that describes human motions can benefit a wide range of fundamental computer vision and graphics tasks, such as providing robustness to videobased human pose estimation, predicting complete body movements for motion capture systems during occlusions, and assisting key frame animation with plausible movements. In this paper, we present a method for learning complex human motions independent of specific tasks using a combined global and local latent space to facilitate coarse and fine-grained modeling. Specifically, we propose a hierarchical motion variational autoencoder (HM-VAE) that consists of a 2-level hierarchical latent space. While the global latent space captures the overall global body motion, the local latent space enables to capture the refined poses of the different body parts. We demonstrate the effectiveness of our hierarchical motion variational autoencoder in a variety of tasks including video-based human pose estimation, motion completion from partial observations, and motion synthesis from sparse key-frames. Even though, our model has not been trained for any of these tasks specifically, it provides superior performance than task-specific alternatives. Our general-purpose human motion prior model can fix corrupted human body animations and generate complete movements from incomplete observations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The modeling of human motions is a core component for many vision tasks, including pose estimation, action recognition, motion synthesis, and motion prediction. Several recent work have demonstrated new capabilities for generating complex body movements and capturing motion from unconstrained videos <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">36]</ref>. While robustness and accuracy is constantly evolving for these methods, highly challenging scenes, occlusions, and body poses can still result in corrupted animations and noise.</p><p>Conventional techniques for reducing artifacts, include temporal filtering <ref type="bibr" target="#b18">[19]</ref>, inverse kinematics <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b9">10]</ref> and statistical human motion priors <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. While ef-fective in reducing unwanted jitters and implausible poses, these methods do not generalize well to complex motions and the results are often inaccurate w.r.t. the ground truth.</p><p>To address this challenge, deep learning-based motion priors were proposed which are particularly effective in representing complex motion variations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. These priors are generally designed for predetermined tasks, such as 3D pose estimation from a video, and a common problem is to be able to cover all possible input cases during training, such as occlusions, motion blur, etc. Ideally, we could build a prior model that describes the space of plausible human body movements, independently of the application and simply plug this model into any system. Training such model would simply consist of collecting high-quality motion capture data (task-generic), instead of fitting for example 3D models to an image (task-specific).</p><p>We introduce a generalized motion prior, that learns complex human body motions from high-fidelity motion capture data <ref type="bibr" target="#b31">[32]</ref>. Similar to the work of <ref type="bibr" target="#b54">[54]</ref> who developed a deep optimized prior for 3D modeling, our prior for motion is a general purpose one. We present a deep generative model based on a joint global and local latent space representation that can accurately capture the poses of different body parts while also modeling the global correlations across the body joints. Specifically, we adopt a two-level hierarchical motion variational autoencoder (HM-VAE) which maps the human motion to global and local latent spaces simultaneously. Our HM-VAE model adopts the recently proposed skeleton-aware architecture <ref type="bibr" target="#b0">[1]</ref> and defines the global and local latent spaces via direct pooling and unpooling operations on the skeleton structure. While our HM-VAE successfully models the local human motion, we introduce an additional trajectory prediction component to model global motions. Taking local joint positions as input, our trajectory model estimates the root joint velocity at each timestep, enabling us to recover human motions in world space.</p><p>We show the generality and effectiveness of our human motion prior on various applications. First, we show that our task-generic model can refine human motions predicted from video <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref> by mapping noisy predictions into our motion prior latent space. We also demonstrate that our model can perform motion completion given partial observations (e.g., the upper body motion only) or motion synthesis given sparse keyframes. In both of these tasks, we optimize for both the global and local latent variables to match the partial observations and restore complete plausible motion sequences. While our model is not trained for any of these tasks specifically, it outperforms task-specific alternatives both qualitatively and quantitatively.</p><p>Our contributions are as follows. First, we present an effective task-generic motion prior model, that can improve the performance of a wide range of applications. Second, we propose a two-level hierarchical motion variational autoencoder (HM-VAE) that consists of a skeleton-aware architecture, allowing it to accurately capture the local motion of body parts and the global correlation between them. Finally, we introduce a trajectory prediction module to model the global trajectory conditioned on the local body motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning Based Priors. The ability of deep neural networks to model data priors has sparked research in a variety of domains. Deep Image Prior (DIP) <ref type="bibr" target="#b43">[43]</ref> shows that a generator network without any learning is an effective prior for image restoration. Given randomly initialized weights, the neural network is able to perform image denoising or super resolution via optimization defined by a task-dependent energy term and a regularizer. A similar idea is proposed and validated in the video domain <ref type="bibr" target="#b25">[26]</ref>, by training a network to mimic specific image operators in a single test video, the learned video prior is able to eliminate temporal inconsistencies in various video processing tasks. Besides discovering priors in the 2D domain, the ability to capture 3D priors is also demonstrated in recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b54">54]</ref>. Point2Mesh <ref type="bibr" target="#b11">[12]</ref> randomly samples a fixed vector and optimizes the network parameters to reconstruct a mesh with geometric details and showcases the effectiveness of self-prior. Deep Optimized Priors <ref type="bibr" target="#b54">[54]</ref> propose to learn a pre-trained prior first which then serves as initialization for optimizing both the latent vector and the decoder parameters given a task-specific objective and regularization loss. In this work, we investigate data-driven priors in the human motion domain and validate the effectiveness of our method by applying it to various human motion tasks without explicitly training for any specific tasks.</p><p>Generative Motion Modeling. With the recent success of learning based methods, several works have focused on generative models for motion synthesis. Martinez et al. <ref type="bibr" target="#b32">[33]</ref> propose a recurrent neural network model for generating future human motion by predicting future joint velocities and adding them to previous joint positions. MT-VAE <ref type="bibr" target="#b53">[53]</ref> propose a probabilistic recurrent neural network method for generating multiple future human motions. Aksan et al. <ref type="bibr" target="#b2">[3]</ref> propose to predict future human motion by exploiting the kinematic structure in human bodies. Following the autoregressive generative model formulation, Motion Transformers <ref type="bibr" target="#b26">[27]</ref> are introduced to model the future pose distribution along with a discrete pose representation, leveraging the advantage of the Transformer <ref type="bibr" target="#b45">[45]</ref> architecture. Motion-VAE <ref type="bibr" target="#b28">[29]</ref> models the future pose distribution given previous pose using a variational autoencoder (VAE) <ref type="bibr" target="#b21">[22]</ref> approach. Normalizing flows is another category of generative models recently applied to human motion modeling. MoGlow <ref type="bibr" target="#b13">[14]</ref> uses normalizing flows for motion modeling and achieve re-alistic motion generation taking root trajectory as the conditioning signal. Recent work also address the problem of motion in-betweening from a generative modeling perspective. Long-term motion in-betweening <ref type="bibr" target="#b57">[57]</ref> uses a generative adversarial neural network (GAN) <ref type="bibr" target="#b8">[9]</ref> approach to generate human motion given sparse key-frames. Robust Motion in-betweening <ref type="bibr" target="#b12">[13]</ref> employs an LSTM to generate a motion sequence given initial frames and end frames while also allowing for motion variations. Here, we focus on extending VAEs to model long-term human motion. Our key difference is to embed multiple frames of motions into a hierarchical global and local latent spaces. Methods like MotionVAE model motion in a per-frame basis while our method maps a full motion sequence into a compact latent space.</p><p>Motion Estimation From Videos. A multitude of optimization-based <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>, learning-based <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b19">20]</ref>, and hybrid methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> have been proposed to tackle the problem of single-image 3D human pose estimation. Its rapid progress has stimulated research interest on the longstanding problem of extracting 3D human motion from videos <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>. VIBE <ref type="bibr" target="#b26">[27]</ref> uses an LSTM to capture temporal information and introduce a discriminator training strategy to ensure the predicted poses lie in a valid manifold. MEVA <ref type="bibr" target="#b30">[31]</ref> presents a coarseto-fine strategy where a valid motion sequence is first extracted conditioned on a latent vector and then refined utilizing person-specific details. TCMR <ref type="bibr" target="#b6">[7]</ref> focuses on avoiding the temporal jitters that exist in the VIBE results and proposes a strategy to explicitly leverage past and future frames to achieve smoother results. Texture-based tracking <ref type="bibr" target="#b52">[52]</ref> is shown to improve the motion stability during optimization. Foot contacts and physically-based models <ref type="bibr" target="#b38">[38]</ref> are also used for estimating realistic human motions from videos. In this work, we are not aiming to design a specific 3D video pose estimation method. Instead, we show that our human motion prior is capable of eliminating jitters and noises that exist in the results of current state-of-the-art methods. We demonstrate that our method can be applied to any pose estimation methods and in our experiments we outperform previous work both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hierarchical Motion VAEs</head><p>The core of our method is a hierarchical motion variational autoencoder (HM-VAE) that models human motion by jointly learning a local and global latent space. Specifically, given a motion sequence x ? R T ?J?D , represented as the D dimensional joint rotations in a fixed time window of size T 1 , we first learn an embedding of x into local and global latent spaces represented by latent codes z l and z g respectively. Assuming the latent space in the local and global levels are independent <ref type="bibr" target="#b27">[28]</ref>, we then model the probability distribution of a motion sequence as:</p><formula xml:id="formula_0">p(x, z) = p(x|z l , z g )p(z l )p(z g ).<label>(1)</label></formula><p>Our variational autoencoder adopts the recently proposed skeleton-aware architecture <ref type="bibr" target="#b0">[1]</ref> to facilitate learning over the humanoid skeleton structure directly. Before we discuss the details of our model, we first provide a brief overview of the skeleton-aware architecture. We refer the reader to the original paper for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>The skeleton-aware architecture consists of three critical components that we adopt in our model design: skeleton convolution, skeleton pooling, and skeleton unpooling.</p><p>Skeleton Convolution. Given a motion sequence x, x ? R T ?J?D , we denote y, y ? R T ?J?D as the updated features after a skeleton convolution operation. For each bone i in the skeleton, the updated feature is calculated as</p><formula xml:id="formula_1">y i = 1 |N d i | j?N d i x j * W i j + b i j ,</formula><p>where the symbol * denotes a one dimensional temporal convolution operation with the temporal filter W i j ? R k?D?D and bias b i j ? R D . D represents the number of temporal filters, k represents the temporal kernel size, and N d i represents the neighboring bones of bone i within distance d. The distance between two bones (j 1 , j 2 ) is defined as the number of bones needed to cross to reach j 2 starting from j 1 along the kinematic chain. The skeleton convolution operation preserves the number of edges J while downsampling the temporal dimension to T . Skeleton Pooling. Skeleton pooling merges the features of connected bones and extracts higher-level motion features by reducing the spatial resolution of the input. The pooling operation is applied to pairs of bones which are connected by a joint with degree of 2. For example, the thigh and calf which are connected by the knee. We recursively search such bone pairs starting from the root node (the hip), and merge their corresponding features using average pooling operation. As we perform pooling, the number of joints is reduced in subsequent layers of the network. Given disjoint sets of pooling bones denoted as {P (1), P (2), ..., P (m)}, the pooling operation is defined as</p><formula xml:id="formula_2">F i = Pool(F j |j ? P (i)).</formula><p>Skeleton Unpooling. The unpooling operation mirrors skeleton pooling. Specifically, given the activation features F defined on a bone b obtained by merging the bones (i, j),</p><formula xml:id="formula_3">hence D = 6.</formula><p>unpooling simply replaces b with the bones i and j where the new bone features are defined as F i = F, F j = F . The number of bones is increased after the unpooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion Prior Learning for Local Motion</head><p>Given a motion sequence x, x ? R T ?J?D , our HM-VAE consists of an encoder and a decoder as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The encoder learns the posterior distribution of the local, z l , and global, z g latent spaces given data x:</p><formula xml:id="formula_4">q(z l , z g |x) = q(z l |f l (x))q(z g |f g (x)),<label>(2)</label></formula><p>where f l (x), f g (x) represent the motion features extracted from different layers in the encoder. Our VAE is then trained by maximizing the modified Evidence Lower BOund (ELBO) <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_5">log p(x) ? E q(z l ,zg|x) [log p(x|z l , z g )]? ?KL(q(z l |f l (x))||p(z l ))? ?KL(q(z g |f g (x))||p(z g )),<label>(3)</label></formula><p>where q(z l , z g |x) is an encoder network that maps the input x into the local and global latent spaces, p(x|z l , z g ) is a decoder network that maps latent variables back into the input x, and p(z l ) and p(z g ) are assumed to be standard normal distributions N (0, I).</p><p>Encoder. The encoder consists of four building blocks B 1 , B 2 , B 3 , B 4 where each building block is a combination of a skeleton convolution, skeleton pooling, and a LeakyReLU activation layer. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we introduce a linear layer W ? R T d?2d h after B 1 and B 4 , mapping motion features of each corresponding block to a latent space. While the shallow layer features F l after B 1 represent the local latent space, the deep layer features F g after B 4 correspond to the global latent space. We enforce a normal distribution on each latent space:</p><formula xml:id="formula_6">z l ? N (? l (F l ), ? l (F l )), z g ? N (? g (F g ), ? g (F g )). (4)</formula><p>Decoder. The decoder has a symmetric architecture to the encoder. Each building block in decoder consists of temporal upsampling, skeleton unpooling, skeleton convolution and LeakyReLU activation layers. Given the latent codes z l and z g , the decoder first maps them to features through linear layers. Temporal upsampling and skeleton unpooling operations are used to increase the number of timesteps and bones gradually. The features obtained from the global latent code after a series of temporal upsampling, skeleton unpooling and convolution are concatenated with the features obtained from the local latent code. A final block of unpooling and convolution operations are used to reconstruct the original motion sequence x. We further add a forward kinematics layer proposed in <ref type="bibr" target="#b46">[46]</ref> to convert x into joint positions P to define an additional joint position reconstruction loss. Also, we convert the 6D rotation representation to the rotation matrix R and use an additional reconstruction loss defined on the rotation matrices. Overall, the reconstruction loss used to train the decoder is defined as:</p><formula xml:id="formula_7">L rec = L 6d + L rot + ?L joints ,<label>(5)</label></formula><formula xml:id="formula_8">L 6d = T t=1 ||x t ? x t || 2 ,<label>(6)</label></formula><formula xml:id="formula_9">L rot = T t=1 ||R t ? R t || 2 ,<label>(7)</label></formula><formula xml:id="formula_10">L joints = T t=1 ||P t ? P t || 2 .<label>(8)</label></formula><p>we experimentally set ? to 10 in our training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Trajectory Prediction</head><p>Given a motion sequence, we use the presented HM-VAE to model the local motion, i.e., the local joint rotations. In addition, we utilize a similar skeleton-aware architecture without reducing temporal dimension to model the global root joint trajectory. Specifically, given a sequence of joint positions denoted as P ? R T ?J?3 , we apply four skeleton convolution layers with skeleton pooling layers to obtain motion features F ? R T ?J ?d . We use a linear layer that takes F as input and estimates the root velocity V ? R T ?3 . By accumulating the root velocity in subsequent frames, we compute the global root trajectory G ? R T ?3 . The root trajectory at any particular time t is defined as G t = t i=0 V i . We train the trajectory estimation module with a loss function that consists of both velocity and trajectory terms:</p><formula xml:id="formula_11">L traj = T t=1 ||V t ? V t || 2 + ||G t ? G t || 2<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Application</head><p>Our HM-VAE provides a generalized motion prior that can be applicable in various tasks like 3D video pose estimation, motion interpolation and motion completion. In this section, we introduce the applications we consider and describe the effective strategy used for each application. We provide qualitative and quantitative results in the next section.</p><p>3D Video Pose Estimation. Our learned motion prior provides an effective strategy to refine video based pose estimations. Concretely, we take potentially noisy pose estimates as input to the encoder, then decode refined poses  using the encoded latent vector. Our HM-VAE is designed for a fixed window size of T frames. In order to have our method process sequences of any length, we could simply partition the input sequence into windows of T frames. However, with no overlap across the time windows, we observe that this may result in discontinuities. Therefore, we propose a sliding window strategy using center frames to process arbitrarily long sequences. Specifically, for each time window we process, we only update the pose of the center frame with the refined result and shift the time window one step. We take the T 2 th frame M T 2 as the refined final result, added to our final refined sequence S. And the window is shifted by one timestep along the input motion sequence for processing the next window. For each window size of pose sequences, we formally define the process as follows, where W 2 represents next window.</p><formula xml:id="formula_12">z = Enc(N 1 , ..., N T ), (10) M 1 , ..., M T 2 , ..., M T = Dec(z),<label>(11)</label></formula><formula xml:id="formula_13">S = S ? M T 2 (12) W 2 = N 2 , ..., N T +1<label>(13)</label></formula><p>Motion Interpolation and Completion. A common setup in motion synthesis is to generate motion sequences given a sparse set of keyframes, which we refer to as motion interpolation. Motion completion, on the other hand, focuses on synthesizing complete body motion from partial observations, e.g., completing the motion of the lower body by observing the upper body. For both motion interpolation and completion tasks, we simply utilize the decoder of HM-VAE to synthesize motion while searching for an optimal latent code to match the given observations (i.e., sparse keyframes or partial body motion). The optimization objective is to minimize the reconstruction error between the given observations and the corresponding decoded poses. We define the reconstruction objective as a combination of three terms including matching the joint rotations using both 6D rotation and rotation matrix representations and matching the joint positions after forward kinematics:</p><formula xml:id="formula_14">L rec = L 6d + L rot + ? 1 L joints<label>(14)</label></formula><p>Concretely, we perform optimization in two phases. Starting with randomly sampled latent vectors z l and z g , in the first phase, we optimize for the latent vectors that minimize L rec as the only objective. The decoder parameters ? are fixed during this phase. In the next phase, we optimize for the decoder parameters ? <ref type="bibr" target="#b54">[54]</ref> while keeping the latent vectors fixed. In this second phase, we introduce a regularization loss to constrain ? and prevent it from deviating too much from the pre-trained parameters ?. Thus, the optimization objective in the second phase becomes:</p><formula xml:id="formula_15">L opt = L rec + ? 2 ||? ? ?|| 2<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first describe the dataset we use for training and evaluation. Then we showcase the results of applying our HM-VAE in the applications we introduced in the previous section. Finally, we perform an ablation study to validate the effectiveness of our overall approach.</p><p>Dataset. We use the AMASS dataset <ref type="bibr" target="#b31">[32]</ref> for training HM-VAE. AMASS dataset is a large collection of 15 motion capture datasets with a unified data representation. The dataset has more than 40 hours of motion data and serves as a great testbed for motion modeling. We use the same validation and testing split introduced in VIBE <ref type="bibr" target="#b22">[23]</ref>. For refining video based pose estimates, we use 3DPW <ref type="bibr" target="#b47">[47]</ref>, a 3D motion in the wild dataset, as our test set. For the motion interpolation task, we train our HM-VAE on the LAFAN1 dataset <ref type="bibr" target="#b12">[13]</ref> to provide quantitative comparisons to the baseline methods. LAFAN1 consists of high-quality motion capture data with specific action types. We follow the data split proposed in <ref type="bibr" target="#b12">[13]</ref> and use subjects 1, 2, 3 and 4 as training, and subject 5 for testing. Implementation Details. We use a batch size of 8 for training. The KL divergence weight ? is set to 0.003. Un-PA-MPJPE MPJPE ACCEL ACCER HD <ref type="bibr" target="#b20">[21]</ref> 72 less noted otherwise, we train HM-VAE with motion sequences of length 64. While training our HM-VAE, to prevent the learning dominated by either shallow or deep latent vector, we use similar strategy proposed in <ref type="bibr" target="#b27">[28]</ref>. We first only train our model with deep latent vector, then start training both shallow and deep latent vectors after 50000 iterations. For the motion interpolation experiments, we found our method converged at around 150 iterations of optimization, with 50 iterations for the first phase and 100 iterations for the second phase. For the motion completion experiments, we found our optimization converged at around 300 iterations with 100 iterations belonging to the first phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>3D Video Pose Estimation. In this section, we show that our model can be used to refine the results of off-the-shelf 3D video pose estimation methods. In order to adapt HM-VAE to different global rotations and frame rate among different datasets, we train our HM-VAE with data augmentation. Our data augmentation consists of different frame rates and random global rotations. Also, we use the HM-VAE model trained with a window size 8 in this application which we observe has a better reconstruction quality. We show quantitative results in <ref type="table" target="#tab_0">Table 1</ref> where we test our method with inputs obtained by both VIBE <ref type="bibr" target="#b22">[23]</ref> and Hu-manDynamics (HD) <ref type="bibr" target="#b20">[21]</ref>. We report errors using the same metrics as VIBE <ref type="bibr" target="#b22">[23]</ref>. Specifically, we report the mean per joint position error with (PA-MPJPE) and without (MPJPE) the Procrustes-alignment, as well as the mean per joint acceleration and acceleration error. In <ref type="figure" target="#fig_1">Figure 3</ref>, we show the acceleration error curves as well as example poses obtained for consecutive timesteps. Compared to current state-ofthe-art methods, our refined motions have smaller acceleration errors. While previous approaches are prone to abrupt changes across consecutive poses as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our model smooths out these noisy estimates. We refer the readers to the supplementary video for a detailed comparison.</p><p>Motion Interpolation. In order to demonstrate the effectiveness of HM-VAE for the motion interpolation task, we compare it to appropriate baseline methods. Specifically, in order to interpolate local joint rotations, we use the standard spherical linear interpolation (Slerp). Since interpolation quality is directly related to the number of missing frames, we perform our evaluations in four settings where 5, 15, 30, 45 frames are missing in each setting. Following the same setting as <ref type="bibr" target="#b12">[13]</ref>, given the starting 10 frames and   ending 1 frame as key frames, we aim to generate the frames in-between. We show quantitative comparisons in terms of local pose estimation in <ref type="table" target="#tab_2">Table 2</ref>. In addition to the metrics introduced before, we also report the global quaternion loss proposed by the original benchmark <ref type="bibr" target="#b12">[13]</ref>. We show that our method outperforms the Slerp baseline quantitatively. We also show that the performance achieved by our human motion prior is competitive against the in-betweening specific method from <ref type="bibr" target="#b12">[13]</ref> in the global quaternion loss metric. Please note that we use the LAFAN1 dataset for this evaluation to compare against the global quaternion errors directly reported by <ref type="bibr" target="#b12">[13]</ref> since their code is not published and the au-thors were not able to run their model on our dataset. We also provide additional qualitative results in the AMASS dataset. We visualize local joint trajectories in <ref type="figure" target="#fig_2">Figure 4</ref> for a walking motion sequence. Our results preserve the original motion patterns while Slerp fails to model the local motion when the interval between two key frames becomes large. We further demonstrate global trajectory interpolation with our method and the alternatives. Specifically, we use our global trajectory estimation module by providing the local motion predicted by our method as well as Slerp. In addition, we also define a simple baseline where we linearly interpolate the global root position of the sparse keyframes (lerp). As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the trajectory estimated by our method more closely resembles the ground truth. We also show a mesh visualization result for motion interpolation in <ref type="figure" target="#fig_4">Figure 6</ref>. For more qualitative results, we encourage readers to check our accompanying video.</p><p>Motion Completion. Given only upper body joint rotations as target, we aim to recover the complete body motion sequences. For this experiment, we use motion sequences from the testing and validation split of the AMASS dataset. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, our approach is able to restore complete motions since the global latent space capture the correlations among different joints. Therefore, the missing lower legs movement that matches the given upper body is retrieved from the learned latent space for human motion.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>In order to motivate the design choices we made, we perform an ablation study where we compare our HM-VAE with a non-hierarchical motion VAE (M-VAE) and a VAE with only temporal convolution layers (TCN-VAE). The temporal convolution layers were used in training an autoencoder for motion processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>. We compare our model to the alternatives in the task of motion reconstruction using the AMASS dataset. Specifically, for each testing sequence, we take the local joint rotations as input to the encoder and then decode the motion from the mean vector. We measure the mean joint reconstruction error as shown in <ref type="table" target="#tab_4">Table 3</ref>. Our HM-VAE model outperforms the M-VAE by a large margin in motion reconstruction evaluation. And the model with skeleton-aware architecture has superior performance than its temporal convolution counterpart. Therefore, we show that skeleton operations from the skeleton-aware architecture are important for  modeling the human body structure in comparison to using standard temporal convolution. Moreover, modeling a global and local motion latent spaces further improve the human motion modeling power of the skeleton-aware architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a task-generic motion prior using a hierarchical motion VAE. We demonstrate the effectiveness of the prior in various applications including 3D video pose estimation, motion interpolation, and motion completion. By learning a global and local embedding, our prior can faithfully model human motion. While our prior enables to refine video-based human motion estimation results by reducing jitters, it also performs on-par with task specific methods for motion interpolation and completion. There are some limitations of our method we would like to address in future work. We observe that there are accumulation of errors when predicting the global trajectory for a long sequence. Exploring more constraints like foot contact during both training and inference might be a potential approach to address this. While we show that our prior is effective in different applications, using few-shot learning to better adapt to specific tasks is another interesting direction. Finally, incorporating certain physical properties and action conditions are also promising directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Model Overview. Left: Model architecture. (We omit activation layers and temporal upsampling layers here for simplicity.) Right: Illustration of receptive field in shallow (B 1 ) and deep layers (B 4 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Acceleration error curves for VIBE<ref type="bibr" target="#b22">[23]</ref> results and our refined results. The right figure shows poses in consecutive timesteps corresponding to the reference images on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Local trajectory comparison for motion interpolation in AMASS data. From left to right is the trajectory when key frame interval is 30, 15, 5 respectively. The upper curves represent the left wrist, the lower curves represent the right ankle. The star symbol represents the starting point, the arrow symbol represent the position of key frames. Our results show similar moving patterns to ground truth, while Slerp differs a lot when key frame interval is large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Global root trajectory comparison for motion interpolation in AMASS data. From left to right is the trajectory (in xy plane) when key frame interval is 30, 15, 5 respectively. The star symbol represents the starting point, the arrow symbol represents the position of key frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Motion interpolation results in AMASS test data. The gray mesh shows key frame poses, the purple mesh show the generated poses. The interval between two key frames is 30 frames. The right figure shows the global trajectory comparison for this motion sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Motion Completion Results. Given upper body joint rotation as optimization objective, the prior model can complete whole motion sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>3D Video Human Pose Estimation Results in 3DPW</figDesc><table><row><cell></cell><cell>.17</cell><cell>115.97</cell><cell>14.96</cell><cell>14.73</cell></row><row><cell>HD [21] w Prior</cell><cell>71.39</cell><cell>113.90</cell><cell>5.21</cell><cell>8.36</cell></row><row><cell>VIBE [23]</cell><cell>56.56</cell><cell>93.59</cell><cell>27.12</cell><cell>27.99</cell></row><row><cell>VIBE [23] w Prior</cell><cell>55.84</cell><cell>92.43</cell><cell>6.03</cell><cell>9.15</cell></row><row><cell>Testing Dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative Evaluation for Motion Interpolation in LAFAN1 Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Motion Reconstruction Results in AMASS test data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our experiments, we use the SMPL<ref type="bibr" target="#b29">[30]</ref> skeleton hence the number of joints J is 24 and we use the continuous 6D rotation representation<ref type="bibr" target="#b56">[56]</ref>,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was conducted at Adobe, at University of Southern California and USC Institute for Creative Technologies. Research was sponsored by the Army Research Office and was supported under Cooperative Agreement Number W911NF-20-2-0053, and sponsored by the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005, the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA; and in part by the ONR YIP grant N00014-17-S-FO14. Affiliation with Pinscreen and the University of California at Berkeley was supported by DARPA under cooperative agreement HR00112020054. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skeletonaware networks for deep motion retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured prediction helps 3d human motion modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>First two authors contributed equally</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Beyond static features for temporally consistent 3d human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno>arXiv-2011, 2020. 3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Generative adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Style-based inverse kinematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2004 Papers</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Metzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11084</idno>
		<title level="m">Point2mesh: A self-prior for deformable meshes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust motion in-betweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="60" to="61" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06598</idno>
		<title level="m">Simon Alexanderson, and Jonas Beskow. Moglow: Probabilistic and controllable motion synthesis using normalising flows</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning motion manifolds with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2015 Technical Briefs</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalizing motion edits with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Ikemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><forename type="middle">Emil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ASME-Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andreas M Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blind video temporal consistency via deep video prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to generate diverse dance motions with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08171</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Progressive learning and disentanglement of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaideep</forename><surname>Vitthal Murkute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashnna</forename><surname>Kumar Gyawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10549</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Character controllers using motion vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Yu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Zinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="40" to="41" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ahmed AA Osman, Dimitrios Tzionas, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contact and human dynamics from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Artist-directed inverse-kinematics using radial basis function interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Charles F Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter-Pike J</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Motionet: 3d human motion reconstruction from monocular video with skeleton consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Kai Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vincent Lepetit, and Pascal Fua</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
	</analytic>
	<monogr>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Temporal motion models for monocular and multiview 3d human body tracking. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="157" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural kinematic networks for unsupervised motion retargetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8639" to="8648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Videomocap: Modeling physically realistic human motion from monocular video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2010 papers</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling 3d human poses from uncalibrated monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiaolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1873" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Inverse kinematics and geometric constraints for articulated figure manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Welman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Theses (School of Computing Science)/Simon Fraser University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10965" to="10974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mt-vae: Learning motion transformations to generate multimodal human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep optimized priors for 3d shape modeling and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07241</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08891,2020.3</idno>
		<title level="m">Generative tweening: Long-term inbetweening of 3d human motions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
