<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Importance Sampling CAMs for Weakly-Supervised Segmentation with Highly Accurate Contours</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvi</forename><surname>Jonnarth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Importance Sampling CAMs for Weakly-Supervised Segmentation with Highly Accurate Contours</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Weak supervision</term>
					<term>semantic segmentation</term>
					<term>importance sampling</term>
					<term>feature similarity</term>
					<term>class activation maps !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classification networks have been used in weakly-supervised semantic segmentation (WSSS) to segment objects by means of class activation maps (CAMs). However, without pixel-level annotations, they are known to (1) mainly focus on discriminative regions, and (2) to produce diffuse CAMs without well-defined prediction contours. In this work, we alleviate both problems by improving CAM learning. First, we incorporate importance sampling based on the class-wise probability mass function induced by the CAMs to produce stochastic image-level class predictions. This results in segmentations that cover a larger extent of the objects, as shown in our empirical studies. Second, we formulate a feature similarity loss term, which further improves the alignment of predicted contours with edges in the image. Furthermore, we shed new light onto the problem of WSSS by measuring the contour F-score as a complement to the common area mIoU metric. We show that our method significantly outperforms previous methods in terms of contour quality, while matching state-of-the-art on region similarity. 60 62 64 66 68 70 72 74 76 78 Region similarity, ? (%) 40 42 44 46 48 50</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE advancements of deep learning methods in recent years have had a major impact on many computer vision tasks, with no exception for semantic segmentation. The ability to automatically segment images has been found useful in many applications, including autonomous driving <ref type="bibr" target="#b0">[1]</ref>, video surveillance <ref type="bibr" target="#b1">[2]</ref>, and medical image analysis <ref type="bibr" target="#b2">[3]</ref>. Fully-supervised segmentation frameworks have achieved remarkable results by utilizing large datasets of pixel-wise annotated images. However, these annotations require a significant manual labelling effort, which increases with the dataset size. Image-level weakly-supervised semantic segmentation (WSSS) aims to alleviate the labelling effort, where, instead of requiring human-annotated pixelwise segmentation masks, the only source of supervision are cheap image-level classification labels. This opens the possibility to train segmentation models on existing largescale datasets where pixel-level labels are non-existent and infeasible to acquire.</p><p>A common approach to WSSS is to first train a classification network with global average pooling (GAP) to produce class activation maps (CAMs) <ref type="bibr" target="#b4">[5]</ref>. The CAMs are used to generate pseudo-labels for supervising the final segmentation network <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, classification networks are known to <ref type="bibr" target="#b0">(1)</ref> mainly focus on discriminative regions as opposed to the whole extent of objects, and (2) to produce overly smooth CAMs without well-defined prediction contours <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this work, we improve the CAMs in these two aspects. We substitute GAP with importance sampling by sampling from the class-wise probability mass function induced by the CAMs to produce stochastic image-level class predictions during training. This leads to segmentations that cover a larger extent of the objects, and not only the discriminative regions. This approach is motivated by the effectiveness of similar stochastic mechanisms, such as particle filters <ref type="bibr" target="#b7">[8]</ref>, random sample consensus <ref type="bibr" target="#b8">[9]</ref>, and the saccadic movement of eyes <ref type="bibr" target="#b9">[10]</ref>.</p><p>Furthermore, we notice a discrepancy in the evaluation arXiv:2203.12459v2 [cs.CV] <ref type="bibr" target="#b22">23</ref> Sep 2022 in WSSS benchmarks compared to related computer vision tasks, such as video object segmentation (VOS). In WSSS benchmarks, only region similarity is evaluated using the mean intersection over union (mIoU) based on the area of segmentation masks. However, a complementary aspect not considered is the contour quality, which might be equally or even more important depending on the application. For example, contours contribute more to the perceived visual quality <ref type="bibr" target="#b10">[11]</ref>, and bear more weight in cases where the segmentations are displayed to a human, such as in virtual green-screen applications. We suggest to use the contour F-score to evaluate the contour quality of WSSS methods, similar to how VOS methods are evaluated in the wellestablished DAVIS benchmark <ref type="bibr" target="#b11">[12]</ref>. We find that current state-of-the-art WSSS methods lack in this regard. Thus, we aim to develop a method that improves upon the contour quality, while keeping the high region similarity of previous works. We propose a new feature similarity loss term (FSL) which aims to match prediction contours with color edges in the image. This significantly improves the contour quality over previous methods, which is shown quantitatively in <ref type="figure" target="#fig_0">Figure 1</ref>, and visually illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Compared to an early version of this method <ref type="bibr" target="#b12">[13]</ref>, we sample several pixels per class during importance sampling, incorporate GAP to implicitly alter the sampling distribution, and fix parameters in FSL instead of learning them during training. A code implementation is available at github. <ref type="bibr" target="#b0">1</ref> Our contributions can be summarized as follows:</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f)</formula><p>? We propose to use importance sampling for pixel-to image-level aggregation in order to cover the whole extent of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We introduce a new feature similarity loss term for improving contours.</p><p>? We suggest to evaluate WSSS methods in terms of contour quality, in addition to region similarity, similar to related computer vision tasks, such as the DAVIS benchmark <ref type="bibr" target="#b11">[12]</ref>.</p><p>? Finally, we perform extensive experiments and ablations on the PASCAL VOC benchmark dataset to validate the effectiveness of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Weakly-supervised semantic segmentation has its background in several sub-fields, for which we describe the related work below. The sub-fields include different types of weak supervision, the learning of class activation maps, pixel-to-image aggregation, the use of feature similarities to refine segmentation predictions, as well as the implicit model distillation used in many WSSS methods. Types of weak supervision. Depending on the application and available data, one might have access to different types of annotations. Therefore, it is of interest to find methods that can learn to segment images based on various types of weak supervision. Semantic segmentation networks have successfully been trained in the past without access to full annotations in the form of segmentation masks. This has been achieved by utilizing different types of weak labels during training, such as bounding boxes <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, scribbles <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, points <ref type="bibr" target="#b18">[19]</ref> and classification labels, in decreasing levels of supervision. Classification labels have been the most popular as they require the least amount of manual labelling effort, and it is clear that they can be gathered from any of the stronger label types. Classification labels have been used for learning CAMs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> and pixel affinities <ref type="bibr" target="#b5">[6]</ref>, constraining the neural network output <ref type="bibr" target="#b19">[20]</ref>, seeding, expanding and constraining the segmentation predictions <ref type="bibr" target="#b20">[21]</ref>, as well as using them in combination with saliency maps <ref type="bibr" target="#b21">[22]</ref>, all in order to increase the quality of the segmentation predictions. In this work, we focus on the lowest level of weak supervision, that is, image-level classification labels.</p><p>Class activation maps. Class activation maps <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b22">[23]</ref> have been used to visualize what part of the input a neural network uses to base its prediction on. CAMs are usually produced using GAP in the final layers, where class predictions are propagated into pixel space. Due to their ability to give an indication of the position and size of objects without ground-truth segmentation masks, they have been adapted in many WSSS methods. Ahn et al. <ref type="bibr" target="#b5">[6]</ref> use CAMs as a starting point for predicting pixel affinities, which they use to produce better segmentation predictions. Wang et al. <ref type="bibr" target="#b3">[4]</ref> improve the generation of CAMs with a siamese network architecture and equivariant regularization to further improve results. Oquab et al. <ref type="bibr" target="#b23">[24]</ref> use global maxpooling to produce image-level class predictions during training, where they use the activation maps for object localization. In this work, we choose a slightly different approach to previous methods by using random sampling instead of pooling to produce classification predictions. We show that this approach leads to better CAMs in terms of activating over the entire extent of objects as opposed to focusing on small discriminative regions. Therefore, it is well suited for the task of semantic segmentation. Lee et al. <ref type="bibr" target="#b6">[7]</ref> also use stochasticity for improving CAMs, where they use spatial dropout for hidden unit selection in order to produce multiple localization maps during both training and inference. Our approach differs in two ways. First, we use a non-uniform sampling strategy based on the CAMs for sampling image-level classification predictions. Second, we only use it during training, thus employing a deterministic inference scheme.</p><p>Pixel-to-image aggregation. Advanced pooling strategies have been extensively studied in the past, and some have suggested to use activation ranking which shares some similarities with importance sampling. Kolesnikov &amp; Lampert <ref type="bibr" target="#b20">[21]</ref> introduce global weighted rank-pooling (GWRP) which aggregates the pixel-level activations by ranking them and assigning a weight based on the rank. Durand et al. <ref type="bibr" target="#b24">[25]</ref> combine the highest and lowest activations to form their spatial pooling strategy. Our approach differs in that neither of the previous works perform sampling, while they both include an implicit object size prior defined by methodrelated parameters.</p><p>Prediction-propagation based on feature similarities. The concept of propagating a signal to spatial neighbours based on feature similarities has been studied in the past, whether it be by computing a weighted mean of nearby pixels for image denoising <ref type="bibr" target="#b25">[26]</ref>, or by letting the raw image features define the pair-wise potentials in a fully connected conditional random field (CRF) for propagating initial segmentation predictions iteratively <ref type="bibr" target="#b26">[27]</ref>. Ahn et al. introduce an additional network for learning classagnostic pixel affinities <ref type="bibr" target="#b5">[6]</ref>. More recently, prediction propagation has been used in an attention-based WSSS approach where the CAM predictions are propagated to similar pixels based on deep features in order to improve segmentation results <ref type="bibr" target="#b3">[4]</ref>. In this work, we propose to do this implicitly through a class-specific feature similarity loss term based on pixel and prediction similarities, which is minimized when the prediction contours of CAMs match the edges in the image.</p><p>Model distillation. Many state-of-the-art WSSS methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref> implicitly take advantage of model distillation, by applying the two-stage framework of first generating pseudo-labels using a classification network, and subsequently training a segmentation network on the generated pseudo-labels in a fully supervised setting. As has been demonstrated in the past <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, large ensembles of models can be distilled into a single model without a significant loss of performance by training the final model to predict the output of the ensemble. Recently, it has also been shown that only one model is sufficient to be used as the "ensemble" to gain better results compared to simply training the final model directly on the data <ref type="bibr" target="#b30">[31]</ref>. This is referred to as selfdistillation. In a sense, the two-stage framework in WSSS is a form of self-distillation, and we also take advantage of this phenomenon by adapting the two stages in generating pseudo-labels for the final segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section we describe our approach of training a network for predicting CAMs, and how we integrate our two main method contributions from Secs. 3.2 and 3.3 for improving them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Computing CAMs</head><p>Let a ? (x) ? [0, 1] W ?H?K denote a class activation map (CAM) which is a function of the input image x, and parameterized by ?, where W , H and K denote the width, height and number of classes (including background) respectively. Furthermore, let s ? (x) ijk represent the unnormalized logit of class k at the position indexed by i and j. Typically, this is the output of the last convolutional layer without activation function. If we model the class probabilities in each pixel as a normalized probability distribution we can estimate the probability that a pixel contains a certain class k using the softmax function as</p><formula xml:id="formula_1">a ? (x) ijk := Pr(z ij = k|x) = e s ? (x) ijk K t=1 e s ? (x)ijt ,<label>(1)</label></formula><p>where z ? R W ?H is the ground-truth segmentation mask and z ij is the class index present at the pixel indexed by i and j. Pr(z ij = k|x) is the estimated probability that class k is present in pixel (i, j) in a given image x.</p><p>Since z is unknown in the weakly supervised setting, the pixel-wise class predictions need to be condensed into an image-level prediction. Commonly, global average pooling (GAP) on the unnormalized logits has been used in WSSS for this purpose <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In this case the image-level probability that class k is present in the image is given by the logistic function?</p><formula xml:id="formula_2">GAP k = e s k e s k + 1 ,<label>(2)</label></formula><p>and where the image-wide average of the logits is given by</p><formula xml:id="formula_3">s k = 1 HW ij s ? (x) ijk .<label>(3)</label></formula><p>However, when minimizing a classification loss in this case, every pixel contributes to the loss. Thus, if an image contains an object of class k, the loss encourages the model to classify every pixel as belonging to that class. This might lead to over-activation, blurry class activations and poorly defined prediction contours, see for example the result of a GAPbased method in <ref type="figure" target="#fig_1">Fig. 2c</ref>. Instead, we choose a different approach.</p><p>Assuming that, in order for an image to be classified as containing an object, it suffices that only one pixel contains that object, then we predict the probability</p><formula xml:id="formula_4">Pr(k ? {z ij } W,H i=1,j=1 |x).</formula><p>We sample a single pixel per class based on the probabilities a ? since this essentially translates to "if at least one pixel contains an object, the whole image contains this object". The first obvious option is to apply global max pooling for this purpose, where the predicted image-level probabilities can be written a?</p><formula xml:id="formula_5">y GMP k := Pr k ? {z ij } W,H i=1,j=1 x = max ij a ? (x) ijk . (4)</formula><p>A shortcoming of max pooling, however, is that it tends to activate over small discriminative regions and does not offer very useful segmentation predictions, even in cases where the classification prediction is correct. This can be observed in <ref type="figure" target="#fig_1">Fig. 2b</ref>. To improve the CAMs in this regard we use importance sampling to produce stochastic image-level class predictions during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Importance Sampling</head><p>As described in the previous section, global max pooling (GMP) tends to activate over small discriminative regions and does not leverage adequate segmentation predictions for pseudo-label generation. To solve this problem we introduce an additional image-level prediction by sampling one pixel for each class using the probability mass function induced by the class activation map a ? . Let us define K probability mass functions, one for each class</p><formula xml:id="formula_6">p k (I, J|x) = Pr(I = i, J = j|x, k) = 1 Z k (a) a ? (x) ijk , (5) where Z k (a) = W i=1</formula><p>H j=1 a ? (x) ijk is a normalizing constant. Now, we sample image coordinates for each class, which we use to extract the class activations. These activations are then interpreted as classification predictions</p><formula xml:id="formula_7">y k = a ? (x)?? k , (?,?) ? p k (I, J|x).<label>(6)</label></formula><p>If the distributions p k were uniform, this method would be similar to GAP since each pixel would have been given the same weight. Note also that in this case the distributions would not be conditioned on the input image x. However, in the case of the activation-based distributions in <ref type="bibr" target="#b4">(5)</ref>, the probability mass depends on the class activations, and pixels with higher activations are more likely to be sampled. Furthermore, pixels with zero activation for a certain class are never sampled and do not contribute to the classification loss term for that class. Note also that since we use softmax in (1), at least one class will have a non-zero probability in every pixel.</p><p>We can further compare GAP and importance sampling by considering the case of small objects. With GAP, in order for a model to perform a correct image-level prediction that an object is present in the image, it would have to output large positive logits at the few pixels resembling the object while outputting small negative logits at the majority of pixels not part of the object. Therefore, the model is not encouraged to predict the absence of objects with high certainty, a constraint not present for importance sampling. Essentially, we avoid the problem of GAP, that pixels, which are correctly classified as not containing an object, get reflected negatively on the loss in cases where the object is present somewhere else in the image.</p><p>If the distributions had instead been defined as the Kronecker delta function and equal to 1 at the maximum activation, we would get max pooling. Similar to max pooling, we choose one pixel to represent the image-level prediction for each class, but we choose them randomly based on the class activations. This allows the model to activate over the whole extent of objects and not only on the most discriminative regions.</p><p>Due to the nature of convolutional neural networks, they tend not to activate only at a specific pixel, but rather over a region of pixels, since the visual features are spatially correlated. Thus, for a model that under-activates, in the worst case we would sample pixels from a small region around discriminant features. Eventually, pixels at the prediction border would be sampled and the model would be encouraged to associate also these pixels with the object, thus covering a larger extent of it. Conversely, for a model that over-activates, pixels that do not correspond to object features would eventually be sampled. This behaviour is penalized in images where the feature is present but not the object. Thus, importance sampling encourages the model to activate only over the objects themselves.</p><p>The parameters ? can be found by minimizing the sum of K binary cross-entropy loss terms</p><formula xml:id="formula_8">L ce (y,?; ?) = ? 1 K K k=1 y k log? k + (1 ? y k ) log(1 ?? k ),<label>(7)</label></formula><p>where y k is the image-level label for class k, which is equal to 1 if class k is present in the image and 0 otherwise. To understand how the behaviour shifts, we train our CAM network using a classification loss containing two crossentropy terms, one for the prediction? computed using either GMP or GAP, and one for the prediction? attained by random sampling according to <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_7">(6)</ref>. Our classification loss term is a convex combination of these two terms</p><formula xml:id="formula_9">L cls (y,?,?) = (1 ? ?)L ce (y,?) + ?L ce (y,?),<label>(8)</label></formula><p>where ? ? [0, 1] is a parameter controlling the weight between the two cross-entropy terms. A value of ? = 0 corresponds to the case described in Sec. 3.1 with no importance sampling, and a value of ? = 1 corresponds to only using stochastic predictions during training. While the first crossentropy term has been successful in classification tasks, it is unclear whether the first, second or a combination of the two is most suitable for weakly-supervised segmentation. In our early experiments we observed that importance sampling improved the CAMs in terms of covering a larger extent of the objects, but the prediction borders did not align with their edges, see <ref type="figure" target="#fig_1">Fig. 2d</ref>. For this reason we introduce a feature similarity loss term, which aims to match the prediction contours with the edges of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Similarity Loss</head><p>Intuitively, similar pixels that are in close proximity have a high probability of being part of the same object. Additionally, if two nearby pixels are dissimilar, there is a chance that they belong to different classes and that the contour runs somewhere between them. Based on this rationale, we formulate a loss term which penalizes dissimilar predictions for nearby similar pixels. Furthermore, similar predictions are discouraged for nearby dissimilar pixels if their predictions are sufficiently dissimilar to begin with. In what follows, we formulate our feature similarity loss term as a function of the pixel-wise class predictions and features. Subsequently, we describe the intuition behind our formulation.</p><p>In the following equations we use a single index for the image coordinates in order to reduce notational clutter. First, let us define a gating function g(a i , a j ) : R 2K ? R + which computes the distance between the predictions a i and a j of the pixels i and j, as well as a function f (d) : [0, 1] ? [?1, 1] which maps the dissimilarity d(x i , x j ) ? [0, 1] between the features x i and x j monotonically to [?1, 1]. Inspired by bilateral filtering <ref type="bibr" target="#b31">[32]</ref>, which considers both geometric and photometric similarity, we formulate the feature similarity loss term</p><formula xml:id="formula_10">L fs (a, x) = ? 1 (HW ) 2 ij w ij g(a i , a j ) f (d(x i , x j )),<label>(9)</label></formula><p>where w ij is a spatial weight which considers the distance between pixels i and j and is used to give a higher weight to pixel pairs which are close to each other. Note that a i and x i are vectors representing respectively the class probability distribution and image features for pixel i. Throughout our experiments we define the weights using a Gaussian neighbourhood</p><formula xml:id="formula_11">w ij = 1 2?? 2 exp ? p i ? p j 2 2 2? 2 ,<label>(10)</label></formula><p>where p i and p j are two-dimensional vectors containing the image coordinates of pixels i and j, and ? is the standard deviation of the Gaussian function to control the size of the considered pixel neighbourhood. Furthermore, we define the gating function g as the squared L 2 distance between the predictions</p><formula xml:id="formula_12">g(a i , a j ) = 1 2 a i ? a j 2 2 ,<label>(11)</label></formula><p>and the function f as</p><formula xml:id="formula_13">f (d(x i , x j )) = tanh ? + log d 1 ? d ,<label>(12)</label></formula><p>where ? is a bias parameter. The logarithm in <ref type="bibr" target="#b11">(12)</ref> computes the logit of the binary decision problem whether or not two pixels are (dis)similar. The tanh function then maps the logit with the added bias from R onto the interval [?1 <ref type="bibr">, 1]</ref>. Thus, f takes the values ?1 and +1 when pixels are similar (d = 0) and dissimilar (d = 1) respectively, and ? controls the crossover point at which f changes sign. For two similar pixels, we have f &lt; 0 and get L fs ? 0 since g ? 0. L fs is thus minimized if g is minimized, i.e. if a i = a j . In the case of two dissimilar pixels on the other hand, i.e. if f &gt; 0, we have L fs ? 0, which is minimized if g is maximized, and occurs when a i and a j are opposite predictions, i.e. if they are two one-hot vectors predicting different classes. However, it is not always the case that two dissimilar pixels are part of different classes. For example, an object could contain some high-frequency texture or be made up of several parts with different visual appearance. However, since the gating function is chosen to be the squared L 2 distance, the gradient of L fs with respect to the predictions is proportional to the difference between the predictions, and equal to zero if a i = a j . Consequently, if two pixels have similar predictions, only a small gradient is propagated through the feature similarity loss, and the total gradient is dominated by the classification loss. This allows the network to classify larger regions to the same class, even though they contain parts with dissimilar features. Thus, the network can activate over the whole extent of objects.</p><p>We use RGB pixel values in [0, 1] for the features x, and choose the dissimilarity</p><formula xml:id="formula_14">d(x i , x j ) = x i ? x j 1 C ,<label>(13)</label></formula><p>which has been used in stereo matching <ref type="bibr" target="#b32">[33]</ref>, where C = 3 is the dimensionality of the feature vectors. Although learnable features would allow for finding image-specific biases, they could potentially lead to trivial solutions or unwanted behaviours if combined with the classification loss, as we would essentially be learning the loss function. Therefore, we stick to RGB features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>This section outlines the experimental results achieved for the methods described above.  <ref type="bibr" target="#b34">[35]</ref>, which is common in VOC experiments, resulting in a total of 10,582 training images. Although ground-truth segmentation masks are available, we only use image-level classification labels for training. Evaluation metrics. For evaluation, we use the commonly adopted mean intersection-over-union (mIoU) metric, or Jaccard index <ref type="bibr" target="#b35">[36]</ref>, based on the area of the segmentations to measure region similarity, denoted J . Additionally, we suggest to further evaluate WSSS methods by measuring contour quality using the F-score computed on the segmentation mask contours, denoted F. We also measure a combined score which is simply an average of the two metrics, denoted J &amp;F . The use of the contour F-score metric is motivated by the fact that it captures a complementary aspect compared to region similarity. Depending on the downstream task, one aspect might be more important than the other. For example, contours contribute more to the perceived visual quality <ref type="bibr" target="#b10">[11]</ref>, and bear more weight in cases where the segmentations are displayed to a human, such as in virtual green-screen applications. Since we do not consider a specific application, it is important to evaluate both aspects simultaneously. The same reasoning has been utilized in similar computer vision tasks, such as in the wellestablished DAVIS video object segmentation benchmark, which measures both region similarity and contour quality, as well as the combined metric J &amp;F <ref type="bibr" target="#b11">[12]</ref>. To the best of our knowledge, we are the first to apply this to WSSS.</p><p>We adapt the code from Perazzi et al. <ref type="bibr" target="#b11">[12]</ref> and compute the F-score by first accumulating bipartite matches of the boundaries between the predicted and ground-truth masks, efficiently approximated using morphological operations. Subsequently, we compute the class-wise F-scores over the dataset, which we then average. Although the VOC segmentation masks contain a thin no-class region between objects, the contour F-score still gives a clear indication of the contour quality since we match the boundaries using dilated regions. Thus, we compute the contour quality by considering this thin no-class region as background.</p><p>Training details. For our CAM network we use the same siamese architecture as our baseline method SEAM <ref type="bibr" target="#b3">[4]</ref> with a ResNet-38 <ref type="bibr" target="#b36">[37]</ref> backbone. As part of the architecture, we also incorporate their method of refining the CAMs using a pixel correlation module (PCM) in the final layer of the network, and adopt their equivariant regularization (ER) and equivariant cross regularization (ECR) loss terms. Thus, our loss function has four terms, namely L cls , L fs , L er and L ecr . We simply sum the four terms, which corresponds to a weight of 1 for each of them, similar to SEAM (for L cls , L er , and L ecr ). We perform minor adjustments to the forward method of the network and the inference scheme to fit with our probabilistic formulation according to (1), <ref type="bibr" target="#b4">(5)</ref> and <ref type="bibr" target="#b5">(6)</ref>. In order to measure the effects of our contributions we do not change any settings or hyperparameters, such as input image dimensions, data augmentation, batch size, number of epochs, optimizer, learning rate, or learning rate schedule. We randomly rescale the input image such that the longest edge is in <ref type="bibr">[448,</ref><ref type="bibr">768]</ref> and extract a crop of size 448 ? 448 for the network input. A list of hyperparameters and other settings can be found in <ref type="table" target="#tab_2">Table 1</ref>. For a fair comparison, we follow the framework in SEAM of training an AffinityNet <ref type="bibr" target="#b5">[6]</ref> to further refine the CAMs before pseudolabel generation. During AffinityNet label generation, we modify the background parameter ? to 2 and 4 when amplifying and weakening the background activations respectively. This was necessary as the values in our CAMs were distributed close to either 0 or 1, while the CAMs from SEAM were distributed more evenly over <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. Other than that, we use the same settings and hyperparameters for AffinityNet training. During pseudo-label generation, we remove segmentation masks for classes that are not present in the image. This information is available in the image-level classification label. As the last step, we train a DeepLab-v1 <ref type="bibr" target="#b37">[38]</ref> network as our final segmentation model, which is supervised by our generated pseudo-labels from the CAM and AffinityNet networks. Both the AffinityNet and DeepLab-v1 networks have the same ResNet-38 <ref type="bibr" target="#b36">[37]</ref> backbone as our CAM network. Similar to SEAM, we use CRF <ref type="bibr" target="#b26">[27]</ref> to produce our segmentation predictions during inference. In our experiments we use two A100 40GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To investigate the effects of the importance sampling and feature similarity loss (FSL) we train our model on the VOC dataset and compare region similarity in terms of mIoU based on the area of predicted segmentations, as well as contour quality in terms of the F-score computed on the contours. All ablations are evaluated on the training set since we use the validation data as a test set to compare with previous methods in subsequent sections. Note, however, that the tuning of hyperparameters differs from the fully  supervised setting since the model training does not involve full segmentation labels, while the evaluation does. Thus, different data is used for training and evaluation, reducing the risk of overfitting. Nevertheless, we observed similar results on the validation and training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Importance Sampling Loss Weight</head><p>In <ref type="figure" target="#fig_2">Fig. 3</ref> we sweep the loss parameter ? from (8) for global max pooling (GMP) and global average pooling (GAP), and plot the region similarity and contour quality for final segmentations. With max pooling in <ref type="figure" target="#fig_2">Fig. 3a</ref> we observe a significant increase in segmentation performance when increasing ?, both in terms of region similarity and contour quality. The highest performance in terms of region similarity occurs at ? = 1, and for contour quality at 0.8. This is due to the fact that the classification loss is computed based on more than just the most activated pixel and thus encourages the model to activate over more than just the most discriminant feature. If the model activates over any class-specific feature, those pixels will be sampled during training and thus the model can learn to associate these features with the corresponding class. We can observe this in <ref type="figure" target="#fig_1">Fig. 2</ref>. With GMP only, the model activates over the most discriminant features, e.g. faces of persons. With importance sampling, it has learned to activate also over other features, such as hands and legs. Similarly, if the model activates over features that are not associated with a certain class it will get penalized in cases where those features are present but not the class. Thus, it will learn to activate only over the objects themselves.</p><p>With GAP in <ref type="figure" target="#fig_2">Fig. 3b</ref> we see a similar trend of better region similarity for larger values of ?. However, the maximum region similarity occurs at ? = 0.8 which indicates that a combination of GAP and importance sampling is better than using only one or the other. We hypothesize that average pooling contributes with a regularizing effect by effectively introducing a bias to the probability distributions p k in <ref type="bibr" target="#b4">(5)</ref>. As explained in Sec. 3.2, GAP corresponds to a uniform distribution. The combination of GAP and importance sampling would then correspond to using a mixture of the activation-based and uniform distributions, and the resulting distribution would be similar to the activationbased distribution but with a non-zero minimum, i.e. a bias. The specific value of ? = 0.8 indicates that this bias should be quite small. The maximum for the contour F-score occurs at a different value of ?, at 0.6. However, the combined metric J &amp;F for ? = 0.8 is slightly higher at 58.2, compared to 57.6 for ? = 0.6, so we choose ? = 0.8 with GAP for the rest of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of the Feature Similarity Loss</head><p>In <ref type="figure" target="#fig_2">Figure 3</ref> we see that the feature similarity loss term gives a performance boost across the board. Both region similarity and contour quality is improved for all values of ?. This is the case for both GMP and GAP. Next, we search for optimal values for the FSL-specific parameters, namely the standard deviation ?, and the bias parameter ?. First, we fix ? = 5.0 and plot region similarity and contour quality as functions of ? in <ref type="figure" target="#fig_4">Figure 4a</ref>. The maximum occurs at 2.5 for both metrics. Subsequently, we fix ? = 2.5 and plot J , F, and J &amp;F as functions of ? in <ref type="figure" target="#fig_4">Figure 4b</ref>. The optimal standard deviation occurs at 7 for the region similarity, and at 5 for the contour quality. However, the region similarity only changes slightly between these values, while the change in contour quality is more notable. This means that the highest combined score is at ? = 5. Thus, we choose ? = 5 for the rest of our experiments, as it provides a good balance between the two metrics.</p><p>The bias parameter ? in <ref type="bibr" target="#b11">(12)</ref> controls what should be considered similar or dissimilar when looking at the difference in RGB values of pixel pairs. A value of 0 means that the breaking point between similar and dissimilar occurs when the normalized L1 distance d between RGB values is 0.5, i.e. the point at which the sign of L fs flips. A positive bias pushes the breaking point lower, meaning that pixels need to be more similar in color in order to be considered similar. An optimum at ? = 2.5 implies that d &lt; 0.076 is required for two pixels to be considered similar.</p><p>To conclude, we fix ? = 2.5, and ? = 5 for the rest of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Number of Sampled Pixels</head><p>Hypothetically, sampling several pixels during importance sampling enables more efficient learning and better CAMs. Therefore, we evaluate our method under a varying number of sampled pixels and average the classification loss L cls accordingly. Tab. 2 shows the mean and standard deviation of J , F, and J &amp;F over five runs on final segmentation predictions. We observe that the performance increases with a larger number of sampled pixels, while the standard deviation decreases at the same time. Furthermore, we observed that, while the maximum score over five runs was only affected slightly, the training could diverge in the early parts of training when using a small number of sampled pixels, resulting in low scores. We did not observe this behaviour for a larger number of pixels. This could be the result of a few unlucky samples during the early iterations, causing the training to become unstable since the predictions are mostly random at this stage. Using more pixels leads to a less sporadic behaviour, and thus, avoids divergence in the early parts of training. Thus, we set the number of samples to 100 for the subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Qualitative Results</head><p>For a qualitative assessment, we display in <ref type="figure" target="#fig_5">Fig. 5</ref> the foreground class activations, together with CAM pseudo-labels and final predictions. Our method manages to successfully segment the images with impressive contour quality, even in low contrast conditions, e.g. black suit on dark background on the fourth row. However, it is less successful on thin structures, e.g. boat masts, or table and chair legs on rows five and six respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Comparison</head><p>We train our segmentation model in the three stages described previously and evaluate the segmentation performance after each stage in terms of (1) CAM pseudo-labels, (2) AffinityNet pseudo-labels, and (3) final segmentation masks. We do the same for our baseline SEAM <ref type="bibr" target="#b3">[4]</ref> and compare the results in Tab. 3, where we measure both region similarity and contour contour quality. Note that the first two rows comparing CAM and AffinityNet results show the metrics computed for pseudo-labels without CRF. The last row compares the results of the final segmentation predictions where CRF has been applied. We observe that our method has significantly better contour quality across the board. The difference is most notable for the CAM results, where we reach an F-score of 40.8 compared to 20.8 for SEAM. Furthermore, the property of accurate contours carries through to the final segmentation model. Both methods benefit from training an AffinityNet where the contour quality and region similarity increase for both methods. We also observe an improvement in terms of region similarity where we reach 67.2 mIoU compared to 64.6 for SEAM. <ref type="table">Table 4</ref> compares the results from SEAM <ref type="bibr" target="#b3">[4]</ref> with our contributions implemented separately. Importance sampling alone slightly reduces the region similarity by -2.1 points, but makes up for it in contour quality, increasing it by +2.2 points. The feature similarity loss, on the other hand, yields strong improvements in both metrics, and results in an increase in the combined score of +2.4 points. Applying both our contributions at once, i.e. yielding our method, results in even stronger improvements, especially in contour quality, which increases by +8.5 points compared to the baseline. In terms of the combined score, our contributions yield strict improvements over the baseline, and when combined, results in an improvement of +5.6 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">State-of-the-art Comparison</head><p>To push the limits of our method even further, we employ a Pretended Under-Fitting strategy and Cyclic Pseudo-masks from PMM <ref type="bibr" target="#b38">[39]</ref> during the final training stage. Furthermore, we substitute the ResNet-38 architecture in the final stage for the more powerful Res2Net-101 <ref type="bibr" target="#b39">[40]</ref> backbone, which has shown to be effective in WSSS <ref type="bibr" target="#b38">[39]</ref>. We refer to this setting as extended training (et).</p><p>In Tab. 5, we evaluate some state-of-the-art methods in terms of both region similarity and contour quality. This  was only possible for methods where a code implementation was readily available, since reproducing the final segmentations was necessary for computing the contour F-score. The table shows the metrics after re-running the codes, including re-training unless weights or pseudo-labels were provided, in order to reproduce the original results as closely as possible. Note, however, that this leads to a lower expected variance for the previous methods where these were available, since the training was resumed from which is indicated by the check mark in this column. Our final method reaches a high region similarity score of 70.1, which is on not far off the best previous method. Moreover, we reach the highest contour quality of 50.3, which is +1.4 points better than PMM <ref type="bibr" target="#b38">[39]</ref> which is the second best in this metric. Finally, for the combined metric J &amp;F, we achieve a score of 60.2, which is the highest score in this comparison, beating the best previous method, MCTformer <ref type="bibr" target="#b41">[42]</ref>, by +0.9 points. To conclude, the results show that our method is well balanced in these two complementary aspects, and an all-around method suitable for a wide range of applications. In Tab. 6 we compare the region similarity on the VOC validation and test sets, with previous WSSS methods as reported in the respective papers. We have excluded methods which make use of additional supervision, either directly using other datasets, or indirectly using saliency maps. For ICAM, we show the result using the extended training and a Res2Net-101 backbone, corresponding to the last row in Tab. 5. We submit the test set segmentation predictions of our best model to the official PASCAL VOC evaluation server to attain the score on the test set. Similar to Tab. 5, we reach a region similarity score close to the current state-ofthe-art in weakly-supervised semantic segmentation, which shows that we can improve the contour quality without sacrificing region similarity performance.</p><formula xml:id="formula_15">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We present two methods for improving CAM learning in weakly-supervised semantic segmentation (WSSS). First, we use importance sampling for producing stochastic imagelevel predictions during training, which results in CAMs that cover a larger extent of the objects. Second, we propose a new feature similarity loss term for aligning prediction contours with edges in the image. Furthermore, we find that previous methods somewhat lack in contour quality and suggest to explicitly evaluate WSSS methods in terms of contour F-score, similar to related computer vision tasks. We find experimentally, that our method outperforms previous methods in terms of contour quality, while being comparable to state-of-the-art methods in terms of region similarity. Considering the two metrics equally, our method outright outperforms the previous state-of-the-art methods. Thus, it is more suitable for a wider range of applications. Arvi Jonnarth is an industrial Ph.D. student at Husqvarna Group, Sweden, and part of the Computer Vision Laboratory, Link?ping University, Sweden. He received his M.Sc. degree in engineering physics from Uppsala University, Sweden (2018). His research interests include deep learning for computer vision and autonomous systems, with a focus on weak supervision and reinforcement learning. He is also part of the Wallenberg AI, Autonomous Systems and Software Program (WASP).</p><p>Michael Felsberg is a full professor at Link?ping University, Sweden. He holds a Ph.D. degree from Kiel University, Germany (2002) and a docent degree from Link?ping University (2005). His research interests include, besides visual object tracking, video object and instance segmentation, point cloud processing, and efficient machine learning techniques. Yushan Zhang is a Ph.D. student at the Computer Vision Laboratory, Link?ping University, Sweden. She is funded by the Wallenberg AI, Autonomous Systems and Software Program (WASP). She received her M.Sc. degree in optical engineering from Beijing Institute of Technology, China. Her current research interests include machine learning and computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Region similarity versus contour quality compared to previous methods, reproduced from original implementations. Our method achieves the highest contour quality while attaining a competitive region similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>CAM comparison. (a) Input image. Pseudo-labels with (b) max pooling, (c) average pooling (SEAM [4]) (d) importance sampling (ours), and (e) importance sampling and feature similarity loss (ours). (f) ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Region similarity (J ) and contour quality (F) on the VOC training set for final segmentations, as functions of the loss parameter ?, with and without the feature similarity loss (FSL), for (a) global max pooling, and (b) global average pooling. FSL parameters are set to ? = 2 and ? = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Region similarity (J ), contour quality (F), and averaged (J &amp;F ) on the VOC training set for final segmentations, as functions of (a) ?, and (b) ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative segmentation results. (a) Input image, (b) foreground class activations, (c) pseudo-labels from our CAM network, (d) final segmentation predictions and (e) ground-truth segmentations. The bottom two rows illustrate failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 Hyperparameters</head><label>1</label><figDesc></figDesc><table><row><cell>and settings</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Region similarity (J ), contour quality (F), and averaged (J &amp;F) on the VOC training set when varying the number of sampled pixels. Mean and standard deviation is reported over five runs ? 3.6 48.5 ? 2.3 57.2 ? 3.0 2 67.6 ? 1.3 49.1 ? 1.3 58.3 ? 1.3 568.0 ? 0.5 49.8 ? 0.5 58.9 ? 0.</figDesc><table><row><cell>#pixels</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>1</cell><cell cols="3">65.9 5</cell></row><row><cell>10</cell><cell cols="3">68.3 ? 0.2 49.5 ? 0.4 58.9 ? 0.3</cell></row><row><cell>20</cell><cell cols="3">68.3 ? 0.5 49.6 ? 0.4 58.9 ? 0.5</cell></row><row><cell>50</cell><cell cols="3">68.2 ? 0.4 49.6 ? 0.4 58.9 ? 0.4</cell></row><row><cell>100</cell><cell cols="3">68.4 ? 0.3 49.8 ? 0.2 59.1 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Baseline comparison with SEAM<ref type="bibr" target="#b3">[4]</ref> in terms of region similarity (J ), contour quality (F), and averaged (J &amp;F ), on the VOC validation set, for CAM and AffinityNet pseudo-labels, and final segmentations. Mean is reported over five runs</figDesc><table><row><cell>Pseudo-labels/</cell><cell>J</cell><cell></cell><cell>F</cell><cell></cell><cell cols="2">J &amp;F</cell></row><row><cell>segmentations</cell><cell cols="6">SEAM Ours SEAM Ours SEAM Ours</cell></row><row><cell>CAMs</cell><cell>52.5</cell><cell>57.2</cell><cell>20.8</cell><cell>40.8</cell><cell>36.7</cell><cell>49.0</cell></row><row><cell>AffinityNet</cell><cell>60.1</cell><cell>65.2</cell><cell>35.7</cell><cell>48.9</cell><cell>47.9</cell><cell>57.0</cell></row><row><cell>Final</cell><cell>64.6</cell><cell>67.2</cell><cell>39.4</cell><cell>47.9</cell><cell>52.0</cell><cell>57.6</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Region similarity (J ), contour quality (F), and combined (J &amp;F ) on the</cell></row><row><cell cols="7">VOC validation set, comparing our contributions separately on SEAM.</cell></row><row><cell></cell><cell cols="4">Mean is reported over five runs</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>J</cell><cell>F</cell><cell cols="2">J &amp;F</cell></row><row><cell>SEAM</cell><cell></cell><cell></cell><cell cols="2">64.6 39.4</cell><cell>52.0</cell><cell></cell></row><row><cell cols="3">SEAM + importance sampling</cell><cell cols="2">62.5 41.6</cell><cell>52.1</cell><cell></cell></row><row><cell cols="5">SEAM + feature similarity loss 65.2 43.6</cell><cell>54.4</cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell cols="2">67.2 47.9</cell><cell>57.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Region similarity (J ), contour quality (F), and averaged (J &amp;F) on the VOC validation set. All values are reproduced from the original implementations, and averaged over five runs. fp is short for fixed parameters, and et indicates extended training from PMM</figDesc><table><row><cell>Method</cell><cell>Backb.</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>SEAM [4]</cell><cell>Res38</cell><cell cols="3">64.6 ? 0.1 39.4 ? 1.5 52.0 ? 0.7</cell></row><row><cell>CDA [41]</cell><cell>Res38</cell><cell cols="3">63.7 ? 0.1 42.7 ? 0.1 53.2 ? 0.1</cell></row><row><cell>PMM [39]</cell><cell>Res38</cell><cell cols="3">67.3 ? 0.2 46.4 ? 0.3 56.9 ? 0.2</cell></row><row><cell>PMM [39]</cell><cell>R2N</cell><cell cols="3">68.8 ? 0.7 48.9 ? 0.5 58.8 ? 0.6</cell></row><row><cell>MCTformer [42]</cell><cell cols="4">DeiT-S 71.0 ? 0.2 47.6 ? 0.3 59.3 ? 0.2</cell></row><row><cell>fp et</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICAM [13]</cell><cell>Res38</cell><cell cols="3">63.9 ? 0.6 43.5 ? 0.8 53.7 ? 0.7</cell></row><row><cell>ICAM (ours)</cell><cell>Res38</cell><cell cols="3">67.2 ? 0.3 47.9 ? 0.3 57.6 ? 0.2</cell></row><row><cell>ICAM (ours)</cell><cell>Res38</cell><cell cols="3">69.1 ? 0.5 48.2 ? 0.6 58.6 ? 0.5</cell></row><row><cell>ICAM (ours)</cell><cell>R2N</cell><cell cols="3">70.1 ? 0.2 50.3 ? 0.4 60.2 ? 0.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">a later stage, as opposed to started from scratch. Thus, the variance in the preceding stages is eliminated. In the table, we denote our method ICAM, short for Importance Sampling CAMs. In the comparison, we include our early version<ref type="bibr" target="#b12">[13]</ref> of ICAM which sets ? and ? to learnable parameters, uses one sampled pixel per class during importance sampling, and sets ? = 1. This setting is distinguished by the fixed parameters (fp) column in the bottom half of Tab. 5. In this paper, we fix the parameters ? and ? according toFig. 4,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg (KAW) Foundation. The computations were conducted at the Berzelius Su-perPOD, provided by the National Supercomputer Centre (NSC), and at the Alvis cluster, provided by the Swedish National Infrastructure for Computing (SNIC), partially funded by the Swedish Research Council (VR) through grant agreement no. 2018-05973.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human segmentation in surveillance video with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gruosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Capece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Erra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1175" to="1199" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="275" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Novel approach to nonlinear/non-gaussian bayesian state estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Salmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings on Radar and Signal Processing</title>
		<imprint>
			<publisher>IET</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="107" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The mechanics of human saccadic eye movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Importance sampling CAMs for weakly-supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jonnarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="2639" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribblesupervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning random-walk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">WILDCAT: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-local means denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="208" to="212" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards understanding ensemble, knowledge distillation and self-distillation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09816</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregation functions to combine rgb color channels in stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jurio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lopez-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paternain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bustince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1247" to="1257" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distribution de la flore alpine dans le bassin des dranses et dans quelques r?gions voisines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin de la Soci?t? Vaudoise des Sciences Naturelles</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="241" to="272" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pseudo-mask matters in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6964" to="6973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context decoupling augmentation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="7004" to="7014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiclass token transformer for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4310" to="4319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4283" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning affinity from attention: End-to-end weakly-supervised semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="16" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="655" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
	<note>in European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation by pixel-to-prototype contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4320" to="4329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ecs-net: Improving weakly supervised semantic segmentation by using connections between class activation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="7283" to="7292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unlocking the potential of ordinary classifier: Class-specific adversarial erasing framework for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6994" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Class re-activation maps for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="969" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Complementary patch for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7242" to="7251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reducing information bottleneck for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-supervised imagespecific prototype exploration for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="4288" to="4298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards noiseless object contours for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="16" to="856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
