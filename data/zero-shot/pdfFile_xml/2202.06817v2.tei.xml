<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CATs++: Boosting Cost Aggregation with Convolutions and Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">CATs++: Boosting Cost Aggregation with Convolutions and Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic visual correspondence</term>
					<term>cost aggregation</term>
					<term>efficient transformer !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cost aggregation is a process in image matching tasks that aims to disambiguate the noisy matching scores. Existing methods generally tackle this by hand-crafted or CNN-based methods, which either lack robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields and inadaptability. In this paper, we introduce Cost Aggregation with Transformers (CATs) to tackle this by exploring global consensus among initial correlation map with the help of some architectural designs that allow us to benefit from global receptive fields of self-attention mechanism. To this end, we include appearance affinity modeling, which helps to disambiguate the noisy initial correlation maps. Furthermore, we introduce some techniques, including multi-level aggregation to exploit rich semantics prevalent at different feature levels and swapping self-attention to obtain reciprocal matching scores to act as a regularization. Although CATs can attain competitive performance, it may face some limitations, i.e., high computational costs, which may restrict its applicability only at limited resolution and hurt performance. To overcome this, we propose CATs++, an extension of CATs. Concretely, we introduce early convolutions prior to cost aggregation with a transformer to control the number of tokens and inject some convolutional inductive bias, then propose a novel transformer architecture for both efficient and effective cost aggregation, which results in apparent performance boost and cost reduction. With the reduced costs, we are able to compose our network with a hierarchical structure to process higher-resolution inputs. We show that the proposed method with these integrated outperforms the previous state-of-the-art methods by large margins. Codes and pretrained weights are available at: https://ku-cvlab.github.io/CATs-PlusPlus-Project-Page/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E STABLISHING dense correspondences across semantically similar images can facilitate many Computer Vision applications, including semantic segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, and image editing <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Unlike classical dense correspondence problems such as stereo matching, geometric matching, or optical flow that consider visually similar images taken under the geometrically constrained settings <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, semantic correspondence poses additional challenges from large intra-class appearance and geometric variations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> caused by the unconstrained settings of the given image pair.</p><p>Recent approaches <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> addressed these challenges by carefully designing deep Convolutional Neural Networks (CNNs)-based models analogously to the classical matching pipeline <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, namely feature extraction, cost aggregation, and flow estimation. Several works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> focused on the feature extraction stage, as it has been demonstrated that the more powerful feature representation the model learns, the more robust matching is obtained <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. However, solely relying on the matching similarity between features without any prior, e.g., spatial smoothness often suffers from the challenges due to ambiguities generated by repetitive patterns or background clutters <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>. On the other hand, some methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> focused on the flow estimation stage either by designing additional CNN as an ad-hoc regressor that predicts the parameters of a single global transformation <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, finding confident matches from correlation maps <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, or directly feeding the correlation maps into the decoder to infer dense correspondences <ref type="bibr" target="#b23">[24]</ref>. However, these methods highly rely on the quality of the initial correlation maps <ref type="bibr" target="#b25">[26]</ref>.</p><formula xml:id="formula_0">*</formula><p>To address this, the latest methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> have focused on the second stage, highlighting the importance of cost aggregation. Since the quality of correlation maps is of prime importance, they proposed to refine the matching scores by formulating the task as Optimal Transport problem <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, re-weighting matching scores by Hough space voting for geometric consistency <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, or utilizing highdimensional 4D or 6D convolutions to find locally consistent matching points <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Although formulated variously, these methods either use hand-crafted techniques that are neither learnable nor robust to severe deformations, nor inherit the limitation of CNNs, which their limited receptive fields make it hard to discriminate incorrect matching points, and they lack an ability to adapt to the input contents due to the inherent characteristic of convolution, i.e., a fixed and shared kernel across all the input pixels.</p><p>In this work, we also focus on the cost aggregation stage and propose a novel cost aggregation network to tackle the aforementioned issues. Our network, called Cost Aggregation with Transformers (CATs), is based on transformer <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which is renowned for its global receptive field and ability to flexibly adapt to consider pairwise interactions among all the input tokens. By considering all the matching scores computed between features of input images globally, our aggregation network explores global consensus and thus refines the ambiguous or noisy matching scores effectively. Specifically, based on the observation that desired correspondence should be aligned at discontinuities with the appearance of images <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, we concatenate an appearance arXiv:2202.06817v2 [cs.CV] <ref type="bibr" target="#b29">30</ref> Oct 2022 We show that our proposed methods perform surprisingly well even for images in the wild. Note that green and red circles denote the correctly predicted points and incorrectly predicted points, respectively, and the red star denotes the ground-truth keypoint. CATs++ finds highly accurate and fine-grained matching points, even surpassing CATs, thanks to its hierarchical architecture as well as its enhanced cost aggregation components.</p><p>embedding to the correlation map, which helps to disambiguate the correlation map within the transformer aggregator. To benefit from hierarchical feature representations, following <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, we use a stack of correlation maps constructed from multi-level features, and propose to effectively aggregate the scores across the multi-level correlation maps. Furthermore, we consider the bidirectional nature of a correlation map to leverage the correlation map from both source and target directions, obtaining reciprocal scores by swapping the pair of dimensions of the correlation map to allow global consensus. In addition to these, we provide residual connections around aggregation networks to ease the learning process.</p><p>Although competitive performance can be attained by CATs, it may face some limitations, i.e., high computational costs induced by the use of full attention adopted by standard transformers <ref type="bibr" target="#b40">[41]</ref>, which restrict its applicability only at limited resolution and result in rather limited performance. To address these, we propose CATs++ to not only alleviate the computational burden but also to enhance the cost aggregation for the improved performance, as exemplified in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, we introduce early convolutions to reduce the costs by controlling the number of tokens and their dimensions and inject some convolutional inductive bias prior to cost aggregation by transformers, which also leads to apparent performance boost as this allows the model capable of aggregating both local and global interactions. Furthermore, we introduce a novel design for transformer, tailored for cost aggregation, which includes reformulation of standard Query-Key-Value (QKV) projection and Feed-Forward Network (FFN). This is achieved by including appearance affinity at the intermediate process of QKV projection to reduce computational burden and utilizing 4D convolutions rather than the linear projection in FFN to not only strengthen the power to consider locality but also to flexibly control the input dimensions. With the reduced costs from the reformulation, we compose a hierarchical architecture, allowing the coarser levels to guide the cost aggregation at finer levels. Lastly, unlike CATs, we refrain from arbitrarily selecting different combinations of feature maps <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b44">[45]</ref> for each dataset, which adds extra burden, but rather use all the feature maps as done in <ref type="bibr" target="#b2">[3]</ref> to exploit richer semantic representations.</p><p>With these combined, we demonstrate our methods on standard benchmarks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> for semantic correspondence. Experimental results on various benchmarks prove the effectiveness of the proposed models over the latest methods for semantic correspondence, clearly outperforming and attaining state-of-theart for all the benchmarks. We also provide an extensive ablation study to validate and analyze components. Finally, we show that the proposed method is robust to both domain and class shift, and also works surprisingly well for a few-shot segmentation task as one of the possible applications.</p><p>This work is extended from the preliminary version of <ref type="bibr" target="#b44">[45]</ref> by (I) leveraging both convolutions and transformers to take the best of two by strengthening the ability to consider locality interactions prior to global consideration of pairwise interactions by transformers and reduce the computational loads, (II) introducing a novel and efficient transformer architecture that can significantly boost the performance and reduce the computational costs, (III) designing a hierarchical architecture that aggregates the cost volume constructed at higher resolution, (IV) setting a new stateof-the-art performance for standard benchmarks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> in semantic correspondence, making an 8.5%p increase from the best-published results, and (V) providing additional experimental results, ablation studies and visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Correspondence</head><p>Methods for semantic correspondence generally follow the classical matching pipeline <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, including feature extraction, cost aggregation, and flow estimation. Most early efforts <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b45">[46]</ref> leveraged the hand-crafted features which are inherently limited in capturing high-level semantics. Though using deep CNNbased features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref> has become increasingly popular thanks to their invariance to deformations, without a means to refine the matching scores independently computed between the features, the performance would be rather limited. Recently, MMNet <ref type="bibr" target="#b47">[48]</ref> attempts to use transformers <ref type="bibr" target="#b40">[41]</ref> to refine the features, focusing on the feature extraction stage, with slight changes made to self-attention computation to compute local self-attention over feature maps to aggregate the local feature values, while we take a different approach, aggregating the cost volume and leveraging both convolutions and transformers to not only inject some convolutional inductive bias and reduce computation cost but also to exploit the merits of both, i.e., locality bias of convolution, adaptability and global receptive fields of transformers.</p><p>To alleviate the limited performance by the use of matching scores without aggregation, several methods focused on the flow estimation stage. Rocco et al. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> proposed an end-to-end network to predict global transformation parameters from the matching scores, and their success inspired many variants <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Among the subsequent works, RTNs <ref type="bibr" target="#b34">[35]</ref> attempted to obtain semantic correspondences through an iterative process of estimating spatial transformations. DGC-Net <ref type="bibr" target="#b19">[20]</ref>, Semantic-GLU-Net <ref type="bibr" target="#b23">[24]</ref>, and DMP <ref type="bibr" target="#b48">[49]</ref> utilize a CNN-based decoder to directly find correspondence fields. Recently, PDC-Net <ref type="bibr" target="#b49">[50]</ref> proposed a flexible probabilistic model that jointly learns the flow estimation and its uncertainty. Arguably, directly regressing correspondences from the initial matching scores highly relies on the quality of them, which motivates aggregation of matching scores for more reliable correspondences.</p><p>Recent numerous methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> thus have focused on the cost aggregation stage to refine the initial matching scores. Among hand-crafted methods, SCOT <ref type="bibr" target="#b22">[23]</ref> formulates semantic correspondence as an Optimal Transport problem and attempts to solve two issues, namely many to one matching and background matching. HPF <ref type="bibr" target="#b20">[21]</ref> first computes appearance matching confidence using hyperpixel features and then uses Regularized Hough Matching (RHM) algorithm for cost aggregation to enforce geometric consistency. DHPF <ref type="bibr" target="#b21">[22]</ref>, which replaces the feature selection algorithm of HPF <ref type="bibr" target="#b20">[21]</ref> with trainable networks, also uses RHM. However, these hand-crafted techniques for refining the matching scores are neither learnable nor robust to severe deformations.</p><p>As learning-based approaches, NC-Net <ref type="bibr" target="#b18">[19]</ref> utilizes 4D convolution to achieve local neighborhood consensus by finding locally consistent matches, and its variants <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref> proposed more cost-efficient methods. PMNC <ref type="bibr" target="#b50">[51]</ref> utilizes a classical algorithm, PatchMatch <ref type="bibr" target="#b51">[52]</ref>, and 4D convolutions to tackle the semantic correspondence task, and proposes a PahchMatch-based optimization strategy to iteratively refine the correspondence field. GO-Cor <ref type="bibr" target="#b25">[26]</ref> proposed an aggregation module that directly improves the correlation maps. GSF <ref type="bibr" target="#b37">[38]</ref> formulated a pruning module to suppress false positives of correspondences in order to refine the initial correlation maps. CHM <ref type="bibr" target="#b27">[28]</ref> goes one step further, proposing a learnable geometric matching algorithm that utilizes 6D convolution. CHMNet <ref type="bibr" target="#b39">[40]</ref> extends CHM <ref type="bibr" target="#b27">[28]</ref> by introducing an efficient kernel decomposition with center-pivot neighbors. Although outstanding performance with the help of additional data augmentation <ref type="bibr" target="#b44">[45]</ref> and multi-level features <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[45]</ref> is achieved, CHMNet <ref type="bibr" target="#b39">[40]</ref> yet suffers from limitations of CNNbased architectures for cost aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Correspondence Applications</head><p>Establishing visual correspondences has been a cornerstone of many Computer Vision applications. To name a few, object tracking <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, video object segmentation (VOS) <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, few-shot segmentation (FSS) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b57">[58]</ref>, visual localization <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b58">[59]</ref>, structure-from-motion (SfM) <ref type="bibr" target="#b59">[60]</ref>, and image retrieval <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref> are the tasks that establishing accurate correspondence fields remain as one of the milestones for their goals. More specifically, object tracking <ref type="bibr" target="#b62">[63]</ref> and video object segmentation <ref type="bibr" target="#b63">[64]</ref> need space-time correspondences between objects of interests for detection or segmentation across different time steps, and FSS <ref type="bibr" target="#b64">[65]</ref> requires segmenting objects of unseen classes by finding correspondences given a few annotated support images and a query image. SfM <ref type="bibr" target="#b59">[60]</ref> and visual localization <ref type="bibr" target="#b61">[62]</ref> are the 3D computer vision tasks to estimate three-dimensional structures by finding correspondences across multiple two-dimensional image sequences. In this paper, we also show how the proposed method is applicable to FSS and visual localization tasks, and evaluate its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformers in Vision</head><p>Transformers <ref type="bibr" target="#b40">[41]</ref>, the de facto standard for Natural Language Processing (NLP) tasks, has recently imposed significant impact on various tasks in Computer Vision fields such as image classification <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b65">[66]</ref>, object detection <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, tracking and matching <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b68">[69]</ref>. ViT <ref type="bibr" target="#b41">[42]</ref>, the first work to propose an endto-end transformer-based architecture for the image classification task, successfully extended the receptive field, owing to its selfattention nature that can capture the global relationships between features. For those works addressing visual correspondence, LoFTR <ref type="bibr" target="#b26">[27]</ref> uses a cross and self-attention module to refine the feature maps conditioned on both input images, and formulate the hand-crafted aggregation layer with dual-softmax <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b69">[70]</ref>, and Optimal Transport <ref type="bibr" target="#b24">[25]</ref> to infer correspondences. In another work, COTR <ref type="bibr" target="#b70">[71]</ref> takes coordinates as input and addresses dense correspondence tasks without the use of a correlation map. Unlike these, for the first time, we propose a transformer-based cost aggregation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation and Overview</head><p>Let us denote a pair of images as source I s and target I t , which represent semantically similar images, and features extracted from I s and I t as D s and D t , respectively. Here, our goal is to establish a dense correspondence field F (i) between two images that are defined for each pixel i, which warps I t towards I s .</p><p>Estimating the correspondence with sole reliance on matching similarities between D s and D t is often challenged by ambiguous matches due to the repetitive patterns or background clutters <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b48">[49]</ref>. To address this, numerous methods proposed cost aggregation techniques that try to refine the initial matching similarities either by formulating the task as Optimal Transport problem <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, using regularized Hough Matching to reweight the costs <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> or adopting 4D or 6D convolutions <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. However, these methods either use handcrafted techniques that are weak to severe deformations or only consider long-range pairwise interactions implicitly by stacking the series of convolution blocks.</p><p>To overcome these, we present transformer-based cost aggregation networks that effectively integrate information present in all matching costs as a whole by considering pairwise interactions explicitly with the help from global receptive fields of attention mechanism, as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. In the following, we explain feature extraction and cost computation and then explain the proposed transformer aggregator. Finally, we introduce additional techniques to further boost the performance to complete the design of cost aggregation networks with transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction and Cost Computation</head><p>To extract dense feature maps from images, as shown in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we leverage multi-level features to capture hierarchical semantic feature representations. To this end, we use multi-level features from different levels of convolutional layers to construct a stack of correlation maps. Specifically, we use CNNs 1 that produce a sequence of L feature maps, and D l represents a feature map at the l-th level. As done in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we use a different combination of multi-level features depending on the dataset trained on, e.g., PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> or SPair-71k <ref type="bibr" target="#b15">[16]</ref>. Given a sequence of feature maps, we resize all the selected feature maps 1. Note that we could also use transformer-based feature backbone networks, e.g., ViT <ref type="bibr" target="#b41">[42]</ref>, which we explore the influence of different feature backbone networks in Section 5.4.  Analogously to the classical matching pipeline, our networks consist of feature extraction, cost aggregation, and flow estimation modules. We first extract multi-level dense features and construct a stack of correlation maps. We then concatenate with embedded features and feed into the transformer-based cost aggregator to obtain a refined correlation map. The flow is then inferred from the refined correlation map.</p><p>to R h?w?c , with height h, width w, and c channels. Note that we down-sample the features to fixed h and w for efficiency. The resized features first undergo l-2 normalization and a correlation map is computed using the inner product between them followed by an activation function such that:</p><formula xml:id="formula_1">C l (i, j) = ReLU D l s (i) ? D l t (j) D l s (i) D l t (j) ,<label>(1)</label></formula><p>where i and j denote 2D spatial positions of feature maps, respectively, and ReLU(?) denotes ReLU activation function <ref type="bibr" target="#b71">[72]</ref>.</p><p>In this way, all pairwise feature matches are computed and stored. We subsequently concatenate the computed correlation maps to obtain a stacked correlation map C ? R hw?hw?L . However, raw matching scores contain numerous ambiguous matching points as exemplified in <ref type="figure">Fig. 3</ref>, which results in inaccurate correspondences.</p><p>To remedy this, we propose cost aggregation networks in the following that aim to refine the ambiguous or noisy matching scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Preliminary: Transformers</head><p>Before moving on to the proposed transformer aggregator, we start by explaining Vision Transformer (ViT) encoder <ref type="bibr" target="#b40">[41]</ref>. Note that the proposed method does not utilize a decoder, which we omit its explanation in this section. Renowned for its global receptive fields, one of the key elements of transformer <ref type="bibr" target="#b40">[41]</ref> is the selfattention mechanism, which enables finding the correlated input tokens by first feeding into scaled dot product attention function (self-attention layer <ref type="bibr" target="#b40">[41]</ref>), normalizing with Layer Normalization (LN) <ref type="bibr" target="#b72">[73]</ref>, and passing the normalized values to an MLP, which is also known as Feed-Forward Network (FFN). Formulating the overall process of vision transformer <ref type="bibr" target="#b41">[42]</ref>, y = T (I), where y serves as the image representation obtained by prepending a learnable embedding prior to feeding into the encoder, it first undergoes Query-Key-Value projection as:</p><formula xml:id="formula_2">Q = P Q (LN(I + E pos )), K = P K (LN(I + E pos )), V = P V (LN(I + E pos )),<label>(2)</label></formula><p>where P Q , P K and P V denote query, key and value linear projection, respectively; I denotes token embeddings; E pos denotes positional embedding. Then the obtained Q, K, and V undergo a self-attention layer:</p><formula xml:id="formula_3">z = SA(Q, K, V ) = softmax(QK T )V,<label>(3)</label></formula><p>where SA(?) denotes Self-Attention. Subsequently, the output undergoes LN and residual connection followed by FFN that consists of two linear transformations with a GELU <ref type="bibr" target="#b73">[74]</ref> activation in between, as shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, which is formulated as:</p><formula xml:id="formula_4">z =? + z pos , x = LN(z), x = P 2 FFN (GELU(P 1 FFN (x))), y =x + z,<label>(4)</label></formula><p>where z pos = I + E pos , and P 1 FFN and P 2 FFN are the first and the second position-wise linear transformations within the feedforward network, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transformer Aggregator</head><p>Several works <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> have shown that given images or features as input, transformers <ref type="bibr" target="#b40">[41]</ref> integrate the global context information by learning to find the attention scores for all pairs of tokens. In this paper, we take a different approach, leveraging the transformers to integrate the pairwise relationships across matching scores to not only discover global consensus but also to carefully consider the pairwise interactions among tokens for an effective cost aggregation in a global manner. Unlike ViT <ref type="bibr" target="#b41">[42]</ref>, we do not use a class token and obtain a refined cost? as output by feeding the stacked raw cost C ? R hw?hw?L to the transformer T , consisting of QKV projections, SA, LN, and FFN:</p><formula xml:id="formula_5">C = T (C).<label>(5)</label></formula><p>The standard transformers receive input as a 1D sequence of token embeddings. To this end, we also treat each spatial location at either source or target as 1D token. We visualize the refined correlation map with self-attention in <ref type="figure">Fig. 5</ref>, where the ambiguities are significantly resolved. In the following, we introduce appearance affinity modeling to disambiguate the noisy correlation maps and multi-level aggregation to effectively aggregate across spatial-and level-dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Appearance Affinity Modelling</head><p>When only matching costs are considered for aggregation, selfattention layer processes the correlation map disregarding the noise involved in the correlation map, which may lead to inaccurate correspondences. Rather than solely relying on a raw correlation map, we additionally provide an appearance embedding from input features to disambiguate the correlation map. The intuition behind is that visually similar points in an image, e.g., color or feature, have similar correspondences, as proven in stereo matching literature, e.g., Cost Volume Filtering (CVF) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>To provide appearance affinity, we propose to concatenate embedded features projected from input features with the correlation map. We first feed the features D l into the linear projection networks, and then concatenate the output along the corresponding dimension, so that the correlation map is augmented such that [C l , P l (D l )] ? R hw?(hw+p) , where [ ? ] denotes concatenation, P denotes linear projection networks, and p is channel dimension of the embedded feature. The transformer can be then formulated as?</p><formula xml:id="formula_6">= T ([C l , P l (D l )] L l=1 ).<label>(6)</label></formula><p>Specifically, within the transformer T , this augmented correlation map then undergoes QKV projection:</p><formula xml:id="formula_7">Q = P Q (LN([C l , P l (D l )] L l=1 + E pos )), K = P K (LN([C l , P l (D l )] L l=1 + E pos )), V = P V (LN([C l , P l (D l )] L l=1 + E pos )),<label>(7)</label></formula><p>where for E pos , we let the networks be aware of the positional information by providing a learnable embedding <ref type="bibr" target="#b41">[42]</ref> rather than a fixed <ref type="bibr" target="#b40">[41]</ref> as shown in <ref type="figure">Fig. 4</ref>. Note that P Q , P K and P V process the stacked correlation map C, while P processes at each l-th level of C. After the QKV projections, following the process of a standard transformer, we feed the output into the self-attention layer, which is formulated in Eq. 3, to aggregate the matching costs concatenated with appearance embedding to reason about their relationships, and then they undergo FFN, which is formulated in Eq. 4. Subsequently, the aggregated augmented correlation map is passed to the linear projection networks, to retain the size of original correlation C, which we obtain? as illustrated in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Multi-level Aggregation</head><p>For each l-th level of stacked correlation maps C = [C l ] L l=1 , levelwise matching scores are available, and we aim to employ all of them present across different levels. In order to employ both hierarchical and spatial semantic representations, we introduce a multi-level aggregation technique, which aggregates the stack of augmented correlation maps in a spatial-and level-wise manner. In this way, the aggregation networks can consider interactions among matching scores at a particular level, as well as across levels.</p><p>Specifically, as shown in <ref type="figure">Fig. 4</ref>, a stack of L augmented correlation maps, [C l , P l (D l )] L l=1 ? R hw?(hw+p)?L , undergo the transformer aggregator. For each l-th augmented correlation map, we aggregate with self-attention layer across all the points in the augmented correlation map, and we refer to this as intracorrelation self-attention. This is shown as a red arrow, which is a level-wise aggregation. In addition, subsequent to this, the correlation map undergoes inter-correlation self-attention across multi-level dimensions, which is a spatial-wise aggregation. Contrary to HPF <ref type="bibr" target="#b20">[21]</ref> that concatenates all the multi-level features to compute a stack of correlation maps and aggregate using RHM, which only limitedly consider the multi-level similarities, within the inter-correlation self-attention layer of the proposed model, the similar matching scores are explicitly explored across multilevel dimensions. In this way, we can embrace richer semantics in different levels of feature maps, as shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cost Aggregation with Transformers</head><p>By leveraging the transformer aggregator, we present a cost aggregation framework with the following additional techniques to improve the performance, and by combining them, we complete the architecture of CATs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Swapping Self-Attention</head><p>In order to impose mutually consistent matching scores, we argue that reciprocal scores should be used to infer confident correspondences. As the correlation map contains bidirectional matching scores, from both target and source perspectives, we can leverage matching similarities from both directions in order to obtain more reciprocal scores as done similarly in other works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b32">[33]</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we first feed the augmented correlation map to the aforementioned transformer aggregator. Then we transpose the output, swapping the pair of dimensions in order to concatenate with the embedded feature from the other image and feed it into the subsequent aggregator. Note that we share the parameters of the transformer aggregators to obtain reciprocal scores. Formally, we define the whole process as follows:</p><formula xml:id="formula_8">S = T ([C l , P l (D l t )] L l=1 ), C = T ([(S l ) T , P l (D l s )] L l=1 ),<label>(8)</label></formula><p>where C T (i, j) = C(j, i) denotes swapping the pair of dimensions corresponding to the source and target images; S denotes the intermediate correlation map before swapping the axis. Note that NC-Net <ref type="bibr" target="#b18">[19]</ref> proposed a similar procedure, but instead of processing serially, they separately process the correlation map and its transposed version and add the outputs, which this procedure is designed to produce a correlation map invariant to the particular order of the input images. Unlike this, we process the correlation map serially, first aggregating the one pair of dimensions and then further aggregating the other pair. In this way, the subsequent attention layer is given cost maps with more consistent matching scores as an input, allowing further reduction of inconsistent matching scores. We justify our design choice in the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Residual Connection</head><p>At the initial phase when the correlation map is fed into the transformers, noisy score maps are inferred due to randomly initialized parameters, which could complicate the learning process.</p><p>To stabilize the learning process and provide a better initialization for the matching, we employ the residual connection. Specifically, we enforce the cost aggregation networks to estimate the residual correlation by adding residual connection around aggregation networks. Now Eq. 8 changes to:</p><formula xml:id="formula_9">S = T ([C l , P l (D l t )] L l=1 ) + C, C = T ([(S l ) T , P l (D l s )] L l=1 ) + C T .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training Objective</head><p>As in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we assume that the ground-truth keypoints are given for each pair of images. We first average the stack of refined correlation maps? ? R hw?hw?L along level dimension and then transform it into a dense flow field F pred using soft-argmax operator <ref type="bibr" target="#b32">[33]</ref>. Subsequently, we compare the predicted dense flow field with the ground-truth flow field F GT obtained by following the protocol of <ref type="bibr" target="#b20">[21]</ref> using input keypoints. For the training objective, we utilize Average End-Point Error (AEPE) <ref type="bibr" target="#b19">[20]</ref>, computed by averaging the Euclidean distance between the ground-truth and estimated flow. We thus formulate the objective function as</p><formula xml:id="formula_10">L = F GT ? F pred 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BOOSTING COST AGGREGATION WITH CONVO-LUTIONS AND TRANSFORMERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation and Overview</head><p>Despite its deliberate design and effectiveness, CATs may struggle with high computational costs induced by the use of standard transformers <ref type="bibr" target="#b40">[41]</ref>, and this could restrict the cost aggregation to be performed only at limited resolutions. More specifically, as stated in some works <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b74">[75]</ref> that computational complexity for self-attention layer is quadratic to token length, and those of QKV projection and FFN are quadratic to feature dimension, processing high-dimensional correlation maps with standard transformers <ref type="bibr" target="#b40">[41]</ref> inevitably face large computational burden.</p><p>To alleviate this, we propose to introduce early convolutions prior to cost aggregation with transformers to control the number of tokens to reduce the costs and additionally inject some convolutional inductive bias to enhance the subsequent cost aggregation, which results in an apparent performance boost. Moreover, to reduce the computational loads from QKV projection and FFN, we introduce a novel transformer architecture for efficient cost aggregation that reformulates the two components by including appearance embedding inside the QKV projection and introducing convolutions to replace linear projections, which not only results in a costs reduction as intended but also an apparent performance boost. Thanks to the reduced costs, we are permitted to process the inputs of higher resolutions and manage to build a hierarchical architecture. Moreover, we deviate from arbitrarily selecting different combinations of feature maps <ref type="bibr" target="#b20">[21]</ref> as was done in CATs, but rather use all the feature maps to exploit richer semantic representations. With these combined, we introduce an extension of CATs, namely CATs++. Affinity Concat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reshape</head><p>Reshape &amp; Linear     <ref type="bibr" target="#b41">[42]</ref> that utilizes a large kernel for nonoverlapping convolutions, our convolutional embedding uses a series of small kernels for overlapping convolutions.</p><formula xml:id="formula_11">Reshape &amp; Linear Attention &amp; Reshape Q K V V K Q ? ? ? ? ? ? ? ? / ! ? ? / ! ? ? ? ? ? ? ? ? / ! ? ? / ! ? ? Positional Embedding LN Intra/Inter Correlation Self-Attention</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Extraction and Cost Computation</head><p>To exploit rich semantics present at different feature levels, we also leverage multi-level features as done in CATs and <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>. However, taking a slightly different approach, we use all the intermediate feature maps as done in <ref type="bibr" target="#b2">[3]</ref>. Specifically, similar to CATs, we produce a sequence of L feature maps, but unlike how CATs selected a specific number of feature maps for each dataset, which is an extra burden, CATs++ use all the feature maps. In this way, not only we do not need to select different combinations of feature maps for each dataset we use, but also we can exploit richer semantics by using all the intermediate feature maps.</p><p>For the cost computation, following <ref type="bibr" target="#b2">[3]</ref>, given a pair of feature maps, D l s and D l t , we compute a correlation map using the inner product between l-2 normalized features. We then collect correlation maps of the same spatial sizes and denote the subset as {C l } l?L q , where L q is a subset of CNN layer indices {1, ..., L} at some pyramid layer q. Lastly, we concatenate all the colleted {C l } l?L q to obtain a hypercorrelation C q ? R hsws?htwt?|L q | , where h s , w s and h t , w t are height and width of feature maps of source and target images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Convolutional Embedding Module</head><p>Without a means to control the high computational costs induced by processing the hypercorrelation C q , the amount of time, resources, and memory required would not be negligible. Considering the quadratic complexity of a standard transformer <ref type="bibr" target="#b40">[41]</ref>, <ref type="figure">Fig. 9</ref>: Visualization of self-attention. Note that CATs++ utilizes hierarchical semantic representations, which the finer level selfattention is shown in the smaller region, and higher q denotes coarser level.</p><formula xml:id="formula_12">(a) Source (b) Target (c) Source q = 5 (d) Source q = 4 (e) Source q = 3 (f) Target q = 5 (g) Target q = 4 (h) Target q = 3</formula><p>simply treating all the spatial dimensions as tokens seems unwise. A better approach and the most straightforward approach, perhaps, would be to reduce the number of tokens by adopting 4D spatial pooling across source and target dimensions. However, this strategy risks losing some information, which we ought to avoid and will be verified in Section 5.4.1. An alternative option could be to patchify the hypercorrelation by splitting it into nonoverlapping tensors and embed with a large learnable kernel as done in ViT <ref type="bibr" target="#b41">[42]</ref>. Na?ve implementation to this could be extending 2D patch embedding to 4D as shown in <ref type="figure" target="#fig_9">Fig. 8 (a)</ref>. However, as claimed in <ref type="bibr" target="#b57">[58]</ref>, non-overlapping operations only provides limited inductive bias, which only the relatively lower translation equivariance is achieved without replacing such operations. Moreover, disregarding window boundaries due to non-overlapping kernels may hurt overall performance due to discontinuity.</p><p>To compensate for the issues, analogously to VAT <ref type="bibr" target="#b57">[58]</ref>, we use a small early convolution layer to process the correlation map prior to transformer aggregation. Given a hypercorrelation C q , we consider receptive fields of 4D window, i.e., m ? m ? m ? m, and build a tensor M q ? R? s??s ??t??t?|L q | , where? and? are the processed sizes as illustrated in <ref type="figure" target="#fig_9">Fig. 8 (b)</ref>. Note that we reduce both spatial dimensions within hypercorrelation for computational efficiency, while VAT <ref type="bibr" target="#b57">[58]</ref> preserves its spatial dimension of query image to obtain a fine-grained segmentation map. During this operation, we locally mix the multi-level similarity vector at each 4D position, which acts as a local inter-correlation aggregation. In this way, we enhance the subsequent cost aggregation with transformers by injecting some convolutional bias to mitigate the above issues and allowing both local and global aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficient Transformer Aggregator</head><p>Although the convolutional embedding module adjusts the number of tokens and feature dimensions to some extent, directly applying standard transformers <ref type="bibr" target="#b40">[41]</ref> is still challenging. This is due to what some works <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b74">[75]</ref> stated that the standard selfattention mechanism has quadratic complexity with respect to the number of tokens, and QKV projections and FFN have quadratic complexity with respect to the feature dimensionality. To address this, recently, numerous works <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> have proposed to reduce the computational burden by introducing an efficient transformer or self-attention, i.e., token pruning <ref type="bibr" target="#b80">[81]</ref>, linear self-attention <ref type="bibr" target="#b76">[77]</ref> and locality-sensitive hashing <ref type="bibr" target="#b75">[76]</ref>. However, although these methods effectively alleviate the computational burden by aiming to either control the number of tokens or reduce the complexity of self-attention, in our context, QKV projections and FFN relatively impose more computational burden as we flatten one of the spatial dimension, i.e., the spatial dimension of source, and level dimension to treat them as feature dimension for a token. This has been relatively underexplored <ref type="bibr" target="#b81">[82]</ref>, and we propose a novel design specialized in matching cost aggregation that addresses the aforementioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Affinity-Aware Convolutional Projection</head><p>As illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref>, the proposed QKV projection differs from that of CATs that follows the standard approach in that it uses convolutions to first process high resolution input to control the subsequent computation by reducing the spatial dimension from hw to hw/s 2 , where s denotes a stride size. By introducing convolutions prior to linear projection, not only we provide efficiency by allowing feature dimension controllable, but also strengthen the power to consider locality. While <ref type="bibr" target="#b82">[83]</ref> tries a similar approach by applying depth-wise separable convolutions to reduce the number of tokens, here we aim to reduce the feature dimension. Subsequently, we concatenate appearance embedding to the intermediate outputs of query and queue projections, introducing appearance affinity inside the transformer architecture. It should be noted that linear projection is included only within the Query and Key projections as we aim to not only utilize and preserve the spatial resolution of the raw correlation map but also to further reduce the computation as we find that it is sufficient to only include appearance affinity information within the attention score computation QK T rather than within the (QK T )?V computation in order to aggregate both the appearance affinity along with the matching scores, which the QK projection is responsible for. Then we further reduce the feature dimension for Q and K projection by feeding them into linear projection, reducing self-attention computational loads. In this way, we benefit from the reduced computation, eliminating one-third of computational loads of value projection, had the linear projection been included.</p><p>Concretely, the process can be expressed as follows:</p><formula xml:id="formula_13">Q q = P q Q ([LN(Q q Q (LN(M q ))), P q (D q )]) + E q pos , K q = P q K ([LN(Q q K (LN(M q ))), P q (D q )]) + E q pos , V q = LN(Q q V (LN(M q ))),<label>(10)</label></formula><p>where Q denotes convolutional projection; E q pos denotes positional embedding at q-th layer. Note that projections are performed at each q-th layer.  <ref type="figure" target="#fig_0">Fig. 10</ref>: Qualitative results on SPair-71k <ref type="bibr" target="#b15">[16]</ref>: keypoints transfer results by DHPF <ref type="bibr" target="#b21">[22]</ref>, CHM <ref type="bibr" target="#b27">[28]</ref>, CATs, and CATs++. Note that the green and red line denote correct and wrong prediction, respectively, with respect to the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Volumetric Convolutional Feed-Forward</head><p>Given outputs of QKV projections, we follow the standard selfattention computation to obtain? as in Eq. 3. Note that we find it relatively less influential to change it to other efficient selfattention computation methods <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b83">[84]</ref>, and we thus use standard self-attention <ref type="bibr" target="#b41">[42]</ref> for simplicity. Subsequent to selfattention computation, in order to model additional locality bias, reduce the number of parameters for better generalization and provide more memory efficiency, we replace linear transformations and ReLU activation with 4D convolutions and GELU, respectively, as shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. Moreover, it should be noted that flattening the correlation volume inevitably requires a linear projection with a large feature dimension, which results in a significant computation load. This provides efficiency benefits to using convolution over a large linear projection layer. The process is defined as:</p><formula xml:id="formula_14">z =? + M q , x = LN(z), x = Q 2 FFN (GELU(Q 1 FFN (x))), y =x + z,<label>(11)</label></formula><p>where Q 1 FFN and Q 2 FFN are the first and the second convolutional projections within the feed-forward network, respectively. We find that this simple change brings a surprisingly apparent performance boost when combined with our proposed architecture, which will be discussed in Section 5.4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Cost Aggregation with Pyramidal Processing</head><p>Combining all the proposed components of CATs++, we manage to reduce the costs and process higher resolution inputs with pyramidal architecture. Specifically, analogous to <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b48">[49]</ref> and inspired by several works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b48">[49]</ref>, we also utilize the coarse-to-fine approach through a pyramidal processing as illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref> to allow matching scores processed at previous levels to guide the aggregation process at finer levels by employing residual connections. In this way, we enforce the networks to focus on learning complementary matching scores <ref type="bibr" target="#b47">[48]</ref> as well as let the coarse level aggregation guide the finer level aggregation.</p><p>Overall, we define the whole process of our pyramidal cost aggregation as follows:</p><formula xml:id="formula_15">C q = T q + (M q , P q (D q )) + T q + ((M q ) T , P q (D q )), M q?1 ? up(? q ) + M q?1 ,<label>(12)</label></formula><p>where T + denotes efficient transformer aggregator and up(?) denotes bilinear upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison to Concurrent Work</head><p>Recently, a concurrent work, Volumetric Aggregation with Transformers (VAT) <ref type="bibr" target="#b57">[58]</ref>, also proposed a Transformer-based architecture to tackle few-shot segmentation task. Although VAT shares a similar high-level idea, which is to combine convolutions and Transformer, and evaluates its effectiveness on both few-shot segmentation and semantic correspondence tasks, there exist apparent differences, which we summarize in the following. First of all, the motivations behind the designs of each architecture differ as their main target tasks are different, i.e., fewshot segmentation and semantic correspondence. Although they may share similar challenges, e.g., background clutters and intraclass variations, at high-level, VAT <ref type="bibr" target="#b57">[58]</ref> designs its architecture to enhance its generalization power to handle new classes beyond training samples tailored for few-shot segmentation, while CATs++ redesigns the components of the standard Transformer for efficient computations tailored for semantic correspondence.</p><p>In this regard, VAT <ref type="bibr" target="#b57">[58]</ref> and our CATs++ formulated different transformer aggregators. It should be noted that the components prior to transformer aggregator, i.e., hypercorrelation and early convolutions, are the designs that both architectures share, which means that the subsequent modules lead to notable variations in effectiveness and efficiency. VAT <ref type="bibr" target="#b57">[58]</ref> adopts Swin Transformer <ref type="bibr" target="#b84">[85]</ref> as its transformer aggregator and extends to handle 4D cost volumes with 4D local windows, while CATs++ redesigns the standard transformer to a more efficient version. To address the complexity, VAT <ref type="bibr" target="#b57">[58]</ref> adopts shifting window multi-head self-attention <ref type="bibr" target="#b84">[85]</ref> that is designed to reduce the computational complexity of a global multi-head self-attention module. Unlike this, a key contribution that differentiates the proposed method to other Transformer-based architecture is that CATs++ reformulates the standard QKV projections and FFN, and proposes affinity-aware convolutional projections and volumetric convolutional feed-forward network, aiming to reduce the memory and computational complexity that are typically high when the input is high dimensional correlation maps.</p><p>More importantly, we argue that the relative positioning bias that Swin Transformer <ref type="bibr" target="#b84">[85]</ref> include is what makes it excel at predicting segmentation map for unseen classes. We observe that relative positioning bias allows greater generalization power, which will be evidenced in <ref type="table" target="#tab_6">Table 5</ref> in experiments, which was also observed in Swin Transformer <ref type="bibr" target="#b84">[85]</ref>, and this led to performance improvement for few-shot segmentation benchmark. However, for the semantic matching task, where generalization power has a relatively smaller influence to performance than to few-shot segmentation, we observe CATs++ outperforms VAT. This can be explained by the special design of CATs++ that integrates convolutions and appearance embedding with the proposed transformer aggregator, which not only allows an improved efficiency, but also a clear difference in architecture that leads to an apparent performance difference to VAT <ref type="bibr" target="#b57">[58]</ref> for semantic correspondence task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>For the backbone feature extractor, we use ResNet-101 <ref type="bibr" target="#b85">[86]</ref> pretrained on ImageNet <ref type="bibr" target="#b86">[87]</ref> for both CATs and CATs++, while other backbone features can also be used. For the hyperparameters of the transformer encoder, we set the number of encoders as 1 for all the transformer aggregators in both CATs and CATs++ based on the ablation study, which we chose to set it as 1 for simplicity. For CATs, we resize the spatial size of the input image pairs to 256?256 and a sequence of selected features is resized to h = 16 and w = 16, while we resize image pairs to 512?512 for CATs++. For pyramidal processing in CATs++, we employ pyramidal layers of q = 3, 4, 5: from conv3 x to conv5 x, similar to <ref type="bibr" target="#b2">[3]</ref>. AdamW <ref type="bibr" target="#b87">[88]</ref> optimizer with an initial learning rate of 3e ? 5 for the aggregators and 3e ? 6 for the backbone features are used, which we gradually decrease during training. To apply data augmentation <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref> with predetermined probabilities to input images at random. Specifically, 50% of the time, we randomly crop the input image, and independently for each augmentation function used in <ref type="bibr" target="#b88">[89]</ref>, we set the probability for applying the augmentation as 20%. We implemented our network using PyTorch <ref type="bibr" target="#b90">[91]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>In this section, we conduct comprehensive experiments for semantic correspondence, by evaluating our approach through comparisons to state-of-the-art methods including CNNGeo <ref type="bibr" target="#b16">[17]</ref>, A2Net <ref type="bibr" target="#b33">[34]</ref>, WeakAlign <ref type="bibr" target="#b17">[18]</ref>, NC-Net <ref type="bibr" target="#b18">[19]</ref>, RTNs <ref type="bibr" target="#b34">[35]</ref>, SFNet <ref type="bibr" target="#b32">[33]</ref>, HPF <ref type="bibr" target="#b20">[21]</ref>, DCC-Net <ref type="bibr" target="#b91">[92]</ref>, ANC-Net <ref type="bibr" target="#b38">[39]</ref>, DHPF <ref type="bibr" target="#b21">[22]</ref>, SCOT <ref type="bibr" target="#b22">[23]</ref>, GSF <ref type="bibr" target="#b37">[38]</ref>, and CHM <ref type="bibr" target="#b27">[28]</ref>, CATs <ref type="bibr" target="#b44">[45]</ref>, PMNC <ref type="bibr" target="#b50">[51]</ref>, MMNet <ref type="bibr" target="#b47">[48]</ref>, and CHMNet <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Datasets</head><p>We consider SPair-71k <ref type="bibr" target="#b15">[16]</ref> which provides a total of 70,958 image pairs with extreme and diverse viewpoints, scale variations, and rich annotations for each image pair, e.g., keypoints, scale difference, truncation and occlusion difference, and clear data split. Previously, for semantic matching, most of the datasets are limited to a small quantity with similar viewpoints and scales <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. As our network relies on transformer which requires a large number of data for training, SPair-71k <ref type="bibr" target="#b15">[16]</ref> makes the use of transformer in our model feasible. We also consider PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> containing 1,351 image pairs from 20 categories and PF-WILLOW <ref type="bibr" target="#b13">[14]</ref> containing 900 image pairs from 4 categories, each dataset providing corresponding ground-truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluation Metric</head><p>For evaluation on SPair-71k <ref type="bibr" target="#b15">[16]</ref>, PF-PASCAL <ref type="bibr" target="#b14">[15]</ref>, and PF-WILLOW <ref type="bibr" target="#b13">[14]</ref>, we employ a percentage of correct keypoints (PCK), computed as the ratio of estimated keypoints within the threshold from ground-truths to the total number of keypoints. Given predicted keypoint k pred and ground-truth keypoint k GT , we count the number of predicted keypoints that satisfy the following condition: d(k pred , k GT ) ? ? ? max(H, W ), where d( ? ) denotes Euclidean distance; ? denotes a threshold. We evaluate on PF-PASCAL with ? img , SPair-71k, and PF-WILLOW with ? bbox . H and W denote height and width of the object bounding box or the entire image, respectively. As stated in <ref type="bibr" target="#b39">[40]</ref>, we additionally report results of ? bbox-kp for PF-WILLOW <ref type="bibr" target="#b13">[14]</ref> for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Correspondence Results</head><p>For a fair comparison, we follow the evaluation protocol of <ref type="bibr" target="#b20">[21]</ref> for SPair-71k <ref type="bibr" target="#b15">[16]</ref>, in which our network is trained on the training split and evaluated on the test split. Similarly, for PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> and PF-WILLOW <ref type="bibr" target="#b13">[14]</ref>, following the common evaluation protocol of <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref>, we train our network on the training split of PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> and then evaluate on the test split of PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> and PF-WILLOW <ref type="bibr" target="#b13">[14]</ref>. All the results of other methods are reported under the identical setting. <ref type="figure" target="#fig_0">Fig. 10</ref> visualizes qualitative results for extremely challenging image pairs. We observe that our methods are capable of suppressing noisy scores and finding accurate correspondences in cases where large scale and geometric variations are present. <ref type="table" target="#tab_1">Table 1</ref> summarizes quantitative results on SPair-71k <ref type="bibr" target="#b15">[16]</ref>, PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> and PF-WILLOW <ref type="bibr" target="#b13">[14]</ref>. In order to ensure a fair comparison, we note whether each method leverages multi-level features and fine-tunes the backbone networks. We additionally denote the types of cost aggregation.</p><p>We first compare the proposed methods with those that do not fine-tune backbone networks. Comparing CATs ? and CATs++ ? to other methods, we find that not only CATs ? outperforms  <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Higher PCK is better. The best results are in bold, and the second best results are underlined. CATs ? (or CATs++ ? ) denotes CATs (or CATs++) without fine-tuning feature backbone. Note that ResNet-101 is used as a backbone feature extractor as a default setting unless stated otherwise, i.e., MMNet <ref type="bibr" target="#b47">[48]</ref>. Feat.-level: Feature-level, FT. feat.: Fine-tune feature.   its effectiveness and highlights that cost aggregation is of prime importance, and leveraging both convolutions and transformers in a right way clearly makes stronger cost aggregation. Note that VAT <ref type="bibr" target="#b57">[58]</ref> also achieves highly competitive results, even surpassing the results of proposed method on PF-WILLOW <ref type="bibr" target="#b13">[14]</ref> at ? = 0.1. Nevertheless, it is shown that the proposed approach outperforms for all other benchmarks, highlighting the superiority of the proposed efficient transformer to Swin Transformer <ref type="bibr" target="#b84">[85]</ref> in VAT <ref type="bibr" target="#b57">[58]</ref> for semantic correspondence task. Now comparing CATs and CATs++, we observe a significant performance boost for both backbone fine-tuned and frozen. Specifically, when the backbone is frozen, 7.6%p increase is observed while there is a 9.9%p increase when backbone is finetuned. This clearly demonstrates the effectiveness extension of CATs. Also, we observe from the results of PF-WILLOW <ref type="bibr" target="#b13">[14]</ref> that more fine details are predicted correctly as well as the generalization power is enhanced. This is confirmed by how CATs++ ? is almost on par with CATs on PF-WILLOW even though their performance gap on PF-PASCAL shows a large gap.</p><p>Additionally, as stated in CHMNet <ref type="bibr" target="#b39">[40]</ref>, for a fair comparison, we report results of PCK @ ? bbox?kp for PF-WILLOW <ref type="bibr" target="#b13">[14]</ref>. It is notable that our methods generally report lower PCK on PF-WILLOW <ref type="bibr" target="#b13">[14]</ref> compared to other state-of-the-art methods. We conjecture that this is due to the provision of sparsely annotated data and the use of transformers, since sparse annotations can hurt the generalization power <ref type="bibr" target="#b39">[40]</ref>. Moreover, it is well known that the transformers lack generalization power with a small dataset size <ref type="bibr" target="#b41">[42]</ref>. When we evaluate on PF-WILLOW, we infer with the model trained on the training split of PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> with sparse keypoint annotations, which only contains 1,351 image pairs, and as an only relatively small quantity of image pairs is available within the PF-PASCAL training split, the proposed method shows low generalization power. Note that to compensate for this issue, we provided data augmentation techniques, which clearly helped to increase the generalization power. From this, we suspect that a provision of more data could help to address the generalization issue, and perhaps the weak supervisions other than sparse keypoints could further enhance its generalization power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation study</head><p>In this section, we show an ablation analysis to validate critical components we made to design our architecture, and provide analysis on the use of different backbone features, the depth of transformer encoder, comparison among different cost aggregators, memory/run-time, and data augmentation. We train all the variants on the training split of SPair-71k <ref type="bibr" target="#b15">[16]</ref> when evaluating on SPair-71k, and train on PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> for evaluating on PF-PASCAL. We measure the PCK, and each ablative experiment is conducted under the same experimental setting for a fair comparison. <ref type="table" target="#tab_3">Table 2</ref> shows the analysis of key components of CATs. We analyze four key components for this ablation study, which include appearance modeling, multi-level aggregation, swapping self-attention, and residual connection. As shown in <ref type="table" target="#tab_4">Table 3</ref>, we also conduct an ablation study on key components of CATs++, which we consider hypercorrelation, convolutional embedding, efficient transformer aggregator, and appearance modeling. For both analyses, we evaluate on SPair-71k benchmark by progressively adding each key component. In this experiments, we report the results for the backbone models with and without fine-tuned. Moreover, to explore how the proposed modules are co-affected, we evaluate the models equipped with different combinations of the modules, and the results are reported in <ref type="table" target="#tab_4">Table 3</ref> from VI to IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Effectiveness of each component</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we first start with CATs by defining the model without any of the key components as a baseline model, which simply feeds the correlation map into the self-attention layer. From I to V, we observe a consistent improvement in performance when each component is added. The results of I show relatively lower PCK, indicating that simply utilizing transformers does not yield high performance, which highlights the importance of each of our proposed components. Interestingly, II shows a large improvement in performance, which we find that the appearance modeling enables the model to refine the ambiguous or noisy matching scores and the use of both appearance and cost volume allows the transformer to relate the pairwise relationships and appearance, helping to find more accurate correspondences. Although a relatively small increase of PCK for III, it shows that the proposed model successfully aggregates the multi-level correlation maps with the help of intra-and inter-correlation selfattention. Furthermore, IV and V show an apparent increase by helping the training process, clearly confirming the significance of both components.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, for CATs++, we define the baseline model that does not utilize appearance affinity, only uses feature maps of the last index at each p-th layer, replaces convolutional embedding with simple 4D max-pooling with linear projection, and utilizes raw correlation map without cost aggregation. In the results, we observe that each component except II contributes to a surprisingly large performance boost. Especially from III to IV, the performance gap is 14.2%p, which clearly demonstrates the effectiveness of the proposed efficient transformer aggregator. It should be noted that V is actually included within our proposed efficient transformer aggregator, which means that the proposed efficient transformer by itself brings a 22.1%p increase. Interestingly, we observe a slight drop in performance when we attempt to use hypercorrelation as shown in II. We conjecture that although the use of multi-level features generally helps to find better correspondences <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b44">[45]</ref>, without a means to aggregate the costs, the use of multi-level features rather make the matching process more complex.</p><p>We also observe consistent improvements in performance even with backbone network fine-tuned when each module is cumulatively stacked. However, the results for VI and VIII deviate from this trend, where the PCK of VIII with backbone network fine-tuned is higher than that of VI despite lower PCK for the backbone model without fine-tuning. Also, we find that although we generally observe consistent performance improvements as we stack up the components, II in <ref type="table" target="#tab_3">Table 2</ref> shows a slight performance drop. This may seem trivial for only a 0.4 % drop, but these could indicate that fine-tuning the backbone network may have had larger influence than introducing a new component. Another notable result is VII, which shows slightly lower PCK to the best PCK reported, indicating that although convolutional embedding can help to achieve higher performance, highly competitive results can be attained even without convolutional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Effectiveness of each component in efficient transformer aggregator</head><p>In this ablation study, we provide an analysis of the proposed efficient transformer aggregator of CATs++ with respect to performance, memory, and number of learnable parameters. For this study, we first define the baseline model; it simply replaces the efficient transformer aggregator with standard transformer <ref type="bibr" target="#b40">[41]</ref>. We investigate the effectiveness of the proposed components by replacing each component in a standard transformer with our proposed component, as illustrated in <ref type="table" target="#tab_5">Table 4</ref>. Note that for calculating both memory consumption and the number of parameters, we exclude memory and parameters of backbone networks and convolutional embedding in order to emphasize the changes made by adopting each of our proposed efficient transformer aggregators.</p><p>Replacing QKV projection with affinity-aware convolutional projection, we observe that the PCK improves by 9.9%p. We also observe a similar increase when we replace feed-forward network with the proposed volumetric convolutional feed-forward. This apparent performance boost demonstrates their effectiveness, and this is further confirmed when two components are combined, which our efficient transformer aggregator obtains PCK of 50.0%p, a 13.3%p higher results than that of the baseline model. We also report memory consumption and the number of learnable parameters to validate their efficiency. Overall, each component reduces the memory consumption by approximately 0.2 GB, which makes roughly 0.3 GB or 11% memory reduction in total. What surprised us is that the number of learnable parameters is reduced by almost 80%, which may have resulted in better generalization power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Why transformers for cost aggregation?</head><p>As our architecture is based on transformers, perhaps it is necessary to compare the effectiveness of transformers to other methods that are capable of relating the tokens of given inputs, i.e., MLP or convolutions. Especially, MLP and MLP mixer <ref type="bibr" target="#b93">[94]</ref> also enjoy from global receptive fields, which is similar to transformers, so we believe that this comparison is necessary to justify our choice to select transformers over others. To this end, we compare the transformer aggregator with all the other cost aggregation methods, and the results are summarized in <ref type="table" target="#tab_6">Table 5</ref>. For this experiment, we simply replace the self-attention module with other aggregator methods without touching any of our contributions. Note that, unlike other methods, 3D convolution is not compatible with our contributions as the different structure of the spatial axis makes them infeasible to directly replace the module, which we have to abandon some of the proposed components, e.g., multi-level aggregation. However, we included 3D convolutions to evaluate the performance as a cost aggregator. Lastly, we adjusted the number of parameters for a fair comparison and report the memory consumption for different cost aggregators to indicate the efficiency. From the results, using simple MLP yields surprisingly competitive results, thanks to its global cost aggregation, but its relatively poor performance highlights the importance of pairwise relationships, as MLP is only responsible for channel-mixing operation. This is similar to convolution, as MLP also lacks the adaptability to input pixels prevents from accurate matching. Using MLP-Mixer <ref type="bibr" target="#b93">[94]</ref>, which includes both channel and token mixing, yields highly competitive results as well, but compared to CATs, the performance is relatively poor, which the inadaptability can be one of the reasons. Given this, we argue that the transformer better considers the pairwise relationships than MLPbased aggregators and shows its superiority to better take global context in a correlation map into account. Now comparing the results to convolution-based aggregators <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b94">[95]</ref>, we observe that the performance gap is quite large for 3D convolutions. This is because we abandoned some of our contributions for this experiment. The use of center-pivot 4D convolution surprisingly performs well, but compared to transformers, the gap seems large. With such differences, CATs obtaining better performance over other methods indicates its superiority and advantage to explicitly learn pairwise relationships with global attention range, process the input as a whole, and remove locality constraint which highlights the importance of our contribution and the effectiveness of transformer aggregator.</p><p>To compare the efficiency of different aggregators, we also note the memory consumption for each. Summarizing the results, we observe that memory consumption gap is quite large between  3D convolution and center-pivot 4D convolution, but others show relatively similar memory consumption, and transformer requires the second lowest memory. Although center-pivot 4D convolutions shows the best efficiency thanks to its deliberate design, considering the large performance gap and relatively small memory consumption gap, we claim that the proposed transformer-based cost aggregator shows its superiority. Additionally, we include the results of the model that employs Relative Positioning Bias (RPB) <ref type="bibr" target="#b84">[85]</ref>. In this experiment, we aim to investigate the reasoning behind the performance difference between this work and that of VAT <ref type="bibr" target="#b57">[58]</ref>. To this end, we include relative positioning bias within the self-attention computation and left other components untouched. In practice, we did not find notable performance changes for semantic matching benchmarks, but rather, we observed a notable performance boosts for FSS-1000 <ref type="bibr" target="#b95">[96]</ref> dataset. Note that we also observed a similar tendency for CATs++, except that we observed a slight performance drop when evaluated on SPair-71k <ref type="bibr" target="#b15">[16]</ref>, confirming a relatively weaker influence of relative positioning bias on a semantic matching task. We thus find that the relative positioning bias can enhance the generalization power, which would explain some portion of performance gap between this work and VAT <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Does different feature backbone matter?</head><p>As shown in <ref type="table" target="#tab_7">Table 6</ref>, we explore the impact of different feature backbones on the performance on SPair-71k <ref type="bibr" target="#b15">[16]</ref> and PF-PASCAL <ref type="bibr" target="#b14">[15]</ref>. We report the results with backbone networks frozen. For subscript single, we use a single-level feature, while for subscript all, every feature map from 12 layers is used for cost construction. For ResNet-101 subscript multi, we use the best layer subset provided by <ref type="bibr" target="#b20">[21]</ref>, while we use hypercorrelation approach for subscript all.</p><p>Summarizing the results, we consistently observe that leveraging multi-level features shows apparent improvements in performance, proving the effectiveness of multi-level aggregation. It is worth noting that DINO, which is more excel at dense tasks than DeiT-B, outperforms DeiT-B when applied to semantic matching. This indicates that the learned representation has a large influence on the final performance. The interesting results we observed are the ones we obtained with CLIP-based backbone. Given ViT w/ CLIP all or ResNet-101 w/ CLIP all as backbone networks, we find that the performance is enhanced compared to the results with conventional ResNet-101. For ViT w/ CLIP all we were surprised to observe such a significant performance boost. Even for PF-PASCAL, the performance boost occurs, confirming that the use of CLIP backbone shows an apparent performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Effects of varying the number of the encoders</head><p>As done in numerous works <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b66">[67]</ref> that utilize transformers, we can also stack more encoders to increase their capacity and validate the effectiveness by varying the number of encoders. We show the effects of varying the number of encoders in <ref type="figure" target="#fig_0">Figure 11</ref>. It is natural that choosing a higher number of encoders inevitably increases the memory consumption and runtime, but it obtains a larger model capacity in return. This is confirmed by the results when the number of encoder is set to 1, where both CATs ? and CATs++ ? report the lowest PCK. This indicates that the increase of the capacity of transformer aggregator clearly helps to boost the performance, which even surpasses the results reported in <ref type="table" target="#tab_1">Table 1</ref>. Interestingly, showing completely contrary results, when the backbone networks are finetuned, the best PCKs are obtained when the number of encoders is set to 1. We suspect that trying to optimize both the backbone networks and the cost aggregator already risks overfitting, and by increasing the capacity of our cost aggregator, the model starts to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Data augmentation</head><p>Transformer is well known for lacking some of the inductive bias and its data-hungry nature thus necessitates a large quantity of training data to be fed <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Recent methods <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b98">[99]</ref> that employ transformers to address Computer Vision tasks have empirically shown that data augmentation techniques have a positive impact on performance. Also, to compensate for low generalization power caused by the provision of sparsely annotated data, i.e., keypoints, we provide a means to address it. Moreover, in the correspondence task, the question of to what extent can data augmentation affect performance has not yet been properly addressed. To this end, from the experiments, we empirically find that data augmentation has consistent positive impacts on performance in semantic correspondence.  In <ref type="table" target="#tab_8">Table 7</ref>, we compared the PCK performance between our variants, DHPF <ref type="bibr" target="#b21">[22]</ref> and CHMNet <ref type="bibr" target="#b39">[40]</ref>. We note if the model is trained with augmentation. For a fair comparison, we evaluate all the methods trained on SPair-71k <ref type="bibr" target="#b15">[16]</ref> using strong supervision, which assumes that the ground-truth keypoints are given. For CHMNet, we use the results reported in <ref type="bibr" target="#b39">[40]</ref>. The results show that compared to DHPF and CHMNet, two typical examples of CNNbased cost aggregation methods, data augmentation technique has a larger influence on CATs in terms of performance. This demonstrates that not only we ease the data-hunger problem inherent in transformers, but also found that applying augmentations for matching has positive effects. Specifically, this technique allowed the performance boosts for DHPF and CHMNet by 2.1%p and 4.3%p, respectively, but it is further boosted for CATs by 6.4%p. This is further confirmed by the results of CATs++, showing that the augmentation technique brings apparent performance improvements by 5.8%p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Serial/Parallel processing</head><p>It is apparent that Equation 9 is not designed for an order-invariant output. Different from NC-Net <ref type="bibr" target="#b18">[19]</ref>, we design CATs in a way that we let the correlation map undergo the self-attention module in a serial manner. We conducted a simple experiment to compare the difference between each approach. Moreover, we also provide a quantitative results comparison of CATs++ to justify our choice. For the experimental setting of serial processing, we sequentially aggregate the correlation map with a shared aggregator, while for parallel processing, we transpose the given input and let the original and transposed correlation maps undergo the same aggregator and add them subsequently. The results are summarized in <ref type="table" target="#tab_9">Table 8</ref>. We argue that although CATs may not support order invariance, adopting serial processing can obtain higher PCK as it has a better capability to reduce inconsistent matching scores by additionally processing the already processed cost map, which we finalize the architecture to include serial processing. However,  <ref type="figure" target="#fig_0">Fig. 12</ref>: Ablation study of image resolution: (a) as varying the image resolutions, (b) comparison between proposed methods at resolution set to 256. Note that we add results for CLIP-ResNet-101 <ref type="bibr" target="#b97">[98]</ref>, which is written as blue color.</p><p>interestingly, CATs++ clearly shows different results, which the parallel processing surpasses serial processing by a large margin. Also, we observe that employing both serial and parallel processing does not yield higher performance than parallel processing. We conjecture that whether the network makes use of convolutions yields different results. More specifically, while token-mixing convolutional operations mix both spatial dimensions of correlation maps, self-attention only considers either source or target spatial dimension, and this makes self-attention suitable to serial processing and convolutions suitable to parallel processing. Note that through parallel processing, CATs++ aggregates local contexts of both spatial dimensions and the following self-attention impart them to all pixels, which allows the best results. We thus argue that employing serial processing for CATs++ may complicate the learning process by performing redundant aggregations, while it can benefit CATs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">GPU memory consumption and run-time comparison</head><p>In <ref type="table" target="#tab_10">Table 9</ref>, we show the memory and run-time comparison to NC-Net <ref type="bibr" target="#b18">[19]</ref>, SCOT <ref type="bibr" target="#b22">[23]</ref>, DHPF <ref type="bibr" target="#b21">[22]</ref>, and CHM <ref type="bibr" target="#b27">[28]</ref>. For a fair comparison, the results are obtained using a single NVIDIA GeForce RTX 2080 Ti GPU and Intel Core i7-10700 CPU. We measure the GPU memory consumption given a single batch. We also measure the inference time for both the process without counting feature extraction and the whole process. Specifically, we note the type of aggregation method in the table, and the reported results represent the memory consumption and running-time of the aggregator, for instance, convolutional embedding module and efficient transformer aggregator for CATs++. Thanks to transformers' fast computation nature, compared to other methods, CATs show much faster inference time, allowing real-time inference. This can benefit several applications which finding correspondences act as one of the milestones, e.g., object tracking and video object segmentation <ref type="bibr" target="#b63">[64]</ref>. Taking an example, an autonomous vehicles require real-time detection, tracking or segmentation of objects for a decision making, and a fast inference time can reduce delays during a decision making process, greatly reducing probability of potential incidents. Note that although CATs++ may suffer from relatively slower inference speed to other works <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, less than 80 ms slower run time is a minor sacrifice for the better performance.</p><p>For the memory consumption, we find that compared to other cost aggregation methods including 4D, 6D convolutions, OT-RHM, and RHM, the transformer aggregator shows comparable efficiency in terms of computational cost. Note that NC-Net and CHM show relatively lower memory consumption to others as they utilize a single feature map while all other methods utilize multilevel feature maps. Also, it is worth noting that CATs++ requires the largest memory consumption. This is because of the use of multi-level features as well as the processing of inputs at higher resolutions, which we show the reduced memory consumption and run-time at a coarse level of CATs++ to confirm this. Although CATs++ managed to reduce computational costs as summarized in <ref type="table" target="#tab_10">Table 9</ref>, processing the correlation maps at higher resolutions inherently requires much larger costs in exchange for large performance gain. Further enhancing the efficiency by balancing with the performance would be a promising direction, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Resolution of input images</head><p>We find that different baseline methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> for semantic correspondence does not use the fixed input resolutions at resolution according to the implementations authors provide. This means that comparing the proposed method to other baseline methods that use different resolutions may make the comparison unfair. We argue that to make a fair comparison, the training resolutions can be different among works, but the evaluation resolutions should be the same. To this end, we provide the results of our proposed methods at different resolutions to make fair comparisons to previous methods first, then we suggest a benchmark for semantic correspondence task for future works.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 12 (a)</ref>, we observe that the higher resolution generally yields higher performance. Specifically, at resolution 400, CATs yields the best results, while at resolution 512, CATs++ yields the best results. This shows that the higher resolution generally helps to gain higher performance. However, it can be seen at resolution 512 that the performance of CATs drops severely. We suspect that this is the limitation of cost aggregation with a standard transformer, which its performance can not infinitely scale as the resolution increases. As seen in <ref type="figure" target="#fig_0">Fig. 12 (a)</ref>, we argue that direct comparison is possible only when the results for different input sizes are given. Now taking this into account, we suggest a fixed benchmark for future works in semantic correspondence task.</p><p>We suggest the evaluation sizes be consistent across all the works for a fair comparison, which we decide as 256, and the results of the proposed methods at 256 evaluation resolution are shown in <ref type="figure" target="#fig_0">Fig. 12 (b)</ref>. As shown, the performance of CATs++ dropped by a quite large margin. More specifically, we observe 6.0%p drop in performance for CATs++ when the evaluation size is changed from 512 to 256. This should make a fair comparison to other works evaluated at 256. Now we introduce a new finding to narrow this gap, which we adopt a new feature backbone, ResNet-101 initialized by pre-trained weights released by CLIP <ref type="bibr" target="#b97">[98]</ref>. We were surprised to observe that by simply replacing the conventional ResNet-101 pre-trained on ImageNet <ref type="bibr" target="#b86">[87]</ref> to that of CLIP, apparent performance boosts can be made. This can be a valuable finding that could bring an apparent performance boost even for other tasks, thanks to the improved training scheme and datasets by CLIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.5">Capturing fine details</head><p>As an extension to CATs, we proposed CATs++, a novel cost aggregation approach that enjoys both reduced costs and boosted performance, and composed to have a hierarchical architecture, Quantitative results as varying the alpha threshold on SPair-71k <ref type="bibr" target="#b15">[16]</ref> and PF-PASCAL <ref type="bibr" target="#b14">[15]</ref>. CATs++ captures finedetails surprisingly well compared to other methods (including CATs), thanks to its hierarchical architecture.</p><p>which as a result, it allowed the model to be excel at capturing fine-details as shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> As shown in <ref type="figure" target="#fig_0">Fig. 14,</ref> where we evaluate DHPF <ref type="bibr" target="#b21">[22]</ref>, CHM <ref type="bibr" target="#b27">[28]</ref>, CATs and CATs++ on SPair-71k <ref type="bibr" target="#b15">[16]</ref> and PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> by varying the alpha threshold used for the PCK evaluation, thanks to its hierarchical architecture, CATs++ successfully captures fine details, accurately finding the matching points, which CATs struggles to find. Note that CATs++ consistently outperforms all other methods except one case where CHM outperforms at ? = 0.01 for PF-PASCAL. Interestingly, when the ? threshold gets lower, CATs perform worse than other methods, indicating that it struggles to capture the fine details, which CATs++ successfully overcomes this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.6">Extension to few-shot segmentation</head><p>We also show that the proposed methods perform well at a rudimentary level even for few-shot segmentation task. Few-shot segmentation aims to reduce reliance on labeled data and only a few support images with their associated masks are given for obtaining the segmentation maps for a query image. For this experiment, we simply replace soft-argmax with a 2D decoder as done in <ref type="bibr" target="#b2">[3]</ref> to produce a segmentation map and we show that CATs and CATs++ both also achieve competitive results on FSS-1000 <ref type="bibr" target="#b95">[96]</ref>. This is because few-shot segmentation shares a similar condition to semantic correspondence task, as it's also challenged by intra-class variations and background clutters.</p><p>In <ref type="table" target="#tab_1">Table 10</ref>, we summarize the quantitative results on FSS-1000 <ref type="bibr" target="#b95">[96]</ref> dataset. FSS-1000 <ref type="bibr" target="#b95">[96]</ref>, a dataset specifically designed for few-shot segmentation, consists of 1000 object classes. Following <ref type="bibr" target="#b95">[96]</ref>, 1000 classes are categorized into 3 splits for training, validation and testing, which consist of 520, 240 and 240 classes, respectively. For the evaluation metric, we employ mean intersection over union (mIoU), which averages over all IoU values for all object classes.</p><p>We observed that CATs++ performs almost on par with other methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref> that are specifically designed for fewshot segmentation task. However, we find that the overall performance is generally lower than those of other works, especially when compared to VAT <ref type="bibr" target="#b57">[58]</ref>. The reason for the performance gap can be explained by the difference in generalization power. As CATs only adopts Transformer as its aggregator, it may suffer from lower generalization power caused by the lack of inductive bias. While other works, including HSNet <ref type="bibr" target="#b2">[3]</ref>, VAT <ref type="bibr" target="#b57">[58]</ref> and CATs++, avoid this by utilizing convolutions for translation equivariance or adopting Swin Transformer <ref type="bibr" target="#b84">[85]</ref> that benefits from relative positioning bias. Although HSNet <ref type="bibr" target="#b2">[3]</ref> and CATs++ benefit from convolutional inductive bias just like VAT <ref type="bibr" target="#b57">[58]</ref> does, the best performance by VAT <ref type="bibr" target="#b57">[58]</ref> can be explained by the joint use of both convolutions and Swin Transformer <ref type="bibr" target="#b84">[85]</ref>, which allowed stronger generalization power compared to HSNet <ref type="bibr" target="#b2">[3]</ref> and CATs++ that only utilize convolutions within their architectures. Nevertheless, our approach showed that it is competent at both tasks despite its relatively lacking generalization power, and even outperforms other works in semantic matching.</p><p>Finally, comparing the memory consumption and the learnable parameters to HSNet <ref type="bibr" target="#b2">[3]</ref> and VAT <ref type="bibr" target="#b57">[58]</ref>, we observe that CATs requires much less memory consumption than all other approaches. It should also be noted that CATs++ is much more lightweight than VAT <ref type="bibr" target="#b57">[58]</ref>, which clearly demonstrates its efficiency. However, we observe that the number of learnable parameters for CATs and CATs++ are larger than other works, which is known to have an inverse relation to generalization power <ref type="bibr" target="#b101">[102]</ref>, which may explain the performance gap when the task is to predict segmenation maps of unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.7">Extension to Visual Localization</head><p>Finally, we show that our approach also benefits the task of visual localization and evaluate on Aachen dataset <ref type="bibr" target="#b105">[106]</ref>. Visual localization aims to determine location from images by estimating the absolute 6 DoF pose of a query image with respect to the corresponding 3D scene model. To evaluate, we submit the results to the online evaluation benchmark <ref type="bibr" target="#b105">[106]</ref>.</p><p>Note that the proposed approach outputs a dense flow field, which makes it require non-trivial implementations and strategies to apply to SfM 3D reconstruction without hurting the performance. To alleviate this issue, we employ SuperPoint <ref type="bibr" target="#b31">[32]</ref> to be responsible for the keypoint detection stage, and we then follow Hloc <ref type="bibr" target="#b106">[107]</ref> outdoor localization pipeline to obtain the final localization results. To train our networks, we employ large-scale outdoor dataset, MegaDepth <ref type="bibr" target="#b107">[108]</ref>. Given output flow maps from our networks, we up-sample the flow map to original image size and utilize keypoint coordinates already obtained from Super-Point <ref type="bibr" target="#b31">[32]</ref> to find the corresponding keypoint at the other image.</p><p>The results are summarized in <ref type="table" target="#tab_1">Table 11</ref>. We observe that the proposed approach struggles to find accurate correspondences given high-resolution images, failing to localize with finedetails. Unlike other works, which include PDCNet <ref type="bibr" target="#b49">[50]</ref>, DualRC-Net <ref type="bibr" target="#b58">[59]</ref> and SuperGlue <ref type="bibr" target="#b24">[25]</ref>, our approach outputs the flow maps at relatively low resolution, i.e., 32?32. This prevents from capturing fine-details, an important aspect for the visual localization task. Nevertheless, although a new state-of-the-art is not attained, a promising future direction is revealed, which is to capture fine details. As a future direction, a module that captures the missing details of the predicted flow can be designed. For example, local cropping around the coarse correspondence may be exploited to find fine details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.8">Limitations</head><p>An apparent limitation of the proposed method is that it directly employs correlation maps, which are computationally expensive to process. This limits the resolutions of the input images the model  can handle, enforcing the model to down-sample, then find the correspondences. Moreover, the proposed approaches assume that a pair of images with common objects, i.e., semantically similar, are given as inputs, making its applicability limited to only when the pair of images with semantically similar objects are given, as shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. Although such pairs can be obtained via image retrieval process, this may be one of the apparent limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have proposed, for the first time, transformerbased cost aggregation networks for semantic correspondence which enables aggregating the matching scores computed between input features, dubbed CATs. We have made several architectural designs in the network architecture, including appearance affinity modelling, multi-level aggregation, swapping self-attention, and residual connection. We have shown that our method surpasses the current state-of-the-art in several benchmarks. Moreover, we extended CATs by introducing early convolutions and efficient transformer aggregator to reduce the computation costs and boost the performance, dubbed CATs++. We demonstrated the effectiveness of the proposed method on several benchmarks, which our method attains outperforms current state-of-the-art with large margin. We also conducted extensive ablation studies to validate our choices and explore its capacity and introduced expansion to other task, few-shot segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Qualitative comparison of (a) CATs and (b) CATs++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Overall network architecture of CATs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Raw Corr. #2 (d) Raw Corr. #4 (e) Raw Corr. #5 (f) Raw Corr. #7 (g) HPF<ref type="bibr" target="#b20">[21]</ref> (h) CATs Visualization of multi-level aggregation: (a) source, (b) target images, (c)-(f) raw correlation maps, respectively, and final correlation maps by (e) HPF<ref type="bibr" target="#b20">[21]</ref> and (f) CATs. Note that HPF and CATs utilize the same feature maps. Compared to HPF, CATs successfully embrace richer semantics in different levels of feature maps. Illustration of Transformer aggregator. Given correlation maps C with projected features, transformer aggregator consisting of intra-and inter-correlation self-attention with LN and MLP refines the inputs not only across spatial domains but across levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 Fig. 5 :</head><label>65</label><figDesc>Source l = 1 (d) Source l = 2 (e) Source l = 3 (f) Source l = 4 (g) Source l = 5 (h) Source l = Visualization of self-attention: Each attention map attends to different aspects, and CATs aggregates the costs, leveraging rich semantic representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Overall network architecture of CATs++. The networks of CATs++ differ from CATs in that all the feature maps are used to compute cost volumes rather than selecting different combinations for different datasets, and early convolution along with the efficient transformer aggregator helps to reduce computational costs and improve performance. Note that we colored the extracted feature maps to indicate that we use all the intermediate feature maps and highlight the difference toFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Intuition of the proposed components: (a) standard transformer-based aggregator (in CATs), (b) efficient transformer-based aggregator (in CATs++). CATs++ replaces the query, key and value projections and feed-forward networks of standard transformer aggregator. With this novel design, we achieve significant performance boost as well as meaningful costs reductions. Convolutional Feed-Forward CATs++: Convolutional Embedding ViT: Patch Embedding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of different embedding strategy. Unlike patch embedding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Effects of varying the number of encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 :</head><label>13</label><figDesc>Qualitative results on non-lambertian objects. The proposed methods can obtain accurate correspondence fields between non-lambertian objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 :</head><label>15</label><figDesc>Failure cases. Given images with irrelevant objects, the proposed method may struggle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Quantitative evaluation on standard benchmarks</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Ablation study of CATs.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SPair-71k</cell></row><row><cell></cell><cell>Components</cell><cell cols="2">? bbox = 0.1 FT. feat.</cell></row><row><cell>(I)</cell><cell>Baseline</cell><cell>26.8</cell><cell>46.7</cell></row><row><cell>(II)</cell><cell>+ Appearance Modelling</cell><cell>33.5</cell><cell>46.3</cell></row><row><cell cols="2">(III) + Multi-level Aggregation</cell><cell>35.9</cell><cell>47.0</cell></row><row><cell>(IV)</cell><cell>+ Swapping Self-Attention</cell><cell>38.8</cell><cell>47.6</cell></row><row><cell>(V)</cell><cell>+ Residual Connection</cell><cell>42.4</cell><cell>49.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Ablation study of CATs++.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SPair-71k</cell></row><row><cell></cell><cell>Components</cell><cell cols="2">? bbox = 0.1 FT. feat.</cell></row><row><cell>(I)</cell><cell>Baseline</cell><cell>22.7</cell><cell>49.8</cell></row><row><cell>(II)</cell><cell>+ Hypercorrelation</cell><cell>20.2</cell><cell>50.2</cell></row><row><cell>(III)</cell><cell>+ Convolutional Embedding</cell><cell>27.9</cell><cell>54.1</cell></row><row><cell>(IV)</cell><cell>+ Efficient Transformer Aggregator</cell><cell>42.1</cell><cell>55.3</cell></row><row><cell>(V)</cell><cell>+ Appearance Modelling</cell><cell>50.0</cell><cell>59.8</cell></row><row><cell>(VI)</cell><cell>(IV) -Convolutional Embedding</cell><cell>47.4</cell><cell>57.8</cell></row><row><cell>(VII)</cell><cell>(V) -Convolutional Embedding</cell><cell>48.5</cell><cell>59.1</cell></row><row><cell cols="2">(VIII) (V) -Hypercorrelation</cell><cell>46.7</cell><cell>58.1</cell></row><row><cell>(IX)</cell><cell>(VIII) -Convolutional Embedding</cell><cell>45.6</cell><cell>55.7</cell></row><row><cell cols="4">DHPF [22], by 5.1%p on SPair-71k, but also CATs++ ? further</cell></row><row><cell cols="4">boosts the performance to achieve 50.0%, which makes it be-</cell></row><row><cell cols="4">yond comparison. Interestingly, the results obtained by CATs++ ?</cell></row><row><cell cols="4">are almost on par with the previous state-of-the-art method,</cell></row><row><cell cols="4">CHMNet [40], even without fine-tuning the backbone. With the</cell></row></table><note>backbone networks fine-tuned, CATs shows highly competitive performance, achieving similar performance to previous state-of- the-art methods, thanks to its ability to explore global consensus and powerful data augmentation technique to fulfill the need of data-hungry transformer. It should be noted that CATs++ sets a new state-of-the-art on almost all the benchmarks, and it surpasses CHMNet by 8.5%p on SPair-71k, which clearly demonstrates</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Component analysis of efficient transformer aggregator. A.A.C: Affinity-aware Convolutional. V.C: Volumetric Convolutional.</figDesc><table><row><cell>Component</cell><cell>SPair-71k ? bbox = 0.1</cell><cell>Memory [GB]</cell><cell>Num. of param.</cell></row><row><cell>Baseline</cell><cell>36.7</cell><cell>2.2</cell><cell>24.9M</cell></row><row><cell>(I) A.A.C QKV projection</cell><cell>46.6</cell><cell>2.0</cell><cell>14.4M</cell></row><row><cell>(II) V.C feed-forward</cell><cell>40.0</cell><cell>2.0</cell><cell>15.9M</cell></row><row><cell>(I+II) Efficient transformer</cell><cell>50.0</cell><cell>1.9</cell><cell>5.5M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation study of cost aggregation methods. Note that we abandon some of our key contributions in order to implement 3D convolutions. RPB: Relative Positioning Bias.</figDesc><table><row><cell>Aggregator</cell><cell cols="2">SPair-71k ? bbox = 0.1 mIoU (1 shot) FSS-1000</cell><cell>Memory [GB]</cell></row><row><cell>MLP</cell><cell>34.4</cell><cell>-</cell><cell>2.0</cell></row><row><cell>MLP-Mixer [94]</cell><cell>39.1</cell><cell>-</cell><cell>2.2</cell></row><row><cell>3D convolutions [95]</cell><cell>30.6</cell><cell>-</cell><cell>2.3</cell></row><row><cell>Center-pivot 4D convolutions [40]</cell><cell>36.9</cell><cell>-</cell><cell>1.7</cell></row><row><cell>Standard Transformer [41]</cell><cell>42.4</cell><cell>80.8</cell><cell>1.9</cell></row><row><cell>Standard Transformer [41], [85] (w/ RPB)</cell><cell>42.6</cell><cell>81.8</cell><cell>1.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Ablation study of feature backbone of CATs.</figDesc><table><row><cell></cell><cell cols="4">Feature Backbone</cell><cell></cell><cell>SPair-71k ? bbox = 0.1</cell><cell>PF-PASCAL ? img = 0.1</cell></row><row><cell></cell><cell cols="4">DeiT-Bsingle [66]</cell><cell></cell><cell>32.1</cell><cell>76.5</cell></row><row><cell></cell><cell cols="3">DeiT-Ball [66]</cell><cell></cell><cell></cell><cell>38.2</cell><cell>87.5</cell></row><row><cell></cell><cell cols="6">DINO w/ ViT-B/16single [97]</cell><cell>39.5</cell><cell>88.9</cell></row><row><cell></cell><cell cols="5">DINO w/ ViT-B/16all [97]</cell><cell>42.0</cell><cell>88.9</cell></row><row><cell></cell><cell cols="4">ResNet-101single [86]</cell><cell></cell><cell>37.4</cell><cell>87.3</cell></row><row><cell></cell><cell cols="4">ResNet-101multi [86]</cell><cell></cell><cell>42.4</cell><cell>89.1</cell></row><row><cell></cell><cell cols="5">ViT w/ CLIPsingle [98]</cell><cell>36.8</cell><cell>85.3</cell></row><row><cell></cell><cell cols="4">ViT w/ CLIPall [98]</cell><cell></cell><cell>47.9</cell><cell>89.0</cell></row><row><cell></cell><cell cols="6">ResNet-101 w/ CLIPsingle [98]</cell><cell>30.8</cell><cell>77.2</cell></row><row><cell></cell><cell cols="6">ResNet-101 w/ CLIPall [98]</cell><cell>45.3</cell><cell>89.4</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCK @ ? = 0.1</cell><cell>44 46 48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell cols="2">CATs ?</cell><cell>CATs</cell><cell></cell></row><row><cell></cell><cell>60</cell><cell cols="2">(a) CATs</cell><cell></cell><cell></cell></row><row><cell></cell><cell>58</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCK @ ? = 0.1</cell><cell>50 52 54 56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell></cell><cell>4</cell></row><row><cell></cell><cell></cell><cell cols="2">CATs++ ?</cell><cell cols="2">CATs++</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Effects of augmentation.</figDesc><table><row><cell>Augment.</cell><cell cols="2">SPair-71k ? bbox = 0.1</cell></row><row><cell>DHPF [22]</cell><cell>37.3</cell><cell>-</cell></row><row><cell>DHPF [22]</cell><cell>39.4</cell><cell>-</cell></row><row><cell>CHMNet [40]</cell><cell>-</cell><cell>47.0</cell></row><row><cell>CHMNet [40]</cell><cell>-</cell><cell>51.3</cell></row><row><cell>CATs</cell><cell>35.1</cell><cell>43.5</cell></row><row><cell>CATs</cell><cell>42.4</cell><cell>49.9</cell></row><row><cell>CATs++</cell><cell>47.3</cell><cell>54.0</cell></row><row><cell>CATs++</cell><cell>50.0</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Comparison of serial/parallel processing.</figDesc><table><row><cell>Serial Parallel</cell><cell>SPair-71k ? bbox = 0.1</cell></row><row><cell>CATs ?</cell><cell>42.4</cell></row><row><cell>CATs ?</cell><cell>38.3</cell></row><row><cell>CATs ?</cell><cell>43.3</cell></row><row><cell>CATs</cell><cell>49.9</cell></row><row><cell>CATs</cell><cell>48.3</cell></row><row><cell>CATs</cell><cell>49.4</cell></row><row><cell>CATs++ ?</cell><cell>45.5</cell></row><row><cell>CATs++ ?</cell><cell>50.0</cell></row><row><cell>CATs++ ?</cell><cell>46.6</cell></row><row><cell>CATs++</cell><cell>54.8</cell></row><row><cell>CATs++</cell><cell>59.8</cell></row><row><cell>CATs++</cell><cell>55.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>GPU memory and run-time comparison. Inference time for aggregator is denoted by (?) and subscript coarse represents the coarsest layer.</figDesc><table><row><cell></cell><cell>Aggregation</cell><cell>Memory [GB]</cell><cell>Run-time [ms]</cell></row><row><cell>NC-Net [19]</cell><cell>4D Conv.</cell><cell>1.2</cell><cell>193.3 (166.1)</cell></row><row><cell>SCOT [23]</cell><cell>OT-RHM</cell><cell>4.6</cell><cell>146.5 (81.6)</cell></row><row><cell>DHPF [22]</cell><cell>RHM</cell><cell>1.6</cell><cell>57.7 (29.5)</cell></row><row><cell>CHM [28]</cell><cell>6D Conv</cell><cell>1.6</cell><cell>47.2 (38.3)</cell></row><row><cell>CATs</cell><cell>Transformer</cell><cell>1.9</cell><cell>34.5 (7.4)</cell></row><row><cell>CATs++</cell><cell>4D Conv. + Transformer</cell><cell>3.1</cell><cell>110.2 (60.6)</cell></row><row><cell>CATs++coarse</cell><cell>4D Conv. + Transformer</cell><cell>1.2</cell><cell>57.4 (3.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10 :</head><label>10</label><figDesc>Quantitative comparison on FSS-1000<ref type="bibr" target="#b95">[96]</ref>.</figDesc><table><row><cell>Backbone feature</cell><cell>Methods</cell><cell cols="2">mIoU 1-shot 5-shot</cell><cell cols="2">Memory # of learnable [GB] params.</cell></row><row><cell></cell><cell>FSOT [101]</cell><cell>82.5</cell><cell>83.8</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet50 [86]</cell><cell>HSNet [3] VAT [58]</cell><cell>85.5 90.1</cell><cell>87.8 90.7</cell><cell>--</cell><cell>2.6M 3.2M</cell></row><row><cell></cell><cell>CATs++</cell><cell>85.2</cell><cell>85.4</cell><cell>-</cell><cell>5.5M</cell></row><row><cell></cell><cell>DAN [100]</cell><cell>85.2</cell><cell>88.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>HSNet [3]</cell><cell>86.5</cell><cell>88.5</cell><cell>2.2</cell><cell>2.6M</cell></row><row><cell>ResNet101 [86]</cell><cell>VAT [58]</cell><cell>90.3</cell><cell>90.8</cell><cell>3.8</cell><cell>3.3M</cell></row><row><cell></cell><cell>CATs</cell><cell>80.8</cell><cell>80.9</cell><cell>1.9</cell><cell>4.6M</cell></row><row><cell></cell><cell>CATs++</cell><cell>85.1</cell><cell>85.3</cell><cell>2.6</cell><cell>5.5M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11 :</head><label>11</label><figDesc>Visual localization on the Aachen day-night dataset<ref type="bibr" target="#b105">[106]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01538</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Foundations and Trends? in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Image alignment and stitching: A tutorial</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="978" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01088</idno>
		<title level="m">Visual attribute transfer through deep image analogy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic attribute matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gan-supervised dense visual alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05143</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spair-71k: A large-scale benchmark for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10543</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Neighbourhood consensus networks,&quot; in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dgc-net: Dense geometric correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glu-net: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gocor: Bringing globally optimized correspondence volumes into your neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Loftr: Detectorfree local feature matching with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00680</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Convolutional hough matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16831</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Superpoint: Selfsupervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parn: Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Guided semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Convolutional hough matching networks for robust and efficient visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05221</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-scale cost aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1590" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cats: Cost aggregation transformers for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2414" to="2422" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-scale matching networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep matching prior: Test-time optimization for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="9907" to="9917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning accurate dense correspondences and when to trust them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01710</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Patchmatch-based neighborhood consensus for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fragoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Aiatrack: Attention in attention for transformer visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Towards grand unification of object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07078</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00875</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Space-time correspondence as a contrastive random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cost aggregation with 4d convolutional swin transformer for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10866</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dual-resolution correspondence networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08844</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Instance-level image retrieval using reranking transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Correlation verification for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="5374" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Disk: Learning local features with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Tyszkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">COTR: Correspondence Transformer for Matching Across Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><forename type="middle">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03860</idno>
		<title level="m">Token pooling in vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Fastformer: Additive attention can be all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09084</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02034</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Sparse is enough in scaling transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaszczur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Nystr?mformer: A nyst?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</title>
		<meeting>the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">14138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Information</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Fewshot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="730" to="746" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII 16</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Few-shot segmentation with optimal transport matching and message flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08518</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Neural network studies, 1. comparison of overfitting and overtraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Luik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="826" to="833" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">D2-net: A trainable cnn for joint description and detection of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8092" to="8101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06195</idno>
		<title level="m">R2d2: repeatable and reliable detector and descriptor</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Darmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01526</idno>
		<title level="m">Ransac-flow: generic two-stage image alignment</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Benchmarking 6dof outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8601" to="8610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">From coarse to fine: Robust hierarchical localization at large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dymczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
