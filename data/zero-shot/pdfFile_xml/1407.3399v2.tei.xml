<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90024</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<email>yuille@stat.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90024</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Articulated pose estimation is one of the fundamental challenges in computer vision. Progress in this area can immediately be applied to important vision tasks such as human tracking <ref type="bibr" target="#b1">[2]</ref>, action recognition <ref type="bibr" target="#b24">[25]</ref> and video analysis.</p><p>Most work on pose estimation has been based on graphical model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>. The graph nodes represent the body parts (or joints), and the edges model the pairwise relationships between the parts. The score function, or energy, of the model contains unary terms at each node which capture the local appearance cues of the part, and pairwise terms defined at the edges which capture the local contextual relations between the parts. Recently, DeepPose <ref type="bibr" target="#b22">[23]</ref> advocates modeling pose in a holistic manner and captures the full context of all body parts in a Deep Convolutional Neural Network (DCNN) <ref type="bibr" target="#b11">[12]</ref> based regressor.</p><p>In this paper, we present a graphical model with image dependent pairwise relations (IDPRs). As illustrated in <ref type="figure">Figure 1</ref>, we can reliably predict the relative positions of a part's neighbors (as well as the presence of the part itself) by only observing the local image patch around it. So in our model the local image patches give input to both the unary and pairwise terms. This gives stronger pairwise terms because data independent relations are typically either too loose to be helpful or too strict to model highly variable poses.</p><p>Our approach requires us to have a method that can extract information about pairwise part relations, as well as part presence, from local image patches. We require this method to be efficient and to share features between different parts and part relationships. To do this, we train a DCNN to output <ref type="figure">Figure 1</ref>: Motivation. The local image measurements around a part, e.g., in an image patch, can reliably predict the relative positions of all its neighbors (as well as detect the part). Center Panel: The local image patch centered at the elbow can reliably predict the relative positions of the shoulder and wrist, and the local image patch centered at the wrist can reliably predict the relative position of the elbow. Left &amp; Right Panels: We define different types of pairwise spatial relationships (i.e., a mixture model) for each pair of neighboring parts. The Left Panel shows typical spatial relationships the elbow can have with its neighbors, i.e., the shoulder and wrist. The Right Panel shows typical spatial relationships the wrist can have with its neighbor, i.e., the elbow. estimates for the part presence and spatial relationships which are used in our unary and pairwise terms of our score function. The weight parameters of different terms in the model are trained using Structured Supported Vector Machine (S-SVM) <ref type="bibr" target="#b23">[24]</ref>. In summary, our model combines the representational flexibility of graphical models, including the ability to represent spatial relationships, with the data driven power of DCNNs.</p><p>We perform experiments on two standard pose estimation benchmarks: LSP dataset <ref type="bibr" target="#b9">[10]</ref> and FLIC dataset <ref type="bibr" target="#b19">[20]</ref>. Our method outperforms the state of the art methods by a significant margin on both datasets. We also do cross-dataset evaluation on Buffy dataset <ref type="bibr" target="#b6">[7]</ref> (without training on this dataset) and obtain strong results which shows the ability of our model to generalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Model</head><p>The Graphical Model and its Variables: We represent human pose by a graphical model G = (V, E) where the nodes V specify the positions of the parts (or joints) and the edges E indicates which parts are spatially related. For simplicity, we impose that the graph structure forms a K?node tree, where K = |V|. The positions of the parts are denoted by l, where l i = (x, y) specifies the pixel location of part i, for i ? {1, . . . , K}. For each edge in the graph (i, j) ? E, we specify a discrete set of spatial relationships indexed by t ij , which corresponds to a mixture of different spatial relationships (see <ref type="figure">Figure 1</ref>). We denote the set of spatial relationships by t = {t ij , t ji |(i, j) ? E}. The image is written as I. We will define a score function F (l, t|I) as follows as a sum of unary and pairwise terms.</p><p>Unary Terms: The unary terms give local evidence for part i ? V to lie at location l i and is based on the local image patch I(l i ). They are of form:</p><formula xml:id="formula_0">U (l i |I) = w i ?(i|I(l i ); ?),<label>(1)</label></formula><p>where ?(.|.; ?) is the (scalar-valued) appearance term with ? as its parameters (specified in the next section), and w i is a scalar weight parameter.</p><p>Image Dependent Pairwise Relational (IDPR) Terms: These IDPR terms capture our intuition that neighboring parts (i, j) ? E can roughly predict their relative spatial positions using only local information (see <ref type="figure">Figure 1</ref>). In our model, the relative positions of parts i and j are discretized into several types t ij ? {1, . . . , T ij } (i.e., a mixture of different relationships) with corresponding mean relative positions r tij ij plus small deformations which are modeled by the standard quadratic deformation term. More formally, the pairwise relational score of each edge (i, j) ? E is given by:</p><formula xml:id="formula_1">R(l i , l j , t ij , t ji |I) = w tij ij , ?(l j ? l i ? r tij ij ) + w ij ?(t ij |I(l i ); ?) + w tji ji , ?(l i ? l j ? r tji ji ) + w ji ?(t ji |I(l j ); ?) ,<label>(2)</label></formula><p>where ?(?l = [?x, ?y]) = [?x ?x 2 ?y ?y 2 ] are the standard quadratic deformation features, ?(.|.; ?) is the Image Dependent Pairwise Relational (IDPR) term with ? as its parameters (specified in the next section), and w tij ij , w ij , w tji ji , w ji are the weight parameters. The notation ., . specifies dot product and boldface indicates a vector.</p><p>The Full Score: The full score F (l, t|I) is a function of the part locations l, the pairwise relation types t, and the input image I. It is expressed as the sum of the unary and pairwise terms:</p><formula xml:id="formula_2">F (l, t|I) = i?V U (l i |I) + (i,j)?E R(l i , l j , t ij , t ji |I) + w 0 ,<label>(3)</label></formula><p>where w 0 is a scalar weight on constant 1 (i.e., the bias term).</p><p>The model consists of three sets of parameters: the mean relative positions r = {r tij ij , r tji ji |(i, j) ? E} of different pairwise relation types; the parameters ? of the appearance terms and IDPR terms; and the weight parameters w (i.e., w i , w tij ij , w ij , w tji ji , w ji and w 0 ). See Section 4 for the learning of these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Dependent Terms and DCNNs</head><p>The appearance terms and IDPR terms depend on the image patches. In other words, a local image patch I(l i ) not only gives evidence for the presence of a part i, but also about the relationship t ij between it and its neighbors j ? N(i), where j ? N(i) if, and only if, (i, j) ? E. This requires us to learn distribution for the state variables i, t ij conditioned on the image patches I(l i ). In order to specify this distribution we must define the state space more precisely, because the number of pairwise spatial relationships varies for different parts with different numbers of neighbors (see <ref type="figure">Figure 1</ref>), and we need also consider the possibility that the patch does not contain a part.</p><p>We define c to be the random variable which denotes which part is present c = i for i ? {1, ..., K} or c = 0 if no part is present (i.e., the background). We define m cN(c) to be the random variable that determines the spatial relation types of c and takes values in M cN(c) . If c = i has one neighbor j (e.g., the wrist), then M iN(i) = {1, . . . , T ij }. If c = i has two neighbors j and k (e.g., the elbow),</p><formula xml:id="formula_3">then M iN(i) = {1, . . . , T ij } ? {1, . . . , T ik }. If c = 0, then we define M 0N(0) = {0}.</formula><p>The full space is represented as:</p><formula xml:id="formula_4">S = ? K c=0 {c} ? M cN(c)<label>(4)</label></formula><p>The size of the space is |S| = K c=0 |M cN(c) |. Each element in this space corresponds to a part with all the types of its pairwise relationships, or the background.</p><p>We use DCNN <ref type="bibr" target="#b11">[12]</ref> to learn the conditional probability distribution p(c, m cN(c) |I(l i ); ?). DCNN is suitable for this task because it is very efficient and enables us to share features. See section 4 for more details.</p><p>We specify the appearance terms ?(.|.; ?) and IDPR terms ?(.|.; ?) in terms of p(c, m cN(c) |I(l i ); ?) by marginalization:</p><formula xml:id="formula_5">?(i|I(l i ); ?) = log(p(c = i|I(l i ); ?)) (5) ?(t ij |I(l i ); ?) = log(p(m ij = t ij |c = i, I(l i ); ?))<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relationship to other models</head><p>We now briefly discuss how our method relates to standard models.</p><p>Pictorial Structure: We recover pictorial structure models <ref type="bibr" target="#b5">[6]</ref> by only allowing one relationship type (i.e., T ij = 1). In this case, our IDPR term conveys no information. Our model reduces to standard unary and (image independent) pairwise terms. The only slight difference is that we use DCNN to learn the unary terms instead of using HOG filters.</p><p>Mixtures-of-parts: <ref type="bibr" target="#b26">[27]</ref> describes a model with a mixture of templates for each part, where each template is called a "type" of the part. The "type" of each part is defined by its relative position with respect to its parent. This can be obtained by restricting each part in our model to only predict the relative position of its parent (i.e., T ij = 1, if j is not parent of i). In this case, each part is associated with only one informative IDPR term, which can be merged with the appearance term of each part to define different "types" of part in <ref type="bibr" target="#b26">[27]</ref>. Also this method does not use DCNNs.</p><p>Conditional Random Fields (CRFs): Our model is also related to the conditional random field literature on data-dependent priors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. The data-dependent priors and unary terms are typically modeled separately in the CRFs. In this paper, we efficiently model all the image dependent terms (i.e. unary terms and IDPR terms) together in a single DCNN by exploiting the fact the local image measurements are reliable for predicting both the presence of a part and the pairwise relationships of a part with its neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inference</head><p>To detect the optimal configuration for each person, we search for the configurations of the locations l and types t that maximize the score function: (l * , t * ) = arg max l,t F (l, t|I). Since our relational graph is a tree, this can be done efficiently via dynamic programming.</p><p>Let K(i) be the set of children of part i in the graph (K(i) = ?, if part i is a leaf), and S i (l i |I) be maximum score of the subtree rooted at part i with part i located at l i . The maximum score of each subtree can be computed as follow:</p><formula xml:id="formula_6">S i (l i |I) = U (l i |I) + k?K(i) max l k ,t ik ,t ki (R(l i , l k , t ik , t ki |I) + S k (l k |I))<label>(7)</label></formula><p>Using Equation 7, we can recursively compute the overall best score of the model, and the optimal configuration of locations and types can be recovered by the standard backward pass of dynamic programming.</p><p>Computation: Since our pairwise term is a quadratic function of locations, l i and l j , the max operation over l k in Equation 7 can be accelerated by using the generalized distance transforms <ref type="bibr" target="#b5">[6]</ref>. The resulting approach is very efficient, taking O(T 2 LK) time once the image dependent terms are computed, where T is the number of relation types, L is the total number of locations, and K is the total number of parts in the model. This analysis assumes that all the pairwise spatial relationships have the same number of types, i.e.,</p><formula xml:id="formula_7">T ij = T ji = T, ?(i, j) ? E.</formula><p>The computation of the image dependent terms is also efficient. They are computed over all the locations by a single DCNN. Applying DCNN in a sliding fashion is inherently efficient, since the computations common to overlapping regions are naturally shared <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>Now we consider the problem of learning the model parameters from images with labeled part locations, which is the data available in most of the human pose datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>. We derive type labels t ij from part location annotations and adopt a supervised approach to learn the model.</p><p>Our model consists of three sets of parameters: the mean relative positions r of different pairwise relation types; the parameters ? of the image dependent terms; and the weight parameters w. They are learnt separately by the K-means algorithm for r, DCNN for ?, and S-SVM for w.</p><p>Mean Relative Positions and Type Labels: Given the labeled positive images {(I n , l n )} N n=1 , let d ij be the relative position from part i to its neighbor j. We cluster the relative positions over the training set {d n ij } N n=1 to get T ij clusters (in the experiments T ij = 11 for all pairwise relations). Each cluster corresponds to a set of instances of part i that share similar spatial relationship with its neighbor part j. Thus we define each cluster as a pairwise relation type t ij from part i to j in our model, and use the center of each cluster as the mean relative position r tij ij associated with each type. In this way, the mean relative positions of different pairwise relation types are learnt, and the type label t n ij for each training instance is derived based on its cluster index. We use K-means in our experiments by setting K = T ij to do the clustering.</p><p>Parameters of Image Dependent Terms: After deriving type labels, each local image patch I(l n ) centered at an annotated part location is labeled with category label c n ? {1, . . . , K}, that indicates which part is present, and also the type labels m n c n N(c n ) that indicate its relation types with all its neighbors. In this way, we get a set of labelled patches {I(l n ), c n , m n c n N(c n ) } KN n=1 from positive images (each positive image provides K part patches), and also a set of background patches {I(l n ), 0, 0} sampled from negative images.</p><p>Given the labelled part patches and background patches, we train a multi-class DCNN classifier by standard stochastic gradient descent using softmax loss. The DCNN consists of five convolutional layers, 2 max-pooling layers and three fully-connected layers with a final |S| dimensions softmax output, which is defined as our conditional probability distribution, i.e., p(c, m cN(c) |I(l i ); ?). The architecture of our network is summarized in <ref type="figure">Figure 2</ref>.</p><p>Weight Parameters: Each pose in the positive image is now labeled with annotated part locations and derived type labels: (I n , l n , t n ). We use S-SVM to learn the weight parameters w. The structure prediction problem is simplified by using 0 ? 1 loss, that is all the training examples either have all dimensions of its labels correct or all dimensions of its labels wrong. We denote the former ones as pos examples, and the later ones as neg examples. Since the full score function (Equation 3) is linear in the weight parameters w, we write the optimization function as:</p><formula xml:id="formula_8">min w 1 2 w, w + C n max(0, 1 ? y n w, ?(I n , l n , t n ) ),<label>(8)</label></formula><p>where y n ? {1, ?1}, and ?(I n , l n , t n ) is a sparse feature vector representing the n-th example and is the concatenation of the image dependent terms (calculated from the learnt DCNN), spatial deformation features, and constant 1. Here y n = 1 if n ? pos, and y n = ?1 if n ? neg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>This section introduces the datasets, clarifies the evaluation metrics, describes our experimental setup, presents comparative evaluation results and gives diagnostic experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>We perform our experiments on two publicly available human pose estimation benchmarks: (i) the "Leeds Sports Poses" (LSP) dataset <ref type="bibr" target="#b9">[10]</ref>, that contains 1000 training and 1000 testing images from sport activities with annotated full-body human poses; (ii) the "Frames Labeled In Cinema" (FLIC) dataset <ref type="bibr" target="#b19">[20]</ref> that contains 3987 training and 1016 testing images from Hollywood movies with annotated upper-body human poses. We follow previous work and use the observer-centric annotations on both benchmarks. To train our models, we also use the negative training images from the INRIAPerson dataset <ref type="bibr" target="#b2">[3]</ref> (These images do not contain people).</p><p>We use the most popular evaluation metrics to allow comparison with previous work. Percentage of Correct Parts (PCP) is the standard evaluation metric on several benchmarks including the LSP dataset. However, as discussed in <ref type="bibr" target="#b26">[27]</ref>, there are several alternative interpretations of PCP that can lead to severely different results. In our paper, we use the stricter version unless otherwise stated, that is we evaluate only a single highest-scoring estimation result for one test image and a body part is considered as correct if both of its segment endpoints (joints) lie within 50% of the length of the ground-truth annotated endpoints (Each test image on the LSP dataset contains only one annotated person). We refer to this version of PCP as strict PCP.</p><p>On the FLIC dataset, we use both strict PCP and the evaluation metric specified with it <ref type="bibr" target="#b19">[20]</ref>: Percentage of Detected Joints (PDJ). PDJ measures the performance using a curve of the percentage of correctly localized joints by varying localization precision threshold. The localization precision threshold is normalized by the scale (defined as distance between left shoulder and right hip) of each ground truth pose to make it scale invariant. There are multiple people in the FLIC images, so each  <ref type="figure">Figure 2</ref>: Architectures of our DCNNs. The size of input patch is 36 ? 36 pixels on the LSP dataset, and 54 ? 54 pixels on the FLIC dataset. The DCNNs consist of five convolutional layers, 2 max-pooling layers and three fully-connected (dense) layers with a final |S| dimensions output. We use dropout, local response normalization (norm) and overlapping pooling (pool) described in <ref type="bibr" target="#b11">[12]</ref>.</p><p>ground truth person is also annotated with a torso detection box. During evaluation, we return a single highest-scoring estimation result for each ground truth person by restricting our neck part to be localized inside a window defined by the provided torso box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation detail</head><p>Data Augmentation: Our DCNN has millions of parameters, while only several thousand of training images are available. In order to reduce overfitting, we augment the training data by rotating the positive training images through 360 ? . These images are also horizontally flipped to double the training images. This increases the number of training examples of body parts with different spatial relationships with its neighbors (See the elbows along the diagonal of the Left Panel in <ref type="figure">Figure 1</ref>). We hold out random positive images as a validation set for the DCNN training. Also the weight parameters w are trained on this held out set to reduce overfitting to training data.</p><p>Note that our DCNN is trained using local part patches and background patches instead of the whole images. This naturally increases the number of training examples by a factor of K (the number of parts). Although the number of dimensions of the DCNN final output also increases linearly with the number of parts, the number of parameters only slightly increase in the last fully-connected layer. This is because most of the parameters are shared between different parts, and thus we can benefit from larger K by having more training examples per parameter. In our experiments, we increase K by adding the midway points between annotated parts, which results in 26 parts on the LSP dataset and 18 parts on the FLIC dataset. Covering a person by more parts also reduces the distance between neighboring parts, which is good for modeling foreshortening <ref type="bibr" target="#b26">[27]</ref>.</p><p>Graph Structure: We define a full-body graph structure for the LSP dataset, and a upper-body graph structure for the FLIC dataset respectively. The graph connects the annotated parts and their midways points to form a tree (See the skeletons in <ref type="figure">Figure 5</ref> for clarification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings:</head><p>We use the same number of types for all pairs of neighbors for simplicity. We set it as 11 on all datasets (i.e., T ij = T ji = 11, ?(i, j) ? E), which results in 11 spatial relation types for the parts with one neighbor (e.g., the wrist), 11 2 spatial relation types for the parts with two neighbors (e.g., the elbow), and so forth (recall <ref type="figure">Figure 1)</ref>. The patch size of each part is set as 36 ? 36 pixels on the LSP dataset, and 54 ? 54 pixels on the FLIC dataset, as the FLIC images are of higher resolution. We use similar DCNN architectures on both datasets, which differ in the first layer because of different input patch sizes. <ref type="figure">Figure 2</ref> visualizes the architectures we used. We use the Caffe <ref type="bibr" target="#b8">[9]</ref> implementation of DCNN in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Benchmark results</head><p>We show strict PCP results on the LSP dataset in <ref type="table" target="#tab_0">Table 1</ref>, and on the FLIC dataset in <ref type="table" target="#tab_1">Table 2</ref>. We also show PDJ results on the FLIC dataset in <ref type="figure" target="#fig_1">Figure 3</ref>. As is shown, our method outperforms state of the art methods by a significant margin on both datasets (see the captions for detailed analysis). <ref type="figure">Figure 5</ref> shows some estimation examples on the LSP and FLIC datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Diagnostic Experiments</head><p>We perform diagnostic experiments to show the cross-dataset generalization ability of our model, and better understand the influence of each term in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-dataset Generalization:</head><p>We directly apply the trained model on the FLIC dataset to the official test set of Buffy dataset <ref type="bibr" target="#b6">[7]</ref> (i.e., no training on the Buffy dataset), which also contains upper-body human poses. The Buffy test set includes a subset of people whose upper-body can be detected. We get the newest detection windows from <ref type="bibr" target="#b4">[5]</ref>, and compare our results to previously published work on this subset.</p><p>Most previous work was evaluated with the official evaluation toolkit of Buffy, which uses a less strict PCP implementation 1 . We refer to this version of PCP as Buffy PCP and report it along with the strict PCP in <ref type="table" target="#tab_3">Table 3</ref>. We also show the PDJ curves in <ref type="figure">Figure 4</ref>. As is shown by both criterions, our method significantly outperforms the state of the arts, which shows the good generalization ability of our method. Also note that both DeepPose <ref type="bibr" target="#b22">[23]</ref> and our method are trained on the FLIC dataset. Compared with <ref type="figure" target="#fig_1">Figure 3</ref>, the margin between our method and DeepPose significantly increases in <ref type="figure">Figure 4</ref>, which implies that our model generalizes better to the Buffy dataset.   Terms Analysis: We design two experiments to better understand the influence of each term in our model. In the first experiment, we use only the unary terms and thus all the parts are localized independently. In the second experiment, we replace the IDPR terms with image independent priors (i.e., in Equation 2, w ij ?(t ij |I(l i ); ?) and w ji ?(t ji |I(l j ); ?) are replaced with scalar prior terms b tij ij and b tji ji respectively), and retrain the weight parameters along with the new prior terms. In this case, our pairwise relational terms do not depend on the image, but instead is a mixture of Gaussian deformations with image independent biases. We refer to the first experiment as Unary-Only and the second one as No-IDPRs, short for No IDPR terms. The experiments are done on the LSP dataset using identical appearance terms for fair comparison. We show strict PCP results in <ref type="table" target="#tab_4">Table 4</ref>. As is shown, all terms in our model significantly improve the performance (see the caption for detail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model over types of spatial relationships. We use DCNNs to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training. <ref type="figure">Figure 5</ref>: Results on the LSP and FLIC datasets. We show the part localization results along with the graph skeleton we used in the model. The last row shows some failure cases, which are typically due to large foreshortening, occlusions and distractions from clothing or overlapping people.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of PDJ curves of elbows and wrists on the FLIC dataset. The legend shows the PDJ numbers at the threshold of 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of strict PCP results on the LSP dataset. Our method improves on all parts by a significant margin, and outperforms the best previously published result<ref type="bibr" target="#b15">[16]</ref> by 5.8% on average. Note that DeepPose uses Person-Centric annotations and is trained with an extra 10,000 images.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Torso</cell><cell>Head</cell><cell>U.arms</cell><cell>L.arms</cell><cell>U.legs</cell><cell>L.legs</cell><cell>Mean</cell></row><row><cell>Ours</cell><cell></cell><cell>92.7</cell><cell>87.8</cell><cell>69.2</cell><cell>55.4</cell><cell>82.9</cell><cell>77.0</cell><cell>75.0</cell></row><row><cell cols="2">Pishchulin et al. [16]</cell><cell>88.7</cell><cell>85.6</cell><cell>61.5</cell><cell>44.9</cell><cell>78.8</cell><cell>73.4</cell><cell>69.2</cell></row><row><cell>Ouyang et al. [14]</cell><cell></cell><cell>85.8</cell><cell>83.1</cell><cell>63.3</cell><cell>46.6</cell><cell>76.5</cell><cell>72.2</cell><cell>68.6</cell></row><row><cell>DeepPose* [23]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>56</cell><cell>38</cell><cell>77</cell><cell>71</cell><cell>-</cell></row><row><cell cols="2">Pishchulin et al. [15]</cell><cell>87.5</cell><cell>78.1</cell><cell>54.2</cell><cell>33.9</cell><cell>75.7</cell><cell>68.0</cell><cell>62.9</cell></row><row><cell cols="2">Eichner&amp;Ferrari [4]</cell><cell>86.2</cell><cell>80.1</cell><cell>56.5</cell><cell>37.4</cell><cell>74.3</cell><cell>69.3</cell><cell>64.3</cell></row><row><cell cols="2">Yang&amp;Ramanan [26]</cell><cell>84.1</cell><cell>77.1</cell><cell>52.5</cell><cell>35.9</cell><cell>69.5</cell><cell>65.6</cell><cell>60.8</cell></row><row><cell>Method</cell><cell cols="3">U.arms L.arms Mean</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>97.0</cell><cell>86.8</cell><cell>91.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MODEC[20]</cell><cell>84.4</cell><cell>52.1</cell><cell>68.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of strict PCP results on the FLIC dataset. Our method significantly outperforms MODEC<ref type="bibr" target="#b19">[20]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Elbows</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Wrists</cell><cell></cell><cell></cell></row><row><cell>Percentage of Detected Joints (PDJ)</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell>0.05 MODEC: 75.5% DeepPose: 91.0% Ours: 94.9%</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>Percentage of Detected Joints (PDJ)</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell>0.05 MODEC: 57.9% DeepPose: 80.9% Ours: 92.0%</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell cols="3">Normalized Precision Threshold</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Normalized Precision Threshold</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Cross-dataset PCP results on Buffy test subset. The PCP numbers are Buffy PCP unless otherwise stated. Note that our method is trained on the FLIC dataset. Cross-dataset PDJ curves on Buffy test subset. The legend shows the PDJ numbers at the threshold of 0.2. Note that both our method and DeepPose<ref type="bibr" target="#b22">[23]</ref> are trained on the FLIC dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Elbows</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Wrists</cell><cell></cell><cell></cell></row><row><cell>Percentage of Detected Joints (PDJ)</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell>0.05 Yang: 80.4% MODEC: 77.0% DeepPose*: 83.4% Ours*: 93.2%</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>Percentage of Detected Joints (PDJ)</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell>0.05 Yang: 57.4% MODEC: 58.8% DeepPose*: 64.6% Ours*: 89.4%</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell cols="3">Normalized Precision Threshold</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Normalized Precision Threshold</cell><cell></cell></row><row><cell cols="3">Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Diagnostic term analysis strict PCP results on the LSP dataset. The unary term alone is still not powerful enough to get good results, even though it's trained by a DCNN classifier. No-IDPRs method, whose pairwise terms are not dependent on the image (see Terms Analysis in Section 5.4), can get comparable performance with the state-of-the-art, and adding IDPR terms significantly boost our final performance to 75.0%.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A part is considered correctly localized if the average distance between its endpoints (joints) and groundtruth is less than 50% of the length of the ground-truth annotated endpoints.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This research has been supported by grants ONR MURI N000014-10-1-0933, ONR N00014-12-1-0883 and ARO 62250-CS. The GPUs used in this research were generously donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive occlusion state estimation for human pose tracking under self-occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Appearance sharing for collective human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2d articulated human pose estimation and retrieval in (almost) unconstrained still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using linking features in learning non-parametric part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to parse images of articulated bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cascaded models for articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
