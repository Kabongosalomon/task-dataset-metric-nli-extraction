<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Delving into Localization Errors for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
							<email>xinzhu.ma@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinmin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<email>danxu@cse.ust.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
							<email>d.zhou@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
							<email>yishuai@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
							<email>hjli@dlut.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Delving into Localization Errors for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D bounding boxes from monocular images is an essential component in autonomous driving, while accurate 3D object detection from this kind of data is very challenging. In this work, by intensive diagnosis experiments, we quantify the impact introduced by each sub-task and found the 'localization error' is the vital factor in restricting monocular 3D detection. Besides, we also investigate the underlying reasons behind localization errors, analyze the issues they might bring, and propose three strategies. First, we revisit the misalignment between the center of the 2D bounding box and the projected center of the 3D object, which is a vital factor leading to low localization accuracy. Second, we observe that accurately localizing distant objects with existing technologies is almost impossible, while those samples will mislead the learned network. To this end, we propose to remove such samples from the training set for improving the overall performance of the detector. Lastly, we also propose a novel 3D IoU oriented loss for the size estimation of the object, which is not affected by 'localization error'. We conduct extensive experiments on the KITTI dataset, where the proposed method achieves real-time detection and outperforms previous methods by a large margin. The code will be made available at: https: //github.com/xinzhuma/monodle. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f t a 3 5 n 2 Y r 4 6 F A d 0 z m 0 M y c H A M 9 w Y = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I V d Q b Q R g z a W C Z g L J E u Y n Z x N x s x e m J l V</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Remarkable progress has been achieved in 3D detection, especially for LiDAR/stereo-based approaches <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38]</ref>, along with the advances in deep neural networks. In contrast, the accuracy of 3D detection from only monocular images <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12]</ref> is obviously lower than that from LiDAR or stereo. In this work, we aim to quantitatively identify the problem and propose our solutions.</p><p>To investigate and quantify the underlying factors that restrict the performance of monocular 3D object detection, we conduct intensive diagnostic experiments for this task, inspired by the error identifying methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref> commonly used in the 2D detection scope. Specifically, we build our baseline model (see Section 3.2 for details) based on CenterNet <ref type="bibr" target="#b43">[44]</ref> and progressively replace predicted items with their ground-truth values. To better analyze the error patterns, we evaluate the results in a range-wise manner and show the summary of those experiments in <ref type="figure" target="#fig_0">Figure 1</ref>. Based on our investigation, we have the following three observations and corresponding designs. Observation 1: The most striking feature in <ref type="figure" target="#fig_0">Figure 1</ref> is the leap in performance when using ground-truth location, reaching a level similar to the state-of-the-art LiDAR-based methods, suggesting the localization error is the key factor in restricting monocular 3D detection. Furthermore, except for depth estimation, detecting the projected center of the 3D object also plays an important role in restoring the 3D position of the object. To this end, we revisit the misalignment between the center of the 2D bounding box and the projected center of the 3D object. Besides, we also confirm the necessity of keeping 2D detection related branches in arXiv:2103.16237v1 [cs.CV] 30 Mar 2021 monocular 3D detector. In this way, 2D detection is used as the correlated auxiliary task to help learning the features shared with 3D detection, which is different from the existing work in <ref type="bibr" target="#b23">[24]</ref> that discards 2D detection.</p><p>Observation 2: An apparent trend reflected in <ref type="figure" target="#fig_0">Figure 1</ref> is that the detection accuracy significantly decreases with respect to the distance (the low performance of very close range objects will be discussed in supplementary materials). More importantly, all the models cannot output any true positive samples beyond a certain distance. We found that it is almost impossible to detect distant objects accurately with existing technologies due to the inevitable localization errors (see Section 4.4 for details). In this case, whether it is beneficial to add these samples into the training set becomes a question. In fact, there is a clear domain gap between 'bad' samples and 'easy-to-detect' samples and forcing the network to learn from those samples will reduce its representative ability for the others, which will thus impair the overall performance. Based on the observation above, we propose two schemes. The first scheme removes distant samples from the training set and the second scheme reduces the training loss weights of these samples.</p><p>Observation 3: We found that, except for localization error, there are also some other vital factors, such as dimension estimation, restricting monocular 3D detection (there is still 27.4% room for improvements even we use the groundtruth location). Existing methods in this scope tend to optimize each component of the 3D bounding box independently, and the studies in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> confirm the effectiveness of this strategy. However, the failure to consider the contribution of each loss item to the final metric (i.e. 3D IoU) may lead to sub-optimal optimization. To alleviate this problem, we propose an IoU oriented loss for 3D size estimation. The new IoU oriented loss dynamically adjust the loss weight for each side in sample level according its contribution rate to the 3D IoU.</p><p>In summary, the key contributions of this paper are as follows: First, we conduct intensive diagnostic experiments for monocular 3D detection. In addition to finding that the 'localization error' is the main problem restricting monocular 3D detection, we also quantify the overall impact of each sub-task. Second, we investigate the underlying reasons behind localization error, analyze the issues it might bring. Accordingly, we propose three novel strategies operating on annotations, training samples, and optimization losses to alleviate problems caused by localization error for boosting the detection.</p><p>Experimental results show the effectiveness of the proposed strategies. In particular, compared with existing bestperforming monocular 3D object detection approaches, the proposed method achieves at least 1.6 points AP 40 improvements on the bird's view detection and 3D object detection in the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Standard monocular 3D detection. Here we briefly review the 'standard' monocular 3D detection approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44]</ref> only use the RGB images, annotations and camera calibrations provided by KITTI dataset. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref> try to improve the representation ability of the models by introducing novel geometric constraints. OFTNet <ref type="bibr" target="#b32">[33]</ref> presents an orthographic feature transform to map image-based features into an orthographic 3D space. MonoDIS <ref type="bibr" target="#b35">[36]</ref> disentangles the loss for 2D/3D detection and jointly trains these two tasks in an end-to-end manner. M3D-RPN <ref type="bibr" target="#b1">[2]</ref> extends the region proposal network (RPN) with 3D box parameters. These works are orthogonal to our analysis to localization error and the proposed strategies for handling it. Monocular 3D detection using additional data. To better estimate the 3D bounding boxes, many methods are proposed for effectively using additional data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref>. Specifically, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b26">27]</ref> use the CAD models as shape templates to get better object geometry. Deep MANTA <ref type="bibr" target="#b4">[5]</ref>, which takes 3D detection as a key-points detection task, uses more detailed annotated locations of keypoints, e.g. wheels, as training labels. Besides, <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref> estimate the depth maps from off-the-shelf depth estimators <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref> trained from larger datasets, and use them to augment the input RGB images. In addition, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26]</ref> propose to transform the estimated depth maps to pseudo-LiDAR representation, before applying existing LiDAR-based 3D detection designs, and achieve promising performance on KITTI benchmark. PatchNet <ref type="bibr" target="#b24">[25]</ref> analyzes the underlying mechanism behind pseudo-LiDAR representation and proposes its corresponding image representation based implementation. Recently, Kinematic3D <ref type="bibr" target="#b2">[3]</ref> propose to use 3D Kalman filter to capture the temporal cues from monocular videos. In contrast, our method does not use any extra data or annotation, and can still achieve better or competitive performance. Misalignment between the definitions of object's center. To recover the 3D object position, there are two groups of methods. The first group <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref> use 2D bounding box to obtain 3D position. In particular, CenterNet <ref type="bibr" target="#b43">[44]</ref> regards the center of the 2D bounding box as the projected 3D position in the image plane and back-project it to 3D space with the help of estimated depth and camera parameters. However, generally speaking, the center of the 2D box and the center of 3D box are not the same. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> regress an offset to compensate for the difference between them. As the second group, SMOKE <ref type="bibr" target="#b23">[24]</ref> removes the 2D detection and directly estimate 3D position using projected 3D center. This work considers the 2D related sub-tasks are redundant because 2D bounding boxes can be generated from 3D detection results. In this work, we revisit this problem and confirm that replacing the 2D center by the projected 3D   center can improve the localization accuracy. Besides, we also find that 2D detection is necessary, because it helps to learn shared features for 3D detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c G X / n p d V s A B 4 U X A = " &gt; A A A B 7 n i c b Z D L S g M x F I Y z 9 V b H W 9 W l m 2 A R X J U Z Q X Q j F t 2 4 r G A v 0 A 7 l T J p p Q z O Z k G S E M v Q h 3 L h Q x I U b 3 8 S 9 G / F t z L R d a O s P g Y / / P 4 e c c 0 L J m T a e 9 + 0 U l p Z X V t e K 6 + 7 G 5 t b 2 T m l 3 r 6 G T V B F a J w l P V C s E T T k T t G 6 Y 4 b Q l F Y U 4 5 L Q Z D q / z v H l P l W a J u D M j S Y M Y + o J F j I C x V r M D X A 7 A 7 Z b K X s W b C C + C P 4 P y 5 Y d 7 I d + + 3 F q 3 9 N n p J S S N q T C E g 9 Z t 3 5 M m y E A Z R j g d u 5 1 U U w l k C H 3 a t i g g p j r I J u O O 8 Z F 1 e j h K l H 3 C 4 I n 7 u y O D W O t R H N r K G M x A z 2 e 5 + V / W T k 1 0 H m R M y N R Q Q a Y f R S n H J s H 5 7 r j H F C W G j y w A U c z O i s k A F B B j L 5 Q f w Z 9 f e R E a J x X / t O L d e u X q F Z q q i A 7 Q I T p G P j p D V X S D a q i O C B q i B / S E n h 3 p P D o v z u u 0 t O D M e v b R H z n v P y</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G m I C J O a 1 H / Z u z X H q g 2 T M k 7 O 4 h p K K A r W Y c R s E 6 q N r s g B L R y e b / g T 4 A X S T A j + e L R q P z 9 e D w q t X K f z b Y i i a D S E g 7 G N A I / t m E K 2 j L C 6 T D b T A y N g f S h S x u O S h D U h O n k 2 i E + d U o b d 5 R 2 J S 2 e q L 8 n U h D G D E T k O g X Y n p n 3 x u J / X i O x n a s w Z T J O L J V k u q i T c G w V H r + O 2 0 x T Y v n A E S C a u V s x 6 Y E G Y l 1 A W R d C M P / y I q m e F 4 K L g l 9 2 a V y j K T L o E J 2 g M x S g S 1 R E t 6 i E K o i g e / S E X t C r p 7 x n 7 8 1 7 n 7 Y u e b O Z A / Q H 3 s c P d T y S w A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t u 7 G J m p g 3 G d h o C d w E q e 5 Z 9 / D O f Q = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M K u I F q G a G G Z g H l A s o T Z y W w y Z n Z m m Z l V w p J / s L F Q x N b K v / A L 7 G z 8 F i e P Q h M P X D i c c y / 3 3 h P E n</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Given are RGB images and the corresponding camera parameters, our goal is to classify and localize the objects of interest in 3D space. Each object is represented by its category, 2D bounding box B 2D , and 3D bounding box B 3D . Specifically, B 2D is represented by its center</p><formula xml:id="formula_0">c i = [x , y ] 2D and size [h , w ] 2D in the image plane, while B 3D is defined by its center [x, y, z] 3D , size [h, w, l] 3D</formula><p>and heading angle ? in the 3D world space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline Model</head><p>Architecture. We build our baseline model based on the anchor-free one-stage detector CenterNet <ref type="bibr" target="#b43">[44]</ref>. Specifically, we use standard DLA-34 <ref type="bibr" target="#b41">[42]</ref> as our backbone for a better speed-accuracy trade-off. On top of this, seven lightweight heads (implemented by one 3 ? 3 conv layer and one 1 ? 1 conv layer) are used for 2D detection and 3D detection. More design choices and implementation details can be found in the supplementary material. 2D detection. For 2D detection task, following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44]</ref>, the proposed model outputs a heatmap to indicate the classification score and the coarse center c = (u, v) of the object. In existing methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>, c is supervised by the groundtruth 2D bounding box center. Another branch predict the offset o i = (?u i , ?v i ) between the coarse center and the real center of 2D bounding box, and we can get the final 2D box center location c i = c + o i . Finally, we use another branch to estimate the size [w , h ] 2D of 2D bounding box. 3D detection. As for 3D detection, a branch is used for predicting the offset o w = (?u w , ?v w ) between the coarse center c and the center of projected 3D bounding box 3D world space by:</p><formula xml:id="formula_1">c w = [x w y w ] T = c + o w .</formula><formula xml:id="formula_2">? ? x y z ? ? 3D = K ?1 c w ? z z = K ?1 ? ? ? x w ? z y w ? z z ? ? 2D ,<label>(1)</label></formula><p>where z is the output of depth branch. Finally, the last two branches are used to predict the 3D size [h, w, l] 3D and orientation ?, respectively.</p><p>Losses. There are seven loss terms in total, one for foreground/background sample classification, two (center and size) for 2D detection, and four (center, depth, size, and heading angle) for 3D detection. We adopt the modified Focal Loss used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref> for classification sub-task. We use L1 Loss without any anchor for center and size regression in 2D detection task. For the 3D detection task, uncertainty modeling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref> (see Section B in the supplementary for details) is used for depth estimation; L1 loss is used for 3D center refinement; and multi-bin loss <ref type="bibr" target="#b28">[29]</ref> (we consider 12 non-overlap equal bins) is used for heading angle estimation. Lastly, for 3D size estimation, we use L1 loss in baseline (without anchor), and the proposed IoU loss in our model. The weights for all loss items are set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Error Analysis</head><p>In this section, we explore what restricts the performance of monocular 3D detection. Inspired by CenterNet <ref type="bibr" target="#b43">[44]</ref> and CornerNet <ref type="bibr" target="#b20">[21]</ref> in the 2D detection field, we conduct an error analysis for different prediction items on KITTI validation set via replacing each predictions with ground truth value and evaluating the performance. Specifically, we replace each output head with its ground truth according to the practice of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, if we replace projected 3D center c w predicted from baseline model with its ground-truth, the accuracy is improved from 11.12% to 18.97%. On the other hand, depth can improve the accuracy to 38.01%. If we consider both depth and projected center, i.e. replacing the predicted 3D locations [x, y, z] 3D with ground-truth results, then the most obvious improvement is  Suppose that all other quantities are correct and the localization error is aligned with the length l (resulting in the maximum tolerance), the IoU can be computed by:</p><formula xml:id="formula_3">IoU = 3.53 ? ? loc 3.53 + ? loc ,<label>(2)</label></formula><p>where ? loc represents the localization error. According to the official setting, the IoU threshold should be set to 0.7, thus the theoretically acceptable maximum error is 0.62m. However, an error of only 4-8 pixels in the image (1-2 pixel in 4? down sampling feature map) will cause the object at 60 meters cannot be detected correctly. Coupled with the errors accumulated by other tasks such as depth estimation ( <ref type="figure" target="#fig_6">Figure 3</ref> shows the errors of depth estimation), it becomes an almost impossible task to accurately estimate the 3D bounding box of distant objects from a single monocular image, unless the depth estimation is accurate enough (not achieved to date).</p><p>To better show the importance of center localization, we show the localization error in 3D space caused by shifting the center in image plane in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Revisiting Center Detection</head><p>Our design for center detection. For estimating the coarse center c, our design is simple. In particular, we 1) use the projected 3D center c w as the ground-truth for the branch estimating coarse center c and 2) force our model to learn features from 2D detection simultaneously. This simple design is from our analysis below. Analysis 1. As shown in <ref type="figure" target="#fig_7">Figure 4</ref>, there is a misalignment between the 2D bounding box center c i and the projected   The key problem here is what should be the supervision for the coarse center c. Some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> choose to use 2D box center c i as its label, which is not related to the 3D object center, making the estimation of the coarse center not aware of the 3D geometry of the object. Here we choose to adopt the projected 3D center c w as the ground-truth for the coarse center c. This helps the branch for estimating the coarse center aware of 3D geometry and more related to the task of estimating 3D object center, which is the key of localization problem (see Section E in supplementary materials for visualizations). Analysis 2. Note that SMOKE <ref type="bibr" target="#b23">[24]</ref> also use the projected 3D center c w as the label of the coarse center c. However, they discard 2D detection related branches while we preserve them. In our design, the coarse center c supervised by the projected 3D center c w is also used for estimating the 2D bounding box center c i . With our design, we force a 2D detection branch to estimate an offset o i = c i ? c between the real 2D center and the coarse 2D center. This makes our model aware of the geometric information of the object.</p><p>Besides, another branch is used to estimate the size of the 2D bounding box so that the shared features can learn some cues that benefit to depth estimation due to the perspective projection. In this way, the 2D detection serves as an auxiliary task that helps to learn better 3D aware features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Samples</head><p>Different from <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref> which force network focus on the 'hard' samples, we argue that ignoring some extremely 'hard' cases can improve the overall performance for the monocular 3D detection task. Both the results shown in <ref type="figure" target="#fig_0">Figure 1</ref> and the analysis conducted in Section 4.4 illustrate there is a strong relationship between the distance of the object and the difficulty of detecting it. According to this, two schemes are proposed on how to generate the objectlevel training weight w i for sample i. Scheme 1, hard coding. This scheme discard all samples over a certain distance:</p><formula xml:id="formula_4">w i = 1 if d i ? s 0 if d i &gt; s<label>(3)</label></formula><p>where d i denotes the depth of sample i, and s is the threshold of depth which is set to 60 meters in our implementation. In this way, the samples with depth larger than s will not be used in the training phase. Scheme 2, soft coding. The other one is soft encoding, and we generate it using a reverse sigmoid-like function:</p><formula xml:id="formula_5">w i = 1 1 + e (di?c)/T ,<label>(4)</label></formula><p>where c and T are the hyper-parameters to adjust the center of symmetry and bending degree, respectively. When c = s and T ? 0, it is equivalent to the hard encoding scheme. When T ? ?, it is equivalent to using the same weight for all samples. By default, c and T are set to 60 and 1, and the empirical experiments in Section 4 find that scheme 1 and scheme 2 are both effective and have similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">IoU Oriented Optimization</head><p>Recently, some LiDAR based 3D detectors <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> applied the IoU oriented optimization <ref type="bibr" target="#b31">[32]</ref>. However, determining the 3D center of object is an very challenging task for monocular 3D detection, and the localization error often reaches several meters (see <ref type="bibr">Section 4.4)</ref>. In this case, localization related sub-tasks (such as depth estimation) will overwhelm others (such as 3D size estimation), if we apply IoU based loss function directly. Moreover, depth estimation from monocular image itself an ill-posed problem, and this kind of contradiction will make the training process collapse. Disentangling each loss item and optimize them independently is a another choice <ref type="bibr" target="#b35">[36]</ref>, but this ignores the correlation of each component to the final result. To alleviate this problem, we propose a IoU oriented optimization for 3D size estimation. Specifically, suppose all prediction items except the 3D size s = [h, w, l] 3D are completely correct, then we can get (details for deriving can be found in supplementary materials):</p><formula xml:id="formula_6">?IoU ?h : ?IoU ?w : ?IoU ?l ? 1 h : 1 w : 1 l .<label>(5)</label></formula><p>Accordingly, we can adjust the weight of each side by its partial derivative w.r.t. IoU (in magnitude), and the loss function of the 3D size estimation can be modified to:</p><formula xml:id="formula_7">L size = || (s ? s * ) s || 1 ,<label>(6)</label></formula><p>where || ? || 1 represent the L 1 norm. Note that, compared with the standard 3D size loss L size = ||s?s * || 1 used in the baseline model, our new loss's magnitude is changed. To compensate it, we compute L size once more, and dynamically generate the compensate weight w s = |L size /L size |, so that the mean value of the final loss function w s ? L size is equal to the standard one. By this way, the proposed loss can be regard as a re-distribution of the standard L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Implementation</head><p>Training. We train our model on two GTX 1080Ti GPUs with a batch size of 16 in an end-to-end manner for 140 epochs. We use Adam optimizer with initial learning rate 1.25e ?3 , and decay it by ten times at 90 and 120 epochs. The weight decay is set to 1e ?5 and the warmup strategy is also used for the first 5 epochs. To avoid over-fitting, we adopt the random cropping/scaling (for 2D detection only) and random horizontal flipping. Under this setting, it takes around 9 hours for whole training process.</p><p>Inference. During the inference phase, we obtain the prediction results from the parallel decoders. To decoding the results, similar to <ref type="bibr" target="#b43">[44]</ref>, we conduct the efficient nonmaxima suppression (NMS) on center detection results using a 3 ? 3 max pooling kernel. Then, we recover 2D/3D bounding boxes according to encoding strategy introduced in Section 3.2 and use the score of center detection as the confidence of predicted results. Finally, we discard predictions with confidence less than 0.2.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Dataset. We evaluate our method on the challenging KITTI dataset <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which provides 7,481 images for training and 7,518 images for testing. Since the ground truth for the test set is not available and the access to the test server is limited, we follow the protocol of prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> to divide the training data into a training set (3,712 images) and a validation set (3,769 images). We conduct ablation studies based on this split and also report final results which trained on all 7,481 images and tested by KITTI official server.</p><p>Metrics. The KITTI dataset provides many widely used benchmarks for autonomous driving scenarios, including 3D detection, bird's eye view (BEV) detection, and average orientation similarity (AOS). We report the Average Precision with 40 recall positions (AP 40 ) <ref type="bibr" target="#b35">[36]</ref> under three difficultly settings (easy, moderate, and hard) for those tasks. We mainly focus on the Car category, and also report the performances of the Pedestrian and Cyclist categories for reference. The default IoU threshold are 0.7, 0.5, 0.5 for these categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>Results on the KITTI test set. As shown in <ref type="table" target="#tab_3">Table 3</ref>, we report our results of the Car category on KITTI test set. Overall, our method achieves superior results over previous methods across all settings under fair conditions. For instance, the proposed method obtains 2.47/2.27/1.64 improvements under easy/moderate/hard setting for 3D detection task. Besides, our method achieves 18.89/90.23 in BEV detection/AOS task under moderate setting, improving previous best results by 4.06/4.12 AP 40 . Compared with the methods with extra data, the proposed method still get comparable performances, which further proves the effectiveness of our model.</p><p>Results on the KITTI validation set. We also present our model's performance on the KITTI validation set in <ref type="table" target="#tab_4">Table 4</ref>. Note that some methods directly use the pre-trained model provided by DORN <ref type="bibr" target="#b12">[13]</ref> as their depth estimator. However, the DORN's training set overlaps with the validation set of KITTI 3D, so we are not comparing these methods here. We can find that the proposed model performs better than all previous methods in 3D detection task. For BEV detec-   <ref type="table">Table 6</ref>: Results on accumulating the proposed approaches on the KITTI validation set. Metric is AP 40 of the Car category for BEV/3D detection. 'p.' denotes using projected 3D center for supervising the coarse center. 'I.' denotes using our IoU loss design. 's.' denotes the design for discarding distant samples. tion task, our method outperforms all methods except for MonoPair. Compared with MonoPair, our method is better at detecting objects under strict conditions (0.7 IoU threshold), while MonoPair is slightly better at catching samples under loose conditions (0.5 IoU threshold). Also note that our method shows better performance consistency between the validation set and test set. This indicates that our method has better generalization ability, which is of great significance in autonomous/assisted driving. Latency analysis. We test the proposed model on a single GTX 1080Ti GPU with a batchsize of 1 for runtime analysis. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the proposed method can run at 25 FPS, meeting the requirement of real-time detection. Specifically, our method runs 4? faster than the twostage detector M3D-RPN. Compared with MonoPair, which shares a similar framework as ours, our method can still save 16 ms for one image in the inference phase, mainly because: 1) we use standard DLA-34 as our backbone, instead of modified DL4-34 with DCN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>. 2) we apply fewer prediction heads in our model. 3) we don't need any postprocessing. SMOKE <ref type="bibr" target="#b23">[24]</ref> can run faster than our method. However, it only conducts 3D detection while the proposed method can perform 2D detection and 3D detection jointly.</p><p>Besides, although the detectors with pretrained depth estimator usually have promising performance, the additional depth estimator introduce lots of computational overheads (e.g. the most commonly used DORN <ref type="bibr" target="#b12">[13]</ref>   <ref type="table">Table 7</ref>: Analysis for center definition and multitask learning. Metrics are AP 40 of the Car category for 2D/3D detection tasks. 'PC', 'RF', and 'MT' represent 'projected 3D center', 'refinement', and 'multi-task learning'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pedestrian/Cyclist Detection</head><p>Here we present the Pedestrian/Cyclist detection results on the KITTI test set in <ref type="table" target="#tab_6">Table 5</ref>. Compared with cars, pedestrians/cyclists are more difficult to detect, and only <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> provide the performances of those categories on KITTI test set. Specifically, the proposed method performs better than <ref type="bibr" target="#b1">[2]</ref> and gets comparable results with <ref type="bibr" target="#b9">[10]</ref>. But it is important to note that, since the number of training samples for those two categories is quite small, the performance may fluctuate to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>Accumulation of the proposed designs. <ref type="table">Table 6</ref> shows experimental results evaluating how the proposed designs contribute to the overall performance for this task. Our design in Section 3.4, which uses projected 3D center for supervising center detection and influencing 2D detection ('+p.' in <ref type="table">Table 6</ref>), improves 3D detection accuracy by 1.5. The IoU loss design in Section 3.6 further improves the accuracy by 0.3. And the design for discarding distant samples in Section 3.5 leads to 0.7 improvement. Supervision for coarse center detection and multi-task learning. We show the performance changes caused by center definition and multi-task learning in <ref type="table">Table 7</ref>. Specifically, from setting a (used in <ref type="bibr" target="#b43">[44]</ref>) and setting b (used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref>) in the table, predicting an offset to compensate for the misalignment between 2D center and projected 3D center can improve the performance of 3D detection significantly. Then, using projected 3D center as the ground truth for coarse detection (setting d, our model) can further improve the performance. Besides, by comparing the setting c used in <ref type="bibr" target="#b23">[24]</ref> and the setting d in our design, we can find the performance of 3D detection benefits from multitask learning (performing 2D detection and 3D detection jointly). Note that the accuracy of 2D detection under setting d is also better than that under setting c, which suggests generating 2D bounding boxes from 3D detection may reduce the quality of the 2D detection results. The above conclusions are also reflected in <ref type="table" target="#tab_3">Table 3</ref> and 4. Training samples. From <ref type="table" target="#tab_9">Table 8</ref>, we can find that both removing some samples from training set appropriately and reducing the training weights of them can improve overall performance. Note that those samples are only a small part   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>We visualize some representative outputs of the proposed method in <ref type="figure" target="#fig_8">Figure 5</ref>. To clearly show the object's position in the 3D world space, we also visualize the LiDAR signals. We can observe that our model outputs remarkably accurate 3D bounding boxes for the cases at a reasonable distance. We also find that our model outputs some false positive samples, e.g. the 3D box on the right in the sixth picture, and the foremost reason for that is the imprecise depth or center estimation. Note that the dimension and orientation estimation for those cases are still accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we systematically analyze the problems in monocular 3D detection and find the localization error is the bottleneck of this task. To alleviate this problem, we first revisit the misalignment between the center of the 2D bounding box and the projected center of 3D object. We argue that directly detecting projected 3D center can reduce the localization error and 2D detection is conducive to optimize 3D detection. Besides, we also find distant samples are almost impossible to detect accurately with the existing technologies, and discarding these samples from the training set will stop them from distracting the network. Finally, we also proposed an IoU oriented loss for 3D size estimation. Extensive experiments on the challenging KITTI dataset show the effectiveness of the proposed strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>Uncertainty modeling. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10]</ref>, we model the heteroscedastic aleatoric uncertainty in the depth estimation sub-task. Specifically, we simultaneously predict the depth d and the standard deviation ? (or variance ? 2 ):</p><formula xml:id="formula_8">[d, ?] = f w (x),<label>(7)</label></formula><p>where x is the input data and f is a convolutional neural network parametrised by the parameters w. Then, we fix a Laplace likelihood to model the uncertainty, and the loss for the depth estimation sub-task can be formulated by:</p><formula xml:id="formula_9">L = ? 2 ? ||d ? d * || 1 + log ?,<label>(8)</label></formula><p>where || ? || 1 denotes the L1 norm and d * is the ground truth value for depth d. Similarly for the Gaussian likelihood:</p><formula xml:id="formula_10">L = 1 2? 2 ||d ? d * || 2 + 1 2 log ? 2 ,<label>(9)</label></formula><p>where || ? || 2 denotes the L2 norm (please refer to <ref type="bibr" target="#b18">[19]</ref> for the derivation of Equation 8 and Equation <ref type="bibr" target="#b8">9</ref>). Note that the uncertainty modeling is not claimed as our contribution. Experimental results. First, from <ref type="figure" target="#fig_9">Figure 6</ref> and <ref type="table" target="#tab_11">Table 9</ref>, we can find that uncertainty-based estimation improves the accuracy of depth map, thereby improving the overall performance of monocular 3D detection. Second, the experimental result also show that modeling uncertainty based on the Laplace distribution (all models in the main paper adopted this setting) is more suitable for our task than Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. IoU Oriented Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Proof of Proposition</head><p>This section provides the proof of the following proposition, which is used in Equations 5 and 6 for IoU oriented optimization in Section 3.6.   Proposition. Suppose all predicted items except the 3D sizes (h, w, l) are completely correct, the contribution ratio of each predicted side to the 3D IoU ?IoU ?h : ?IoU ?w : ?IoU ?l can be approximated to 1 h : 1 w : 1 l . Proof. Given the above conditions, the 3D IoU metric can be formulated as:</p><formula xml:id="formula_11">IoU = i?{h,w,l} min(i, i * ) h ? w ? l + h * ? w * ? l * ? i?{h,w,l} min(i, i * ) ,<label>(10)</label></formula><p>where (h * , w * , l * ) denotes the ground truth of 3D size (h, w, l). With the different relationship between the prediction and the ground truth of the 3D size, we can obtain the following cases: Case 1: If h ? h * , w ? w * , and l ? l * , the Equation 10 can be simplified as:</p><formula xml:id="formula_12">IoU = h ? w ? l h * ? w * ? l * ,<label>(11)</label></formula><p>and we further compute the partial derivative of 3D IoU with respect to the variable h as</p><formula xml:id="formula_13">?IoU ?h = w ? l h * ? w * ? l * ,<label>(12)</label></formula><p>where ?IoU ?h represents the partial derivative of 3D IoU with respect to the variable h, analogically for ?IoU ?w and ?IoU ?l . Then, combining the derivative of 3D IoU with respect to h, w, and l, the contribution ratio of each predicted side can be given as:</p><formula xml:id="formula_14">?IoU ?h : ?IoU ?w : ?IoU ?l = 1 h : 1 w : 1 l .<label>(13)</label></formula><p>Case 2: If h &gt; h * , w &gt; w * , and l &gt; l * , the Equation 10</p><p>can be simplified as:</p><formula xml:id="formula_15">IoU = h * ? w * ? l * h ? w ? l ,<label>(14)</label></formula><p>and similar to Equation 12 and 13, we can derive the same conclusion as Case 1. Case 3: If h &gt; h * , w ? w * , and l ? l * , then we represent the 3D IoU as:</p><formula xml:id="formula_16">IoU = h * ? w ? l h ? w ? l + h * ? w * ? l * ? h * ? w ? l .<label>(15)</label></formula><p>By calculating the derivative of 3D IoU with respect to h, w, and l respectively, we can get the contribution ratio of each predicted side:</p><formula xml:id="formula_17">?IoU ?h : ?IoU ?w : ?IoU ?l = w ? l h * ? w * ? l * : 1 w : 1 l .<label>(16)</label></formula><p>Case 4: If h &gt; h * , w &gt; w * , and l ? l * , similarly, we can get the IoU formulation as:</p><formula xml:id="formula_18">IoU = h * ? w * ? l h ? w ? l + h * ? w * ? l * ? h * ? w * ? l .<label>(17)</label></formula><p>Similar to previous steps, the formulation of each side's contribution rate to the 3D IoU is given as:</p><formula xml:id="formula_19">?IoU ?h : ?IoU ?w : ?IoU ?l = 1 h : 1 w : h * ? w * ? l * h ? w ? l ? l .<label>(18)</label></formula><p>The other cases are similar to Case 3 and Case 4. When h ? h * , w ? w * , and l ? l * , we can get the Equation 5 used in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Experiments</head><p>We report the improvement introduced by the proposed loss function in the main paper. To further validate the effectiveness of it, we also implement the 3D GIoU loss <ref type="bibr" target="#b40">[41]</ref> for reference. Specifically, we add the 3D GIoU loss as a regularization item as in <ref type="bibr" target="#b40">[41]</ref>, investigating different weights considered in our baseline model, and the AP 40 of cars on the moderate setting on KITTI validation set (Table 10) show that our IoU oriented optimization improves accuracy but 3D-GIoU with different weights does not.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance for the Close Objects</head><p>The <ref type="figure" target="#fig_0">Figure 1</ref> in the main paper provides lots of insights to us. Except for the observations analyzed in the main paper, we also found that the performance degrades for the very close object. Here we provides our analysis for this. In particular, there are three main reasons in total. a) The close-range objects tend to have larger center misalignment (see <ref type="figure" target="#fig_6">Figure 3</ref> for the statistics). b) The objects at closer ranges are usually more truncated, e.g. the red car (depth=3.7, truncation=0.88) and the black car (depth=6.2, truncation=0.34) in <ref type="figure" target="#fig_10">Figure 7</ref>. c) The training samples in the close range are fewer. For example, there are 5,979 cars in [5m, 15m] and 6,707 cars in [10m, 20m] on the KITTI trainval set, and the distribution for those samples are summarized in <ref type="table" target="#tab_0">Table 11</ref>. Note that the KITTI annotate the difficulty of each samples according to its size of 2D bounding box, occlusion, and truncation. The instance with 'un-Known' tag usually means that it is extremely difficult to detect and is ignored in evaluation. With that in mind, the effective samples of those two ranges are 4,522 and 6,149. In summary, the low performance of the very close objects is caused by the limited training samples (c) and the large proportion of hard cases (a, b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Learned features</head><p>From <ref type="figure" target="#fig_7">Figure 4</ref> in the main paper, we can see there is a misalignment between the center of the 2D bounding box and the projected center of the 3D object, especially for close objects (see <ref type="figure" target="#fig_6">Figure 3</ref> and <ref type="figure" target="#fig_7">Figure 4</ref>). Accordingly, we propose our solution for this problem. Here we visualize the learned features of coarse center detection branch in <ref type="figure" target="#fig_10">Figure 7</ref> to show the effectiveness of the proposed method. The qualitative results clearly show that using projected 3D center as ground truth can make the coarse center more accurate, thereby improving the localization accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Comparison of qualitative results</head><p>Visualizations in the image plane. We show more qualitative results of M3D-RPN (the best of all open-source standard monocular 3D detector) and the proposed method in <ref type="figure" target="#fig_11">Figure 8</ref>. We use red circle to highlight the main differences of each pair of images, and we can find that our method performs better than M3D-RPN for dense objects. Visualizations in the 3D world space. We also visualize the 3D bounding boxes in the 3D world space for better presentation. As shown in <ref type="figure" target="#fig_12">Figure 9</ref>, the proposed model outputs better results than M3D-RPN, especially for the orientation estimation. Representative failure case. We show a typical error pattern in monocular 3D object detection in <ref type="figure" target="#fig_0">Figure 10</ref>. We can observe that the projected 3D bounding boxes fit the object's appearance tightly in the image plane. However, from the visualization results in the 3D world space, this is a clear false positive because the depth is inaccurate (the outline of the object can be perceived through the point clouds, best viewed with zooming in). Note that this problem is common in the monocular 3D detection task, which suggests that depth estimation is a key factor restricting this task.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>DepthFigure 1 :</head><label>1</label><figDesc>AP40: 9.97) w/ gt proj. center (overall AP40: 16.54) w/ gt depth (overall AP40: 35.11) w/ gt location (overall AP40: 72.36) Range-wise evaluation on the KITTI validation set. Metric is AP 40 of the Car category under moderate setting. The sampling interval is 10 m. For example, the corresponding value at horizontal axis 20 represents the overall performance of all samples between 15 m and 25 m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>w</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z s w M C Y 1 r 6 6 Z U N 6 L V / J y d M S x I n Q E = " &gt; A A A B 6 H i c b Z D J S g N B E I Z r 4 h b H L e r R S 2 M Q P I U Z Q f Q i B r 1 4 T M A s k A y h p 1 O T t O l Z 6 O 4 R w p A n 8 O J B E a / 6 M N 6 9 i G 9 j Z z l o 4 g 8 N H / 9 f R V e V n w i u t O N 8 W 7 m l 5 Z X V t f y 6 v b G 5 t b 1 T 2 N 2 r q z i V D G s s F r F s + l S h 4 B H W N N c C m 4 l E G v o C G / 7 g e p w 3 7 l E q H k e 3 e p i g F 9 J e x A P O q D Z W t d 8 p F J 2 S M x F Z B H c G x c s P + y J 5 / 7 I r n c J n u x u z N M R I M 0 G V a r l O o r 2 M S s 2 Z w J H d T h U m l A 1 o D 1 s G I x q i 8 r L J o C N y Z J w u C W J p X q T J x P 3 d k d F Q q W H o m 8 q Q 6 r 6 a z 8 b m f 1 k r 1 c G 5 l / E o S T V G b P p R k A q i Y z L e m n S 5 R K b F 0 A B l k p t Z C e t T S Z k 2 t 7 H N E d z 5 l R e h f l J y T 0 t O 1 S m W r 2 C q P B z A I R y D C 2 d Q h h u o Q A 0 Y I D z A E z x b d 9 a j 9 W K 9 T k t z 1 q x n H / 7 I e v s B L u G Q L Q = = &lt; / l a t e x i t &gt; h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G v h X 5 V W W Z 8 6 a 1 2 T W u K d q G 8 3 3 T v c = " &gt; A A A B 6 H i c b Z D L S s N A F I Z P 6 q 3 G W 9 W l m 2 A R X J V E E N 2 I R T c u W 7 A X a E O Z T E / a s Z N J m J k I J f Q J 3 L h Q x K 0 + j H s 3 4 t s 4 v S y 0 9 Y e B j / 8 / h z n n B A l n S r v u t 5 V b W l 5 Z X c u v 2 x u b W 9 s 7 h d 2 9 u o p T S b F G Y x 7 L Z k A U c i a w p p n m 2 E w k k i j g 2 A g G 1 + O 8 c Y 9 S s V j c 6 m G C f k R 6 g o W M E m 2 s K u 8 U i m 7 J n c h Z B G 8 G x c s P + y J 5 / 7 I r n c J n u x v T N E K h K S d K t T w 3 0 X 5 G p G a U 4 8 h u p w o T Q g e k h y 2 D g k S o / G w y 6 M g 5 M k 7 X C W N p n t D O x P 3 d k Z F I q W E U m M q I 6 L 6 a z 8 b m f 1 k r 1 e G 5 n z G R p B o F n X 4 U p t z R s T P e 2 u k y i V T z o Q F C J T O z O r R P J K H a 3 M Y 2 R / D m V 1 6 E + k n J O y 2 5 V b d Y v o K p 8 n A A h 3 A M H p x B G W 6 g A j W g g P A A T / B s 3 V m P 1 o v 1 O i 3 N W b O e f f g j 6 + 0 H N P G Q M Q = = &lt; / l a t e x i t &gt; l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n r 3 / I o q C x h w N n j i x y K a P Q D o r w E 0 = " &gt; A A A B 8 n i c b Z D L S s N A F I Y n 9 V b j r e r S z W A R X J V E E N 2 I R T c u K 9 g L p L F M p p N 2 6 C Q T Z k 6 U E v I Y b l w o 0 q 3 v 4 d 6 N + D Z O L w t t / W H g 4 / / P Y c 4 5 Q S K 4 B s f 5 t g p L y y u r a 8 V 1 e 2 N z a 3 u n t L v X 0 D J V l N W p F F K 1 A q K Z 4 D G r A w f B W o l i J A o E a w a D 6 3 H e f G B K c x n f w T B h f k R 6 M Q 8 5 J W A s L 2 s H I a b 5 f f a Y d 0 p l p + J M h B f B n U H 5 8 s O + S E Z f d q 1 T + m x 3 J U 0 j F g M V R G v P d R L w M 6 K A U 8 F y u 5 1 q l h A 6 I D 3 m G Y x J x L S f T U b O 8 Z F x u j i U y r w Y 8 M T 9 3 Z G R S O t h F J j K i E B f z 2 d j 8 7 / M S y E 8 9 z M e J y m w m E 4 / C l O B Q e L x / r j L F a M g h g Y I V d z M i m m f K E L B X M k 2 R 3 D n V 1 6 E x k n F P a 0 4 t 0 6 5 e o W m K q I D d I i O k Y v O U B X d o B q q I 4 o k e k I v 6 N U C 6 9 l 6 s 0 b T 0 o I 1 6 9 l H f 2 S 9 / w D L q p S V &lt; / l a t e x i t &gt; c w &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u j Z d E Q J j s t N E t S 1 K 0 1 a V 4 r B t D m A = " &gt; A A A B 8 n i c b Z D L S g M x F I Y z X u t 4 q 7 p 0 E y y C q z I j i G 7 E o h u X F e w F 2 r F k 0 k w b m k m G 5 I x Q h n k M N y 4 U 6 d b 3 c O 9 G f B v T y 0 J b f w h 8 / P 8 5 5 J w T J o I b 8 L x v Z 2 l 5 Z X V t v b D h b m 5 t 7 + w W 9 / b r R q W a s h p V Q u l m S A w T X L I a c B C s m W h G 4 l C w R j i 4 G e e N R 6 Y N V / I e h g k L Y t K T P O K U g L V a W T u M M M 0 f M p 5 3 i i W v 7 E 2 E F 8 G f Q e n q w 7 1 M R l 9 u t V P 8 b H c V T W M m g Q p i T M v 3 E g g y o o F T w X K 3 n R q W E D o g P d a y K E n M T J B N R s 7 x s X W 6 O F L a P g l 4 4 v 7 u y E h s z D A O b W V M o G / m s 7 H 5 X 9 Z K I b o I M i 6 T F J i k 0 4 + i V G B Q e L w / 7 n L N K I i h B U I 1 t 7 N i 2 i e a U L B X c u 0 R / P m V F 6 F + W v b P y t 6 d V 6 p c o 6 k K 6 B A d o R P k o 3 N U Q b e o i m q I I o W e 0 A t 6 d c B 5 d t 6 c 0 b R 0 y Z n 1 H K A / c t 5 / A L Z k l I c = &lt; / l a t e x i t &gt; c i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 D H T S V Y d K V R x S E 9 c K 2 M B w p 9 1 t 9Q = " &gt; A A A B 7 n i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F c F V m B N G N W H T j s o K 9 Q D u U T J p p Q z O Z k G S E M v Q h 3 L h Q x I U b 3 8 S 9 G / F t z L R d a O s P g Y / / P 4 e c c 0 L J m T a e 9 + 0 U l p Z X V t e K 6 + 7 G 5 t b 2 T m l 3 r 6 G T V B F a J w l P V C v E m n I m a N 0 w w 2 l L K o r j k N N m O L z O 8 + Y 9 V Z o l 4 s 6 M J A 1 i 3 B c s Y g Q b a z W z T h g h M u 6 W y l 7 F m w g t g j + D 8 u W H e y H f v t x a t / T Z 6 S U k j a k w h G O t 2 7 4 n T Z B h Z R j h d O x 2 U k 0 l J k P c p 2 2 L A s d U B 9 l k 3 D E 6 s k 4 P R Y m y T x g 0 c X 9 3 Z D j W e h S H t j L G Z q D n s 9 z 8 L 2 u n J j o P M i Z k a q g g 0 4 + i l C O T o H x 3 1 G O K E s N H F j B R z M 6 K y A A r T I y 9 k G u P 4 M + v v A i N k 4 p / W v F u v X L 1 C q Y q w g E c w j H 4 c A Z V u I E a 1 I HA E B 7 g C Z 4 d 6 T w 6 L 8 7 r t L T g z H r 2 4 Y + c 9 x 9 v + p K g &lt; / l a t e x i t &gt; c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U l 4 x y n s d l r m v C 3 J 3 x k X n H q W Q W x o = " &gt; A A A B 8 n i c b Z D L S g M x F I b P e K 3 j r e r S T b A I r s q M I L o R i 2 5 c V r A X a M e S S T N t a C Y Z k o x Q h n k M N y 4 U 6 d b 3 c O 9 G f B v T y 0 J b f w h 8 / P 8 5 5 J w T J p x p 4 3 n f z t L y y u r a e m H D 3 d z a 3 t k t 7 u 3 X t U w V o T U i u V T N E G v K m a A 1 w w y n z U R R H I e c N s L B z T h v P F K l m R T 3 Z p j Q I M Y 9 w S J G s L F W K 2 u H E Z L 5 Q 8 b y T r H k l b 2 J 0 C L 4 M y h d f b i X y e j L r X a K n + 2 u J G l M h S E c a 9 3 y v c Q E G V a G E U 5 z t 5 1 q m m A y w D 3 a s i h w T H W Q T U b O 0 b F 1 u i i S y j 5 h 0 M T 9 3 Z H h W O t h H N r K G J u + n s / G 5 n 9 Z K z X R R Z A x k a S G C j L 9 K E o 5 M h K N 9 0 d d p i g x f G g B E 8 X s r I j 0 s c L E 2 C u 5 9 g j + / M q L U D 8 t + 2 d l 7 8 4 r V a 5 h q g I c w h G c g A / n U I F b q E I N C E h 4 g h d 4 d Y z z 7 L w 5 o 2 n p k j P r O Y A / c t 5 / A M j Q l J M = &lt; / l a t e x i t &gt; o i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R s f u h z q 1 p h b F 8 D S L + b 8 i Z h p l 8 q M = " &gt; A A A B 8 n i c b Z D L S s N A F I Y n 9 V b j r e r S z W A R X J V E E N 2 I R T c u K 9 g L p L F M p p N 2 6 C Q T Z k 6 U E v I Y b l w o 0 q 3 v 4 d 6 N + D Z O L w t t / W H g 4 / / P Y c 4 5 Q S K 4 B s f 5 t g p L y y u r a 8 V 1 e 2 N z a 3 u n t L v X 0 D J V l N W p F F K 1 A q K Z 4 D G r A w f B W o l i J A o E a w a D 6 3 H e f G B K c x n f w T B h f k R 6 M Q 8 5 J W A s L 2 s H I Z b 5 f f a Y d 0 p l p + J M h B f B n U H 5 8 s O + S E Z f d q 1 T + m x 3 J U 0 j F g M V R G v P d R L w M 6 K A U 8 F y u 5 1 q l h A 6 I D 3 m G Y x J x L S f T U b O 8 Z F x u j i U y r w Y 8 M T 9 3 Z G R S O t h F J j K i E B f z 2 d j 8 7 / M S y E 8 9 z M e J y m w m E 4 / C l O B Q e L x / r j L F a M g h g Y I V d z M i m m f K E L B X M k 2 R 3 D n V 1 6 E x k n F P a 0 4 t 0 6 5 e o W m K q I D d I i O k Y v O U B X d o B q q I 4 o k e k I v 6 N U C 6 9 l 6 s 0 b T 0 o I 1 6 9 l H f 2 S 9 / w D e F p S h &lt; / l a t e x i t &gt; o w &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U T g R D B f a X b b 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>L h k m 0 = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + R o e f G l l k L 3 k J k Z L h e Q s 8 k R w k n 0 = " &gt; A A A B 7 X i c b Z A 9 S w N B E I b n 4 l c 8 v 6 K W N o t B s A p 3 g m g j B m 0 s I 5 g P S I 6 w t 9 l L 1 u z t H b t z Q g j 5 D z Y W i t h Y + F P s b c R / 4 y a x 0 M Q X F h 7 e d 4 a d m T C V w q D n f T m 5 h c W l 5 Z X 8 q r u 2 v r G 5 V d j e q Z k k 0 4 x X W S I T 3 Q i p 4 V I o X k W B k j d S z W k c S l 4 P + 5 f j v H 7 H t R G J u s F B y o O Y d p W I B K N o r V o L e x x p u 1 D 0 S t 5 E Z B 7 8 H y i e v 7 t n 6 e u n W 2 k X P l q d h G U x V 8 g k N a b p e y k G Q 6 p R M M l H b i s z P K W s T 7 u 8 a V H R m J t g O J l 2 R A 6 s 0 y F R o u 1 T S C b u 7 4 4 h j Y 0 Z x K G t j C n 2 z G w 2 N v / L m h l G p 8 F Q q D R D r t j 0 o y i T B B M y X p 1 0 h O Y M 5 c A C Z V r Y W Q n r U U 0 Z 2 g O 5 9 g j + 7 M r z U D s q + c c l 7 9 o r l i 9 g q j z s w T 4 c g g 8 n U I Y r q E A V G N z C P T z C k 5 M 4 D 8 6 z 8 z I t z T k / P b v w R 8 7 b N w T f k m k = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A X 5 x y 6 N h r E G 4 k E R 0 M h Q B 4 D N 3 k u k = " &gt; A A A B 7 X i c b V D J S g N B E O 1 x j X G L e l S k M Q i e w o w g e g x 6 8 Z i A W S A Z Q k 2 n k 7 T p Z e j u E c K Q o 3 c v H h T x 6 i / k O 7 z 5 D f 6 E n e W g i Q 8 K H u 9 V U V U v i j k z 1 v e / v K X l l d W 1 9 c x G d n N r e 2 c 3 t 7 d f N S r R h F a I 4 k r X I z C U M 0 k r l l l O 6 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>G n j u l 9 O Z m l 5 Z X U t u 5 7 b 2 N z a 3 s n v 7 t W 1 T B S h N S K 5 V M 0 A a 8 q Z o D X D D K f N W F E c B Z w 2 g s H l 2 G / c U a W Z F D d m G F M / w j 3 B Q k a w s V L 9 v p O e X o 0 6 + Y J b d C d A i 8 S b k U L p o P r N 3 s s f l U 7 + s 9 2 V J I m o M I R j r V u e G x s / x c o w w u k o 1 0 4 0 j T E Z 4 B 5 t W S p w R L W f T q 4 d o W O r d F E o l S 1 h 0 E T 9 P Z H i S O t h F N j O C J u + n v f G 4 n 9 e K z H h h Z 8 y E S e G C j J d F C Y c G Y n G r 6 M u U 5 Q Y P r Q E E 8 X s r Y j 0 s c L E 2 I B y N g R v / u V F U j 8 t e m d F t 2 r T K M M U W T i E I z g B D 8 6 h B N d Q g R o Q u I U H e I J n R z q P z o v z O m 3 N O L O Z f f g D 5 + 0 H P g + S m w = = &lt; / l a t e x i t &gt; w 2D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w w K x j R B u f j C b 9 w K + W W a + B o N K 6 S E = " &gt; A A A B 7 X i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u Q L Q M 0 c I y A f O A Z A m z k 9 l k z O z M M j M r h C X / Y G O h i K 2 V f + E X 2 N n 4 L U 4 e h S Y e u H A 4 5 1 7 u v S e I O d P G d b + c z M r q 2 v p G d j O 3 t b 2 z u 5 f f P 2 h o m S h C 6 0 R y q V o B 1 p Q z Q e u G G U 5 b s a I 4 C j h t B s O r i d + 8 p 0 o z K W 7 N K K Z + h P u C h Y x g Y 6 X G o J u W r s f d f M E t u l O g Z e L N S a F 8 V P t m 7 5 W P a j f / 2 e l J k k R U G M K x 1 m 3 P j Y 2 f Y m U Y 4 X S c 6 y S a x p g M c Z + 2 L R U 4 o t p P p 9 e O 0 a l V e i i U y p Y w a K r + n k h x p P U o C m x n h M 1 A L 3 o T 8 T + v n Z j w 0 k + Z i B N D B Z k t C h O O j E S T 1 1 G P K U o M H 1 m C i W L 2 V k Q G W G F i b E A 5 G 4 K 3 + P I y a Z S K 3 n n R r d k 0 K j B D F o 7 h B M 7 A g w s o w w 1 U o Q 4 E 7 u A B n u D Z k c 6 j 8 + K 8 z l o z z n z m E P 7 A e f s B J w i S j A = = &lt; / l a t e x i t &gt; h 2D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n Z g 7 m Z 9 5 a z c 5 e Y R J 8 O Q Z g R H Z q q A = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I V d Q b Q R g z a W C Z g L J E u Y n Z x N x s x e m J k V 4 p I n s L F Q x F Y f x t 5 G f B s n i Y U m / j D w 8 f / n M O c c P x F c a c f 5 s n I L i 0 v L K / l V e 2 1 9 Y 3 O r s L 1 T V 3 E q G d Z Y L G L Z 9 K l C w S O s a a 4 F N h O J N P Q F N v z B 5 T h v 3 K J U P I 6 u 9 T B B L 6 S 9 i A e c U W 2 s 6 l 2 n U H R K z k R k H t w f K J 6 / 2 2 f J 2 6 d d 6 R Q + 2 t 2 Y p S F G m g m q V M t 1 E u 1 l V G r O B I 7 s d q o w o W x A e 9 g y G N E Q l Z d N B h 2 R A + N 0 S R B L 8 y J N J u 7 v j o y G S g 1 D 3 1 S G V P f V b D Y 2 / 8 t a q Q 5 O v Y x H S a o x Y t O P g l Q Q H Z P x 1 q T L J T I t h g Y o k 9 z M S l i f S s q 0 u Y 1 t j u D O r j w P 9 a O S e 1 x y q k 6 x f A F T 5 W E P 9 u E Q X D i B M l x B B W r A A O E e H u H J u r E e r G f r Z V q a s 3 5 6 d u G P r N d v S i m Q P w = = &lt; / l a t e x i t &gt; z Visualization of the notations of 2D bounding box in the feature map scale (left), 3D bounding box in the 3D world space (middle), and orientation of the object from bird's view (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Statistics. Top: the misalignment (in pixel, collected on the KITTI trainval set under moderate setting) between the center of 2D bounding box and the projected 3D center in the image plane. Bottom: the depth errors (in meter, trained on the KITTI training set, tested on the validation set) These two statistics are presented as the function of the depth (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the misalignment between the center of the 2D bounding box (blue) and the projected 3D center (red) in image plane. center c w of the 3D bounding box. According to the formulation in Equation 1, the projected 3D center c w should be the key for recovering the 3D object center [x, y, z] 3D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the KITTI test set. These results are based on proposed model trained on the KITTI trainval set, running at 25 FPS. We use blue, green, and red boxes to denote cars, pedestrians, and cyclists. LiDAR signals are only used for visualization. Best viewed in color with zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Errors of depth estimation. We show the errors of depth estimation as a function of the depth (x-axis) for the plain scheme (top) and the uncertainty aware scheme based on the Laplace likelihood (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison for the learned features of coarse center detection task on the KITTI validation set. Top: the input image. Middle: the features of the coarse center detection branch supervised by 2D center. Bottom: the features of the coarse center detection branch supervised by projected 3D center. We use the write circle to highlight the ground truth projected 3D center for better comparison. Best viewed in color with zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparison on the KITTI validation set. We visualize the 3D bounding boxes in the image plane. Results are from M3D-RPN (left) and our method (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative comparison on the KITTI validation set. We visualize the 3D bounding boxes in the 3D world space. Results are from M3D-RPN (top) and our method (middle). We also show the corresponding 2D image (bottom) for reference. Best viewed in color with zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Failure case. We show a representative failure case which is caused by the inaccurate depth estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>With the known camera intrinsic matrix K ? R 3?3 , we can recover the center of object in baseline 11.12 ground truth 99.97 w/ gt proj. center 23.90 w/o gt proj. center 46.33 w/ gt depth 38.01 w/o gt depth 25.25 w/ gt 3D location 78.84 w/o gt 3D location 12.13 Error analysis. Left: We replace the outputs of 3D detection related branches with the ground truth values. Right: We replace the values of ground truth with the predicted results. Metric is AP 40 for 3D detection under moderate setting on the KITTI val set. 'proj. center' denotes the projected 3D center c w on the image plane.</figDesc><table><row><cell>w/ gt 3D size</cell><cell>11.96 w/o gt 3D size</cell><cell>80.50</cell></row><row><cell>w/ gt orientation</cell><cell>11.88 w/o gt orientation</cell><cell>70.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Localization error (in meter) caused by center shifting in image plane (in pixel).observed. Therefore, the low accuracy of monocular 3D detection is mainly caused by localization error. On the other hand, according to Equation 1, depth estimation and center localization jointly determine the position of the object in 3D world space. Compared with the ill-posed depth estimation from a monocular image, improving the accuracy of center detection is a more feasible way.Table 2shows localization errors introduced by inaccurate center detection. Furthermore, the mean shape of cars in KITTI dataset is [1.53m, 1.63m, 3.53m] for [h, w, l] 3D .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of the Car category on the KITTI test set. Methods are ranked by moderate setting (same as KITTI leaderboard). We highlight the best results in bold and the second place in underlined. 15.62 11.88 48.53 35.94 28.59 53.35 39.60 31.76 MonoPair [10] 16.28 12.30 10.42 24.12 18.17 15.76 55.38 42.39 37.99 61.06 47.63 41.92 Ours 17.45 13.66 11.68 24.97 19.33 17.01 55.41 43.42 37.81 60.73 46.87 41.89 Improvement +1.17 +1.36 +1.26 +0.85 +1.16 +1.25 +0.03 +1.03 -0.18 -0.33 -0.80 -0.03</figDesc><table><row><cell>Method</cell><cell cols="3">3D@IOU=0.7 Easy Mod. Hard</cell><cell cols="3">BEV@IOU=0.7 Easy Mod. Hard</cell><cell cols="3">3D@IOU=0.5 Easy Mod. Hard Easy Mod. Hard BEV@IOU=0.5</cell></row><row><cell>CenterNet [44]</cell><cell>0.60</cell><cell>0.66</cell><cell>0.77</cell><cell>3.46</cell><cell>3.31</cell><cell>3.21</cell><cell cols="3">20.00 17.50 15.57 34.36 27.91 24.65</cell></row><row><cell cols="2">MonoGRNet [30] 11.90</cell><cell>7.56</cell><cell>5.76</cell><cell cols="6">19.72 12.81 10.15 47.59 32.28 25.50 48.53 35.94 28.59</cell></row><row><cell>MonoDIS [36]</cell><cell>11.06</cell><cell>7.60</cell><cell>6.37</cell><cell cols="3">18.45 12.58 10.66</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>M3D-RPN [2]</cell><cell cols="2">14.53 11.07</cell><cell>8.65</cell><cell>20.85</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of the Car category on the KITTI validation set. Methods are ranked by moderate setting (same as KITTI leaderboard). We highlight the best results in bold and the second place in underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Ped. 10.99 / 10.02 7.04 / 6.68 6.29 / 5.53</figDesc><table><row><cell>Method</cell><cell>Cat.</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>M3D-RPN [2]</cell><cell>Ped.</cell><cell>5.65 / 4.92</cell><cell cols="2">4.05 / 3.48 3.29 / 2.94</cell></row><row><cell>MonoPair [10] Ours</cell><cell>Ped.</cell><cell cols="3">10.73 / 9.64 6.96 / 6.55 6.20 / 5.44</cell></row><row><cell cols="2">M3D-RPN [2] Cyc.</cell><cell>1.25 / 0.94</cell><cell cols="2">0.81 / 0.65 0.78 / 0.47</cell></row><row><cell cols="2">MonoPair [10] Cyc.</cell><cell>4.76 / 3.79</cell><cell cols="2">2.87 / 2.12 2.42 / 1.83</cell></row><row><cell>Ours</cell><cell>Cyc.</cell><cell>5.34 / 4.59</cell><cell cols="2">3.28 / 2.66 2.83 / 2.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Benchmark for Pedestrian/Cyclist detection on the KITTI test set. Metric is AP 40 for BEV/3D detection task at 0.5 IoU threshold. +I.+s. 24.97 / 17.45 19.33 / 13.66 17.01 / 11.68</figDesc><table><row><cell></cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>baseline</cell><cell cols="3">20.29 / 14.51 16.15 / 11.12 14.07 / 9.97</cell></row><row><cell>+ p.</cell><cell cols="3">23.10 / 15.78 18.15 / 12.65 16.11 / 10.62</cell></row><row><cell>+ p.+I.</cell><cell cols="3">23.89 / 16.12 18.34 / 12.97 16.69 / 10.99</cell></row><row><cell>+ p.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>takes about 400 ms to process a standard KITTI image. See KITTI Depth Benchmark for more details). / 13.98 92.31 / 10.81 84.75 / 9.59 c -94.55 / 12.31 88.79 / 10.30 79.29 / 8.82 d 98.42 / 16.08 92.74 / 13.04 83.04 / 11.16</figDesc><table><row><cell></cell><cell cols="2">PC RF MT</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>a</cell><cell>-</cell><cell>-</cell><cell>98.08 / 1.32</cell><cell>92.31 / 1.04</cell><cell>84.75 / 1.16</cell></row><row><cell>b</cell><cell>-</cell><cell></cell><cell>98.08</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>= 40, T =1 14.50 11.74 9.95 + soft encoding, c = 60, T =1 17.50 13.54 11.32 + soft encoding, c = 60, T =5 17.25 13.03 11.01</figDesc><table><row><cell></cell><cell>Easy Mod. Hard</cell></row><row><cell>baseline</cell><cell>16.12 12.97 10.99</cell></row><row><cell>+ hard encoding, s = 40</cell><cell>14.25 11.25 9.63</cell></row><row><cell>+ hard encoding, s = 60</cell><cell>17.45 13.66 11.68</cell></row><row><cell>+ soft encoding, c</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Analysis for training samples. Metrics is AP 40 of the Car category for 3D detection.of the whole training set and will not affect the representation learning of the network to the whole dataset. For example, in the 7,481 images in trainval set, only 1,301/767 samples beyond 60/65 meters, accounting for 4.5%/2.7% of the total 28,742 samples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>/ 13.18 14.24 / 10.15 12.13 / 8.45 Gaussian 18.68 / 13.20 14.22 / 10.41 12.08 / 8.69 Laplace 20.29 / 14.51 16.15 / 11.12 14.07 / 9.97</figDesc><table><row><cell>uncert.</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>-</cell><cell>18.56</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Analysis for the designs of depth estimation. Metrics are AP 40 of the Car category for BEV/3D detection tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Ablation study for the proposed loss function and 3D GIoU loss on the KITTI validation set. Metric is AP 40 of the Car category under moderate setting.</figDesc><table><row><cell>Range</cell><cell cols="5">Easy Moderate Hard UnKnown Total</cell></row><row><cell cols="2">[5m, 15m] 2,131</cell><cell>1,428</cell><cell>963</cell><cell>1,457</cell><cell>5,979</cell></row><row><cell cols="2">[10m, 20m] 2,639</cell><cell>1,840</cell><cell>1,670</cell><cell>558</cell><cell>6,707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Data distribution for the car samples located in [5m, 15m] and [10m, 20m]. The data is collected from the KITTI trainval set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by SenseTime, the Australian Research Council Grant DP200103223, and Australian Medical Research Future Fund MRFAI000085.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>This document provides additional technical details, experimental results, theoretical analysis, and qualitative results to the main paper. Specifically, in Section B, we provide more details on the implementation of the depth estimation sub-task, and Section C shows the details and ablations about the proposed IoU oriented loss. Section D provides more discussion which is omitted in the main paper. Finally, Section E presents more visual results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tide: A general toolbox for identifying object detection errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9287" to="9296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with decoupled structured polygon estimation and heightguided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12536" to="12545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yodsawalai</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qieyun</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Geometry and uncertainty in deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Guy</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smoke: Singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection: From single to multi-class recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Songhua He, and Jiahua Zhu. 3d-giou: 3d generalized intersection over union for object detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
