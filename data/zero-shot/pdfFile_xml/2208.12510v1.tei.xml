<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partially Relevant Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianke</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Partially Relevant Video Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">MM &apos;22</title>
						<meeting> <address><addrLine>Lisboa, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3547976</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Query: Sheldon was browsing a book whilst sitting at the couch.</p><p>Query: Penny walks in the living room and sits down on the couch, holding her cup of coffee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevant moment</head><p>Detected key clip Attention weights on frames <ref type="figure">Figure 1</ref>: Two textual queries partially relevant to a given video. Only a specific moment in the video is relevant to the corresponding query, while the other frames are irrelevant. We formulate the task of partially relevant video retrieval (PRVR) as a multiple instance learning problem, and propose a MS-SL network. MS-SL first detects a key clip that is most likely to be relevant to the query. Then, the importance of each frame is measured in a fine-grained temporal scale under the guidance of the key clip. The final similarity is computed by jointly considering the query's similarities with the key clip and the frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Current methods for text-to-video retrieval (T2VR) are trained and tested on video-captioning oriented datasets such as MSVD, MSR-VTT and VATEX. A key property of these datasets is that videos are assumed to be temporally pre-trimmed with short duration, whilst the provided captions well describe the gist of the video content. Consequently, for a given paired video and caption, the video is supposed to be fully relevant to the caption. In reality, however, as queries are not known a priori, pre-trimmed video clips may not contain sufficient content to fully meet the query. This suggests a gap between the literature and the real world. To fill the gap, we propose in this paper a novel T2VR subtask termed Partially Relevant Video Retrieval (PRVR). An untrimmed video is considered to be partially relevant w.r.t. a given textual query if it contains a moment relevant to the query. PRVR aims to retrieve such partially relevant videos from a large collection of untrimmed * Corresponding authors: Xirong Li and Xun Wang</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>videos. PRVR differs from single video moment retrieval and video corpus moment retrieval, as the latter two are to retrieve moments rather than untrimmed videos. We formulate PRVR as a multiple instance learning (MIL) problem, where a video is simultaneously viewed as a bag of video clips and a bag of video frames. Clips and frames represent video content at different time scales. We propose a Multi-Scale Similarity Learning (MS-SL) network that jointly learns clip-scale and frame-scale similarities for PRVR. Extensive experiments on three datasets (TVR, ActivityNet Captions, and Charades-STA) demonstrate the viability of the proposed method. We also show that our method can be used for improving video corpus moment retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advent of the big data era, millions of videos are uploaded to the Internet every day. There is an increasing need of retrieving videos from the big data. As common users prefer to express their information need by natural-language queries, research on text-tovideo retrieval (T2VR) is important <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">60]</ref>. Given a query in the form of a natural language sentence, T2VR asks to retrieve videos that are semantically relevant to the given query from a gallery of videos. Current methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> for T2VR are trained and tested on video-captioning oriented datasets such as MSVD <ref type="bibr" target="#b3">[4]</ref>, MSRVTT <ref type="bibr" target="#b58">[59]</ref> and VATEX <ref type="bibr" target="#b53">[54]</ref>. A key property of these datasets is that videos are assumed to be temporally pre-trimmed with short duration, whilst the provided captions well describe the gist of the video content. Consequently, for a given paired video and caption, the video is supposed to be fully relevant to the caption. In reality, however, as queries are not known a priori, pre-trimmed video clips may not contain sufficient content to fully meet the query. This suggests a gap between the literature and the real world.</p><p>To fill the above gap, we propose in this paper a novel T2VR subtask termed Partially Relevant Video Retrieval (PRVR). An untrimmed video is considered to be partially relevant w.r.t. a given textual query as long as the video contains a (short) moment relevant w.r.t. the query, see <ref type="figure">Fig. 1</ref>. PRVR aims to retrieve such partially relevant videos from a large collection of untrimmed videos. Because where the relevant moment is localized and how long it lasts are both unknown (see <ref type="figure" target="#fig_1">Fig. 2</ref>), PRVR is more challenging than the conventional T2VR task.</p><p>Observing the connection between PRVR and Multiple Instance Learning (MIL) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref> at a high level, we tackle the new task by a multi-scale MIL approach. In the current context, a video is simultaneously viewed as a bag of video clips and a bag of video frames. Clips and frames represent video content at different temporal scales, which is helpful for handling moments of varying temporal lengths. Besides, based on the multi-scale video representation, we propose Multi-Scale Similarity Learning (MS-SL) network. In MS-SL, we regard the clip-scale as the coarse temporal granularity as it is typically of longer duration. Besides, the frame-scale is regraded as the fine-grained temporal granularity, as frames usually reflect more detailed content of videos. The multi-scale similarity learning consists of a clip-scale SL branch built on clip-scale video representation and a frame-scale SL branch built on frame-scale video representation. They are jointly learned in a coarse-to-fine manner. Note that the two similarity learning branches are not independent. In clip-scale SL, a key clip that is most likely to be relevant to the query will be detected. Then the clip-scale similarity is computed as the similarity between the key clip and the query. Additionally, the key clip is regarded as a guide for frame-scale SL to measure the importance of each frame in a fine-grained temporal scale. The frame-scale similarity is computed as the similarity between the weighted frames with the query. Finally, the clip-scale similarity and the frame-scale similarity are jointly used to measure the final video-text similarity.</p><p>It is worth noting that PRVR differs from Single Video Moment Retrieval (SVMR) <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b68">69]</ref> and Video Corpus Moment Retrieval (VCMR) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b67">68]</ref>, as the latter two are to retrieve moments rather than untrimmed videos. Additionally, although our model  is proposed for PRVR, it can also be used for improving VCMR. In sum, our main contributions are as follows:</p><p>? We propose a new T2VR subtask named PRVR, where an untrimmed video is considered to be partially relevant with respect to a given textual query if it contains a moment relevant to the query. PRVR aims to retrieve such partially relevant videos from a large collection of untrimmed videos.</p><p>? We formulate the PRVR subtask as a MIL problem, simultaneously viewing a video as a bag of video clips and a bag of video frames. Clips and frames represent video content at different temporal scales. Based on multi-scale video representation, we propose MS-SL to compute the relevance between videos and queries in a coarse-to-fine manner.</p><p>? Extensive experiments on three datasets (TVR <ref type="bibr" target="#b28">[29]</ref>, ActivityNet Captions <ref type="bibr" target="#b27">[28]</ref>, and Charades-STA <ref type="bibr" target="#b18">[19]</ref>) demonstrate the viability of the proposed method for PRVR. We also show that our method can be used for improving video corpus moment retrieval. Source code and datasets are available at http://danieljf24.github.io/prvr 2 RELATED WORK T2VR. The T2VR task has gained much attention in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60]</ref>, aiming to retrieve relevant videos by a given query from a set of pre-trimmed video clips. The retrieved clips are supposed to be fully relevant to the given query. A common solution for T2VR is to first encode videos and textual queries and then map them into common embedding spaces where the crossmodal similarity is measured. Therefore, current works mainly focus on video encoding <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref>, sentence encoding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>, and their cross-modal similarity learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b62">63]</ref>. Different from the above works, we consider a more realistic scenario, where videos are supposed to be partially relevant to a specific query. We thus focus more on how to measure the partial relevance between textual queries and videos. VMR. The VMR task is to retrieve moments semantically relevant to the given query from a given single untrimmed video or a large collection of untrimmed videos. The former is known as SVMR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70]</ref>, and the latter is known as VCMR <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b65">66]</ref>. In SVMR, existing methods mainly concentrate on how to precisely localize temporal boundings of target moments, and could be typically classified as proposal-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b66">67]</ref> and proposal-free methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>Proposal-based methods first generate multiple moment proposals, then match them with a query to determine the most relevant one from the proposals. Without generating moment proposals, proposal-free methods predict the start and end time points of the target moment based on the fused video-query feature. As the extension of SVMR, the VCMR task is to retrieve moments (or video segments) that are semantically relevant w.r.t. a given query from a collection of untrimmed videos. The state-of-the-art methods (e.g., ReLoCLNet <ref type="bibr" target="#b67">[68]</ref> and XML <ref type="bibr" target="#b28">[29]</ref>) for VCMR have a two-stage workflow. The first stage is to retrieve a number of candidate videos which may contain the target moment, while the second stage is to retrieve moments from the candidate videos.</p><p>Different from video moment retrieval aiming to retrieve moments, our proposed PRVR task aims to retrieve untrimmed videos. Besides, while PRVR is similar to VCMR's first stage yet requires no moment-level annotations as commonly needed for VCMR. Therefore, a method for PRVR can in principle be used to improve a two-stage method for VCMR, and our proposed model is designed for PRVR, it can also be used for improving VCMR.</p><p>MIL. MIL <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref> is a classical framework for learning from weakly annotated data, and widely used for classification tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>. In MIL, a sample is defined as a bag of multiple instances, and there is only a label associated with the bag instead of the instance. Besides, a bag is positive if the bag contains at least one positive instance and negative if it contains no such positive instance. Existing MIL methods could be roughly grouped into instance-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref> and embedding-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref>. The former typically predicts a score of each instance in the bag and aggregates them to generate a bag score. The latter usually aggregates embedding of all instances into a bag embedding, then outputs a bag score based on the bag embedding. In this work, we formulate the PRVR task as a MIL problem. Different from the previous MIL works that usually regrade a sample as a specific bag of instances, in this work a video is simultaneously viewed as a bag of video clips and a bag of video frames. Moreover, we employ MIL for the retrieval task instead of the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD</head><p>We formulate PRVR as a MIL problem. As moments relevant to queries typically show large variations in their temporal lengths, we devise multi-scale video representation to represent videos at multiple temporal scales, obtaining a bag of video clips of varying lengths and a bag of video frames. Based on the two bags, we further propose multi-scale similarity learning to measure the partial queryvideo relevance, see <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation of PRVR</head><p>Given a natural language query, the task of PRVR aims to retrieve videos containing a moment that is semantically relevant to the given query, from a large corpus of untrimmed videos. As the moment referred to by the query is typically a small part of a video, we argue that the query is partially relevant to the video. It is worth pointing out that PRVR is different from conventional T2V retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>, where videos are pre-trimmed and much shorter, and queries are usually fully relevant to the whole video. To build a PRVR model, a set of untrimmed videos are given for training, where each video is associated with multiple natural language sentences. Each sentence describes the content of a specific moment in the corresponding video. Note that we do not have access to the start/end time points of the moments (moment annotations) referred to by the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Representation</head><p>For sentence representation, we adopt the method by Lei et al. <ref type="bibr" target="#b28">[29]</ref>, considering its good performance on VCMR. Specifically, given a sentence consisting of words, a pre-trained RoBERTa <ref type="bibr" target="#b37">[38]</ref> is firstly employed to extract word features. Then a fully connected (FC) layer with a ReLU activation is utilized to map the word features into a lower-dimensional space. After adding the learned positional embedding to the mapped features, a standard Transformer layer <ref type="bibr" target="#b50">[51]</ref> is further employed to obtain a sequence of -dimensional contextualized word feature vectors = { } =1 ? R ? . In the Transformer, the features are fed to a multi-head attention layer followed by a feed-forward layer, and both layers are connected with residual connection <ref type="bibr" target="#b22">[23]</ref> and layer normalization <ref type="bibr" target="#b1">[2]</ref>. Finally, a sentence-level representation ? R is obtained by employing a simple attention on :</p><formula xml:id="formula_0">= ?? =1 ? , = ( T ),<label>(1)</label></formula><p>where denotes softmax layer, ? R ?1 is trainable vector, and ? R 1? indicates the attention vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Scale Video Representation</head><p>Given an untrimmed video, we first represent it by a sequence of -dimensional feature vectors ? R ? , where denotes the number of the vectors. The feature sequence is obtained by extracting frame-level features using a pre-trained 2D CNN, or extracting segment-level features using a pre-trained 3D CNN. For the ease of description, we regard as a sequence of frame-level features in the following. Based on , we construct multi-scale video representation, jointly using a clip-scale feature learning branch and a frame-scale feature learning branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Clip-scale video representation.</head><p>Before constructing video clips, we first downsample the input in the temporal domain to reduce the length of the feature sequence, which helps reduce the computational complexity of the model. Specifically, given a sequence of frame feature vectors as the input, we downsample them into a fixed number of feature vectors, where each feature vector is obtained by mean pooling over the corresponding multiple consecutive frame features. Then, the video is described by a sequence of new feature vectors ? R ? , where indicates the number of the corresponding feature vectors. In order to make the features more compact, we employ an FC layer with a ReLU activation. Moreover, we also use standard Transformer <ref type="bibr" target="#b50">[51]</ref> with a learned positional embedding to improve the temporal dependency of the features. Formally, through an FC layer and a one-layer Transformer, we obtain ? ? R ? :</p><formula xml:id="formula_1">? = { 1 , 2 , ..., } = ( ( ) + )<label>(2)</label></formula><p>where denotes the output of the positional embedding. The reduced feature array ? is further used for clip-scale video representation learning.</p><p>For clip construction, we employ a multi-scale sliding window strategy to generate video clips, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Note that different from previous work <ref type="bibr" target="#b65">[66]</ref> where video clips are of equal length and non-overlapping, our video clips are of varied lengths and overlapping. Concretely, we apply sliding windows of different sizes over ? along its temporal dimension with a stride of 1. Given a sliding window of size , a clip feature is obtained by mean pooling over the features within the given window. The resultant feature sequence is denoted as ? . Consequently, by jointly employing sliding windows of varied sizes as {1, 2, ..., }, we are able to obtain</p><formula xml:id="formula_2">{? 1 , ? 2 , ..., ? }.</formula><p>Putting them together, a video can be represented a sequence of video clips ? R ? :</p><formula xml:id="formula_3">= {? 1 , ? 2 , ..., ? } = { 1 , 2 , ..., },<label>(3)</label></formula><p>where ? R denotes the feature representation of -th clip, is the number of all generated clips which meets = ( + 1)/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Frame-scale video representation.</head><p>As the initial frame features are extracted independently, they naturally lack temporal dependency. To bring such dependency back, we again utilize Transformers. Specifically, given the frame feature sequence , we first utilize an FC layer with ReLU activation to reduce the dimensionality of the input, followed by a standard Transformer with a positional embedding layer. The re-encoded frame features, denoted ? R ? , are computed as:</p><formula xml:id="formula_4">= { 1 , 2 , ..., } = ( ( ) + ).<label>(4)</label></formula><p>Note that the network structures of Transformer, FC and PE are the same as that in the clip-scale branch, but their trainable parameters are not shared. This allows each branch to learn parameters suitable for their own scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-Scale Similarity Learning</head><p>As we have no priori about where the relevant content is localized in PRVR, it is challenging to directly compute video-text similarity on a fine-grained scale. Here, we propose multi-scale similarity learning, which computes the similarity in a coarse-to-fine manner. It first detects a key clip that is most likely to be relevant to the query. Then, the importance of each frame is measured in a finegrained temporal scale under the guidance of the key clip. The final similarity is computed by jointly considering the query's similarities with the key clip and the frames. The hypothesis here is that if one model briefly knows coarse relevant content with respect to the query, it will help the model to find more relevant content on a more fine-grained scale accurately. The framework of the multiscale similarity learning is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.1</head><p>Clip-scale Similarity. Given a video as a sequence of video clips, we first measure the cross-modal similarity of each video clip with the query, and then aggregate the instance similarities to obtain the clip-scale similarity. Specifically, given a sequence of video clips = { 1 , 2 , ..., }, we use cosine similarity between each instance representation and the query representation, followed by a max-pooling operator on the similarities. More formally, the clip-scale similarity is obtained as:</p><formula xml:id="formula_5">( , ) = max{ ( 1 , ), ( 2 , ), ..., ( , )},<label>(5)</label></formula><p>where (?) denotes the cosine similarity function. The max-pooling determines the clip with the highest similarity, and we then utilize its similarity with the query as the whole video's similarity with the query. Besides, we select this clip as key clip, which is used for the latter frame-scale similarity learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Frame-scale Similarity.</head><p>To obtain the frame-scale similarity, we first aggregate a sequence of frame feature vectors to a feature vector under the guidance of the key clip obtained in Section 3.4.1, and then compute its similarity with the query as the frame-scale similarity. Specifically, given a sequence of video frames = { 1 , 2 , ..., }, we devise a Key Clip Guided Attention (KCGA) to aggregate frame features. The implementation of KCGA borrows the idea of multi-head self-attention (MHSA) mechanism in Transformer <ref type="bibr" target="#b50">[51]</ref>. MHSA first projects the input into queries, keys, values, and then computes the output as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Different from MHSA utilizes the same input to construct queries, keys, and values, here we take the feature vector of the key clip as the query, and the video frame features as keys and values. Formally, the aggregated frame feature vector is obtained as:</p><formula xml:id="formula_6">= (?T ) T , = , = ,<label>(6)</label></formula><p>where?? R ?1 indicates the feature vector of the key clip, ? R ? and ? R ? are two trainable projection matrices. The dot product measures the similarity between frames and the key clip, resulting in larger values for frames that are more similar to the key clip. Therefore, frames that are more similar to the key clip will have greater attention weights.</p><p>Finally, the frame-scale similarity is measured as the cosine similarity between the aggregated frame feature vector and query feature vector , namely:</p><formula xml:id="formula_7">( , ) = ( , ).<label>(7)</label></formula><p>3.4.3 Similarity Learning. In this section, we first introduce the definition of positive and negative pairs for similarity learning. Inspired by MIL <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>, we define that a query and video pair is positive if the video contains certain content that is relevant to the query, and negative if no relevant content in the video. Based on the above definition, we jointly use the triplet ranking loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> and InfoNCE loss <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b67">68]</ref> that are widely used in retrieve related tasks, and found them complementary. Given a positive video-query pair ( , ), the triplet ranking loss over the mini-batch B is defined as:</p><formula xml:id="formula_8">L = 1 ?? ( , ) ?B [ (0, + ( ? , ) ? ( , )) + (0, + ( , ? ) ? ( , ))],<label>(8)</label></formula><p>where is the margin constant, (?) denotes the similarity function which we can use the clip-scale similarity (?) or the frame-scale similarity (?) . Besides, ? and ? respectively indicate a negative sentence sample for and a negative video sample for . The negative samples are randomly sampled from the mini-batch at the beginning of the training, while being the hardest negative samples after 20 epochs.</p><p>Given a positive video-query pair ( , ), the infoNCE loss over the mini-batch B is computed as:</p><formula xml:id="formula_9">L = ? 1 ?? ( , ) ?B ( , ) ( , ) + ? ?N ( ? , ) + ( , ) ( , ) + ? ?N ( , ? ) ,<label>(9)</label></formula><p>where N denotes all negative queries of the video in the minibatch, while N denotes all negative videos of the query in the mini-batch.</p><p>As the previous work <ref type="bibr" target="#b31">[32]</ref> has concluded that using one loss per similarity function performs better than using one loss on the summation of multiple similarities, we employ the above two losses on both clip-scale similarity and frame-scale similarity, instead of their sum. Finally, our model is trained by minimizing the following overall training loss:</p><formula xml:id="formula_10">L = L + L + 1 L + 2 L ,<label>(10)</label></formula><p>where L and L denote the triplet ranking loss using the clipscale similarity (?) and frame-scale similarity (?) respectively, and accordingly for L and L . 1 and 2 are hyper-parameters to balance the contribution of infoNCE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Inference</head><p>After the model has been trained, the similarity between a video and a sentence query is computed as the sum of their clip-level similarity and frame-level similarity, namely:</p><formula xml:id="formula_11">( , ) = ( , ) + (1 ? ) ( , )<label>(11)</label></formula><p>where is a hyper-parameter to balance the importance of two similarities, ranging within [0, 1]. Given a query, we sort all videos from the video gallery in descending order according to their similarity with respect to the given video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Setup</head><p>4.1.1 Datasets. In order to verify the viability of our proposed model for PRVR, queries that are partially relevant to videos are required. As videos in popular T2VR datasets such as MSR-VTT <ref type="bibr" target="#b58">[59]</ref>, MSVD <ref type="bibr" target="#b3">[4]</ref> and VATEX <ref type="bibr" target="#b53">[54]</ref> are supposed to be fully relevant to the queries, they are not suited for our experiments. Here, we re-purpose three datasets commonly used for VCMR, i.e., TVR <ref type="bibr" target="#b28">[29]</ref>, Activitynet Captions <ref type="bibr" target="#b27">[28]</ref>, and Charades-STA <ref type="bibr" target="#b18">[19]</ref>, considering their natural language queries partially relevant with the corresponding videos (a query is typically associated with a specific moment in a video). <ref type="table" target="#tab_0">Table 1</ref> summarizes the brief statistics of these datasets, including average lengths of moments and videos, and the average moment length proportion in the whole video (moment-to-video ratio). Note that as we focus on retrieving videos, moment annotations provided by these datasets are not used in our proposed new PRVR task. TV show Retrieval (TVR) <ref type="bibr" target="#b28">[29]</ref> is a multimodal dataset originally for video corpus moment retrieval, where videos are paired with subtitles that are generated by automatic speech recognition. It contains 21.8K videos collected from 6 TV shows, and each video is associated with 5 natural language sentences that describe a specific moment in the video. As a moment is typically a part of a video, we assume that sentences are partially relevant to videos, and use them to evaluate our model. Following <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b67">68]</ref>, we utilize 17,435 videos with 87,175 moments for training and 2,179 videos with 10,895 moments for testing. ActivityNet Captions <ref type="bibr" target="#b27">[28]</ref> is originally developed for dense video captioning task, and is now a popular dataset for single video moment retrieval. It contains around 20K videos from Youtube, and the average length of videos is the largest among the three datasets we used. On average, each video has around 3.7 moments with corresponding sentence descriptions.We use the popular data partition used in <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>Charades-STA <ref type="bibr" target="#b18">[19]</ref> is a dataset for single video moment retrieval. It contains 6,670 videos with 16,128 sentence descriptions. Each video has around 2.4 moments with corresponding sentence descriptions on average. We utilize the official data partition for model training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics.</head><p>To evaluate PRVR models, we utilize the rank-based metrics, namely @ ( = 1, 5, 10, 100), which are commonly used for the conventional text-to-video retrieval <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53]</ref>. @ is the fraction of queries that correctly retrieve desired items in the top of the ranking list. The performance is reported in percentage (%). Higher @ means better performance. For overall comparison, we also report the Sum of all Recalls (SumR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details.</head><p>We use PyTorch as our deep learning environment, and we will release our source code. For video feature, on TVR, we utilize the feature provided by <ref type="bibr" target="#b28">[29]</ref>, that is, 3,072-D visual feature obtained by the concatenation of frame-level ResNet152 <ref type="bibr" target="#b22">[23]</ref> feature and segment-level I3D <ref type="bibr" target="#b2">[3]</ref> feature. For ease of reference, we refer to it as ResNet152-I3D. On ActivityNet-Captions and Charades-STA, we only utilize the same I3D feature, which are respectively provided by <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b42">[43]</ref>. For sentence feature, we use the 768-D RoBERTa feature provided by <ref type="bibr" target="#b28">[29]</ref> on TVR, where RoBERTa is finetuned on the queries and subtitle sentences of TVR. On ActivityNet-Captions and Charades-STA, we use the 1,024-D RoBERTa feature extracted by ourselves using the open RoBERTa toolkit 1 . Due to the limited space of the paper, we present more detailed implementation details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Baseline Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baseline selection.</head><p>As models specifically designed for PRVR are non-existing, we compare with models targeted at conventional T2VR and models developed for VCMR. Given the rich literature, 1 https://pytorch.org/hub/pytorch_fairseq_roberta/ we have to be selective, choosing open-source models for fair and reproducible comparison. In particular, we choose the following nine T2VR models, i.e., VSE++ <ref type="bibr" target="#b14">[15]</ref>, W2VV <ref type="bibr" target="#b9">[10]</ref>, CE <ref type="bibr" target="#b36">[37]</ref>, W2VV++ <ref type="bibr" target="#b30">[31]</ref>, DE <ref type="bibr" target="#b10">[11]</ref>, HTM <ref type="bibr" target="#b41">[42]</ref>, HGR <ref type="bibr" target="#b6">[7]</ref>, DE++ <ref type="bibr" target="#b11">[12]</ref> and RIVRL <ref type="bibr" target="#b12">[13]</ref>, and the following two VCMR models, i.e., XML <ref type="bibr" target="#b28">[29]</ref> and ReLoCLNet <ref type="bibr" target="#b67">[68]</ref>. Both XML and ReLoCLNet are two-stage, where a first-stage module is used to retrieve candidate videos followed by a second-stage module to localize specific moments in the candidate videos. As moment annotations are unavailable for PRVR, we have re-trained XML and ReLoCLNet (with their moment localization modules removed) using the same video features as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Results on the TVR dataset. <ref type="table" target="#tab_1">Table 2</ref> summarizes the performance comparison on TVR. Our proposed model consistently outperforms all conventional T2VR models with a clear margin. Even the best performing model RIVRL among the T2VR models, our model outperforms it by 36.8 in terms of SumR. As these models focus on the whole similarity between videos and queries, the results allow us to conclude that such similarity modeling is sub-optimal for PRVR. For the models of the second group, i.e., ReLoCLNet and XML, they perform better than the conventional T2VR models, but they are still worse than ours. ReLoCLNet and XML focus on retrieving moment, which to some extent model the partial relevance, but they compute the similarity only in terms of a specific scale. By contrast, we compute the similarity in terms of both clip scale and frame scale. The results demonstrate the effectiveness of our proposed multi-scale similarity learning for PRVR. Note that when using the extra subtitle feature provided by <ref type="bibr" target="#b28">[29]</ref>, our model obtains better performance (R@1 of 24.0 and SumR of 220.8).</p><p>To gain a further understanding of the individual models, we define moment-to-video ratio (M/V) for query, which is measured by its corresponding moment's length ratio in the entire video. The smaller M/V indicates less relevant content while more irrelevant content with respect to the query. Besides, the smaller M/V to some extent means a lower relevance of a query to its corresponding  <ref type="table" target="#tab_0">SumR   W2VV,TMM18  HGR,CVPR20   HTM,ICCV19  CE,BMVC19   W2VV++,MM19  VSE++,BMVC19   DE,CVPR19  DE++,TPAMI21</ref> RIVRL,TCSVT22 XML,ECCV20</p><p>ReLoCLNet,SIGIR21 Ours <ref type="figure">Figure 5</ref>: Performance of different models on different types of queries. Queries are grouped according to their M/V.  video, while the larger one indicates a higher relevance. According to M/V, queries can be automatically classified into different groups, which enables a fine-grained analysis of how a specific model responds to the different types of queries. On TVR, the 10,895 test queries are split according to their M/V into six groups, with the performance of each group shown in <ref type="figure">Fig. 5</ref>. Unsurprisingly, our model consistently performs the best in all groups. Observing the figure from left to right, the average performance of the twelve compared models increases along with the M/V, from 106.8, 114.2 114.3, 118.6, 125.8 and 127.7. The performance in the group with the lowest M/V is the smallest, while the group with the highest M/V is the largest. The result allows us to conclude that the current video retrieval baseline models better address queries of larger relevance to the corresponding video. By contrast, the performance we achieved is more balanced in all groups. This result shows that our proposed model is less sensitive to irrelevant content in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results on Activitynet</head><p>Captions and Charades-STA. The performance of different models on Activitynet Captions and Charades-STA are summarized in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref>, respectively. On both datasets, our model is still at the leading position.The results again verify the effectiveness of our model for measuring partial relevance between videos and queries. Interestingly, we observe that HTM performs badly on TVR and Activitynet Captions, while on Charades-STA it achieves the best SumR score among the T2VR models. We speculate it is due to the fact that Charades-STA has the least training data among the three datasets. Besides, the model structure of HTM is very simple, respectively using an FC layer with gating mechanism to embed videos and sentences into a common space, showing an advantage of training on small-scale data. For our proposed model, it consistently performs the best on the three datasets of the varying number of training samples, which to some extent shows that our model is not sensitive to the scale of training data. <ref type="table" target="#tab_4">Table 5</ref> summarizes the model complexity comparison in terms of the time complexity and memory consumption. For a specific method, its time complexity is measured as FLOPs it takes to encode a given video-text pair. In terms of FLOPs, our model is at the mid-level, slightly slower than XML and ReLoCLNet, yet faster than RIVRL, DE and HGR. In terms of memory consumption, our model requires more memory than the majority of compared models, which is mainly due to the usage of transformer and multi-scale video representations. However, we found that our model takes about 0.2 seconds to retrieve videos from 20,000 candidate untrimmed videos, given that the video embeddings are pre-computed. The retrieval speed is adequate for instant response. Top-K videos for moment retrieval  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison on Model Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PRVR for VCMR</head><p>Our PRVR model can also be used in the first stage of VCMR.To that end, we replace the first stage of two VCMR models, i.e., XML <ref type="bibr" target="#b28">[29]</ref> and ReLoCLNet <ref type="bibr" target="#b67">[68]</ref>, with our model. Both visual and subtitle features are used for video representation. <ref type="figure">Fig. 6</ref> shows the performance of the original models and the replaced ones on the TVR dataset. Here, we report SumR, the sum of R1/R5/R10/R100. Replacing the first stage with our model improves both XML and ReLoCLNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.1</head><p>The effectiveness of multi-scale branches. To examine the usefulness of the multi-scale branches, we compare the counterpart without the clip-scale branch or the frame-scale branch. As shown in <ref type="table" target="#tab_5">Table 6</ref>, removing any branch results in clear performance degeneration. The result not only demonstrates the effectiveness of the multi-scale solution, but also shows the complementary of the clip-scale and the frame-scale branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.2</head><p>The effectiveness of key clip guided attention. Additionally, we also compare the model w/o key clip guide, which is implemented by replacing key clip guided attention with a simple attention. The simple attention is implemented as Eq. 1 without any guide. As <ref type="table" target="#tab_5">Table 6</ref> shows, our model with the full setup still performs better, which shows the importance of key clip guided attention for PRVR.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.3</head><p>The effectiveness of the combination of triplet ranking loss and InfoNCE loss. To validate the choice of joint use of the two losses, we compare the results of using either triplet ranking loss or infoNCE loss. As shown in <ref type="table" target="#tab_5">Table 6</ref>, triplet ranking loss and InfoNCE give comparable results when used alone, but they are much worse than the model with the full setup of jointly using both. The result demonstrates the benefit of using these two losses jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.4</head><p>The effect of on the retrieval performance. The influence of the hyper-parameter in Eq. 11 is studied as follows. We try with its value ranging from 0.1 to 0.9 with an interval of 0.1. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, when the is larger than 0.3, the performance of using multi-scale similarity are all over 170, which consistently outperform the counterparts using the frame-scale or the clip-scale similarity alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we have proposed a novel T2VR subtask termed PRVR. Different from the conventional T2VR where a query is usually fully relevant to the corresponding video, it is typically partially relevant in PRVR. Besides, videos in the conventional T2VR are temporally pre-trimmed with short durations, while videos are untrimmed in PRVR and a video is typically partially relevant to multiple sentences of different semantics. Additionally, PRVR differs from SVMR and VCMR, as the latter two are to retrieve moments rather than untrimmed videos. Towards PRVR, we have formulated it as a MIL problem, and propose MS-SL which computes the similarity on both clip scale and frame scale in a coarse-to-fine manner. Extensive experiments on three datasets have verified the effectiveness of MS-SL for PRVR, and have shown that it can also be used for improving VCMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>We report more experimental results and more technical details which are not included in the paper due to space limit:</p><p>? More comparisons on the TVR dataset, including extra ablation studies to explore the significance of the transformer module, comparing with models using extra subtitles and comparing with conventional T2VR models using clips (Section A.1). ? Performance comparison on pre-trimmed video datasets (Section A.2). ? Distribution of moment-to-video ratios on Charades-STA (Section A.3.1). ? More technical details of our method (Section A.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 More Experiments on TVR</head><p>A.1.1 Ablations on the usage of the transformer. Among the compared methods on the TVR dataset, the top three ranked methods (ReLoCLNet, XML and RIVRL) use Transformer modules: ReLo-CLNet and XML utilize Transformers for text and video representation, while RIVRL uses a Transformer for video representation. So we conduct extra ablation study to explore whether is the usage of the Transformer module mainly contributes the significant performance improvement of our method. We replace all the three Transformer modules in our model with 1D-CNN, bi-GRU, bi-LSTM, respectively. As shown in the <ref type="table" target="#tab_6">Table 7</ref>, our model in the Transformerfree setup obtains SumR of 161.6, 170.2 and 162.3, respectively <ref type="bibr" target="#b28">[29]</ref> dataset. While their performance is worse than the counterpart using Transformer, they are still better than the best baseline, i.e., , ReLoCLNet with SumR of 157.1.</p><p>A.1.2 Comparison with models using extra subtitles. As the TVR dataset is a multimodal dataset where each video is additionally associated with subtitle (dialogue) texts, in this experiment we compare with models using extra subtitles. Here, we do not compare with conventional T2VR models, as they do not support using extra subtitles. We compare with XML <ref type="bibr" target="#b28">[29]</ref> and ReLoCLNet <ref type="bibr" target="#b67">[68]</ref>, and use the same 768-D subtitles features provided by <ref type="bibr" target="#b28">[29]</ref> as extra video   <ref type="table" target="#tab_7">Table 8</ref>, and our proposed model again performs the best.</p><p>A.1.3 Comparison with conventional T2VR models using clips. As the conventional T2VR models are typically designed for retrieving video clips, in this experiment we employ conventional T2VR models using clips to explore their potential for PRVR. Specifically, during the inference, we first split videos into multiple clips, and then compute the similarity of each clip with the query. The maximum similarity is regarded as the final similarity between the video and the query.  Besides, we adopt two strategies to generate clips from video, i.e., content-agnostic strategy and content-aware strategy. The content-agnostic strategy first splits the video into video units evenly, then constructs video clips by using a specific video unit or concatenating adjacent video units. The larger means generating more video clips, and = 1 indicates using the whole video for inference. The content-aware strategy generates video clips by a scene detector toolkit 2 with an own provided threshold , which automatically splits the video into individual clips according to their content change. The smaller threshold means generating more video clips.</p><p>We conduct experiments with top-4 performing T2VR models on TVR, i.e., VSE++, DE, DE++, RIVRL, and the results are shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. As shown in 8 (a), all the models achieve performance gains when is large than 1, and obtain the best performance when = 3. Recall that = 1 indicates using the whole video for inference. The results allow us to conclude that the T2VR models could be improved by splitting the video into multiple clips with the content-agnostic strategy. However, their performance is still much worse than our proposed model which achieves a SumR score of 172.4 on TVR. <ref type="figure" target="#fig_9">Fig. 8 (b)</ref> shows the results when the content-aware strategy is used. Note that = indicates using the whole video without splitting. We found that splitting the video into multiple clips by the content-aware strategy result in relative performance degeneration, which shows the scene detector is not suitable for PRVR. Besides, we speculate it is due to the content in a moment may has scene changes, and the scene detector is likely to split a moment into multiple parts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results on Pre-trimmed Datasets</head><p>Although our proposed model is designed for untrimmed video, it can also be utilized for retrieving pre-trimmed by text. Therefore, we conduct experiments on MSR-VTT <ref type="bibr" target="#b58">[59]</ref> and MSVD <ref type="bibr" target="#b3">[4]</ref>, two commonly used pre-trimmed datasets for T2VR. For MSR-VTT, we follow the official partition, where 6513 video clips for training, 497 video clips for validation and the remaining 2,990 video clips for testing. For MSVD, we also follow the official partition, 1200 video clips are used for training, 100 video clips for validation and 670 video clips for testing Following the previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref>, we use the concatenation of 2048-dim ResNeXt-101 and 2048-dim ResNet-152 features as the video feature. For text representation, we use the open RoBERTa toolkit to extract 1,024-D sentence feature. The results are shown in the <ref type="table" target="#tab_8">Table 9</ref>, where all the methods use the same video feature. Note that not all the compared methods report their performance on both datasets. As expected, our model is not on par with the state-of-the-art models on the two pre-trimmed datasets. Recall that the rationale for our proposed model is to first detect a key clip that is most likely to be relevant to the query and then measure the importance of other frames in a fine-grained temporal scale under the guidance of the key clip. As for pre-trimmed videos in MSR-VTT and MSVD, the majority of their frames are actually relevant w.r.t. the associated descriptions, making key clip detection unnecessary. Our method is thus suboptimal for text-to-video retrieval on MSR-VTT and MSVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Others</head><p>A.3.1 Distribution of Moment-to-Video Ratios. <ref type="figure" target="#fig_10">Fig. 9</ref> shows the distribution of moment-to-video ratio on Charades-STA. Momentto-video ratio indicates the moment's length ratio in the entire video. Moments on Charades-STA show a large variance in their temporal lengths. A.3.2 More Implementation Details. For the video representation module, we set the fixed number to 32 in the downsampling strategy. Besides, we set the maxmium frame number to 128. Once the number of frame is over , it will be downsampled to . For sentences, we set the maximum length of query to 30 on TVR and Charades-STA, 64 on ActivityNet Captions, and the words outside the maximum length are simply discarded. For the Transformer module used in our model, we set its hidden size = 384, and 4 attention heads are employed. For hyper-parameters in the loss functions, we empirically set 1 =0.02 and 2 =0.04 to make all loss elements have a similar loss value at the beginning of the model training. For model training, we utilize an Adam optimizer with a mini-batch size of 128. The initial learning rate is set to 0.00025, and we take a learning rate adjustment schedule similar to <ref type="bibr" target="#b28">[29]</ref>. Early stop occurs if the validation performance does not improve in ten consecutive epochs. The maximal number of epochs is set to 100. Note that we will release our source code and data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of moment-to-video ratio on (a) TVR and (b) ActivityNet Captions. Moment-to-video ratio indicates the moment's length ratio in the entire video. Moments show a large variance in their temporal lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The framework of our proposed model for partially relevant video retrieval. # denotes a temporal sliding window of size with a stride of 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The illustration of multi-scale similarity learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 Figure 6 :</head><label>76</label><figDesc>Performance of XML and ReLoCLNet without/with our model as the first stage for VCMR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The influence of the hyper-parameter in Eq. 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Performance of four conventional T2VR models using clips generated by (a) content-agnostic strategy and (b) content-aware strategy on the TVR dataset. Their performance is still much worse than our proposed model which achieves a SumR score of 172.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Distribution of moment-to-video ratio on Charades-STA. Moment-to-video ratio indicates the moment's length ratio in the entire video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Brief statistics of three public datasets used in our experiments. The length is measured in seconds.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Average length moments videos</cell><cell cols="2">Moment-to-video ratio min max mean</cell></row><row><cell>TVR</cell><cell>9.1</cell><cell>76.2</cell><cell>0.48% 100%</cell><cell>11.9%</cell></row><row><cell>Activitynet Captions</cell><cell>36.2</cell><cell>117.6</cell><cell>0.48% 100%</cell><cell>30.8%</cell></row><row><cell>Charades-STA</cell><cell>8.1</cell><cell>30.0</cell><cell>4.3% 100%</cell><cell>26.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of PRVR on the TVR dataset. Models are sorted in ascending order in terms of their overall performance. Visual feature: ResNet152-I3D.</figDesc><table><row><cell>Model</cell><cell cols="4">R@1 R@5 R@10 R100</cell><cell>SumR</cell></row><row><cell>T2VR models:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2VV, TMM18 [10]</cell><cell>2.6</cell><cell>5.6</cell><cell>7.5</cell><cell>20.6</cell><cell>36.3</cell></row><row><cell>HGR, CVPR20 [7]</cell><cell>1.7</cell><cell>4.9</cell><cell>8.3</cell><cell>35.2</cell><cell>50.1</cell></row><row><cell>HTM, ICCV19 [42]</cell><cell>3.8</cell><cell>12.0</cell><cell>19.1</cell><cell>63.2</cell><cell>98.2</cell></row><row><cell>CE, BMVC19 [37]</cell><cell>3.7</cell><cell>12.8</cell><cell>20.1</cell><cell>64.5</cell><cell>101.1</cell></row><row><cell>W2VV++, MM19 [31]</cell><cell>5.0</cell><cell>14.7</cell><cell>21.7</cell><cell>61.8</cell><cell>103.2</cell></row><row><cell>VSE++, BMVC19 [15]</cell><cell>7.5</cell><cell>19.9</cell><cell>27.7</cell><cell>66.0</cell><cell>121.1</cell></row><row><cell>DE, CVPR19 [11]</cell><cell>7.6</cell><cell>20.1</cell><cell>28.1</cell><cell>67.6</cell><cell>123.4</cell></row><row><cell>DE++, TPAMI21 [12]</cell><cell>8.8</cell><cell>21.9</cell><cell>30.2</cell><cell>67.4</cell><cell>128.3</cell></row><row><cell>RIVRL, TCSVT22 [13]</cell><cell>9.4</cell><cell>23.4</cell><cell>32.2</cell><cell>70.6</cell><cell>135.6</cell></row><row><cell cols="3">VCMR models w/o moment localization:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XML, ECCV20 [29]</cell><cell>10.0</cell><cell>26.5</cell><cell>37.3</cell><cell>81.3</cell><cell>155.1</cell></row><row><cell>ReLoCLNet, SIGIR21[68]</cell><cell>10.7</cell><cell>28.1</cell><cell>38.1</cell><cell>80.3</cell><cell>157.1</cell></row><row><cell>Ours</cell><cell>13.5</cell><cell>32.1</cell><cell>43.4</cell><cell>83.4</cell><cell>172.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of PRVR on the ActivityNet Captions dataset. Visual feature: I3D.</figDesc><table><row><cell>Model</cell><cell cols="4">R@1 R@5 R@10 R100</cell><cell>SumR</cell></row><row><cell>T2VR models:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2VV [10]</cell><cell>2.2</cell><cell>9.5</cell><cell>16.6</cell><cell>45.5</cell><cell>73.8</cell></row><row><cell>HTM [42]</cell><cell>3.7</cell><cell>13.7</cell><cell>22.3</cell><cell>66.2</cell><cell>105.9</cell></row><row><cell>HGR [7]</cell><cell>4.0</cell><cell>15.0</cell><cell>24.8</cell><cell>63.2</cell><cell>107.0</cell></row><row><cell>RIVRL [13]</cell><cell>5.2</cell><cell>18.0</cell><cell>28.2</cell><cell>66.4</cell><cell>117.8</cell></row><row><cell>VSE++ [15]</cell><cell>4.9</cell><cell>17.7</cell><cell>28.2</cell><cell>67.1</cell><cell>117.9</cell></row><row><cell>DE++ [12]</cell><cell>5.3</cell><cell>18.4</cell><cell>29.2</cell><cell>68.0</cell><cell>121.0</cell></row><row><cell>DE [11]</cell><cell>5.6</cell><cell>18.8</cell><cell>29.4</cell><cell>67.8</cell><cell>121.7</cell></row><row><cell>W2VV++ [31]</cell><cell>5.4</cell><cell>18.7</cell><cell>29.7</cell><cell>68.8</cell><cell>122.6</cell></row><row><cell>CE [37]</cell><cell>5.5</cell><cell>19.1</cell><cell>29.9</cell><cell>71.1</cell><cell>125.6</cell></row><row><cell cols="4">VCMR models w/o moment localization:</cell><cell></cell><cell></cell></row><row><cell>ReLoCLNet [68]</cell><cell>5.7</cell><cell>18.9</cell><cell>30.0</cell><cell>72.0</cell><cell>126.6</cell></row><row><cell>XML [29]</cell><cell>5.3</cell><cell>19.4</cell><cell>30.6</cell><cell>73.1</cell><cell>128.4</cell></row><row><cell>Ours</cell><cell>7.1</cell><cell>22.5</cell><cell>34.7</cell><cell>75.8</cell><cell>140.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of PRVR on the Charades-STA dataset.</figDesc><table><row><cell>Visual feature: I3D.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">R@1 R@5 R@10 R100</cell><cell>SumR</cell></row><row><cell>T2VR models:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W2VV [10]</cell><cell>0.5</cell><cell>2.9</cell><cell>4.7</cell><cell>24.5</cell><cell>32.6</cell></row><row><cell>VSE++ [15]</cell><cell>0.8</cell><cell>3.9</cell><cell>7.2</cell><cell>31.7</cell><cell>43.6</cell></row><row><cell>W2VV++ [31]</cell><cell>0.9</cell><cell>3.5</cell><cell>6.6</cell><cell>34.3</cell><cell>45.3</cell></row><row><cell>HGR [7]</cell><cell>1.2</cell><cell>3.8</cell><cell>7.3</cell><cell>33.4</cell><cell>45.7</cell></row><row><cell>CE [37]</cell><cell>1.3</cell><cell>4.5</cell><cell>7.3</cell><cell>36.0</cell><cell>49.1</cell></row><row><cell>DE [11]</cell><cell>1.5</cell><cell>5.7</cell><cell>9.5</cell><cell>36.9</cell><cell>53.7</cell></row><row><cell>DE++ [12]</cell><cell>1.7</cell><cell>5.6</cell><cell>9.6</cell><cell>37.1</cell><cell>54.1</cell></row><row><cell>RIVRL[13]</cell><cell>1.6</cell><cell>5.6</cell><cell>9.4</cell><cell>37.7</cell><cell>54.3</cell></row><row><cell>HTM [42]</cell><cell>1.2</cell><cell>5.4</cell><cell>9.2</cell><cell>44.2</cell><cell>60.0</cell></row><row><cell cols="4">VCMR models w/o moment localization:</cell><cell></cell><cell></cell></row><row><cell>ReLoCLNet [68]</cell><cell>1.2</cell><cell>5.4</cell><cell>10.0</cell><cell>45.6</cell><cell>62.3</cell></row><row><cell>XML [29]</cell><cell>1.6</cell><cell>6.0</cell><cell>10.1</cell><cell>46.9</cell><cell>64.6</cell></row><row><cell>Ours</cell><cell>1.8</cell><cell>7.1</cell><cell>11.8</cell><cell>47.7</cell><cell>68.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Model comparison in terms of FLOPs and memory consumption.W2VV HGR HTM CE W2VV++ VSE++ DE DE++ RIVRL XML ReLoCLNet Ours</figDesc><table><row><cell cols="2">FLOPs (G)</cell><cell></cell><cell>0.42</cell><cell>2.96</cell><cell>0.06</cell><cell>0.06</cell><cell>0.4</cell><cell>0.20</cell><cell>5.24</cell><cell>5.30</cell><cell>8.64</cell><cell>0.80</cell><cell>0.96</cell><cell>1.22</cell></row><row><cell cols="3">Memory (MiB)</cell><cell>1231</cell><cell cols="3">8555 1225 1435</cell><cell>1281</cell><cell>1299</cell><cell cols="2">5837 3515</cell><cell>4809</cell><cell>2451</cell><cell>2673</cell><cell>5349</cell></row><row><cell>1</cell><cell>5</cell><cell>10</cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the TVR dataset.</figDesc><table><row><cell>Model</cell><cell cols="5">R@1 R@5 R@10 R100 SumR</cell></row><row><cell>Full setup</cell><cell>13.5</cell><cell>32.1</cell><cell>43.4</cell><cell>83.4</cell><cell>172.4</cell></row><row><cell>w/o frame-scale branch</cell><cell>12.3</cell><cell>30.5</cell><cell>41.5</cell><cell>82.3</cell><cell>166.6</cell></row><row><cell>w/o clip-scale branch</cell><cell>8.0</cell><cell>21.0</cell><cell>30.0</cell><cell>74.0</cell><cell>133.0</cell></row><row><cell>w/o key clip guide</cell><cell>12.2</cell><cell>30.6</cell><cell>41.0</cell><cell>82.4</cell><cell>166.3</cell></row><row><cell>w/o InfoNCE</cell><cell>11.3</cell><cell>29.1</cell><cell>40.1</cell><cell>81.3</cell><cell>161.8</cell></row><row><cell>w/o Triplet loss</cell><cell>11.2</cell><cell>29.2</cell><cell>40.4</cell><cell>81.9</cell><cell>162.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablations on the usage of the Transformer.</figDesc><table><row><cell></cell><cell cols="5">R@1 R@5 R@10 R@100 SumR</cell></row><row><cell cols="2">ReLoCLNet (best baseline) 10.7</cell><cell>28.1</cell><cell>38.1</cell><cell>80.3</cell><cell>157.1</cell></row><row><cell>Ours (1D-CNN)</cell><cell>10.6</cell><cell>29.3</cell><cell>39.9</cell><cell>81.9</cell><cell>161.6</cell></row><row><cell>Ours (bi-LSTM)</cell><cell>11.2</cell><cell>29.0</cell><cell>40.2</cell><cell>81.9</cell><cell>162.3</cell></row><row><cell>Ours (bi-GRU)</cell><cell>12.4</cell><cell>31.6</cell><cell>42.9</cell><cell>83.3</cell><cell>170.2</cell></row><row><cell>Ours (Transformer)</cell><cell>13.5</cell><cell>32.1</cell><cell>43.4</cell><cell>83.4</cell><cell>172.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance of PRVR with using subtitle features.</figDesc><table><row><cell cols="6">Visual feature:ResNet152-I3D, Subtitle feature:RoBERTa.</cell></row><row><cell>Method</cell><cell cols="5">R@1 R@5 R@10 R100 SumR</cell></row><row><cell>XML[29]</cell><cell>17.4</cell><cell>39.3</cell><cell>51.5</cell><cell>89.1</cell><cell>197.3</cell></row><row><cell>ReLoCLNet[68]</cell><cell>19.1</cell><cell>40.3</cell><cell>51.5</cell><cell>87.0</cell><cell>197.9</cell></row><row><cell>Ours</cell><cell>24.0</cell><cell>47.8</cell><cell>58.8</cell><cell>90.2</cell><cell>220.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Performance comparison on the MSR-VTT and MSVD dataset. Visual feature:ResNeXt101+ResNet-152.</figDesc><table><row><cell></cell><cell cols="4">R@1 R@5 R@10 SumR</cell></row><row><cell>On MSR-VTT:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE, BMVC19[37]</cell><cell>7.9</cell><cell>23.6</cell><cell>34.6</cell><cell>66.1</cell></row><row><cell>VSE++, BMVC19[15]</cell><cell>8.7</cell><cell>24.3</cell><cell>34.1</cell><cell>67.1</cell></row><row><cell>DE, CVPR19[11]</cell><cell>11.1</cell><cell>29.4</cell><cell>40.3</cell><cell>80.8</cell></row><row><cell>W2VV++, MM19[31]</cell><cell>11.1</cell><cell>29.6</cell><cell>40.5</cell><cell>81.2</cell></row><row><cell>DE++, TPAMI21[12]</cell><cell>11.6</cell><cell>30.3</cell><cell>41.3</cell><cell>83.2</cell></row><row><cell>HGR, CVPR20[7]</cell><cell>11.1</cell><cell>30.5</cell><cell>42.1</cell><cell>83.7</cell></row><row><cell>SEA, TMM21[32]</cell><cell>12.4</cell><cell>32.1</cell><cell>43.3</cell><cell>87.8</cell></row><row><cell>RIVRL, TCSVT22[13]</cell><cell>13.0</cell><cell>33.4</cell><cell>44.8</cell><cell>91.2</cell></row><row><cell>Ours</cell><cell>11.3</cell><cell>30.4</cell><cell>42.2</cell><cell>83.9</cell></row><row><cell>On MSVD:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DE, CVPR19[11]</cell><cell>20.3</cell><cell>46.8</cell><cell>59.7</cell><cell>126.8</cell></row><row><cell cols="2">CF-GNN, TMM21[53] 22.8</cell><cell>50.9</cell><cell>63.6</cell><cell>137.3</cell></row><row><cell>W2VV++, MM19[31]</cell><cell>22.4</cell><cell>51.6</cell><cell>64.8</cell><cell>138.8</cell></row><row><cell>SEA, TMM21[32]</cell><cell>24.6</cell><cell>55.0</cell><cell>67.9</cell><cell>147.5</cell></row><row><cell>Ours</cell><cell>22.0</cell><cell>52.6</cell><cell>67.2</cell><cell>141.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/Breakthrough/PySceneDetect/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking the bottom-up framework for query-based video localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chujie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10551" to="10558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting visual features from text for image and video caption retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3377" to="3388" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual encoding for video retrieval by text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12763</idno>
		<title level="m">Temporal localization of moments in video collections with natural language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VSE++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep MIML network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="14747" to="14350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting visual semantic reasoning for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caili</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1005" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast video moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">COOT: Cooperative hierarchical transformer for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22605" to="22618" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained Cross-modal Alignment Network for Text-Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3826" to="3834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CONQUER: Contextual query-aware ranking for video corpus moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wing Kwong</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3900" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aozhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th European Conference on Computer Vision</title>
		<meeting>the 17th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xiuqiang He, and Yueting Zhuang. 2021. Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weike</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="1114" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TVR: A large-scale dataset for video-subtitle moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="447" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">W2VV++: Fully deep learning for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhineng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1786" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SEA: Sentence encoder assembly for video retrieval by textual queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4351" to="4362" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Modal Multi-Instance Learning for Retinal Disease Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youxin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2474" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context-aware biaffine localizing network for temporal sentence grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulai</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11235" to="11244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Jointly cross-and self-modal graph attention network for query-based moment localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4070" to="4078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Progressive Semantic Matching for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanhua</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5083" to="5091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">CLIP4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Local-global video-text interactions for temporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10810" to="10819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text-based localization of moments in a video corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8886" to="8899" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fine-grained iterative attention network for temporal language localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4280" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial-temporal graphs for cross-modal text2video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04881</idno>
		<title level="m">Multiple instance learning with graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporally grounding language queries in videos by contextual boundary-aware prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12168" to="12175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning Coarse-to-Fine Graph Neural Networks for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">VATEX: A large-scale, high-quality multilingual dataset for videoand-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Siamese Alignment Network for Weakly Supervised Video Moment Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visual co-occurrence alignment learning for weakly-supervised video moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1459" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">HANet: Hierarchical Alignment Networks for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3518" to="3527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Boundary proposal network for two-stage natural language video localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoning</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2986" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1339" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deconfounded video moment retrieval with causal intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video moment retrieval with cross-modal neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1204" to="1216" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semantic conditioned dynamic modulation for temporal sentence grounding in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9159" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A hierarchical multi-modal encoder for moment localization in video corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheide</forename><surname>Chammas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09046</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1247" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Video corpus moment retrieval with contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangli</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick Siow Mong</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="685" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning 2d temporal adjacent networks for moment localization with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12870" to="12877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Progressive localization networks for language-based moment localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications, and Applications</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
