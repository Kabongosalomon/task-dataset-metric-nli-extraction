<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
							<email>xuezhem@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>ehovy@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word-and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks -Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets -97.55% accuracy for POS tagging and 91.21% F1 for NER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linguistic sequence labeling, such as part-ofspeech (POS) tagging and named entity recognition (NER), is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. Natural language processing (NLP) systems, like syntactic parsing <ref type="bibr" target="#b40">(Nivre and Scholz, 2004;</ref><ref type="bibr" target="#b37">McDonald et al., 2005;</ref><ref type="bibr" target="#b23">Koo and Collins, 2010;</ref><ref type="bibr" target="#b32">Ma and Zhao, 2012a;</ref><ref type="bibr" target="#b33">Ma and Zhao, 2012b;</ref><ref type="bibr" target="#b3">Chen and Manning, 2014;</ref><ref type="bibr" target="#b30">Ma and Hovy, 2015)</ref> and entity coreference resolution <ref type="bibr" target="#b39">(Ng, 2010;</ref>, are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.</p><p>Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) <ref type="bibr" target="#b44">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b42">Passos et al., 2014;</ref><ref type="bibr" target="#b29">Luo et al., 2015)</ref>, which rely heavily on hand-crafted features and taskspecific resources. For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER. However, such task-specific knowledge is costly to develop <ref type="bibr" target="#b31">(Ma and Xia, 2014)</ref>, making sequence labeling models difficult to adapt to new tasks or new domains.</p><p>In the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, have been broadly applied to NLP problems with great success. <ref type="bibr" target="#b6">Collobert et al. (2011)</ref> proposed a simple but effective feed-forward neutral network that independently classifies labels for each word by using contexts within a window with fixed size. Recently, recurrent neural networks (RNN) <ref type="bibr" target="#b17">(Goller and Kuchler, 1996)</ref>, together with its variants such as long-short term memory (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b12">Gers et al., 2000)</ref> and gated recurrent unit (GRU) <ref type="bibr" target="#b5">(Cho et al., 2014)</ref>, have shown great success in modeling sequential data. Several RNN-based neural network models have been proposed to solve sequence labeling tasks like speech recognition <ref type="bibr" target="#b18">(Graves et al., 2013)</ref>, POS tagging  and <ref type="bibr">NER (Chiu and Nichols, 2015;</ref><ref type="bibr" target="#b20">Hu et al., 2016)</ref>, achieving competitive performance against traditional models. However, even systems that have utilized distributed representations as inputs have used these to augment, rather than replace, hand-crafted features (e.g. word spelling and capitalization patterns). Their performance drops rapidly when the models solely depend on neural embeddings.</p><p>In this paper, we propose a neural network architecture for sequence labeling. It is a truly endto-end model requiring no task-specific resources, feature engineering, or data pre-processing beyond pre-trained word embeddings on unlabeled corpora. Thus, our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains. We first use convolutional neural networks (CNNs) <ref type="bibr" target="#b26">(LeCun et al., 1989)</ref> to encode character-level information of a word into its character-level representation. Then we combine character-and word-level representations and feed them into bi-directional LSTM (BLSTM) to model context information of each word. On top of BLSTM, we use a sequential CRF to jointly decode labels for the whole sentence. We evaluate our model on two linguistic sequence labeling tasks -POS tagging on Penn Treebank WSJ <ref type="bibr" target="#b36">(Marcus et al., 1993)</ref>, and NER on English data from the CoNLL 2003 shared task <ref type="bibr">(Tjong Kim Sang and De Meulder, 2003)</ref>. Our end-to-end model outperforms previous stateof-the-art systems, obtaining 97.55% accuracy for POS tagging and 91.21% F1 for NER. The contributions of this work are (i) proposing a novel neural network architecture for linguistic sequence labeling. (ii) giving empirical evaluations of this model on benchmark data sets for two classic NLP tasks. (iii) achieving state-of-the-art performance with this truly end-to-end system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Architecture</head><p>In this section, we describe the components (layers) of our neural network architecture. We introduce the neural layers in our neural network oneby-one from bottom to top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN for Character-level Representation</head><p>Previous studies <ref type="bibr" target="#b45">(Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b5">Chiu and Nichols, 2015)</ref> have shown that CNN is an effective approach to extract morphological information (like the prefix or suffix of a word) from characters of words and encode it into neural representations. <ref type="figure" target="#fig_0">Figure 1</ref> shows the CNN we use to extract character-level representation of a given word. The CNN is similar to the one in <ref type="bibr" target="#b5">Chiu and Nichols (2015)</ref>, except that we use only character embeddings as the inputs to CNN, without character type features. A dropout layer <ref type="bibr">(Srivastava et al., 2014)</ref> is applied before character embeddings are input to CNN. Dashed arrows indicate a dropout layer applied before character embeddings are input to CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bi-directional LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">LSTM Unit</head><p>Recurrent neural networks (RNNs) are a powerful family of connectionist models that capture time dynamics via cycles in the graph. Though, in theory, RNNs are capable to capturing long-distance dependencies, in practice, they fail due to the gradient vanishing/exploding problems <ref type="bibr" target="#b1">(Bengio et al., 1994;</ref><ref type="bibr" target="#b41">Pascanu et al., 2012)</ref>.</p><p>LSTMs (Hochreiter and Schmidhuber, 1997) are variants of RNNs designed to cope with these gradient vanishing problems. Basically, a LSTM unit is composed of three multiplicative gates which control the proportions of information to forget and to pass on to the next time step.  Formally, the formulas to update an LSTM unit at time t are:</p><formula xml:id="formula_0">i t = ?(W i h t?1 + U i x t + b i ) f t = ?(W f h t?1 + U f x t + b f ) c t = tanh(W c h t?1 + U c x t + b c ) c t = f t c t?1 + i t c t o t = ?(W o h t?1 + U o x t + b o ) h t = o t tanh(c t )</formula><p>where ? is the element-wise sigmoid function and is the element-wise product. x t is the input vector (e.g. word embedding) at time t, and h t is the hidden state (also called output) vector storing all the useful information at (and before) time t.</p><formula xml:id="formula_1">U i , U f , U c , U o denote the weight matrices of different gates for input x t , and W i , W f , W c , W o are the weight matrices for hidden state h t . b i , b f , b c , b o denote the bias vectors.</formula><p>It should be noted that we do not include peephole connections <ref type="bibr" target="#b13">(Gers et al., 2003)</ref> in the our LSTM formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">BLSTM</head><p>For many sequence labeling tasks it is beneficial to have access to both past (left) and future (right) contexts. However, the LSTM's hidden state h t takes information only from past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work  is bi-directional LSTM (BLSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CRF</head><p>For sequence labeling (or general structured prediction) tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence. For example, in POS tagging an adjective is more likely to be followed by a noun than a verb, and in NER with standard BIO2 annotation (Tjong Kim Sang and Veenstra, 1999) I-ORG cannot follow I-PER. Therefore, we model label sequence jointly using a conditional random field (CRF) <ref type="bibr" target="#b25">(Lafferty et al., 2001)</ref>, instead of decoding each label independently.</p><p>Formally, we use z = {z 1 , ? ? ? , z n } to represent a generic input sequence where z i is the input vector of the ith word. y = {y 1 , ? ? ? , y n } represents a generic sequence of labels for z. Y(z) denotes the set of possible label sequences for z. The probabilistic model for sequence CRF defines a family of conditional probability p(y|z; W, b) over all possible label sequences y given z with the following form:</p><formula xml:id="formula_2">p(y|z; W, b) = n i=1 ? i (y i?1 , y i , z) y ?Y(z) n i=1 ? i (y i?1 , y i , z)</formula><p>where ? i (y , y, z) = exp(W T y ,y z i + b y ,y ) are potential functions, and W T y ,y and b y ,y are the weight vector and bias corresponding to label pair (y , y), respectively.</p><p>For CRF training, we use the maximum conditional likelihood estimation. For a training set {(z i , y i )}, the logarithm of the likelihood (a.k.a. the log-likelihood) is given by:</p><formula xml:id="formula_3">L(W, b) = i log p(y|z; W, b) Maximum likelihood training chooses parameters such that the log-likelihood L(W, b) is maxi- mized.</formula><p>Decoding is to search for the label sequence y * with the highest conditional probability:</p><formula xml:id="formula_4">y * = argmax y?Y(z) p(y|z; W, b)</formula><p>For a sequence CRF model (only interactions between two successive labels are considered), training and decoding can be solved efficiently by adopting the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">BLSTM-CNNs-CRF</head><p>Finally, we construct our neural network model by feeding the output vectors of BLSTM into a CRF layer. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the architecture of our network in detail.</p><p>For each word, the character-level representation is computed by the CNN in <ref type="figure" target="#fig_0">Figure 1</ref> with character embeddings as inputs. Then the character-level representation vector is concatenated with the word embedding vector to feed into the BLSTM network. Finally, the output vectors of BLSTM are fed to the CRF layer to jointly decode the best label sequence. As shown in <ref type="figure">Fig</ref> improve the performance of our model (see Section 4.5 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network Training</head><p>In this section, we provide details about training the neural network. We implement the neural network using the Theano library <ref type="bibr" target="#b2">(Bergstra et al., 2010)</ref>. The computations for a single model are run on a GeForce GTX TITAN X GPU. Using the settings discussed in this section, the model training requires about 12 hours for POS tagging and 8 hours for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter Initialization</head><p>Word Embeddings. We use Stanford's publicly available GloVe 100-dimensional embeddings 1 trained on 6 billion words from Wikipedia and web text <ref type="bibr" target="#b43">(Pennington et al., 2014)</ref> We also run experiments on two other sets of published embeddings, namely Senna 50dimensional embeddings 2 trained on Wikipedia and Reuters RCV-1 corpus <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>, and Google's Word2Vec 300-dimensional embeddings 3 trained on 100 billion words from Google News <ref type="bibr" target="#b38">(Mikolov et al., 2013)</ref>. To test the effectiveness of pretrained word embeddings, we experimented with randomly initialized embeddings with 100 dimensions, where embeddings are uni-</p><formula xml:id="formula_5">formly sampled from range [? 3 dim , + 3 dim ]</formula><p>where dim is the dimension of embeddings <ref type="bibr">(He et al., 2015)</ref>. The performance of different word embeddings is discussed in Section 4.4. Character Embeddings.</p><p>Character embeddings are initialized with uniform samples from , where r and c are the number of of rows and columns in the structure <ref type="bibr" target="#b16">(Glorot and Bengio, 2010)</ref>. Bias vectors are initialized to zero, except the bias b f for the forget gate in LSTM , which is initialized to 1.0 <ref type="bibr">(Jozefowicz et al., 2015)</ref>.</p><formula xml:id="formula_6">[? 3 dim , + 3 dim ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization Algorithm</head><p>Parameter optimization is performed with minibatch stochastic gradient descent (SGD) with batch size 10 and momentum 0.9. We choose an initial learning rate of ? 0 (? 0 = 0.01 for POS tagging, and 0.015 for NER, see Section 3.3.), and the learning rate is updated on each epoch of training as ? t = ? 0 /(1 + ?t), with decay rate ? = 0.05 and t is the number of epoch completed. To reduce the effects of "gradient exploding", we use a gradient clipping of 5.0 <ref type="bibr" target="#b41">(Pascanu et al., 2012)</ref>. We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam <ref type="bibr" target="#b22">(Kingma and Ba, 2014)</ref> or RMSProp <ref type="bibr" target="#b8">(Dauphin et al., 2015)</ref>, but none of them meaningfully improve upon SGD with momentum and gradient clipping in our preliminary experiments. Early Stopping. We use early stopping <ref type="bibr" target="#b14">(Giles, 2001;</ref><ref type="bibr" target="#b18">Graves et al., 2013)</ref> based on performance on validation sets. The "best" parameters appear at around 50 epochs, according to our experiments. Fine Tuning. For each of the embeddings, we fine-tune initial embeddings, modifying them during gradient updates of the neural network model by back-propagating gradients. The effectiveness of this method has been previously explored in sequential and structured prediction problems <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr">Peng and Dredze, 2015)</ref>. Dropout Training. To mitigate overfitting, we apply the dropout method (Srivastava et al., 2014) to regularize our model. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> and 3, we apply dropout on character embeddings before inputting to CNN, and on both the input and output vectors of BLSTM. We fix dropout rate at 0.5 for all dropout layers through all the experiments. We obtain significant improvements on model performance after using dropout (see Section 4.5).   <ref type="bibr" target="#b44">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b7">Dai et al., 2015;</ref><ref type="bibr" target="#b26">Lample et al., 2016)</ref>. The corpora statistics are shown in <ref type="table" target="#tab_3">Table 2</ref>. We did not perform any pre-processing for data sets, leaving our system truly end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tuning Hyper-Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We first run experiments to dissect the effectiveness of each component (layer) of our neural network architecture by ablation studies. We compare the performance with three baseline systems -BRNN, the bi-direction RNN; BLSTM, the bidirection LSTM, and BLSTM-CNNs, the combination of BLSTM with CNN to model characterlevel information. All these models are run using Stanford's GloVe 100 dimensional word embeddings and the same hyper-parameters as shown in  <ref type="table">Table 3</ref>: Performance of our model on both the development and test sets of the two tasks, together with three baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc. <ref type="bibr" target="#b15">Gim?nez and</ref><ref type="bibr">M?rquez (2004) 97.16 Toutanova et al. (2003)</ref> 97.27 <ref type="bibr" target="#b35">Manning (2011)</ref> 97.28 <ref type="bibr" target="#b6">Collobert et al. (2011) ?</ref> 97.29 Santos and Zadrozny (2014) ? 97.32 <ref type="bibr" target="#b46">Shen et al. (2007)</ref> 97.33 <ref type="bibr" target="#b48">Sun (2014)</ref> 97.36 <ref type="bibr" target="#b47">S?gaard (2011)</ref> 97.50 This paper 97.55 results reported by previous work <ref type="bibr" target="#b45">(Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b5">Chiu and Nichols, 2015)</ref>. Finally, by adding CRF layer for joint decoding we achieve significant improvements over BLSTM-CNN models for both POS tagging and NER on all metrics. This demonstrates that jointly decoding label sequences can significantly benefit the final performance of neural network models.  <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>, which is a feed-forward neural network model using capitalization and discrete suffix features, and data pre-processing. Moreover, our model achieves 0.23% improvements on accuracy over the "CharWNN" <ref type="bibr" target="#b45">(Santos and Zadrozny, 2014)</ref>, which is a neural network model based on Senna and also uses CNNs to model characterlevel representations. This demonstrates the effectiveness of BLSTM for modeling sequential data Model F1 Chieu and <ref type="bibr">Ng (2002)</ref> 88.31 <ref type="bibr" target="#b11">Florian et al. (2003)</ref> 88.76 <ref type="bibr" target="#b0">Ando and Zhang (2005)</ref> 89 <ref type="formula">.</ref>  <ref type="bibr" target="#b29">Luo et al. (2015)</ref> 91.20 This paper 91.21 and the importance of joint decoding with structured prediction model. Comparing with traditional statistical models, our system achieves state-of-the-art accuracy, obtaining 0.05% improvement over the previously best reported results by <ref type="bibr" target="#b47">S?gaard (2011)</ref>. It should be noted that  also evaluated their BLSTM-CRF model for POS tagging on WSJ corpus. But they used a different splitting of the training/dev/test data sets. Thus, their results are not directly comparable with ours. <ref type="table" target="#tab_8">Table 5</ref> shows the F1 scores of previous models for NER on the test data set from CoNLL-2003 shared task. For the purpose of comparison, we list their results together with ours. Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other three neural models, namely the LSTM-CRF proposed by , LSTM-CNNs pro-  <ref type="table">Table 6</ref>: Results with different choices of word embeddings on the two tasks (accuracy for POS tagging and F1 for NER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Previous Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">POS Tagging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">NER</head><p>posed by <ref type="bibr" target="#b5">Chiu and Nichols (2015)</ref>, and the LSTM-CRF by <ref type="bibr" target="#b26">Lample et al. (2016)</ref>.  utilized discrete spelling, POS and context features, Chiu and Nichols (2015) used charactertype, capitalization, and lexicon features, and all the three model used some task-specific data preprocessing, while our model does not require any carefully designed features or data pre-processing.</p><p>We have to point out that the result (90.77%) reported by Chiu and Nichols <ref type="formula">(2015)</ref> is incomparable with ours, because their final model was trained on the combination of the training and development data sets 4 . To our knowledge, the previous best F1 score (91.20) 5 reported on CoNLL 2003 data set is by the joint NER and entity linking model <ref type="bibr" target="#b29">(Luo et al., 2015)</ref>. This model used many hand-crafted features including stemming and spelling features, POS and chunks tags, WordNet clusters, Brown Clusters, as well as external knowledge bases such as Freebase and Wikipedia. Our end-to-end model slightly improves this model by 0.01%, yielding a state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Word Embeddings</head><p>As mentioned in Section 3.1, in order to test the importance of pretrained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as a random sampling method, to initialize our model. <ref type="table">Table 6</ref> gives the performance of three different word embeddings, as well as the randomly sampled one. According to the results in <ref type="table">Table 6</ref>, models using pretrained word embeddings obtain a significant improvement as opposed to the ones using random embeddings. Comparing the two tasks, NER relies <ref type="bibr">4</ref> We run experiments using the same setting and get 91.37% F1 score. <ref type="bibr">5</ref> Numbers are taken from the <ref type="table">Table 3</ref> of the original paper <ref type="bibr" target="#b29">(Luo et al., 2015)</ref>. While there is clearly inconsistency among the precision (91.5%), recall (91.4%) and F1 scores (91.2%), it is unclear in which way they are incorrect.   more heavily on pretrained embeddings than POS tagging. This is consistent with results reported by previous work <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b5">Chiu and Nichols, 2015)</ref>. For different pretrained embeddings, Stanford's GloVe 100 dimensional embeddings achieve best results on both tasks, about 0.1% better on POS accuracy and 0.9% better on NER F1 score than the Senna 50 dimensional one. This is different from the results reported by <ref type="bibr" target="#b5">Chiu and Nichols (2015)</ref>, where Senna achieved slightly better performance on NER than other embeddings. Google's Word2Vec 300 dimensional embeddings obtain similar performance with Senna on POS tagging, still slightly behind GloVe. But for NER, the performance on Word2Vec is far behind GloVe and Senna. One possible reason that Word2Vec is not as good as the other two embeddings on NER is because of vocabulary mismatch -Word2Vec embeddings were trained in casesensitive manner, excluding many common symbols such as punctuations and digits. Since we do not use any data pre-processing to deal with such common symbols or rare words, it might be an issue for using Word2Vec. <ref type="table" target="#tab_11">Table 7</ref> compares the results with and without dropout layers for each data set. All other hyperparameters remain the same as in <ref type="table" target="#tab_2">Table 1</ref>. We observe a essential improvement for both the two tasks. It demonstrates the effectiveness of dropout in reducing overfitting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Dropout</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">OOV Error Analysis</head><p>To better understand the behavior of our model, we perform error analysis on Out-of-Vocabulary words (OOV). Specifically, we partition each data set into four subsets -in-vocabulary words (IV), out-of-training-vocabulary words (OOTV), out-of-embedding-vocabulary words (OOEV) and out-of-both-vocabulary words (OOBV). A word is considered IV if it appears in both the training and embedding vocabulary, while OOBV if neither. OOTV words are the ones do not appear in training set but in embedding vocabulary, while OOEV are the ones do not appear in embedding vocabulary but in training set. For NER, an entity is considered as OOBV if there exists at lease one word not in training set and at least one word not in embedding vocabulary, and the other three subsets can be done in similar manner. <ref type="table" target="#tab_12">Table 8</ref> informs the statistics of the partition on each corpus. The embedding we used is Stanford's GloVe with dimension 100, the same as Section 4.2. <ref type="table" target="#tab_14">Table 9</ref> illustrates the performance of our model on different subsets of words, together with the baseline LSTM-CNN model for comparison. The largest improvements appear on the OOBV subsets of both the two corpora. This demonstrates that by adding CRF for joint decoding, our model is more powerful on words that are out of both the training and embedding sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In recent years, several different neural network architectures have been proposed and successfully applied to linguistic sequence labeling such as POS tagging, chunking and NER. Among these neural architectures, the three approaches most similar to our model are the BLSTM-CRF model proposed by , the LSTM-CNNs model by <ref type="bibr" target="#b5">Chiu and Nichols (2015)</ref> and the BLSTM-CRF by <ref type="bibr" target="#b26">Lample et al. (2016)</ref>.  used BLSTM for word-level representations and CRF for jointly label decoding, which is similar to our model. But there are two main differences between their model and ours. First, they did not employ CNNs to model character-level information. Second, they combined their neural network model with handcrafted features to improve their performance, making their model not an end-to-end system. Chiu and Nichols <ref type="formula">(2015)</ref> proposed a hybrid of BLSTM and CNNs to model both character-and word-level representations, which is similar to the first two layers in our model. They evaluated their model on NER and achieved competitive performance. Our model mainly differ from this model by using CRF for joint decoding. Moreover, their model is not truly end-to-end, either, as it utilizes external knowledge such as character-type, capitalization and lexicon features, and some data preprocessing specifically for NER (e.g. replacing all sequences of digits 0-9 with a single "0"). Recently, <ref type="bibr" target="#b26">Lample et al. (2016)</ref> proposed a BLSTM-CRF model for NER, which utilized BLSTM to model both the character-and word-level information, and use data pre-processing the same as <ref type="bibr" target="#b5">Chiu and Nichols (2015)</ref>. Instead, we use CNN to model character-level information, achieving better NER performance without using any data preprocessing.</p><p>There are several other neural networks previously proposed for sequence labeling. <ref type="bibr" target="#b24">Labeau et al. (2015)</ref> proposed a RNN-CNNs model for German POS tagging. This model is similar to the LSTM-CNNs model in <ref type="bibr" target="#b5">Chiu and Nichols (2015)</ref>, with the difference of using vanila RNN instead of LSTM. Another neural architecture employing CNN to model character-level information is the "CharWNN" architecture <ref type="bibr" target="#b45">(Santos and Zadrozny, 2014)</ref> which is inspired by the feed-forward network <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>. CharWNN obtained near state-of-the-art accuracy on English POS tagging (see Section 4.3 for details). A similar model has also been applied to Spanish and Portuguese NER (dos <ref type="bibr" target="#b9">Santos et al., 2015</ref> and <ref type="bibr" target="#b53">Yang et al. (2016)</ref> also used BSLTM to compose character embeddings to word's representation, which is similar to <ref type="bibr" target="#b26">Lample et al. (2016)</ref>. Peng and Dredze (2016) Improved NER for Chinese Social Media with Word Segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a neural network architecture for sequence labeling. It is a truly end-toend model relying on no task-specific resources, feature engineering or data pre-processing. We achieved state-of-the-art performance on two linguistic sequence labeling tasks, comparing with previously state-of-the-art systems.</p><p>There are several potential directions for future work. First, our model can be further improved by exploring multi-task learning approaches to combine more useful and correlated information. For example, we can jointly train a neural network model with both the POS and NER tags to improve the intermediate representations learned in our network. Another interesting direction is to apply our model to data from other domains such as social media (Twitter and Weibo). Since our model does not require any domain-or taskspecific knowledge, it might be effortless to apply it to these domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The convolution neural network for extracting character-level representations of words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 gives the basic structure of an LSTM unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of LSTM unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>ure 3, dropout layers are applied on both the input and output vectors of BLSTM. Experimental results show that using dropout significantly The main architecture of our neural network. The character representation for each word is computed by the CNN inFigure 1. Then the character representation vector is concatenated with the word embedding before feeding into the BLSTM network. Dashed arrows indicate dropout layers applied on both the input and output vectors of BLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where we set dim = 30.</figDesc><table><row><cell cols="2">Weight Matrices and Bias Vectors. Matrix pa-</cell></row><row><cell cols="2">rameters are randomly initialized with uniform</cell></row><row><cell>samples from [?</cell><cell>6 r+c , +</cell></row></table><note>6 r+c ]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Thus, for the tasks of POS tagging and NER we try to share as many hyper-parameters as possible. Note that the final hyper-parameters for these two tasks are almost the same, except the initial learning rate. We set the state size of LSTM to 200. Tuning this parameter did not significantly impact the performance of our model. For CNN, we use 30 filters with window length 3.</figDesc><table><row><cell>4 Experiments</cell></row></table><note>summarizes the chosen hyper-parameters for all experiments. We tune the hyper-parameters on the development sets by random search. Due to time constrains it is infeasible to do a ran- dom search across the full hyper-parameter space.4.1 Data Sets As mentioned before, we evaluate our neural net- work model on two sequence labeling tasks: POS tagging and NER.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Corpora statistics. SENT and TOKEN refer to the number of sentences and tokens in each data set.</figDesc><table><row><cell>POS Tagging. For English POS tagging, we use</cell></row><row><cell>the Wall Street Journal (WSJ) portion of Penn</cell></row><row><cell>Treebank (PTB) (Marcus et al., 1993), which con-</cell></row><row><cell>tains 45 different POS tags. In order to com-</cell></row><row><cell>pare with previous work, we adopt the standard</cell></row><row><cell>splits -section 0-18 as training data, section 19-</cell></row><row><cell>21 as development data and section 22-24 as test</cell></row><row><cell>data (Manning, 2011; S?gaard, 2011).</cell></row></table><note>NER. For NER, We perform experiments on the English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named entities: PERSON, LOCATION, ORGA- NIZATION, and MISC. We use the BIOES tag- ging scheme instead of standard BIO2, as pre- vious studies have reported meaningful improve- ment with this scheme</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>According to the results shown in Table 3, BLSTM obtains better performance than BRNN on all evaluation metrics of both the two tasks. BLSTM-CNN models significantly outperform the BLSTM model, showing that characterlevel representations are important for linguistic sequence labeling tasks. This is consistent with 96.76 92.04 89.13 90.56 87.05 83.88 85.44 BLSTM 96.88 96.93 92.31 90.85 91.57 87.77 86.23 87.00 BLSTM-CNN 97.34 97.33 92.52 93.64 93.07 88.53 90.21 89.36 BRNN-CNN-CRF 97.46 97.55 94.85 94.63 94.74 91.35 91.06 91.21</figDesc><table><row><cell></cell><cell cols="2">POS</cell><cell></cell><cell></cell><cell>NER</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell></cell><cell>Test</cell></row><row><cell>Model</cell><cell cols="3">Acc. Acc. Prec. Recall</cell><cell>F1</cell><cell>Prec. Recall</cell><cell>F1</cell></row><row><cell>BRNN</cell><cell>96.56</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>POS tagging accuracy of our model on test data from WSJ proportion of PTB, together with top-performance systems. The neural network based models are marked with ?.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>illustrates the results of our model for</cell></row><row><cell>POS tagging, together with seven previous top-</cell></row><row><cell>performance systems for comparison. Our model</cell></row><row><cell>significantly outperform Senna</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>NER F1 score of our model on test data set from CoNLL-2003. For the purpose of comparison, we also list F1 scores of previous topperformance systems. ? marks the neural models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>97.06 97.11 99.97 93.51 89.25 Yes 97.86 97.46 97.55 99.63 94.74 91.21</figDesc><table><row><cell></cell><cell></cell><cell>POS</cell><cell></cell><cell></cell><cell>NER</cell></row><row><cell></cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>No</cell><cell>98.46</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results with and without dropout on two tasks (accuracy for POS tagging and F1 for NER).</figDesc><table><row><cell></cell><cell>POS</cell><cell></cell><cell cols="2">NER</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>IV</cell><cell cols="4">127,247 125,826 4,616 3,773</cell></row><row><cell>OOTV</cell><cell>2,960</cell><cell>2,412</cell><cell cols="2">1,087 1,597</cell></row><row><cell>OOEV</cell><cell>659</cell><cell>588</cell><cell>44</cell><cell>8</cell></row><row><cell>OOBV</cell><cell>902</cell><cell>828</cell><cell>195</cell><cell>270</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Statistics of the partition on each corpus. It lists the number of tokens of each subset for POS tagging and the number of entities for NER.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://nlp.stanford.edu/projects/ glove/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://ronan.collobert.com/senna/ 3 https://code.google.com/archive/p/ word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang2005] Rie Kubota</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a cpu and gpu math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for scientific computing conference (SciPy)</title>
		<meeting>the Python for scientific computing conference (SciPy)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2014] Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2014</title>
		<meeting>EMNLP-2014<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition: a maximum entropy approach using global information</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<editor>Ng2002] Hai Leong Chieu and Hwee Tou Ng</editor>
		<meeting>CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols ; Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Named entity recognition with bidirectional lstm-cnns</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<editor>Hong-Jie Dai, Po-Ting Lai, Yung-Chun Chang, and Richard Tzong-Han Tsai</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Harm de Vries, Junyoung Chung, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04390</idno>
	</analytic>
	<monogr>
		<title level="m">Rmsprop and equilibrated adaptive learning rates for non-convex optimization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting named entity recognition with neural character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Dos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NEWS 2015 The Fifth Named Entities Workshop</title>
		<meeting>NEWS 2015 The Fifth Named Entities Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2015</title>
		<meeting>ACL-2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL-2003</title>
		<meeting>HLT-NAACL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich Caruana Steve Lawrence Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">402</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Svmtool: A general pos tagger generator based on support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Gim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Gim?nez and M?rquez2004</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Glorot and Bengio2010</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
	<note>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and J?rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2016</title>
		<meeting>ACL-2016<address><addrLine>Berlin, Germany, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kingma and Ba2014</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient third-order dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2010</title>
		<meeting>ACL-2010<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-lexical neural architecture for finegrained pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Labeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="232" to="237" />
		</imprint>
		<respStmt>
			<orgName>Matthieu Labeau, Kevin L?ser</orgName>
		</respStmt>
	</monogr>
	<note>Alexandre Allauzen, and Rue John von Neumann</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML-2001</title>
		<meeting>ICML-2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">951</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-2016</title>
		<meeting>NAACL-2016<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
		</imprint>
	</monogr>
	<note>Neural architectures for named entity recognition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2009</title>
		<meeting>ACL-2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
	<note>Lin and Wu2009</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2015</title>
		<meeting>EMNLP-2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2015</title>
		<meeting>EMNLP-2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, and Zaiqing Nie</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient inner-to-outer greedy algorithm for higher-order labeled dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hovy2015] Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP-2015</title>
		<meeting>the EMNLP-2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia2014] Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2014</title>
		<meeting>ACL-2014<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1337" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao2012a] Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters</title>
		<meeting>COLING 2012: Posters<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Probabilistic models for high-order projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao2012b] Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04174</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised ranking model for entity coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-2016</title>
		<meeting>NAACL-2016<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Partof-speech tagging from 97% to 100%: is it time for some linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online largemargin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2005</title>
		<meeting>ACL-2005<address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-25" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised noun phrase coreference research: The first fifteen years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1396" to="1411" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-2010</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deterministic dependency parsing of English text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-2004</title>
		<meeting>COLING-2004<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08-23" />
			<biblScope unit="page" from="64" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012. On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Passos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2014</title>
		<meeting>CoNLL-2014<address><addrLine>Ann Arbor, Michigan; Lisbon, Portugal; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Nanyun Peng and Mark Dredze</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-2016</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2014</title>
		<meeting>EMNLP-2014<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2009</title>
		<meeting>CoNLL-2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
	<note>Ratinov and Roth2009</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zadrozny2014]</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML-2014</title>
		<meeting>ICML-2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Guided learning for bidirectional sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2007</title>
		<meeting>ACL-2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="760" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note>Dropout: A simple way to prevent neural networks from overfitting</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Structure regularization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2402" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<editor>Tjong Kim Sang and De Meulder2003] Erik F. Tjong Kim Sang and Fien De Meulder</editor>
		<meeting>CoNLL-2003<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representing text chunks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL&apos;99</title>
		<editor>Tjong Kim Sang and Veenstra1999] Erik F. Tjong Kim Sang and Jorn Veenstra</editor>
		<meeting>EACL&apos;99<address><addrLine>Bergen, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="173" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT-2003</title>
		<meeting>NAACL-HLT-2003</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06270</idno>
		<title level="m">Multi-task cross-lingual sequence tagging from scratch</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
