<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Class-Conditional Assumption: A Primary Attempt to Combat Instance-Dependent Label Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">VIVO AI Lab 3 Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
							<email>gy.chen@siat.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhao</surname></persName>
							<email>jingwei.zhao@vivo.com</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">VIVO AI Lab 3 Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
							<email>pheng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Class-Conditional Assumption: A Primary Attempt to Combat Instance-Dependent Label Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning under label noise has seen numerous advances recently, while existing theoretical findings and empirical results broadly build up on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label. In this work, we present a theoretical hypothesis testing and prove that noise in real-world dataset is unlikely to be CCN, which confirms that label noise should depend on the instance and justifies the urgent need to go beyond the CCN assumption.The theoretical results motivate us to study the more general and practical-relevant instancedependent noise (IDN). To stimulate the development of theory and methodology on IDN, we formalize an algorithm to generate controllable IDN and present both theoretical and empirical evidence to show that IDN is semantically meaningful and challenging. As a primary attempt to combat IDN, we present a tiny algorithm termed self-evolution average label (SEAL), which not only stands out under IDN with various noise fractions, but also improves the generalization on realworld noise benchmark Clothing1M. Our code is released 1 . Notably, our theoretical analysis in Section 2 provides rigorous motivations for studying IDN, which is an important topic that deserves more research attention in future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Noisy labels are unavoidable in practical applications, where instances are usually labeled by workers on crowdsourcing platforms <ref type="bibr" target="#b42">(Yan et al. 2014;</ref><ref type="bibr" target="#b32">Schroff, Criminisi, and Zisserman 2010)</ref>. Unfortunately, Deep Neural Networks (DNNs) can memorize noisy labels easily but generalize poorly on clean test data <ref type="bibr" target="#b47">(Zhang et al. 2017)</ref>. Hence, how to mitigate the effect of noisy labels in the training of DNNs has attracted considerable attention recently. Most existing works, for their theoretical analysis or noise synthesizing in experiments, follow the class-conditional noise (CCN) assumption <ref type="bibr" target="#b33">(Scott, Blanchard, and Handy 2013;</ref><ref type="bibr" target="#b48">Zhang and Sabuncu 2018;</ref><ref type="bibr" target="#b24">Menon et al. 2020;</ref><ref type="bibr" target="#b21">Ma et al. 2020)</ref>, where the label noise is independent of its input features conditional on the latent true label. In fact, instances with the same label can be entirely different, hence the probability of mislabeling should be highly dependent on the specific instance. As shown in the first row of <ref type="figure" target="#fig_0">Fig. 1</ref>, the second right image is likely to be mislabeled as the number 6 and the fourth right image is likely to be manually mislabeled as the number 7; in the second row, the last image is more likely to be mislabeled as the ship. In this paper, our first contribution (Section 2) is to present a theoretical hypothesis testing on the well-known real-world dataset, Clothing1M, to demonstrate the urgent need to go beyond the CCN assumption in practical applications. Meanwhile, we discuss the challenge of instance-dependent noise (IDN) with both theoretical and empirical evidence. Some pioneer efforts has been contributed to IDN, but most results are restricted to binary classification <ref type="bibr" target="#b25">(Menon, van Rooyen, and Natarajan 2018;</ref><ref type="bibr" target="#b4">Bootkrajang and Chaijaruwanich 2018;</ref><ref type="bibr" target="#b7">Cheng et al. 2020)</ref> or based on assumptions such as the noise is parts-dependent <ref type="bibr" target="#b38">(Xia et al. 2020)</ref>.</p><p>To stimulate the development of theory and methodology on more practical-relevant IDN, we propose an algorithm to generate controllable IDN and present extensive characterizations of training under IDN, which is our second contribution (Section 3). Our third contribution (Section 4) is to propose an algorithm termed self-evolution average label (SEAL) to defend IDN, motivated by an experimental observation that the DNN's output corresponding to the latent true label can be activated with oscillation before memorizing noise. Specifically, SEAL provides instance-dependent label correction by averaging predictions of a DNN on each instance over the whole training process, then retrains a classifier using the averaged soft labels. The superior performance of SEAL is verified on extensive experiments, including synthetic/realworld datasets under IDN of different noise fractions, and the large benchmark Clothing1M <ref type="bibr" target="#b40">(Xiao et al. 2015)</ref> with real-world noise .</p><p>2 From CCN to IDN -Theoretical Evidence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Considering a c-class classification problem, let X be the feature space, Y = {1, ..., c} be the label space, (X, Y ) ? X ? Y be the random variables with distribution D X,Y and D = {(x i , y i )} n i=1 be a dataset containing i.i.d. samples drawn from D X,Y . In practical applications, the true label Y may not be observable. Instead, we have an observable distribution of noisy labels (X,? ) ?D X,? and a dataset D = {(x i ,? i )} n i=1 drawn from it. A classifier f : X ? P c is defined by a DNN that outputs a probability distribution over all classes, where P c = {s ? R c + : s 1 = 1}. By default, the probability is obtained by a softmax function <ref type="bibr" target="#b10">(Goodfellow, Bengio, and Courville 2016)</ref> at the output of f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Beyond the CCN assumption</head><p>The CCN assumption is commonly used in previous works, as clearly stated in theoretical analysis <ref type="bibr" target="#b4">(Blum and Mitchell 1998;</ref><ref type="bibr" target="#b43">Yan et al. 2017;</ref><ref type="bibr" target="#b28">Patrini et al. 2016;</ref><ref type="bibr" target="#b48">Zhang and Sabuncu 2018;</ref><ref type="bibr" target="#b41">Xu et al. 2019;</ref><ref type="bibr" target="#b24">Menon et al. 2020;</ref><ref type="bibr" target="#b21">Ma et al. 2020)</ref> or inexplicitly used in experiments for synthetizing noisy labels <ref type="bibr" target="#b13">(Han et al. 2018b;</ref><ref type="bibr" target="#b45">Yu et al. 2019;</ref><ref type="bibr" target="#b0">Arazo et al. 2019;</ref><ref type="bibr" target="#b19">Li, Socher, and Hoi 2020;</ref><ref type="bibr" target="#b20">Lukasik et al. 2020)</ref>. Under the CCN assumption, the observed label? is independent of X conditioning on the latent true label Y . Definition 1. (CCN Model) Under the CCN assumption, there is a noise transition matrix M ? [0, 1] c?c and we observe samples (X,? ) ?D = CCN(D, M ), where first we draw (X, Y ) ? D as usual, then flip Y to produce? according to the conditional probability defined by M , i.e., Pr(? = q|Y = p) = M p,q , where p, q ? Y.</p><p>We have seen various specific cases of CCN, including uniform/symmetric noise <ref type="bibr" target="#b31">(Ren et al. 2018;</ref><ref type="bibr" target="#b0">Arazo et al. 2019;</ref><ref type="bibr" target="#b5">Chen et al. 2019a;</ref><ref type="bibr" target="#b20">Lukasik et al. 2020)</ref>, pair/asymmetric noise <ref type="bibr" target="#b13">(Han et al. 2018b;</ref><ref type="bibr" target="#b6">Chen et al. 2019b)</ref>, tri/column/blockdiagonal noise <ref type="bibr" target="#b12">(Han et al. 2018a</ref>). Since the noise transition process is fully specified by a matrix M , one can mitigate the effect of CCN by modeling M <ref type="bibr" target="#b29">(Patrini et al. 2017;</ref><ref type="bibr" target="#b15">Hendrycks et al. 2018;</ref><ref type="bibr" target="#b12">Han et al. 2018a;</ref><ref type="bibr" target="#b39">Xia et al. 2019;</ref><ref type="bibr" target="#b44">Yao et al. 2019)</ref>. Alternatively, several robust loss functions <ref type="bibr" target="#b26">(Natarajan et al. 2013;</ref><ref type="bibr" target="#b29">Patrini et al. 2017;</ref><ref type="bibr" target="#b48">Zhang and Sabuncu 2018;</ref><ref type="bibr" target="#b41">Xu et al. 2019)</ref> have been proposed and justified. Many other works do not focus on theoretical analysis, yet propose methods based on empirical findings or intuitions, such as sample selection <ref type="bibr" target="#b13">(Han et al. 2018b;</ref><ref type="bibr" target="#b34">Song, Kim, and Lee 2019;</ref><ref type="bibr" target="#b45">Yu et al. 2019)</ref>, sample weighting <ref type="bibr" target="#b31">(Ren et al. 2018</ref>) and label correction <ref type="bibr" target="#b0">Arazo et al. 2019)</ref>.</p><p>Intuitively, it can be problematic to assign the same noise transition probability to diverse samples in a same class, which is illustrated by examples in <ref type="figure" target="#fig_0">Fig. 1</ref>. Theoretically, we can justify the need to go beyond the CCN assumption with the following theorem. Theorem 1. (CCN hypothesis testing) Given a noisy dataset with n instances, considering random sampling a validation setD = {(x i ,? i )} m i=1 , m &lt; n, and training a network f on the rest instances. After training, the validation error</p><formula xml:id="formula_0">onD is?r 0?1 D [f ] = m i=1 1 m 1(f(x i ) =? i ), where 1(?) is</formula><p>the indicator function. Let w p = Pr[Y = p], p ? Y be the fraction of samples in each class, then the following holds given the CCN assumption.</p><formula xml:id="formula_1">Pr 1 ? c p=1 w p max q?Y M p,q ??r 0?1 D [f ] ? ? ? e ?2m? 2<label>(1)</label></formula><p>Proof. Let er 0?1</p><formula xml:id="formula_2">D [f ] = E (X,? )?D 1(f(X) =? )</formula><p>be the generalization error on noisy distribution. For any f , the CCN assumption implies f (X) is independent of? conditional on Y , then we have,</p><formula xml:id="formula_3">er 0?1 D [f ] = 1 ? E (X,? )?D 1(f(X) =? ) = 1 ? c p=1 w p Pr[f (X) =? |Y = p] = 1 ? c p=1 w p c q=1 Pr[f (X) = q,? = q|Y = p] = 1 ? c p=1 w p c q=1 Pr[f (X) = q|Y = p] ? M p,q ? 1 ? c p=1 w p max q?Y M p,q .</formula><p>Note that the error?r 0?1 D [f ] is estimated on validation samples that are not used when training f , hence 1(f(x i ) = y i ), i = 1, ? ? ? , m, are m i.i.d Bernoulli random variables with expectation er 0?1 D [f ]. Using Hoeffding's inequality, we have, ?? &gt; 0,</p><formula xml:id="formula_4">Pr 1 ? c p=1 w p max q?Y M p,q ??r 0?1 D [f ] ? ? ? Pr er 0?1 D [f ] ??r 0?1 D [f ] ? ? ? e ?2m? 2 .</formula><p>Now we apply Theorem 1 to the widely used noise benchmark Clothing1M, which contains 1M noisy training samples of clothing images in 14 classes. We train a ResNet-50 on 500K random sampled instances, validate on the rest and obtain a validation accuracy?r 0?1 D [f ] = 0.1605. The original paper <ref type="bibr" target="#b40">(Xiao et al. 2015)</ref> provides additional refined labels and a noise confusion matrix that can be an estimator of M under the CCN assumption. Moreover, we estimate w p using the proportion of labels on the 14k refined subset. In this way, we get 1 ? c p=1 w p max q?Y M p,q = 0.3817. Now we have 1? c p=1 w p max q?Y M p,q ??r 0?1 D [f ] = 0.2212. By substituting ? = 0.2212 to Eq. (1), we see that this result happens with probability lower than 10 ?21250 , which is statistically impossible. This contradiction implies that the CCN assumption does not hold on Clothing1M. In fact, this result is explainable by analyzing the difference between CCN and IDN. Under the CCN assumption, in each class, label noise is independent of input features, hence the network can not generalize well on such independent noise. Thus we derive a high validation error conditioning on CCN. While practically, we obtain an error much lower than the derived one, which implies that the network learned feature-dependent noise that can be generalized to the noisy validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The IDN model and its challenges</head><p>Now both theoretical evidence and the intuition imply that label noise should be dependent on input features, yet limited research efforts have been devoted to IDN. For binary classification under IDN, there have been several pioneer theoretical analysis on robustness <ref type="bibr" target="#b25">(Menon, van Rooyen, and Natarajan 2018;</ref><ref type="bibr" target="#b4">Bootkrajang and Chaijaruwanich 2018)</ref> and sample selection methods <ref type="bibr" target="#b7">(Cheng et al. 2020)</ref>, mostly restricted to small-scale machine learning such as logistic regression. Under deep learning scenario, <ref type="bibr" target="#b38">Xia et al. (2020)</ref> combat IDN by assuming that the noise is parts-dependent, hence they can estimate the noise transition for each part. <ref type="bibr" target="#b36">Thulasidasan et al. (2019)</ref> investigate the co-occurrence of noisy labels with underlying features by adding synthetic features, such as the smudge, to mislabeled instances. However, this is not the typical realistic IDN where the noisy label should be dependent on inherent input features. As presented in Definition 2, we can model instancedependent mislabelling among given classes, where the noise transition matrix is a function of X. Note that both IDN and CCN consider close-set noise, as contrast to a specific label noise termed open-set noise , where the noisy instances does not belong to any considered classes. The graphical model of label noise is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. CCN can be seen as a degenerated case of IDN such that all instances have the same noise transition matrix.</p><formula xml:id="formula_5">Definition 2. (IDN Model) Under the IDN model, M : X ? [0, 1] c?c is a function of X. We observe samples (X,? ) ? D = IDN(D, M ), where first we draw (X, Y ) ? D as usual, then flip Y to produce? according to the conditional probability defined by M (X) , i.e., Pr(? = q|Y = p) = M p,q (X), where p, q ? Y.</formula><p>Many existing robust loss functions <ref type="bibr" target="#b26">(Natarajan et al. 2013;</ref><ref type="bibr" target="#b29">Patrini et al. 2017;</ref><ref type="bibr" target="#b48">Zhang and Sabuncu 2018;</ref><ref type="bibr" target="#b41">Xu et al. 2019)</ref> have theoretical guarantees derived from the CCN assumption but not IDN. Some sample selection algorithms <ref type="bibr" target="#b23">(Malach and Shalev-Shwartz 2017;</ref><ref type="bibr" target="#b13">Han et al. 2018b;</ref><ref type="bibr" target="#b45">Yu et al. 2019;</ref><ref type="bibr" target="#b19">Li, Socher, and Hoi 2020)</ref>, targeting at selecting clean samples from the noisy training set, work quite well under CCN. Though these methods does not directly rely on the CCN assumption, it can be more challenging to identify clean samples under IDN since the label noise is correlated with inherent input features that result in confusion.</p><p>Theoretically, we show that the optimal sample selection exists under CCN but may fail under IDN. This is because under IDN, even if we select all clean samples, the distribution of X can be different to its original distribution. While for CCN, we can select an optimal subset in theory. The key issue is whether the following holds for any p ? Y.</p><formula xml:id="formula_6">supp(P (X|? = Y, Y = p)) ? = supp(P (X|Y = p)), (2)</formula><p>where supp(?) is the support of a distribution. For CCN, since X is independent of? conditioning Y , the equality in Eq.</p><p>(2) holds. While for IDN, it mostly does not hold. For example, if samples near the decision boundary are more likely to be mislabeled, then the support is misaligned for clean samples, which means learning with selected clean samples is statistically inconsistent <ref type="bibr" target="#b7">(Cheng et al. 2020)</ref>. More characterizations will be presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A typical controllable IDN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Enabling controllable experiments</head><p>The rapid advance of research on CCN not only attributes to simplicity of the noise model, but also the easy generation process of synthetic noise. We are able to conduct experiments on synthetic CCN with varying noise fractions by randomly flipping labels according to the conditional probability defined by M , which enable us to characterize DNNs trained with CCN <ref type="bibr" target="#b1">(Arpit et al. 2017;</ref><ref type="bibr" target="#b6">Chen et al. 2019b</ref>), develop algorithms accordingly and quickly verify the idea. Similarly, it is desired to easily generate IDN with any noise fraction for any given benchmark dataset. A practical solution is to model IDN using DNNs' prediction error because the error is expected to be challenging for DNNs. To yield calibrated softmax output for IDN generation, <ref type="bibr">Berthon et al. (2020)</ref> train a classifier on a small subset, calibrate the classifier on another clean validation set <ref type="bibr" target="#b11">(Guo et al. 2017)</ref>, and then use predictions on the rest instances to obtain noisy labels. It does not generate noise for the whole dataset and the noise largely depends on the small training subset.</p><p>To stimulate the development of theory and methodology, we propose a novel IDN generator in Algorithm 1. Our labeler follows the intuition that 'hard' instances are more likely to be mislabeled <ref type="bibr" target="#b9">(Du and Cai 2015;</ref><ref type="bibr" target="#b25">Menon, van Rooyen, and Natarajan 2018)</ref>. Given a dataset D = {(x i , y i )} n i=1 with labels believed to be clean, we normally train a DNN for T epochs and get a sequence of networks with various classification performance. For each instance, if many networks predict a high probability on a class different to the labeled one, it means that it is hard to clearly distinguish the instance from this class. Therefore, we can compute the score of mislabeling N (x) and the potential noisy label?(x) as follow:</p><formula xml:id="formula_7">S = T t=1 S t /T ? R n?c , N (x i ) = max k =yi S i,k ,?(x i ) = arg max k =yi S i,k ,<label>(3)</label></formula><formula xml:id="formula_8">where S t = [f t (x i )] n i=1</formula><p>is DNN's output at t-th epoch. The average prediction here reveals the DNN's confusion on instances throughout training. We flip the label of p% instances Algorithm 1 IDN Generation.</p><formula xml:id="formula_9">Input: Clean samples D = {(x i , y i )} n i=1 , a targeted noise fraction p, epochs T . Initialize a network f . for t = 1 to T do for batches {(x i , y i )} i?B do</formula><p>Train f on {(x i , y i )} i?B using cross-entropy loss:</p><formula xml:id="formula_10">L CE = ? 1 |B| i?B log(f t yi (x i )) end for Record output S t = [f t (x i )] n i=1 ? R n?c . end for Compute N (x i ),?(x i ) using {S t } T t=1 (Eq. (3)). Compute the index set I = {p% arg max 1?i?n N (x i )}. Flip? i =? i if i ? I else keep? i = y i . Output: A dataset with IDN:D = {(x i ,? i )} n i=1 .</formula><p>with highest mislabeling scores, where p is the targeted noise fraction. In essence, Algorithm 1 uses predictions of the DNN to synthetize noisy labels, while it stands out for being able to generate noisy labels of any noise ratio for the whole training set, requiring simply a single round of training on given labels. The noise is instance-dependent since it comes from the prediction error on each instance. Moreover, it is a typical challenging IDN since the error is exactly the class hard for the DNN to distinguish. In Appendix A, examples of noisy samples show that the noise is semantically meaningful.  The curves of testing accuracy and CSR presented in <ref type="figure" target="#fig_3">Fig. 4</ref> show typical characterizations of the memorization effect. Similar to CCN, the model achieves maximum testing accuracy before memorizing all training samples under IDN, which suggests that DNNs can learn general patterns first. Moreover, the CSR increases during training, suggesting that DNNs learn gradually more complex hypotheses. It is worth noting that under IDN, both peak testing accuracy and CSR are lower, and the gap between peak and converged testing accuracy is smaller. On MNIST, the testing accuracy decreases since very early stage of training, suggesting that the memorizing of noise dominates learning of real data. Therefore, we conclude that the memorization effect still exists under IDN, but it is less significant compared to CCN.  <ref type="figure" target="#fig_2">Fig. 3</ref>. We plot the entry of softmax output corresponding to the noisy label and true label throughout training. DNNs will eventually memorize the wrong label, while during training, the output corresponding to the true label can be largely activated with oscillation. The intensity of oscillation and the epoch when the memorization happens is quite different for each instance. Algorithm 2 An iteration of SEAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Characterizations of training with IDN</head><formula xml:id="formula_11">Input: Noisy samplesD = {(x i ,? i )} n i=1</formula><p>, epochs T , soft labels from the last iterationS (optional).</p><p>Initialize a network f . ifS is not available then # The initial iteration, using one-hot noisy labelsS =</p><formula xml:id="formula_12">[e? i ] n i=1 ? R n?c where e? i is the one-hot label. end if for t = 1 to T do for batches {(x i ,S i )} i?B do Train f on {(x i ,S i )} i?B using the loss: L SEAL = ? 1 |B| i?B c k=1S i,k log(f t k (x i )) end for Record outputS t = [f t (x i )] n i=1 ? R n?c . end for UpdateS = T t=1S t /T ? R n?c .</formula><p>Output: Trained f ,S (can be used in next iteration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEAL: a primary attempt to combat IDN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>To mitigate the effect of label noise, we propose a practical algorithm termed self-evolution average label (SEAL). SEAL provides instance-dependent label correction by averaging predictions of a DNN on each instance over the whole training process, then retrains a classifier using the averaged soft labels. An iteration of SEAL is outlined in Algorithm 2, and we can apply SEAL with multiple iterations.</p><p>Here we discuss the intuitions of SEAL. Without loss of generality, assume there exists a latent optimal distribution of true label for each instance. Let S * i ? P c be the latent optimal label distribution of the i-th instance. S * i can be one-hot for a confident instance and be soft otherwise. Intuitively, we can image S * i as the output of an oracle DNN. Considering training a DNN on a c-class noisy datasetD = {(x i ,? i )} n i=1 for sufficient many T epochs until converged, we let f t (x i ) be the output on x i at t-th epoch. Based on the oscillations show in <ref type="figure" target="#fig_2">Figure 3</ref>, we roughly approximate the output on x i at t-th epoch as</p><formula xml:id="formula_13">f t (x i ) = ? t i ? t i + (1 ? ? t i )e? i ,<label>(5)</label></formula><p>where t ? {1, 2, ? ? ? , T }, e? i is the one-hot label, ? t i ? [0, 1] are coefficients dependent on instances and the network,</p><formula xml:id="formula_14">w t i ? P c are i.i.d. random vectors with E[w t i ] = S * i .</formula><p>The approximation may be inaccurate at the early stage of training because the network does not learn useful features and it is better to add a term for random predictions in the approximation. Still, we ignore this term because random predictions do not introduce bias toward any class and the effect is mitigated by taking the average. With the approximation, we intuitively compare with the prediction of a random epoch f ? (x i ), where ? is a random epoch such that Pr[? = t] = 1/T, ?t ? {1, 2, ? ? ? , T }. LetS ? R n?c be the soft labels obtained by SEAL and ? denote a norm on P c , it is not difficult to see that for any training instance</p><formula xml:id="formula_15">x i , E[S i ] ? S * i ? e? i ? S * i , (6) var(S i,k ) ? var(f ? k (x i ))</formula><p>, ?k ? {1, 2, ? ? ? , c}. (7) That is, SEAL yields instance-dependent label correction that is expected to be better than the given noisy labels and the label correction has lower variance due to taking the average.</p><p>We can run SEAL for multiple iteration to further correct the noise, and we term this 'self-evolution'. We take the soft label (denoted asS <ref type="bibr">[m]</ref> i , m ? 0) of the last iteration as input and outputS</p><p>[m+1] i . Using similar approximation as Eq. (5) by replacing the training label e? i withS <ref type="bibr">[m]</ref> i , SEAL is expected to produce labels that gradually approach the optimal ones,</p><formula xml:id="formula_16">E[S [m+1] i ] ? S * i ? S [m] i ? S * i .</formula><p>(8) A concern of SEAL is the increased computational cost due to retraining the network. In experiments, we focus on verifying the idea of SEAL and we retrain networks from the scratch in each iteration to show the evolution under exactly the same training process, resulting in scaled computational cost. While in practice, we may save computational cost by reserving the best model (e.g., using a noisy validation set) and training for less epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEAL v.s. related methods</head><p>Using predictions of DNNs has long been adopted in distillation <ref type="bibr" target="#b16">(Hinton, Vinyals, and Dean 2015)</ref> and robust training algorithms that use pseudo labels <ref type="bibr" target="#b30">(Reed et al. 2015;</ref><ref type="bibr" target="#b22">Ma et al. 2018;</ref><ref type="bibr" target="#b35">Tanaka et al. 2018;</ref><ref type="bibr" target="#b34">Song, Kim, and Lee 2019;</ref><ref type="bibr" target="#b27">Nguyen et al. 2019;</ref><ref type="bibr" target="#b0">Arazo et al. 2019)</ref>. SEAL provides an elegant solution that is simple, effective and has empirical and theoretical intuitions. Taking the average of predictions, motivated by the activation and oscillation of softmax output at the entry of true label, provides label correction. SEAL is different to vanilla distillation <ref type="bibr" target="#b16">(Hinton, Vinyals, and Dean 2015)</ref>: in the presence of label noise, simply distilling knowledge from a converged teacher network, which memorizes noisy labels, can not correct the noise.</p><p>Compared with existing pseudo-labeling methods, SEAL does not require carefully tuning hyperparameters to ensure that (i) the DNN learns enough useful features and (ii) the DNN dose not fit too much noise. It is challenging to compromise between (i) and (ii) in learning with IDN. We have shown that the memorization on correct/noisy labels can be quite different for each training instance. However, the above (i) and (ii) are typically required in existing methods <ref type="bibr" target="#b30">(Reed et al. 2015;</ref><ref type="bibr" target="#b22">Ma et al. 2018;</ref><ref type="bibr" target="#b35">Tanaka et al. 2018;</ref><ref type="bibr" target="#b34">Song, Kim, and Lee 2019;</ref><ref type="bibr" target="#b27">Nguyen et al. 2019;</ref><ref type="bibr" target="#b0">Arazo et al. 2019)</ref>. For example, one usually needs to tune a warm-up epoch <ref type="bibr" target="#b35">(Tanaka et al. 2018;</ref><ref type="bibr" target="#b34">Song, Kim, and Lee 2019;</ref><ref type="bibr" target="#b0">Arazo et al. 2019)</ref> before which no label correction is applied. A small warm-up epoch results in underfitting on useful features while a large one yields overfitting on noise. Worse still, one may need to tune an adaptive weight during training to determine how much we trust predictions of the DNN <ref type="bibr" target="#b30">(Reed et al. 2015;</ref><ref type="bibr" target="#b22">Ma et al. 2018)</ref>. As theoretically shown by <ref type="bibr" target="#b8">Dong et al. (2019)</ref>, the conditions are very strict for DNNs to converge and not to fit noise.</p><p>When implementing SEAL, there is no specific hyperparameters other than the canonical hyperparameters such as the training epoch and learning rate. To determine these canonical hyperparameters, we simply need to examine the training accuracy on the noisy dataset. Since SEAL averages predictions throughout training, the label correction can be effective even if the DNN memorizes noise when converged. Therefore, our criterion of choosing hyperparameters is to make sure the training accuracy is converged and it is as high as possible. Moreover, the model architecture and training hyperparameters can be shared in each iteration of SEAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Empirical evaluation</head><p>Experimental setup. Our experiments focus on challenging IDN and real-world noise. We demonstrate the performance of SEAL on MNIST and CIFAR-10 (Krizhevsky and Hinton 2009) with varying IDN fractions as well as largescale real-world noise benchmark Clothing1M <ref type="bibr" target="#b40">(Xiao et al. 2015)</ref>. We use a CNN on MNIST and the Wide ResNet 28?10 (Zagoruyko and Komodakis 2016) on CIFAR-10. On Clothing1M, we use the ResNet-50 <ref type="bibr" target="#b14">(He et al. 2016)</ref> following the benchmark setting <ref type="bibr" target="#b29">(Patrini et al. 2017;</ref><ref type="bibr" target="#b35">Tanaka et al. 2018;</ref><ref type="bibr" target="#b41">Xu et al. 2019</ref>  SEAL corrects label noise. We first evaluate distances between the true label and the soft label obtained by SEAL for all noisy instances. For each noisy instance, the distance is</p><formula xml:id="formula_17">d(S i , y i ) = S i ? e yi 1 / e? i ? e yi 1<label>(9)</label></formula><p>where the denominator is to normalize the distance such that d(S i , y i ) ? [0, 1]. Before running SEAL, the label is initialized as the one-hot observed label e? i , hence the distance concentrates at 1.0. In <ref type="figure">Fig. 5</ref>, we show histograms of distance distribution for all noisy instances. When running SEAL iteratively, the distribution moves toward the left (distance reduced), suggesting that the updated soft label approaches the true label. This verifies that SEAL can correct label noise on varying datasets and noise fractions. To further investigate  individual instances, we defineN (x)-the confidence that a label needs correction and?(x)-the label correction</p><formula xml:id="formula_18">N (x i ) = max k =?iS i,k ,?(x i ) = arg max k =?iS i,k .<label>(10)</label></formula><p>In <ref type="figure">Fig. 6</ref>, we present examples with the highestN (x) in each class, with the noisy observed label and proposed label correction (in parentheses) annotated on top of each image. SEAL can identify and correct noisy labels.</p><p>SEAL improves generalization under IDN. We conduct experiments on MNIST and CIFAR-10 with IDN of varying noise fractions, compared with extensive baselines including (i) cross-entropy (CE) loss; (ii) Forward (Patrini , which trains a network to estimate an instanceindependent noise transition matrix then corrects the loss; (iii) Co-teaching <ref type="bibr" target="#b13">(Han et al. 2018b)</ref>, where two classifiers select small-loss instances to train each other; (iv) Generalized Cross Entropy (GCE) loss, which is a robust version of CE loss with theoretical guarantee under CCN; (v) deep abstaining classifier (DAC) <ref type="bibr" target="#b36">(Thulasidasan et al. 2019)</ref>, which gives option to abstain samples depending on the cross-entropy error and an abstention penalty; (vi) Determinant based Mutual Information (DMI), which is an information-theoretic robust loss. The number of iterations is 10 on MNIST and 3 on CIFAR-10. SEAL consistently achieves the best generalization performance, as shown in <ref type="table" target="#tab_3">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref>, where we report the accuracy at the last epoch and repeat each experiment three times.</p><p>SEAL improves generalization under real-world noise.</p><p>Clothing1M <ref type="bibr" target="#b40">(Xiao et al. 2015</ref>) is a large-scale real-world dataset of clothes collected from shopping websites, with noisy labels assigned by the surrounding text. Following the benchmark setting <ref type="bibr" target="#b29">(Patrini et al. 2017;</ref><ref type="bibr" target="#b35">Tanaka et al. 2018;</ref><ref type="bibr" target="#b41">Xu et al. 2019)</ref>, the training set consists of 1M noisy instances and the additional validation, testing sets consist of 14K, 10K clean instances. The number of SEAL iterations is 3.</p><p>In <ref type="table" target="#tab_5">Table 3</ref>, we present the test accuracy. By default, SEAL is implemented with normal cross-entropy, where we see 1.56% absolute improvement. Notably, SEAL also improves advanced training algorithms such as DMI <ref type="bibr" target="#b41">(Xu et al. 2019)</ref> when we use the method as initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we theoretically justify the urgent need to go beyond the CCN assumption and study IDN. We formalize an algorithm to generate controllable IDN which is semantically meaningful and challenging. As a primary attempt to combat IDN, we propose a method SEAL, which is effective for both synthetic IDN and real-world noise.</p><p>Notably, our theoretical analysis in Section 2 provides rigorous motivations for studying IDN. Learning with IDN is an important topic that deserves more research attention in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of 8 in MNIST (first row) and Airplane in CIFAR-10 (second row). It is problematic to assume a same probability of mislabeling for diverse samples in each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The graphical model of label noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of softmax outputs on the noisy label and latent true label. The x-axis is training epoch and the y-axis is DNN's output probability. The airp. is airplane for short. IDN easily, the generalization performance degenerates at early stages of training. The observation is consistent with the findings on real-world noise presented by<ref type="bibr" target="#b17">Jiang et al. (2020)</ref>.The memorization effect is less significant. The memorization effect<ref type="bibr" target="#b1">(Arpit et al. 2017</ref>) is a critical phenomenon of DNNs trained with CCN: DNNs first learn simple and general patterns of the real data before fitting noise. It has motivated extensive robust training algorithms. The memorization effect is characterized by the testing accuracy and critical sample ratio (CSR)<ref type="bibr" target="#b1">(Arpit et al. 2017)</ref> during training, where CSR estimates the density of decision boundaries. A sample x is a critical sample if there exists ax, s.t., arg max k f k (x) = arg max k f k (x), s.t. x ?x ? ? r. (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training/testing accuracy and critical sample ratio throughout training on IDN and CCN with varying noise fractions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Histograms of distance distribution, where distance is evaluated between the true label and the soft label obtained by SEAL in 1-3 iteration. The noisy label and label correction (in parentheses) obtained from SEAL. The airp. and auto. are airplane and automobile for short.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>It is easier for DNNs to fit IDN. Firstly, let us focus on the training/testing curves inFig. 4. For IDN and CCN with the same noise fraction, the training accuracy is higher under IDN. This implies that it is easier for DNNs to fit IDN. The finding is consistent with our intuition since noisy labels under IDN are highly correlated with input features that can mislead DNNs. In this sense, IDN is more difficult to mitigate because the feature-dependent noise is very confusing for</figDesc><table /><note>To combat label noise, we can firstly characterize behaviors of DNNs trained with noise. For example, the memoriza- tion effect (Arpit et al. 2017) under CCN claims that DNNs tend to learn simple and general patterns first before memo- rizing noise, which has motivated extensive robust training algorithms. While our understanding on IDN is still limited. Here we present some empirical findings on IDN, to help researchers understand the behaviors of DNNs trained with IDN and to motivate robust training methods. We conduct ex- periments on MNIST and CIFAR-10 under IDN with varying noise fractions generated by Algorithm 1. For CCN, we use the most studied uniform noise. In all experiments throughout this paper, the DNN model and training hyperparameters we use are consistent. More details on experimental settings are summarized in Section 4.3 and Appendix B.DNNs, which can easily result in overfitting. Moreover, the peak testing accuracy before convergence, which implies the DNN learns general patterns first (Arpit et al. 2017), is much lower under IDN. This suggests that due to DNNs can fit</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies (%) on MNIST under instance-dependent label noise with different noise fractions.</figDesc><table><row><cell>Method</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>CE</cell><cell cols="4">94.07 ?0.29 ?0.56 ?0.09 ?0.56 85.62 75.75 65.83</cell></row><row><cell>Forward</cell><cell cols="4">93.93 ?0.14 ?0.92 ?0.81 ?0.42 85.39 76.29 68.30</cell></row><row><cell>Co-teaching</cell><cell cols="4">95.77 ?0.03 ?0.19 ?0.35 ?0.84 91.07 86.20 79.30</cell></row><row><cell>GCE</cell><cell cols="4">94.56 ?0.31 ?0.47 ?0.43 ?0.58 86.71 78.32 69.78</cell></row><row><cell>DAC</cell><cell cols="4">94.13 ?0.02 ?0.56 ?0.58 ?0.78 85.63 75.82 65.69</cell></row><row><cell>DMI</cell><cell cols="4">94.21 ?0.12 ?0.42 ?0.64 ?0.73 87.02 76.19 67.65</cell></row><row><cell>SEAL</cell><cell cols="4">96.75 ?0.08 ?0.33 ?0.15 ?0.41 93.63 88.52 80.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies (%) on CIFAR-10 under instance-dependent label noise with different noise fractions.</figDesc><table><row><cell>Method</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>CE</cell><cell cols="4">91.25 ?0.27 ?0.11 ?0.05 ?0.29 86.34 80.87 75.68</cell></row><row><cell>Forward</cell><cell cols="4">91.06 ?0.02 ?0.11 ?2.66 ?0.47 86.35 78.87 71.12</cell></row><row><cell>Co-teaching</cell><cell cols="4">91.22 ?0.25 ?0.20 ?0.17 ?0.47 87.28 84.33 78.72</cell></row><row><cell>GCE</cell><cell cols="4">90.97 ?0.21 ?0.23 ?0.15 ?0.39 86.44 81.54 76.71</cell></row><row><cell>DAC</cell><cell cols="4">90.94 ?0.09 ?0.13 ?0.46 ?0.32 86.16 80.88 74.80</cell></row><row><cell>DMI</cell><cell cols="4">91.26 ?0.06 ?0.16 ?0.57 ?0.85 86.57 81.98 77.81</cell></row><row><cell>SEAL</cell><cell cols="4">91.32 ?0.14 ?0.09 ?0.01 ?0.05 87.79 85.30 82.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Testing accuracy (%) on Clothing1M. The * marks published results.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>CE *</cell><cell>68.94</cell></row><row><cell>Forward *</cell><cell>69.84</cell></row><row><cell>Co-teaching</cell><cell>70.15</cell></row><row><cell>GCE *</cell><cell>69.09</cell></row><row><cell>Joint Optimization *</cell><cell>72.16</cell></row><row><cell>DMI *</cell><cell>72.46</cell></row><row><cell>CE</cell><cell>69.07</cell></row><row><cell>SEAL</cell><cell>70.63</cell></row><row><cell>DMI</cell><cell>72.27</cell></row><row><cell>SEAL (DMI)</cell><cell>73.40</cell></row><row><cell>et al. 2017)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The work is supported by the Key-Area Research and Development Program of Guangdong Province, China (2020B010165004) and the National Natural Science Foundation of China (Grant Nos.: 62006219, U1813204).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Examples of noisy samples As an example, each row corresponds to instances of the same true class, consisting of two correct instances and eight mislabeled instances. The airp. and auto. are airplane and automobile for short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More details on experiments</head><p>? On MNIST, we use a convolution neural network (CNN) with the standard input 28?28 and 4 layers as follows: [conv 5?5, filters 20, stride 1, relu, maxpool /2]; [conv 5?5, filters 50, stride 1, relu, maxpool /2]; [fully connect 4*4*50?500, relu]; [fully connect 500?10, softmax]. Models are trained for 50 epochs with a batch size of 64 and we report the testing accuracy at the last epoch. For the optimizer, we use SGD with a momentum of 0.5, a learning rete of 0.01, without weight decay.</p><p>? On CIFAR-10, we use the Wide ResNet 28?10. Models are trained for 150 epochs with a batch size of 128 and we report the testing accuracy at the last epoch. From <ref type="figure">Fig 4</ref> in the main paper, we can see that the epoch of 150 is sufficient large for the training accuracy to converges to 100%. For the optimizer, we use SGD with a momentum of 0.9 and a weight decay of 5 ? 10 ?4 The learning rate is initialized as 0.1 and is divided by 5 after 60 and 120 epochs. We apply the standard data augmentation on CIFAR-10: horizontal random flip and 32?32 random crop after padding 4 pixels around images. The standard normalization with mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010) is applied before feeding images to the network.</p><p>? On Clothing1M, following the benchmark setting <ref type="bibr" target="#b29">(Patrini et al. 2017;</ref><ref type="bibr" target="#b35">Tanaka et al. 2018;</ref><ref type="bibr" target="#b41">Xu et al. 2019)</ref>, we use the ResNet-50 pre-trained on ImageNet and access the clean validation set consisting of 14K instances to do model selection. Models are trained for 10 epochs with a batch size of 256 on the noisy training set consisting of 1M instances. For the optimizer, we use SGD with a momentum of 0.9 and a weight decay of 10 ?3 . We use a learning rate of 10 ?3 in the first 5 epochs and 10 ?4 in the second 5 epochs in all experiments except for DMI <ref type="bibr" target="#b41">(Xu et al. 2019)</ref>, where the learning rate is 10 ?6 and 0.5 ? 10 ?6 according to its original paper. We apply the standard data augmentation: horizontal random flip and 224?224 random crop. Before feeding images to the network, we normalize each image with mean and std from ImageNet, i.e., mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225). Considering that a pre-trained model and a clean validation are accessed in all methods, we do not reinitialize our model in each SEAL iteration, instead, we start the training on top of the best model from the last iteration.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised Label Noise Modeling and Loss Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="arXiv">arXiv:2001.03772</idno>
		<title level="m">Confidence Scores Make Instance-dependent Labelnoise Learning Possible</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards instance-dependent label noise-tolerant classification: a probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bootkrajang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaijaruwanich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Combining labeled and unlabeled data with co-training</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A meta approach to defend noisy labels by the manifold regularizer PSDR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01255</idno>
		<title level="m">Distillation ? Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modelling class noise with symmetric and asymmetric distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Normalized Loss Functions for Deep Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality-Driven Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can gradient clipping mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from binary labels with instance-dependent noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01842</idno>
		<title level="m">Self: Learning to filter noisy labels with self-ensembling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Loss factorization, weakly supervised learning and label noise robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training Deep Neural Networks on Noisy Labels with Bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to Reweight Examples for Robust Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harvesting image databases from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Classification with asymmetric label noise: Consistency and maximal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Handy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SELFIE: Refurbishing Unclean Samples for Robust Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combating Label Noise in Deep Learning using Abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parts-dependent label noise: Towards instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Are Anchor Points Really Indispensable in Label-Noise Learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">L DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust semi-supervised learning through label aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Safeguarded Dynamic Label Regression for Generalized Noisy Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How does Disagreement Help Generalization against Label Corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
