<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
							<email>lvpengyuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
							<email>zhangchengquan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Liu</surname></persName>
							<email>liushanshan07@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Qiao</surname></persName>
							<email>qiaomeina@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
							<email>xuyangliu@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wu</surname></persName>
							<email>wuliang11@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yao</surname></persName>
							<email>yaokun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<email>hanjunyu@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<email>wangjingdong@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a model pretraining technique, named MaskOCR, for text recognition. Our text recognition architecture is an encoder-decoder transformer: the encoder extracts the patch-level representations, and the decoder recognizes the text from the representations. Our approach pretrains both the encoder and the decoder in a sequential manner. (i) We pretrain the encoder in a self-supervised manner over a large set of unlabeled real text images. We adopt the masked image modeling approach, which shows the effectiveness for general images, expecting that the representations take on semantics. (ii) We pretrain the decoder over a large set of synthesized text images in a supervised manner and enhance the language modeling capability of the decoder by randomly masking some text image patches occupied by characters input to the encoder and accordingly the representations input to the decoder. Experiments show that the proposed MaskOCR approach achieves superior results on the benchmark datasets, including Chinese and English text images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Optical character recognition aims to recognize texts within a digital image, e.g., a scanned document, a photo of a document, a scene-photo, and so on. It has wide-range applications, such as visual search, document digitization, and so on. Optical character recognition generally consists of two tasks: text detection, localizing the text region, and text recognition, identifying the text from the localized region, which is the interest of this paper.</p><p>There are three main pipelines for text recognition. (i) The character-based pipeline <ref type="bibr">[48; 23; 29]</ref> localizes each character, performs character recognition, and then groups them into words. (ii) The word-based pipeline <ref type="bibr" target="#b20">[21]</ref> performs word classification directly. (iii) The sequence-based pipeline <ref type="bibr">[42; 43; 50; 12]</ref> regards text recognition as a sequence labeling problem, and is adopted by most deep learning methods.</p><p>We follow the sequence-based pipeline and adopt an encoder-decoder transformer for text recognition. The encoder is a ViT architecture <ref type="bibr" target="#b10">[11]</ref>, a sequence of self-attention and FFN blocks, for text image patch representation extraction. The decoder is formed with the DETR-style <ref type="bibr" target="#b2">[3]</ref> decoder, a sequence of self-attention, cross-attention and FFN blocks, mapping the patch representations to a text with a expected role of language modeling <ref type="bibr" target="#b26">[27]</ref>.</p><p>Our main work lies in exploring the pretraining technology for text recognition. Our approach, called MaskOCR, pretrains both the encoder and the decoder with the masking strategy in a sequential manner. We follow the self-supervised pretraining framework and adopt a masked image modeling approach to pretrain the encoder for semantic patch representation learning. We divide the text image into a set of vertical patches, and randomly mask some patches that may contain a part of some character, or some whole characters. We predict the representations of the masked patches from the visible patches in the representation space learned from the encoder, and map the predicted representations to the masked patch images.</p><p>We pretrain the decoder in a supervised manner with the masking strategy for language modeling over synthesized text images. We fix the pretrained encoder and only update the decoder, so that this pretraining task explores the language rule and the encoder is not affected by the synthesized text image style that might be different from the downstream tasks.</p><p>We validate the effectiveness of the proposed MaskOCR approach on the benchmark datasets, including Chinese and English text images. The experiments show that our approach achieves superior results over previous text recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scene text recognition There are three main kinds of solutions to text recognition: characterbased <ref type="bibr">[48; 23; 29]</ref>, word-based <ref type="bibr" target="#b20">[21]</ref>, and sequence-based <ref type="bibr">[42; 43; 50; 12]</ref>. The character-based methods recognize text image by performing character localization, character classification and character grouping. The word-based methods directly treat each word as a category and recognition text by image classification. The sequence-based methods regard text recognition as a sequence labeling problem. Besides CTC <ref type="bibr" target="#b13">[14]</ref> and attention mechanism <ref type="bibr">[43; 34; 50; 12]</ref> are used to align the input image patch sequence and the output character sequence. The sequence-based solution has been extensively studied recently because ground-truth labeling is easier and the framework is more flexible. The architecture for the sequence-based solution consists of two main modules: encoder and decoder. The encoder aims to learn semantic representations for text images, and it can be formed using convolutional networks or transformers. The decoder aims to recognize the characters from the representations and map them to the texts. Different from the previous sequence-based methods <ref type="bibr">[42; 43; 12; 54; 1; 30; 50; 49]</ref>, which consists of convolutional layers or recurrent layers, our proposed method is a fully transformer network which is simple but more effective.</p><p>Pretraining Representation pretraining, e.g., supervised or self-supervised pretraining on ImageNet in computer vision and self-supervised pretraining in natural language processing, has shown to be beneficial to downstream tasks. The self-supervised pretraining that is not limited by lableled data has attracted a lot of attention. In <ref type="bibr">[17; 7; 8]</ref>, the representation pretraining is learned with siamese networks by contrastive learning. In <ref type="bibr">[16; 2; 10]</ref>, the vision and nlp models are masked autoencoders, which enhance the representation by learning to predict the masked content.</p><p>Encoder pretraining for text recognition Recently, some pretraining methods were used to learn the representations for text recognition. TrOCR <ref type="bibr" target="#b26">[27]</ref> pretraining the recognition model on synthesized data by supervised learning. In <ref type="bibr">[1; 30; 54]</ref>, self-supervised contrastive pretraining were introduced to learn representations from the input images.</p><p>Differently, the encoder pretraining of MaskOCR follows the masked image modeling framework, which is recently studied in computer vision and shows superior performance over contrastive pretraining <ref type="bibr">[6; 16]</ref>, to learn robust character representations from unlabeled text images.</p><p>Decoder pretraining for text recognition The attention decoder <ref type="bibr">[43; 26; 50]</ref> in some sense is able to learn the text relation from the training data. Besides, the masked language modeling approach, such as BERT <ref type="bibr" target="#b9">[10]</ref>, is introduced to model the relation of the representations or the relation of the characters that are output from the CTC <ref type="bibr" target="#b13">[14]</ref> or attention decoder. In <ref type="bibr" target="#b11">[12]</ref>, a masked language model is pretrained for correcting the predicted results from the visual model. In <ref type="bibr" target="#b26">[27]</ref>, the linguistic information are modeling by employing supervised learning on synthesized data.</p><p>We also attempt to pretrain the decoder from the synthesized data for enhancing the capability of solving the task: grouping the frame-wise predictions corresponding to the same character region and exploring the linguistic rule by masking some representations output from the encoder. This is different from <ref type="bibr" target="#b26">[27]</ref>: the encoder is also pretrained, potentially degrading the encoding quality due to using the synthesized data for pretraining. <ref type="figure">Figure 1</ref>: Encode-decoder transformer for text recognition. The encoder extracts a sequence of patch representations, and the decoder maps the patch representations to a sequence of representations, followed by a linear layer to recognize the sequence of characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R O N A L D O _</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder-Decoder Transformer for Text Recognition</head><p>We adopt the encoder-decoder transformer architecture for text recognition. The encoder extracts a sequence of patch representations, and the decoder maps the patch representations to a text. <ref type="figure">Figure 1</ref> illustrates the encoder-decoder transformer architecture.</p><p>Encoder. The encoder receives an image, I ? R 3?H?W , as the input. We partition the image horizontally into a set of M vertical patches, [p 1 , p 2 , . . . , p M ]. The size of each patch is H ? W/M . We process the flattened patches using linear projection to get patch embeddings. We add the 1D positional embeddings, which is enough as we partition the images vertically. We use the ViT <ref type="bibr" target="#b10">[11]</ref>, consisting of a sequence of multi-head self-attention and FFN units, as the encoder and learn the patch-level representations, F = [f 1 , f 2 , . . . , f M ], for the text image.</p><p>Decoder. We form the text recognition decoder by following the decoder style of the DETR <ref type="bibr" target="#b2">[3]</ref> that is original designed for object detection. A decoder is composed of a multi-head self-attention unit, a multi-head cross-attention unit, and an FFN unit.</p><p>The decoder transforms the N input embeddings, C = [c 1 , c 2 , . . . , c N ], called character queries, into output embeddings, which are then independently mapped into characters through a linear classifier. Multi-head self-attention is performed over the character queries for the first decoder layer, or the output embeddings of the previous decoder layer. Multi-head cross-attention, also called encoder-decoder attention, uses the encoder outputs F as the keys and the values and the character queries for the first decoder layer, or the output embeddings of the previous decoder layer as the queries.</p><p>The multi-head self-attention, crucial for duplicate detection and non-maximum suppression removal in DETR, plays the role of merging the duplicate predictions from the patches that correspond to the same character region and promoting one prediction and suppressing other duplicate predictions.</p><p>Our decoder self-attention is different from masked attention that is widely used in machine translation <ref type="bibr" target="#b45">[46]</ref> and also used for text recognition <ref type="bibr" target="#b26">[27]</ref>. The decoder based on masked attention models the language in a unidirectional way <ref type="bibr" target="#b26">[27]</ref>, our decoders models the language in a bidirectional manner. During inference, the decoder based on masked attention predicts the letters one by one sequentially. In contrast, our decoder is more efficient and predicts the characters simultaneously.</p><p>Loss. We denote the character predictions by Y = [y 1 y 2 . . . y N ]. Assuming N is larger than the number of characters in the text image. We consider the ground truth as Y * = [y * 1 y * 2 . . . y * N ] padded with an end of sentence symbol <ref type="bibr">[EOS]</ref>. The loss function is formulated as follows,</p><formula xml:id="formula_0">(Y, Y * ) = 1 L + 1 L+1 l=1 CE(y l , y * l ),<label>(1)</label></formula><p>Latent contextual regressor Alignment <ref type="figure">Figure 2</ref>: Encoder pretraining architecture. We adopt a masked image modeling approach, CAE <ref type="bibr" target="#b5">[6]</ref> to pretrain the encoder for text image representation learning. Slightly different from the CAE, we use the image patches (RGB) as the prediction targets. In this example, six image patches (top) are visible patches, and the other four (bottom) to be predicted are masked patches.</p><p>where CE(?, ?) is the cross-entropy loss. L is the number of characters in the text image. To balance the number of [EOS] and other characters, we only employ loss function on the characters as well as the first [EOS].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Masked Encoder Pretraining</head><p>We follow the masked image modeling framework, which is recently studied for general image representation pretraining, and adopt the context autoencoder-style method <ref type="bibr" target="#b5">[6]</ref> to pretrain the encoder for text image representation learning.</p><p>The encoder pretraining process is given as follows. The text image is divided into a set of visible image patches and a set of masked image patches. The visible image bands are sent to the encodoer, generating the representations of visible patches. Then, the representations of visible patches are fed into a latent contextual regressor with mask queries, predicting the representations for masked patches Z m which is expected to be close to the representations Z * m of masked patches directly computed from the encoder. Last, the representations of masked patches go into the image decoder, predicting the targets T m . <ref type="figure">Figure 2</ref> illustrates the encoder pretraining architecture.</p><p>We adopt the patch partitioning strategy, same with encoder-decoder transformer for text recognition, for encoder pretraining, and partition the text image into a sequence of M vertical patches with the size being H ? W/M . The number M of patches is usually larger than the number of characters contained in the text image, thus a patch is only a portion of a character. When many patches are masked, there are two cases: a part for some characters is masked, or some characters are completely masked.</p><p>Considering that the DALL-E <ref type="bibr" target="#b39">[40]</ref> tokenizer used in CAE <ref type="bibr" target="#b5">[6]</ref>, that is learned over the general images, and we handle special images: text images, we use the patch RGB values, processed with layer normalization (Gaussian normalization), to form the targets. The loss function for encoder pretraining is a combination of representation alignment loss and prediction loss, and given as follows,</p><formula xml:id="formula_1">t (T m , T * m ) + ? z (Z m , Z * m ).<label>(2)</label></formula><p>Here, both losses t (?, ?) and z (?, ?) are the MSE loss. ? is the tradeoff parameter, and set to be 0.05 in our implementation.</p><p>There are two main kinds of prediction tasks. One is to predict a part of a character from a visible part of the character and other characters. The other one is to predict some whole character from other characters. We assume that the predictions are more reliable if the representations for the patches take on semantics. <ref type="figure">Figure 3</ref>: Decoder pretraining architecture. The whole pipeline is similar with the one in <ref type="figure">Figure 1</ref>.</p><formula xml:id="formula_2">R O N O L D O _</formula><p>The difference is that the input to the encoder are the visible patches. The visible patches are formed by masking the image patches that correspond to the target characters (the patch number may be greater than the character number). The input representations to the decoder are a combination of encoded representations and zero representations added to the positions (gray boxes) where the masked patches are. The prediction targets are the characters that are masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Masked Decoder Pretraining</head><p>We pretrain the decoder on synthesized text images using the supervision loss. Considering the style of synthesized text images might be different from the real text images, we keep the encoder (learned from encoder pretraining) not updated and only optimize the decoder, so that this pretraining stage does not influence the representation quality.</p><p>In addition to removing duplicate predictions, the decoder is capable of modeling the language for better text recognition. To further enhance the capability of language modeling, we adopt the idea of masked language modeling and introduce a masked decoder pretraining scheme. We randomly mask some characters and accordingly the patches, and send the remaining visible patches to the encoder, obtain the representations of visible patches, F v . Then, we insert the zero representations </p><formula xml:id="formula_3">(? l , Y * l ) = 1 L L l=1 CE(y n l , y * n l ),<label>(3)</label></formula><p>where L is the number of masked characters, and {n 1 , n 2 , . . . , n L } are the positions of the masked characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Chinese text line images. The pretraining set consists of 100 million unlabeled text line images collected from practical scenarios for encoder pretraining, and 100 million synthetic text line images for decoder pretraining. The real images are collected from document and street view, and the text in them are almost in Chinese. We collect text corpus from Chinese corpus 2 , and generate 100 million images with 64 commonly used fonts using Text Render 3 . Specifically, for each synthetic sample, the text transcription as well as the character bounding boxes are given.</p><p>We first pretrain the encoder and decoder serially on the collected real images and the synthetic images, and then evaluate our model on a large-scale Chinese text image benchmark BCTR <ref type="bibr" target="#b4">[5]</ref>. BCTR consists of four subsets (scene, web, document, and handwriting) and provides 1.4 million fully labeled images in total. The scene subset is derived from some scene text datasets, including RCTW <ref type="bibr" target="#b43">[44]</ref>, ReCTS <ref type="bibr" target="#b52">[53]</ref>, LSVT <ref type="bibr" target="#b44">[45]</ref>, ArT <ref type="bibr" target="#b8">[9]</ref>, and CTW <ref type="bibr" target="#b50">[51]</ref>, resulting in 636,455 images. The web subset is constructed based on the MTWI <ref type="bibr" target="#b17">[18]</ref> dataset and contains 140589 text images. The document subset is composed of 500000 synthetic text images generated by Text Render in document style. The handwriting subset is collected from a handwriting dataset SCUT-HCCDoc <ref type="bibr" target="#b51">[52]</ref>, and 116643 text images are included.</p><p>English text word images. We collect about 4.2 million unlabeled English word images from COCO-Text <ref type="bibr" target="#b46">[47]</ref> and Book32 <ref type="bibr" target="#b18">[19]</ref> for encoder pretraining. To be specific, we obtain about 0.5 million cropped images from COCO-Text with the official annotations. Besides, about 3.7 million cropped images are collected from Book32 with the predicted bounding boxes yielded by DB 4 <ref type="bibr" target="#b27">[28]</ref>.</p><p>In addition, we also synthesize 100 million English word images for decoder pretraining. Similarly, we collect corpus from WikiText103 <ref type="bibr" target="#b34">[35]</ref> and generate synthetic images with Text Render and 10 commonly used English fonts.</p><p>Following <ref type="bibr">[43; 50; 12; 49; 54]</ref>, two synthetic dastsets MJSynth <ref type="bibr" target="#b19">[20]</ref> and SynthText <ref type="bibr" target="#b14">[15]</ref> are used for the training of downstream recognition task. We evaluate our model on six public scene text datasets: ICDAR 2013 (IC13) <ref type="bibr" target="#b24">[25]</ref>, Street View Text (SVT) <ref type="bibr" target="#b47">[48]</ref>, IIIT5K-Words (IIIT5K) <ref type="bibr" target="#b35">[36]</ref>), ICDAR 2015 (IC15) <ref type="bibr" target="#b23">[24]</ref>, Street View Text-Perspective (SVTP) <ref type="bibr" target="#b36">[37]</ref>, and CUTE80 (CUTE) <ref type="bibr" target="#b40">[41]</ref>). The samples in the first three datasets are all regular text images and the remaining datasets may contain perspective or curved text images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Encoder-decoder transformer. The image patches are fed into a linear projection layer, and then sent to the ViT. Three ViT structures are studied: ViT-S (12 transformer blocks with dimension 384), ViT-B (12 transformer blocks with dimension 768), and ViT-L (24 transformer blocks with dimension 1024). The decoder consists of four decoder layers, each of which includes a self-attention unit, a cross-attention unit, and an FFN unit. Each attention module is a 12-head attention with dimension 384.</p><p>We train the encoder-deocoder transformer with AdamW optimizer <ref type="bibr" target="#b31">[32]</ref>, cosine learning rate decay <ref type="bibr" target="#b30">[31]</ref>, a weight decay of 0.05, a drop path ratio of 0.1 and a batch size of 512. When the model is trained from scratch, the learning rate is set to 1e ? 3. Otherwise, the model is optimized with a initial learning rate of 1e ? 4. We set the training epochs as 120 and 20 for the Chinese text line recognition model and the English word recognition model with a warm-up of 5 epochs and 0.5 epoch respectively.</p><p>Masked encoder pretraining. The CAE regressor consists of four regressor layers. Each layer includes a cross-attention unit, where the queries are learnable mask queries for the first layer and the output embeddings of the previous layer for other layers, and an FFN unit. The CAE decoder consists of four layers, and each layer includes a self-attention unit and an FFN unit. Each attention module is also a 12-head attention with dimension 384. Following <ref type="bibr" target="#b15">[16]</ref>, we use the normalized pixel values of each masked patch as task.</p><p>We optimize the model with AdamW optimizer and set learning rate with the linear learning rate scaling rule <ref type="bibr" target="#b12">[13]</ref>: lr = base_lr ? batchsize/256. By default, the base_lr is set to 1.5e ? 4 with cosine learning rate decay and a 0.5 epoch warm-up. We train the encoder for 10 epochs with the batch size being 4096 for ViT-S and ViT-B, and 2048 for ViT-L due to the memory limitation of GPU.</p><p>Masked decoder pretraining. We mask some characters and accordingly mask the patches that contain the characters. This might lead to that a different number of patches are masked for different text images as one character may correspond to a different number of patches. We adopt masked attention to replace the original attention in the encoder with the parameters unchanged.</p><p>We pretrain the decoder for 5 epochs with a batch size of 512, a initial learning rate of 1e ? 4, a 0.5 epoch warmup and a cosine learning rate decay.</p><p>Data preprocessing. Since the Chinese text line images vary greatly in width, we resize the height of the input image to 32 with the aspect ratio kept and pad the width of the input images to 400. For the English word samples, we directly resize all input images to 32 ? 128. We set the width of the split vertical patch to 4 for all datasets by default. During the training of downstream recognition, some data augmentations like rotation, distortion, and colorJitter are also used. <ref type="table">Table 1</ref>: Ablation about encoder pretraining. We report the text recognition results for three cases: randomly initialized encoder, pretrained encoder fixed during downstream task training, and pretrained encoder fine-tuned during downstream task training. One can see that the score is also good when the pretrained encoder is fixed, indicating that the pretrained encoder is already able to learn the semantics. Fine-tuning pretrained encoder outperforms fixing pretrained encoder, implying that the adaptation to downstream tasks, exploiting the domain knowledge, is helpful. The superiority over random encoder demonstrates the effectivess of encoder pretraining.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We evaluate BCTR by first processing the predictions and ground truth with the following rules as <ref type="bibr" target="#b4">[5]</ref>: (i) convert the full-width characters to half-width characters; (ii) convert all traditional Chinese characters to simplified characters; (iii) convert all English characters to lowercase; (iv) remove all spaces. After that, we computer the accuracy in sentence level over each subsets and the whole dataset (avg).</p><p>To evaluate the six scene English text datasets, we follow <ref type="bibr">[43; 50; 12; 49; 54]</ref> and evaluate the recognition performance of our model with the case-insensitive word accuracy. We also report the average accuracy (avg) over both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this section, we conduct ablation studies on BCTR dataset to verify the effectiveness of pretraining. All experiments are conducted on 8 A100 GPUs with the ViT-B as encoder.</p><p>Encoder pretraining. We compare the results of downstream tasks for three cases: (i) randomly initialized encoder, (ii) pretrained encoder fixed during downstream task training, and (iii) pretrained encoder fine-tuned during downstream task training. The results are given in <ref type="table">Table 1</ref>. We use linear probing (ii) to evaluate the pretraining quality of representation learning by fixing the pretrained encoder and only update the rest modules. The acceptable result in <ref type="table">Table 1</ref> shows that the pretrained encoder can learn meaningful representations from the input image. We also fine-tune the pretrained encoder during the downstream tasking, and the accuracy is improved by 4.0% over training from scratch, also showing the effectiveness of the encoder pretraining.</p><p>We also explore the effect of different masking ratios: 0.30, 0.45, and 0.60. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. The masking ratio of 0.45 works better in downstream recognition task. We observe that our optimal masking ratio is not so high as <ref type="bibr" target="#b15">[16]</ref> (0.75), perhaps because the text images have higher information density.</p><p>Decoder pretraining. We report the results for: (i) randomly initialized decoder with fixing pretrained encoder, (ii) pretrained encoder and decoder fixed during downstream task training (only fine-tuning the linear classifiers), (iii) pretrained decoder fine-tuned only during downstream task training, and (iv) fine-tuning both the encoder and the decoder. The results are given in <ref type="table">Table 3</ref>. When conducting linear probing (ii), our method achieves the accuracy of 47.8%, showing that our pretraining can learn useful representations for the downstream recognition task. We also finetune the decoder with the encoder fixed and fine-tuned, and 0.8% (73.6% v.s. 74.4%) and 1.0% (79.8% v.s. <ref type="table">Table 3</ref>: Ablation about decoder pretraining. We consider four cases: randomly initialized decoder, pretrained decoder fixed during downstream task training (only fine-tuning the linear classifier), pretrained decoder fine-tuned during downstream task training, and fine-tuning both the encoder and the decoder. The first three cases do not fine-tune the pretrained encoder. For the case of fixing pretrained decoder, we only fine-tune the linear classifier. The result (47.8%) shows that the pretrained decoder has certain capability about decoding. The reason that the result is not high is that the dataset for pretraining the decoder is synthesized and there is a domain difference from the downstream dataset, BCTR.    80.8%) improvements are obtained respectively, further validating the effectiveness of our decoder pretraining.</p><p>We report the results of decoder pretraining for two cases in <ref type="table" target="#tab_3">Table 4</ref>: masking patches and not masking patches. Compared to the model of without decoder pretraining, the two cases of decoder pretraining contribute an improvement of 0.7% and 1.0% respectively. Besides, the better results from masking patches suggest that the ability of language modeling is enhanced.</p><p>We also study the performance for the case of retraining the encoder from the pretrained weights when pretraining the decoder. The results are shown in <ref type="table" target="#tab_4">Table 5</ref>. The accuracy of 76.7% is achieved, which is better than the model from scratch (76.7%v.s.75.8%). However, the accuracy drops by 4.1% when compared with our serially pretraining (76.7%v.s.80.8%), which indicates that the pretrained encoder is affected by the synthesized text images.</p><p>Vertical patch size. We consider two kinds of patch sizes and evaluate the performance over the BCTR dataset without pretraining. The results are shown in <ref type="table" target="#tab_5">Table 6</ref>. The larger patch size achieves worse performance and drops the accuracy by 3.6%, probably due to the embedding token of larger patch size has higher information density, which is more difficult to learn.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-art Methods</head><p>Chinese Text Line Recognition. We evaluate the ability of our model to recognize Chinese text line on the BCTR dataset. We set the number of input embeddings N to 40, since most of the samples in BCTR have less than 40 characters. We show the results of our method and representative existing methods on the BCTR dataset in <ref type="table" target="#tab_6">Table 7</ref>. When training from scratch, our method with ViT-B as encoder outperforms all the previous methods which with the similar model size. Specifically, our method is better than the previous best method TransOCR <ref type="bibr" target="#b3">[4]</ref> by 3.0% (72.8%v.s.75.8%). When training with pretrained encoder and decoder, our models outperforms all the previous methods and surpass the previous best results by large margins. In detail, our method shows steady improvement with the increase of the model size, and improve over the state-of-the-art by 5.3%, 8.0%, and 9.8% respectively.</p><p>English scene text recognition. Following <ref type="bibr">[42; 43]</ref>, we set the number of input embeddings N to 25 which exceeds the lengths of most English words. Since scene text appeared in natural scenes always with distortions or irregular layout, we employ a spatial transformer network <ref type="bibr" target="#b21">[22]</ref> which is adopted in <ref type="bibr" target="#b42">[43]</ref> to rectify the input image and train it with our recognizer jointly. We report the results of our method and representative existing methods on English Scene text datasets in <ref type="table" target="#tab_7">Table  8</ref>. Our small model achieves the best performance over the previous counterparts with the similar model size. Specifically, our method achieves better accuracy than PerSec <ref type="bibr" target="#b29">[30]</ref> which pretrained with 100 million real data, while we only use 4.2 million real data for pretraining. ABINet <ref type="bibr" target="#b11">[12]</ref> and its extension ConCLR <ref type="bibr" target="#b53">[54]</ref>, performing similarly to our approach with small ViT, use an explicit pretrained masked language model to correct the predicted result iteratively, which is complementary to our approach and may benefit our approach. Our method benefit from bigger models: ours large model pushs the SOTA result to a new level: 93.8% .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The core of the proposed approach, MaskOCR, for text recognition lies in that we pretrain the recognition model, including both the encoder and the decoder. The encoder pretraining is able to benefit from large scale real text images that are easily available without the need of text annotation. The decoder pretraining is able to benefit from the synthetic text images that are also easily available with the character-level annotation easily obtained. Experiments verify the effectiveness of encoderdecoder pretraining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F</head><label></label><figDesc>m to the positions corresponding to the masked patches, and feed the combined representations F = [F v F m ] with the corresponding positional embeddings added into the decoder, predicting the text:? = [? 1?2 . . .? N ]. The loss function is similar to BERT and is merely about masked characters,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation about masking ratio in encoder pretraining. In our experiments, we choose 0.45 as the masking ratio.</figDesc><table><row><cell>Masking ratio</cell><cell>Scene</cell><cell>Web</cell><cell>Document</cell><cell>Handwriting</cell><cell>Avg</cell></row><row><cell>0.30</cell><cell>71.5</cell><cell>73.1</cell><cell>99.1</cell><cell>61.8</cell><cell>79.3</cell></row><row><cell>0.45</cell><cell>72.3</cell><cell>73.7</cell><cell>99.2</cell><cell>62.5</cell><cell>79.8</cell></row><row><cell>0.60</cell><cell>72.0</cell><cell>73.6</cell><cell>99.1</cell><cell>60.7</cell><cell>79.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation about the masking strategy in decoder pretraining. It can be seen that masking helps the performance, especially for scene.</figDesc><table><row><cell></cell><cell>Scene</cell><cell>Web</cell><cell>Document</cell><cell>Handwriting</cell><cell>Avg</cell></row><row><cell>w/o masking</cell><cell>73.1</cell><cell>74.6</cell><cell>99.3</cell><cell>63.6</cell><cell>80.5</cell></row><row><cell>w/ masking</cell><cell>73.9</cell><cell>74.8</cell><cell>99.3</cell><cell>63.7</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Studying if the pretrained encoder is simultaneously retrained during decoder pretraining. The performance for encoder retraining is inferior. The reason is that the style of synthesized text image is different from the downstream text images.</figDesc><table><row><cell></cell><cell>Scene</cell><cell>Web</cell><cell>Document</cell><cell>Handwriting</cell><cell>Avg</cell></row><row><cell>Retraining encoder</cell><cell>69.0</cell><cell>71.4</cell><cell>99.0</cell><cell>53.3</cell><cell>76.7</cell></row><row><cell>Fixing encoder</cell><cell>73.9</cell><cell>74.8</cell><cell>99.3</cell><cell>63.7</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation about the vertical patch size.</figDesc><table><row><cell>Patch size</cell><cell>Scene</cell><cell>Web</cell><cell>Document</cell><cell>Handwriting</cell><cell>Avg</cell></row><row><cell>32 ? 4</cell><cell>68.8</cell><cell>70.7</cell><cell>98.6</cell><cell>49.4</cell><cell>75.8</cell></row><row><cell>32 ? 8</cell><cell>64.0</cell><cell>67.3</cell><cell>97.5</cell><cell>43.3</cell><cell>72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Text recognition results on the BCTR dataset.</figDesc><table><row><cell>Methods</cell><cell cols="7">Pretraining Scene Web Document Handwriting Avg #Params</cell></row><row><cell>CRNN [42]</cell><cell>?</cell><cell>53.4</cell><cell>54.5</cell><cell>97.5</cell><cell>46.4</cell><cell>67.0</cell><cell>-</cell></row><row><cell>ASTER [43]</cell><cell>?</cell><cell>54.5</cell><cell>52.3</cell><cell>93.1</cell><cell>38.9</cell><cell>64.7</cell><cell>-</cell></row><row><cell>MORAN [33]</cell><cell>?</cell><cell>51.8</cell><cell>49.9</cell><cell>95.8</cell><cell>39.7</cell><cell>64.3</cell><cell>-</cell></row><row><cell>SAR [26]</cell><cell>?</cell><cell>62.5</cell><cell>54.3</cell><cell>93.8</cell><cell>31.4</cell><cell>67.3</cell><cell>-</cell></row><row><cell>SRN [50]</cell><cell>?</cell><cell>60.1</cell><cell>52.3</cell><cell>96.7</cell><cell>18.0</cell><cell>65.0</cell><cell>-</cell></row><row><cell>SEED [39]</cell><cell>?</cell><cell>49.6</cell><cell>46.3</cell><cell>93.7</cell><cell>32.1</cell><cell>61.2</cell><cell>-</cell></row><row><cell>TransOCR[4]</cell><cell>?</cell><cell>63.3</cell><cell>62.3</cell><cell>96.9</cell><cell>53.4</cell><cell>72.8</cell><cell>84M</cell></row><row><cell>MaskOCR (ours, ViT-S) MaskOCR (ours, ViT-B) MaskOCR (ours, ViT-L)</cell><cell>? ? ?</cell><cell>71.4 73.9 76.2</cell><cell>72.5 74.8 76.8</cell><cell>98.8 99.3 99.4</cell><cell>55.6 63.7 67.9</cell><cell>78.1 80.8 82.6</cell><cell>36M 100M 318M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Text recognition results on six English scene text datasets.</figDesc><table><row><cell>Methods</cell><cell cols="5">Pretraining IC13 SVT IIIT5K IC15 SVTP CUTE Avg #Params</cell></row><row><cell>ASTER [43]</cell><cell>?</cell><cell>91.8 89.5 93.4</cell><cell>76.1 78.5</cell><cell>79.5 86.7</cell><cell>-</cell></row><row><cell>SEED [39]</cell><cell>?</cell><cell>92.8 89.6 93.8</cell><cell>80.0 81.4</cell><cell>83.6 88.3</cell><cell>-</cell></row><row><cell>PIMNet [38]</cell><cell>?</cell><cell>95.2 91.2 95.2</cell><cell>83.5 84.3</cell><cell>84.4 90.5</cell><cell>-</cell></row><row><cell>SRN [50]</cell><cell>?</cell><cell>95.5 91.5 94.8</cell><cell>82.7 85.1</cell><cell>87.8 90.4</cell><cell>55M</cell></row><row><cell>VisionLan [49] ABINet [12] ConCLR [54] PerSec [30]</cell><cell>? ? ? ?</cell><cell>95.7 91.7 95.8 97.4 93.5 96.2 97.7 94.3 96.5 97.2 94.6 96.3</cell><cell>83.7 86.0 86.0 89.3 85.4 89.3 84.4 89.5</cell><cell>88.5 91.2 89.2 92.7 91.3 92.8 90.2 92.4</cell><cell>33M 37M 37M -</cell></row><row><cell>MaskOCR (ours, ViT-S) MaskOCR (ours, ViT-B) MaskOCR (ours, ViT-L)</cell><cell>? ? ?</cell><cell>97.1 95.7 95.5 98.1 94.7 95.8 97.8 94.1 96.5</cell><cell>87.0 90.1 87.3 89.9 88.7 90.2</cell><cell cols="2">90.3 92.9 89.2 93.1 92.7 93.8 315M 31M 97M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/crownpku/awesome-chinese-nlp 3 https://github.com/oh-my-ocr/text_renderer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/MhLiao/DB</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence contrastive learning for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviad</forename><surname>Aberdam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Slossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shai Mazor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15302" to="15312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beit: BERT pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/2106.08254</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene text telescope: Text-focused scene image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Benchmarking chinese text recognition: Datasets, baselines, and an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengnan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<idno>abs/2112.15093</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shentong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2202.03026</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9620" to="9629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Chee Kheng Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><forename type="middle">Chet</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ni</surname></persName>
		</author>
		<title level="m">ChuanMing Fang, Shuaitao Zhang, and Junyu Han. ICDAR2019 robust reading challenge on arbitrary-shaped text -rrc-art. In ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1571" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7098" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Xin Zhang, and Lianwen Jin. ICPR2018 contest on robust reading for multi-type web images. In ICPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Judging a book by its cover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kenji Iwana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<idno>abs/1610.09204</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1406.2227</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<title level="m">Faisal Shafait, Seiichi Uchida, and Ernest Valveny. ICDAR 2015 competition on robust reading. In ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lluis Gomez i Bigorda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
			<pubPlace>Joan Mas, David Fern?ndez Mota, Jon Almaz?n, and Llu?s-Pere de las Heras</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Sergi Robles Mestre</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Trocr: Transformer-based optical character recognition with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Dinei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Flor?ncio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<idno>abs/2109.10282</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceiving stroke-semantic context: Hierarchical contrastive learning for robust scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mobai</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MORAN: A multi-object rectified attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhang</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>2d attentional irregular scene text recognizer. CoRR, abs/1906.05708</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<editor>Richard Bowden, John P. Collomosse, and Krystian Mikolajczyk</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Trung Quy Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pimnet: A parallel, iterative and mimicking network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2046" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SEED: semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13525" to="13534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhar</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ASTER: an attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<title level="m">ICDAR2017 competition on reading chinese text in the wild (RCTW-17). In ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1429" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Chee Kheng Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><forename type="middle">Chet</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<title level="m">ICDAR 2019 competition on large-scale street view text with partial labeling -RRC-LSVT. In ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1557" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cocotext: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1601.07140</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">From two to one: A new scene text recognizer with visual language modeling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenggao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14174" to="14183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12110" to="12119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Ling</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">A large chinese text dataset in the wild. JCST</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="509" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scut-hccdoc: A new benchmark dataset of handwritten chinese text in unconstrained camera-captured documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="page">107559</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<title level="m">ICDAR 2019 robust reading challenge on reading chinese text on signboard. In ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Context-based contrastive learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
