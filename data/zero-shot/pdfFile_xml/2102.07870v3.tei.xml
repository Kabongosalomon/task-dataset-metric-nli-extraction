<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Momentum Residual Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Sander</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ablin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
						</author>
						<title level="a" type="main">Momentum Residual Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The training of deep residual neural networks (ResNets) with backpropagation has a memory cost that increases linearly with respect to the depth of the network. A way to circumvent this issue is to use reversible architectures. In this paper, we propose to change the forward rule of a ResNet by adding a momentum term. The resulting networks, momentum residual neural networks (Momentum ResNets), are invertible. Unlike previous invertible architectures, they can be used as a dropin replacement for any existing ResNet block. We show that Momentum ResNets can be interpreted in the infinitesimal step size regime as secondorder ordinary differential equations (ODEs) and exactly characterize how adding momentum progressively increases the representation capabilities of Momentum ResNets: they can learn any linear mapping up to a multiplicative factor, while ResNets cannot. In a learning to optimize setting, where convergence to a fixed point is required, we show theoretically and empirically that our method succeeds while existing invertible architectures fail. We show on CIFAR and ImageNet that Momentum ResNets have the same accuracy as ResNets, while having a much smaller memory footprint, and show that pre-trained Momentum ResNets are promising for fine-tuning models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Problem setup. As a particular instance of deep learning <ref type="bibr" target="#b30">(LeCun et al., 2015;</ref><ref type="bibr" target="#b18">Goodfellow et al., 2016)</ref>, residual neural networks <ref type="bibr">(He et al., 2016, ResNets)</ref> have achieved great empirical successes due to extremely deep representations and their extensions keep on outperforming state of the art on real data sets <ref type="bibr" target="#b28">(Kolesnikov et al., 2019;</ref><ref type="bibr">Touvron et al.,</ref> Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). 2019). Most of deep learning tasks involve graphics processing units (GPUs), where memory is a practical bottleneck in several situations <ref type="bibr">(Wang et al., 2018;</ref><ref type="bibr" target="#b41">Peng et al., 2017;</ref><ref type="bibr">Zhu et al., 2017)</ref>. Indeed, backpropagation, used for optimizing deep architectures, requires to store values (activations) at each layer during the evaluation of the network (forward pass). Thus, the depth of deep architectures is constrained by the amount of available memory. The main goal of this paper is to explore the properties of a new model, Momentum ResNets, that circumvent these memory issues by being invertible: the activations at layer n is recovered exactly from activations at layer n + 1. This network relies on a modification of the ResNet's forward rule which makes it exactly invertible in practice. Instead of considering the feedforward relation for a ResNet (residual building block)</p><formula xml:id="formula_0">x n+1 = x n + f (x n , ? n ),<label>(1)</label></formula><p>we define its momentum counterpart, which iterates v n+1 = ?v n + (1 ? ?)f (x n , ? n )</p><formula xml:id="formula_1">x n+1 = x n + v n+1 ,<label>(2)</label></formula><p>where f is a parameterized function, v is a velocity term and ? ? [0, 1] is a momentum term. This radically changes the dynamics of the network, as shown in the following figure.  <ref type="figure">Figure 1</ref>. Comparison of the dynamics of a ResNet (left) and a Momentum ResNet with ? = 0.9 (right) with tied weights between layers, ?n = ? for all n. The evolution of the activations at each layer is shown (depth 15). Models try to learn the mapping x ? ?x 3 in R. The ResNet fails (the iterations approximate the solution of a first-order ODE, for which trajectories don't cross, cf. Picard-Lindelof theorem) while the Momentum ResNet leverages the changes in velocity to model more complex dynamics.</p><p>In contrast with existing reversible models, Momentum ResNets can be integrated seamlessly in any deep architec-arXiv:2102.07870v3 <ref type="bibr">[cs.</ref>LG] 22 Jul 2021 ture which uses residual blocks as building blocks (cf. in Section 3).</p><p>Contributions. We introduce momentum residual neural networks (Momentum ResNets), a new deep model that relies on a simple modification of the ResNet forward rule and which, without any constraint on its architecture, is perfectly invertible. We show that the memory requirement of Momentum ResNets is arbitrarily reduced by changing the momentum term ? (Section 3.2), and show that they can be used as a drop-in replacement for traditional ResNets.</p><p>On the theoretical side, we show that Momentum ResNets are easily used in the learning to optimize setting, where other reversible models fail to converge (Section 3.3). We also investigate the approximation capabilities of Momentum ResNets, seen in the continuous limit as second-order ODEs (Section 4). We first show in Proposition 3 that Momentum ResNets can represent a strictly larger class of functions than first-order neural ODEs. Then, we give more detailed insights by studying the linear case, where we formally prove in Theorem 1 that Momentum ResNets with linear residual functions have universal approximation capabilities, and precisely quantify how the set of representable mappings for such models grows as the momentum term ? increases. This theoretical result is a first step towards a theoretical analysis of representation capabilities of Momentum ResNets.</p><p>Our last contribution is the experimental validation of Momentum ResNets on various learning tasks. We first show that Momentum ResNets separate point clouds that ResNets fail to separate (Section 5.1). We also show on image datasets (CIFAR-10, CIFAR-100, ImageNet) that Momentum ResNets have similar accuracy as ResNets, with a smaller memory cost (Section 5.2). We also show that parameters of a pre-trained model are easily transferred to a Momentum ResNet which achieves comparable accuracy in only few epochs of training. We argue that this way to obtain pre-trained Momentum ResNets is of major importance for fine-tuning a network on new data for which memory storage is a bottleneck. We provide a Pytorch package with a method that takes a torchvision ResNet model and returns its Momentum counterpart that achieves similar accuracy with very little refit. We also experimentally validate our theoretical findings in the learning to optimize setting, by confirming that Momentum ResNets perform better than RevNets <ref type="bibr" target="#b17">(Gomez et al., 2017)</ref>. Our code is available at https: //github.com/michaelsdr/momentumnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and previous works.</head><p>Backpropagation. Backpropagation is the method of choice to compute the gradient of a scalar-valued function. It operates using the chain rule with a backward traversal of the computational graph <ref type="bibr" target="#b2">(Bauer, 1974)</ref>. It is also known as reverse-mode automatic differentiation <ref type="bibr" target="#b3">(Baydin et al., 2018;</ref><ref type="bibr" target="#b46">Rumelhart et al., 1986;</ref><ref type="bibr">Verma, 2000;</ref><ref type="bibr" target="#b20">Griewank &amp; Walther, 2008)</ref>. The computational cost is similar to the one of evaluating the function itself. The only way to back-propagate gradients through a neural architecture without further assumptions is to store all the intermediate activations during the forward pass. This is the method used in common deep learning libraries such as Pytorch <ref type="bibr" target="#b40">(Paszke et al., 2017)</ref>, Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and JAX <ref type="bibr" target="#b27">(Jacobsen et al., 2018)</ref>. A common way to reduce this memory storage is to use checkpointing: activations are only stored at some steps and the others are recomputed between these check-points as they become needed in the backward pass (e.g., <ref type="bibr" target="#b36">Martens &amp; Sutskever (2012)</ref>).</p><p>Reversible architectures. However, models that allow backpropagation without storing any activations have recently been developed. They are based on two kinds of approaches. The first is discrete and relies on finding ways to easily invert the rule linking activation n to activation n+1 <ref type="bibr" target="#b17">(Gomez et al., 2017;</ref><ref type="bibr" target="#b5">Chang et al., 2018;</ref><ref type="bibr" target="#b22">Haber &amp; Ruthotto, 2017;</ref><ref type="bibr" target="#b27">Jacobsen et al., 2018;</ref><ref type="bibr" target="#b4">Behrmann et al., 2019)</ref>. In this way, it is possible to recompute the activations on the fly during the backward pass: activations do not have to be stored. However, these methods either rely on restricted architectures where there is no straightforward way to transfer a well performing non-reversible model into a reversible one, or do not offer a fast inversion scheme when recomputing activations backward. In contrast, our proposal can be applied to any existing ResNet and is easily inverted. The second kind of approach is continuous and relies on ordinary differential equations (ODEs), where ResNets are interpreted as continuous dynamical systems <ref type="bibr">(Weinan, 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2018;</ref><ref type="bibr" target="#b52">Teh et al., 2019;</ref><ref type="bibr" target="#b50">Sun et al., 2018;</ref><ref type="bibr">Weinan et al., 2019;</ref><ref type="bibr" target="#b34">Lu et al., 2018;</ref><ref type="bibr" target="#b49">Ruthotto &amp; Haber, 2019)</ref>. This allows one to import theoretical and numerical advances from ODEs to deep learning. These models are often called neural ODEs  and can be trained by using an adjoint sensitivity method <ref type="bibr" target="#b43">(Pontryagin, 2018)</ref>, solving ODEs backward in time. This strategy avoids performing reverse-mode automatic differentiation through the operations of the ODE solver and leads to a O(1) memory footprint. However, defining the neural ODE counterpart of an existing residual architecture is not straightforward: optimizing ODE blocks is an infinite dimensional problem requiring a non-trivial time discretization, and the performances of neural ODEs depend on the numerical integrator for the ODE <ref type="bibr" target="#b21">(Gusak et al., 2020)</ref>. In addition, ODEs cannot always be numerically reversed, because of stability issues: numerical errors can occur and accumulate when a system is run backwards <ref type="bibr" target="#b16">(Gholami et al., 2019;</ref><ref type="bibr" target="#b52">Teh et al., 2019)</ref>. Thus, in practice, neural ODEs are seldom used in standard deep learning settings. Nevertheless, recent works <ref type="bibr">(Zhang et al., 2019;</ref><ref type="bibr" target="#b44">Queiruga et al., 2020)</ref> incorporate ODE blocks in neural architectures to achieve comparable accuracies to ResNets on CIFAR.</p><p>Representation capabilities. Studying the representation capabilities of such models is also important, as it gives insights regarding their performance on real world data. It is well-known that a single residual block has universal approximation capabilities <ref type="bibr" target="#b11">(Cybenko, 1989)</ref>, meaning that on a compact set any continuous function can be uniformly approximated with a one-layer feedforward fully-connected neural network. However, neural ODEs have limited representation capabilities. <ref type="bibr" target="#b52">Teh et al. (2019)</ref> propose to lift points in higher dimensions by concatenating vector fields of data with zeros in an extra-dimensional space, and show that the resulting augmented neural ODEs (ANODEs) achieve lower loss and better generalization on image classification and toy experiments. <ref type="bibr" target="#b33">Li et al. (2019)</ref> show that, if the output of the ODE-Net is composed with elements of a terminal family, then universal approximation capabilities are obtained for the convergence in L p norm for p &lt; +?, which is insufficient <ref type="bibr" target="#b53">(Teshima et al., 2020)</ref>. In this work, we consider the representation capabilities in L ? norm of the ODEs derived from the forward iterations of a ResNet. Furthermore, <ref type="bibr">Zhang et al. (2020)</ref> proved that doubling the dimension of the ODE leads to universal approximators, although this result has no application in deep learning to our knowledge. In this work, we show that in the continuous limit, our architecture has better representation capabilities than Neural ODEs. We also prove its universality in the linear case.</p><p>Momentum in deep networks. Some recent works <ref type="bibr" target="#b26">(He et al., 2020;</ref><ref type="bibr" target="#b7">Chun et al., 2020;</ref><ref type="bibr" target="#b38">Nguyen et al., 2020;</ref><ref type="bibr" target="#b32">Li et al., 2018)</ref> have explored momentum in deep architectures. However, these methods differ from ours in their architecture and purpose. <ref type="bibr" target="#b7">Chun et al. (2020)</ref> introduce a momentum to solve an optimization problem for which the iterations do not correspond to a ResNet. <ref type="bibr" target="#b38">Nguyen et al. (2020</ref><ref type="bibr">) (resp. He et al. (2020</ref>) add momentum in the case of RNNs (different from ResNets) where the weights are tied to alleviate the vanishing gradient issue (resp. link the key and query encoder layers). <ref type="bibr" target="#b32">Li et al. (2018)</ref> consider a particular case where the linear layer is tied and is a symmetric definite matrix. In particular, none of the mentioned architectures are invertible, which is one of the main assets of our method.</p><p>Second-order models We show that adding a momentum term corresponds to an Euler integration scheme for integrating a second-order ODE. Some recently proposed architectures <ref type="bibr" target="#b39">(Norcliffe et al., 2020;</ref><ref type="bibr" target="#b48">Rusch &amp; Mishra, 2021;</ref><ref type="bibr" target="#b34">Lu et al., 2018;</ref><ref type="bibr" target="#b37">Massaroli et al., 2020)</ref> are also motivated by secondorder differential equations. <ref type="bibr" target="#b39">Norcliffe et al. (2020)</ref> introduce second-order dynamics to model second-order dynamical systems, whereas our model corresponds to a discrete set of equations in the continuous limit. Also, in our method, the neural network only acts on x, so that although momentum increases the dimension to 2d, the computational burden of a forward pass is the same as a ResNet of dimension d. <ref type="bibr" target="#b48">Rusch &amp; Mishra (2021)</ref> propose second-order RNNs, whereas our method deals with ResNets. Finally, the formulation of LM-ResNet in <ref type="bibr" target="#b34">Lu et al. (2018)</ref> differs from our forward pass (x n+1 = x n + ?v n + (1 ? ?)f (x n , ? n )), even though they both lead to second-order ODEs. Importantly, none of these second-order formulations are invertible. Notations For d ? N * , we denote by R d?d , GL d (R) and D C d (R) the set of real matrices, of invertible matrices, and of real matrices that are diagonalizable in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Momentum Residual Neural Networks</head><p>We now introduce Momentum ResNet, a simple transformation of any ResNet into a model with a small memory requirement, and that can be seen in the continuous limit as a second-order ODE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Momentum ResNets</head><p>Adding a momentum term in the ResNet equations.</p><p>For any ResNet which iterates (1), we define its Momentum counterpart, which iterates (2), where (v n ) n is the velocity initialized with some value v 0 in R d , and ? ? [0, 1] is the so-called momentum term. This approach generalizes gradient descent algorithm with momentum <ref type="bibr" target="#b45">(Ruder, 2016)</ref>, for which f is the gradient of a function to minimize.</p><p>Initial speed and momentum term. In this paper, we consider initial speeds v 0 that depend on x 0 through a simple relation. The simplest options are to set v 0 = 0 or v 0 = f (x 0 , ? 0 ). We prove in Section 4 that this dependency between v 0 and x 0 has an influence on the set of mappings that Momentum ResNets can represent. The parameter ? controls how much a Momentum ResNet diverges from a ResNet, and also the amount of memory saving. The closer ? is to 0, the closer Momentum ResNets are to ResNets, but the less memory is saved. In our experiments, we use ? = 0.9, which we find to work well in various applications.</p><p>Invertibility. Procedure (2) is inverted through</p><formula xml:id="formula_2">x n = x n+1 ? v n+1 , v n = 1 ? (v n+1 ? (1 ? ?)f (x n , ? n )) ,<label>(3)</label></formula><p>so that activations can be reconstructed on the fly during the backward pass in a Momentum ResNet. In practice, in order to exactly reverse the dynamics, the information lost by the finite-precision multiplication by ? in (2) has to be efficiently stored. We used the algorithm from <ref type="bibr" target="#b35">Maclaurin et al. (2015)</ref> to perform this reversible multiplication. It consists in maintaining an information buffer, that is, an integer that stores the bits that are lost at each iteration, so that multiplication becomes reversible. We further describe the procedure in Appendix C. Note that there is always a small loss of floating point precision due to the addition of the learnable mapping f . In practice, we never found it to be a problem: this loss in precision can be neglected compared to the one due to the multiplication by ?. Drop-in replacement. Our approach makes it possible to turn any existing ResNet into a reversible one. In other words, a ResNet can be transformed into its Momentum counterpart without changing the structure of each layer. For instance, consider a ResNet-152 <ref type="bibr" target="#b25">(He et al., 2016)</ref>. It is made of 4 layers (of depth 3, 8, 36 and 3) and can easily be turned into its Momentum ResNet counterpart by changing the forward equations (1) into (2) in the 4 layers. No further change is needed and Momentum ResNets take the exact same parameters as inputs: they are a drop-in replacement. This is not the case of other reversible models. Neural ODEs  take continuous parameters as inputs. i-ResNets <ref type="bibr" target="#b4">(Behrmann et al., 2019)</ref> cannot be trained by plain SGD since the spectral norm of the weights requires constrained optimization. i-RevNets <ref type="bibr" target="#b27">(Jacobsen et al., 2018)</ref> and RevNets <ref type="bibr" target="#b17">(Gomez et al., 2017)</ref> require to train two networks with their own parameters for each residual block, split the inputs across convolutional channels, and are half as deep as ResNets: they do not take the same parameters as inputs. <ref type="table" target="#tab_0">Table 1</ref> summarizes the properties of reversible residual architectures. We discuss in further details the differences between RevNets and Momentum ResNets in sections 3.3 and 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory cost</head><p>Instead of storing the full data at each layer, we only need to store the bits lost at each multiplication by ? (cf. "intertibility"). For an architecture of depth k, this corresponds to storing log 2 (( 1 ? ) k ) values for each sample ( k(1??) ln(2) if ? is close to 1). To illustrate, we consider two situations where storing the activations is by far the main memory bottleneck. First, consider a toy feedforward architecture</p><formula xml:id="formula_3">where f (x, ?) = W T 2 ?(W 1 x + b), with x ? R d and ? = (W 1 , W 2 , b), where W 1 , W 2 ? R p?d and b ? R p ,</formula><p>with a depth k ? N. We suppose that the weights are the same at each layer. The training set is composed of n vectors x 1 , ..., x n ? R d . For ResNets, we need to store the weights of the network and the values of all activations for the training set at each layer of the network. In total, the memory needed is O(k ? d ? n batch ) per iteration. In the case of Momentum ResNets, if ? is close to 1 we get a memory requirement of O((1 ? ?) ? k ? d ? n batch ). This proves that the memory dependency in the depth k is arbitrarily reduced by changing the momentum ?. The memory savings are confirmed in practice, as shown in <ref type="figure">Figure 2</ref>.  <ref type="figure">Figure 2</ref>. Comparison of memory needed (calculated using a profiler) for computing gradients of the loss, with ResNets (activations are stored) and Momentum ResNets (activations are not stored). We set n batch = 500, d = 500 and ? = 1 ? 1 50k at each depth. Momentum ResNets give a nearly constant memory footprint.</p><p>As another example, consider a ResNet-152 <ref type="bibr" target="#b25">(He et al., 2016)</ref> which can be used for ImageNet classification <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. Its layer named "conv4 x" has a depth of 36: it has 40 M parameters, whereas storing the activations would require storing 50 times more parameters. Since storing the activations is here the main obstruction, the memory requirement for this layer can be arbitrarily reduced by taking ? close to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The role of momentum</head><p>When ? is set to 0 in (2), we recover a ResNet. Therefore, Momentum ResNets are a generalization of ResNets. When ? ? ? 1, one can scale f ? 1 1?? f to get in (2) a symplectic scheme <ref type="bibr" target="#b23">(Hairer et al., 2006)</ref> that recovers a special case of other popular invertible neural network: RevNets <ref type="bibr" target="#b17">(Gomez et al., 2017)</ref> and Hamiltonian Networks <ref type="bibr" target="#b5">(Chang et al., 2018)</ref>.</p><formula xml:id="formula_4">A RevNet iterates v n+1 = v n +?(x n , ? n ), x n+1 = x n +?(v n+1 , ? n ),<label>(4)</label></formula><p>where ? and ? are two learnable functions.</p><p>The usefulness of such architecture depends on the task. RevNets have encountered success for classification and regression. However, we argue that RevNets cannot work in some settings. For instance, under mild assumptions, the RevNet iterations do not have attractive fixed points when the parameters are the same at each layer: ? n = ?, ? n = ? . We rewrite (4) as</p><formula xml:id="formula_5">(v n+1 , x n+1 ) = ?(v n , x n ) with ?(v, x) = (v + ?(x, ?), x + ?(v + ?(x, ?), ? )).</formula><p>Proposition 1 (Instability of fixed points). Let (v * , x * ) a fixed point of the RevNet iteration (4). Assume that ? (resp. ?) is differentiable at x * (resp. v * ), with Jacobian matrix</p><formula xml:id="formula_6">A (resp. B) ? R d?d . The Jacobian of ? at (v * , x * ) is J(A, B) = Id d A B Id d +BA .</formula><p>If A and B are invertible, then there exists ? ? Sp (J(A, B)) such that |?| ? 1 and ? = 1.</p><p>This shows that (v * , x * ) cannot be a stable fixed point. As a consequence, in practice, a RevNet cannot have converging iterations: according to (4), if x n converges then v n must also converge, and their limit must be a fixed point. The previous proposition shows that it is impossible.</p><p>This result suggests that RevNets should perform poorly in problems where one expects the iterations of the network to converge. For instance, as shown in the experiments in Section 5.3, this happens when we use reverible dynamics in order to learn to optimize <ref type="bibr" target="#b35">(Maclaurin et al., 2015)</ref>. In contrast, the proposed method can converge to a fixed point as long as the momentum term ? is strictly less than 1. Remark. Proposition 1 has a continuous counterpart. Indeed, in the continuous limit, <ref type="formula" target="#formula_4">(4)</ref> </p><formula xml:id="formula_7">writesv = ?(x, ?),? = ?(v, ? ). The corresponding Jacobian in (v * , x * ) is 0 A B 0 .</formula><p>The eigenvalues of this matrix are the square roots of those of AB: they cannot all have a real part &lt; 0 (same stability issue in the continuous case). Neural ODEs: ResNets as first-order ODEs. The ResNets equation <ref type="formula" target="#formula_0">(1)</ref> with initial condition x 0 (the input of the ResNet) can be seen as a discretized Euler scheme of the ODE? = f (x, ?) with x(0) = x 0 . Denoting T a time horizon, the neural ODE maps the input x(0) to the output x(T ), and, as in <ref type="bibr" target="#b6">Chen et al. (2018)</ref>, is trained by minimizing a loss L(x(T ), ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Momentum ResNets as continuous models</head><p>Momentum ResNets as second-order ODEs. Let ? = 1 1?? . We can then rewrite <ref type="formula" target="#formula_1">(2)</ref> as</p><formula xml:id="formula_8">v n+1 = v n + f (x n , ? n ) ? v n ? , x n+1 = x n + v n+1 ,</formula><p>which corresponds to a Verlet integration scheme <ref type="bibr" target="#b23">(Hairer et al., 2006)</ref> with step size 1 of the differential equation ?? +? = f (x, ?). Thus, in the same way that ResNets can be seen as discretization of first-order ODEs, Momentum ResNets can be seen as discretization of second-order ones. <ref type="figure" target="#fig_2">Figure 3</ref> sums up these ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Representation capabilities</head><p>We now turn to the analysis of the representation capabilities of Momentum ResNets in the continuous setting. In particular, we precisely characterize the set of mappings representable by Momentum ResNets with linear residual functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Representation capabilities of first-order ODEs</head><p>We consider the first-order model</p><formula xml:id="formula_9">x = f (x, ?) with x(0) = x 0 .<label>(5)</label></formula><p>We denote by ? t (x 0 ) the solution at time t starting at initial condition</p><formula xml:id="formula_10">x(0) = x 0 . It is called the flow of the ODE. For all t ? [0, T ],</formula><p>where T is a time horizon, ? t is a homeomorphism: it is continuous, bijective with continuous inverse.</p><p>First-order ODEs are not universal approximators.</p><p>ODEs such as <ref type="formula" target="#formula_9">(5)</ref> are not universal approximators. Indeed, the function mapping an initial condition to the flow at a certain time horizon T cannot represent every mapping x 0 ? h(x 0 ). For instance when d = 1, the mapping x ? ?x cannot be approximated by a first-order ODE, since 1 should be mapped to ?1 and 0 to 0, which is impossible without intersecting trajectories <ref type="bibr" target="#b52">(Teh et al., 2019)</ref>. In fact, the homeomorphisms represented by <ref type="formula" target="#formula_9">(5)</ref> are orientation-preserving: if K ? R d is a compact set and h : K ? ? R d is a homeomorphism represented by (5), then h is in the connected component of the identity function on K for the topology of the uniform convergence (see details in Appendix B.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Representation capabilities of second-order ODEs</head><p>We consider the second-order model for which we recall that Momentum ResNets are a discretization:</p><formula xml:id="formula_11">?? +? = f (x, ?) with (x(0),?(0)) = (x 0 , v 0 ). (6)</formula><p>In Section 3.3, we showed that Momentum ResNets generalize existing models when setting ? = 0 or 1. We now state the continuous counterparts of these results. Recall that 1 1?? = ?. When ? ? ? 0, we recover the first-order model. Proposition 2 (Continuity of the solutions). We let x * (resp. x ? ) be the solution of (5) (resp. (6)) on [0, T ], with initial conditions</p><formula xml:id="formula_12">x * (0) = x ? (0) = x 0 and? ? (0) = v 0 . Then x ? ? x * ? ? ? 0 as ? ? ? 0.</formula><p>The proof of this result relies on the implicit function theorem and can be found in Appendix A.1. Note that Proposition 2 is true whatever the initial speed v 0 . When ? ? ? +?, one needs to rescale f to study the asymptotics: the solution of? + 1 ?? = f (x, ?) converges to the solution of x = f (x, ?) (see details in Appendix B.1). These results show that in the continuous regime, Momentum ResNets also interpolate between? = f (x, ?) and? = f (x, ?).</p><p>Representation capabilities of a model <ref type="formula">(6)</ref> on the x space. We recall that we consider initial speeds v 0 that can depend on the input</p><formula xml:id="formula_13">x 0 ? R d (for instance v 0 = 0 or v 0 = f (x 0 , ? 0 )). We therefore assume ? t : R d ? R d such that ? t (x 0 ) is solution of (6). We emphasize that ? t is not always a homeomorphism. For instance, ? t (x 0 ) = x 0 exp (?t/2) cos (t/2) solves? +? = ? 1 2 x(t) with (x(0),?(0)) = (x 0 , ? x0</formula><p>2 ). All the trajectories intersect at time ?. It means that Momentum ResNets can learn mappings that are not homeomorphisms, which suggests that increasing ? should lead to better representation capabilities. The first natural question is thus whether, given h :</p><formula xml:id="formula_14">R d ? ? R d , there exists some f such that ? t asso- ciated to (6) satisfies ?x ? R d , ? 1 (x) = h(x).</formula><p>In the case where v 0 is an arbitrary function of x 0 , the answer is trivial since <ref type="formula">(6)</ref> can represent any mapping, as proved in Appendix B.2. This setting does not correspond to the common use case of ResNets, which take advantage of their depth, so it is important to impose stronger constraints on the dependency between v 0 and x 0 . For instance, the next proposition shows that even if one imposes v 0 = f (x 0 , ? 0 ), a second-order model is at least as general as a first-order one. Proposition 3 (Momentum ResNets are at least as general). There exists a functionf such that for all x solution of (5), x is also solution of the second-order model ?? +? =f (x, ?) with (x(0),?(0)) = (x 0 , f (x 0 , ? 0 )).</p><p>Furthermore, even with the restrictive initial condition v 0 = 0, x ? ?x for ? &gt; ?1 can always be represented by a second-order model <ref type="formula">(6)</ref> (see details in Appendix B.4). This supports the claim that the set of representable mappings increases with ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Universality of Momentum ResNets with linear residual functions</head><p>As a first step towards a theoretical analysis of the universal representation capabilities of Momentum ResNets, we now investigate the linear residual function case. Consider the second-order linear ODE</p><formula xml:id="formula_15">?? +? = ?x with (x(0),?(0)) = (x 0 , 0),<label>(7)</label></formula><p>with ? ? R d?d . We assume without loss of generality that the time horizon is T = 1. We have the following result. Proposition 4 (Solution of <ref type="formula" target="#formula_15">(7)</ref>). At time 1, (7) defines the linear mapping</p><formula xml:id="formula_16">x 0 ? ? 1 (x 0 ) = ? ? (?)x 0 where ? ? (?) = e ? 1 2? +? n=0 1 (2n)! + 1 2?(2n + 1)! ? ? + Id d 4? 2 n .</formula><p>Characterizing the set of mappings representable by <ref type="formula" target="#formula_15">(7)</ref> is thus equivalent to precisely analyzing the range ? ? (R d?d ).</p><p>Representable mappings of a first-order linear model.</p><formula xml:id="formula_17">When ? ? ? 0, Proposition 2 shows that ? ? (?) ? ? ? 0 (?) = exp ?.</formula><p>The range of the matrix exponential is indeed the set of representable mappings of a first order linear model</p><formula xml:id="formula_18">x = ?x with x(0) = x 0<label>(8)</label></formula><p>and this range is known <ref type="bibr" target="#b1">(Andrica &amp; Rohan, 2010)</ref> </p><formula xml:id="formula_19">to be ? 0 (R d?d ) = exp (R d?d ) = {M 2 | M ? GL d (R)}.</formula><p>This means that one can only learn mappings that are the square of invertible mappings with a first-order linear model <ref type="formula" target="#formula_18">(8)</ref>.</p><p>To ease the exposition and exemplify the impact of increasing ? &gt; 0, we now consider the case of matrices with real coefficients that are diagonalizable in C, D C d (R). Note that the general setting of arbitrary matrices is exposed in Appendix A.4 using Jordan decomposition. Note also that <ref type="bibr" target="#b24">(Hartfiel, 1995)</ref>. Using Theorem 1 from <ref type="bibr" target="#b10">Culver (1966)</ref></p><formula xml:id="formula_20">D C d (R) is dense in R d?d</formula><formula xml:id="formula_21">, we have that if D ? D C d (R)</formula><p>, then D is represented by a first-order model <ref type="formula" target="#formula_18">(8)</ref> if and only if D is non-singular and for all eigenvalues ? ? Sp(D) with ? &lt; 0, ? is of even multiplicity order. This is restrictive because it forces negative eigenvalues to be in pairs. We now generalize this result and show that increasing ? &gt; 0 leads to less restrictive conditions. <ref type="figure">Figure 4</ref>. Left: Evolution of ?? defined in Theorem 1. ?? is non increasing, stays close to 0 when ? 1 and close to ?1 when ? ? 2. Right: Evolution of the real eigenvalues ?1 and ?2 of representable matrices in D C d (R) by <ref type="formula" target="#formula_15">(7)</ref> when d = 2 for different values of ?. The grey colored areas correspond to the different representable eigenvalues. When ? = 0, ?1 = ?2 or ?1 &gt; 0 and ?2 &gt; 0. When ? &gt; 0, single negative eigenvalues are acceptable.</p><formula xml:id="formula_22">? = + ? ? = 0.5 ? = 0 ?1 ? ?1 | ? 1 ? 2 ? ? ?</formula><p>Representable mappings by a second-order linear model. Again, by density and for simplicity, we focus on matrices in D C d (R), and we state and prove the general case in Appendix A.4, making use of Jordan blocks decomposition of matrix functions <ref type="bibr" target="#b15">(Gantmacher, 1959)</ref> and localization of zeros of entire functions <ref type="bibr" target="#b47">(Runckel, 1969)</ref>. The range of ? ? over the reals has for form ? ? (R) = [? ? , +?[. It plays a pivotal role to control the set of representable mappings, as stated in the theorem bellow. Its minimum value can be computed con-</p><formula xml:id="formula_23">veniently since it satisfies ? ? = min ??R G ? (?) where G ? (?) exp (? 1 2? )(cos(?) + 1 2?? sin(?)). Theorem 1 (Representable mappings with linear residual functions). Let D ? D C d (R)</formula><p>. Then D is represented by a second-order model <ref type="formula" target="#formula_15">(7)</ref> if and only if ?? ? Sp(D) such that ? &lt; ? ? , ? is of even multiplicity order.</p><p>Theorem 1 is illustrated in <ref type="figure">Figure 4</ref>. A consequence of this result is that the set of representable linear mappings is strictly increasing with ?. Another consequence is that one can learn any mapping up to scale using the ODE <ref type="formula" target="#formula_15">(7)</ref>:</p><formula xml:id="formula_24">if D ? D C d (R)</formula><p>, there exists ? ? &gt; 0 such that for all ? ? Sp(? ? D), one has ? &gt; ? ? . Theorem 1 shows that ? ? D is represented by a second-order model <ref type="formula" target="#formula_15">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We now demonstrate the applicability of Momentum ResNets through experiments. We used Pytorch and Nvidia Tesla V100 GPUs. We experimentally validate the representation capabilities of Momentum ResNets on a challenging synthetic classification task. As already noted <ref type="bibr" target="#b52">(Teh et al., 2019)</ref>, neural ODEs ultimately fail to break apart nested rings. We experimentally demonstrate the advantage of Momentum ResNets by separating 4 nested rings (2 classes). We used the same structure for both models:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Point clouds separation</head><formula xml:id="formula_25">f (x, ?) = W T 2 tanh(W 1 x + b) with W 1 , W 2 ? R 16?2 , b ? R 16</formula><p>, and a depth 15. Evolution of the points as depth increases is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The fact that the trajectories corresponding to the ResNet panel don't cross is because, with this depth, the iterations approximate the solution of a first order ODE, for which trajectories cannot cross, due to the Picard-Lindelof theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image experiments</head><p>We also compare the accuracy of ResNets and Momentum ResNets on real data sets: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b29">(Krizhevsky et al., 2010)</ref> and ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009</ref>).</p><p>We used existing ResNets architectures. We recall that Momentum ResNets can be used as a drop-in replacement and that it is sufficient to replace every residual building block with a momentum residual forward iteration. We set ? = 0.9 in the experiments. More details about the experimental setup are given in Appendix D. Results on CIFAR-10 and CIFAR-100. For these data sets, we used a ResNet-101 <ref type="bibr" target="#b25">(He et al., 2016</ref>) and a Momentum ResNet-101 and compared the evolution of the test error and test loss. Two kinds of Momentum ResNets were used: one with an initial speed v 0 = 0 and the other one where the initial speed v 0 was learned: v 0 = f (x 0 ). These experiments show that Momentum ResNets perform similarly to ResNets. Results are summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Effect of the momentum term ?. Theorem 1 shows the effect of ? on the representable mappings for linear ODEs.</p><p>To experimentally validate the impact of ?, we train a Momentum ResNet-101 on CIFAR-10 for different values of the momentum at train time, ? train . We also evaluate Momentum ResNets trained with ? train = 0 and ? train = 1 with no further training for several values of the momentum at test time, ? test . In this case, the test accuracy never decreases by more than 3%. We also refit for 20 epochs Momentum ResNets trained with ? train = 0 and ? train = 1. This is sufficient to obtain similar accuracy as models trained from scratch. Results are shown in <ref type="figure">Figure 6</ref> (upper row). This indicates that the choice of ? has a limited impact on accuracy. In addition, learning the parameter ? does not affect the accuracy of the model. Since it also breaks the method described in 3.2, we fix ? in all the experiments.</p><p>Results on ImageNet. For this data set, we used a ResNet-101, a Momentum ResNet-101, and a RevNet-101. For the latter, we used the procedure from <ref type="bibr" target="#b17">Gomez et al. (2017)</ref> and adjusted the depth of each layer for the model to have approximately the same number of parameters as the original ResNet-101. Evolution of test errors are shown in <ref type="figure">Figure 6</ref> (lower row), where comparable performances are achieved.</p><p>Memory costs. We compare the memory (using a memory profiler) for performing one epoch as a function of the batch size for two datasets: ImageNet (depth of 152) and CIFAR-10 (depth of 1201). Results are shown in <ref type="figure" target="#fig_4">Figure 7</ref> and illustrate how Momentum ResNets can benefit from increased batch size, especially for very deep models. We also Momentum ResNet (v0 = f (x0)) RevNet <ref type="figure">Figure 6</ref>. Upper row: Robustness of final accuracy w.r.t ? when training Momentum ResNets 101 on CIFAR-10. We train the networks with a momentum ?train and evaluate their accuracy with a different momentum ?test at test time. We optionally refit the networks for 20 epochs. We recall that ?train = 0 corresponds to a classical ResNet and ?train = 1 corresponds to a Momentum ResNet with optimal memory savings. Lower row: Top-1 classification error on ImageNet (single crop) for 4 different residual architectures of depth 101 with the same number of parameters. Final test accuracy is 22% for the ResNet-101 and 23% for the 3 other invertible models.</p><p>In particular, our model achieve the same performance as a RevNet with the same number of parameters.</p><p>show in <ref type="figure" target="#fig_4">Figure 7</ref> the final test accuracy for a full training of Momentum ResNets on CIFAR-10 as a function of the memory used (directly linked to ? (section 3.2)).  Ability to perform pre-training and fine-tuning. It has been shown <ref type="bibr" target="#b51">(Tajbakhsh et al., 2016)</ref> that in various medical imaging applications the use of a pre-trained model on ImageNet with adapted fine-tuning outperformed a model trained from scratch. In order to easily obtain pre-trained Momentum ResNets for applications where memory could be a bottleneck, we transferred the learned parameters of a ResNet-152 pre-trained on ImageNet to a Momentum ResNet-152 with ? = 0.9. In only 1 epoch of additional training we reached a top-1 error of 26.5% and in 5 additional epochs a top-1 error of 23.5%. We then empirically compared the accuracy of these pre-trained models by finetuning them on new images: the hymenoptera 1 data set. As a proof of concept, suppose we have a GPU with 3 Go of RAM. The images have a resolution of 500 ? 500 pixels so that the maximum batch size that can be taken for finetuning the ResNet-152 is 2, against 4 for the Momentum ResNet-152. As suggested in <ref type="bibr" target="#b51">Tajbakhsh et al. (2016)</ref> ("if the distance between the source and target applications is significant, one may need to fine-tune the early layers as well"), we fine-tune the whole network in this proof of concept experiment. In this setting the Momentum ResNet leads to faster convergence when fine-tuning, as shown in <ref type="figure" target="#fig_5">Figure 8</ref>: Momentum ResNets can be twice as fast as ResNets to train when samples are so big that only few of them can be processed at a time. In contrast, RevNets <ref type="bibr" target="#b17">(Gomez et al., 2017)</ref> cannot as easily be used for fine-tuning since, as shown in (4), they require to train two distinct networks.</p><p>Continuous training. We also compare accuracy when using first-order ODE blocks  and secondorder ones on CIFAR-10. In order to emphasize the influence of the ODE, we considered a neural architecture which down-sampled the input to have a certain number of channels, and then applied 10 successive ODE blocks. Two types of blocks were considered: one corresponded to the first-order ODE (5) and the other one to the second-order ODE (6). Training was based on the odeint function imple-mented by <ref type="bibr" target="#b6">Chen et al. (2018)</ref>. <ref type="figure">Figure 9</ref> shows the final test accuracy for both models as a function of the number of channels used. As a baseline, we also include the final accuracy when there are no ODE blocks. We see that an ODE Net with momentum significantly outperforms an original ODE Net when the number of channels is small. Training took the same time for both models.  <ref type="figure">Figure 9</ref>. Accuracy after 120 iterations on CIFAR-10 with or without momentum, when varying the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Learning to optimize</head><p>We conclude by illustrating the usefulness of our Momentum ResNets in the learning to optimize setting, where one tries to learn to minimize a function. We consider the Learned-ISTA (LISTA) framework <ref type="bibr" target="#b19">(Gregor &amp; LeCun, 2010)</ref>. Given a matrix D ? R d?p , and a hyper-parameter ? &gt; 0, the goal is to perform the sparse coding of a vector y ? R d , by finding x ? R p that minimizes the Lasso cost function L y (x) 1 2 y ? Dx 2 + ? x 1 <ref type="bibr">(Tibshirani, 1996)</ref>. In other words, we want to compute a mapping y ? argmin x L y (x). The ISTA algorithm <ref type="bibr" target="#b12">(Daubechies et al., 2004)</ref> solves the problem, starting from x 0 = 0, by iterating x n+1 = ST(x n ? ?D (Dx n ? y), ??), with ? &gt; 0 a step-size. Here, ST is the soft-thresholding operator. The idea of <ref type="bibr" target="#b19">Gregor &amp; LeCun (2010)</ref> is to view L iterations of ISTA as the output of a neural network with L layers that iterates x n+1 = g(x n , y, ? n ) ST(W 1 n x n +W 2 n y, ??), with parameters ? (? 1 , . . . , ? L ) and ? n (W 1 n , W 2 n ). We call ?(y, ?) the network function, which maps y to the output x L . Importantly, this network can be seen as a residual network, with residual function f (x, y, ?) = g(x, y, ?) ? x. ISTA corresponds to fixed parameters between layers: W 1 n = Id p ? ?D D and W 2 n = ?D , but these parameters can be learned to yield better performance. We focus on an "unsupervised" learning setting, where we have some training examples y 1 , . . . , y Q , and use them to learn parameters ? that quickly minimize the Lasso function L. In other words, the parameters ? are estimated by minimizing the cost function ? ? Q q=1 L yq (?(y q , ?)). The performance of the network is then measured by computing the testing loss, that is the Lasso loss on some unseen testing examples.</p><p>We consider a Momentum ResNet and a RevNet variant of LISTA which use the residual function f . For the RevNet, the activations x n are first duplicated: the network has twice as many parameters at each layer. The matrix D is generated with i.i.d. Gaussian entries with p = 32, d = 16, and its columns are then normalized to unit variance. Training and testing samples y are generated as normalized Gaussian i.i.d. entries. More details on the experimental setup are added in Appendix D. The next <ref type="figure">Figure 10</ref> shows the test loss of the different methods, when the depth of the networks varies.  <ref type="figure">Figure 10</ref>. Evolution of the test loss for different models as a function of depth in the Learned-ISTA (LISTA) framework.</p><p>As predicted by Proposition 1, the RevNet architecture fails on this task: it cannot have converging iterations, which is exactly what is expected here. In contrast, the Momentum ResNet works well, and even outperforms the LISTA baseline. This is not surprising: it is known that momentum can accelerate convergence of first order optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper introduces Momentum ResNets, new invertible residual neural networks operating with a significantly reduced memory footprint compared to ResNets. In sharp contrast with existing invertible architectures, they are made possible by a simple modification of the ResNet forward rule. This simplicity offers both theoretical advantages (better representation capabilities, tractable analysis of linear dynamics) and practical ones (drop-in replacement, speed and memory improvements for model fine-tuning). Momentum ResNets interpolate between ResNets (? = 0) and RevNets (? = 1), and are a natural second-order extension of neural ODEs. As such, they can capture non-homeomorphic dynamics and converging iterations. As shown in this paper, the latter is not possible with existing invertible residual networks, although crucial in the learning to optimize setting. In Section A we give the proofs of all the Propositions and the Theorem. In Section B we give other theoretical results to validate statements made in the paper. Section C presents the algorithm from <ref type="bibr" target="#b35">Maclaurin et al. (2015)</ref>. Section D gives details for the experiments in the paper. We derive the formula for backpropagation in Momentum ResNets in Section E. Finally, we present additional figures in Section F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>Notations</p><formula xml:id="formula_26">? C ? 0 ([0, 1], R d )</formula><p>is the set of infinitely differentiable functions from [0, 1] to R d with value 0 in 0.</p><p>? If f : U ? V ? W is a function, we denote by ? u f , when it exists, the partial derivative of f with respect to u ? U .</p><p>? For a matrix A ? R d?d , we denote by (? ? z) a the Jordan block of size a ? N associated to the eigenvalue z ? C .</p><p>A.0. Instability of fixed points -Proof of Proposition 1</p><p>Proof.</p><p>Since (x * , v * ) is a fixed point of the RevNet iteration, we have</p><formula xml:id="formula_27">?(x * ) = 0 ?(v * ) = 0</formula><p>Then, a first order expansion, writing x = x * + ? and v = v * + ? gives at order one</p><formula xml:id="formula_28">?(v, x) = (v * + ? + A?, x * + ? + B(? + A?))<label>(9)</label></formula><p>We therefore obtain at order one</p><formula xml:id="formula_29">?(v, x) = ?(v * , x * ) + J(A, B) ? ?</formula><p>which shows that J <ref type="figure">(A, B)</ref> is indeed the Jacobian of ? at (v * , x * ). We now turn to a study of the spectrum of J(A, B). We let ? ? C an eigenvalue of J <ref type="figure">(A, B)</ref>, and vectors u ? C d , w ? C d such that (u, w) is the corresponding eigenvector, and study the eigenvalue equation</p><formula xml:id="formula_30">J(A, B) u w = ? u w</formula><p>which gives the two equations u + Aw = ?u (10)</p><formula xml:id="formula_31">w + Bu + BAw = ?w<label>(11)</label></formula><p>We start by showing that ? = 1 by contradiction. Indeed, if ? = 1, then (10) gives Aw = 0, which implies w = 0 since A is invertible. Then, (11) gives Bu = 0, which also implies u = 0. This contradicts the fact that (u, v) is an eigenvector (which is non-zero by definition).</p><p>Then, the first equation <ref type="formula" target="#formula_0">(10)</ref> gives Aw = (? ? 1)u, and multiplying (11) by A on the left gives</p><formula xml:id="formula_32">?ABu = (? ? 1) 2 u<label>(12)</label></formula><p>We also cannot have ? = 0, since it would imply u = 0. Then, dividing (12) by ? shows that (??1) 2 ? is an eigenvalue of AB.</p><p>Next, we let ? = 0 the eigenvalue of AB such that ? = (??1) 2 ? . The equation can be rewritten as the second order equation</p><formula xml:id="formula_33">? 2 ? (2 + ?)? + 1 = 0</formula><p>This equation has two solutions ? 1 (?), ? 2 (?), and since the constant term is 1, we have ? 1 (?)? 2 (?) = 1. Taking modulus, we get |? 1 (?)||? 2 (?)| = 1, which shows that necessarily, either |? 1 (?)| ? 1 or |? 1 (?)| ? 1.</p><p>Now, the previous reasoning is only a necessary condition on the eigenvalues, but we can now prove the advertised result by going backwards: we let ? = 0 an eigenvalue of AB, and u ? C d the associated eigenvector. We consider ? a solution of ? 2 ? (2 + ?)? + 1 = 0 such that |?| ? 1 and ? = 1. Then, we consider w = (? ? 1)A ?1 u. We just have to verify that (u, v) is an eigenvector <ref type="figure">of J(A, B)</ref>. By construction, (10) holds. Next, we have</p><formula xml:id="formula_34">A(w + Bu + BAw) = (? ? 1)u + ABu + (? ? 1)ABu = (? ? 1)u + ?ABu</formula><p>Leveraging the fact that u is an eigenvector of AB, we have ?ABu = ??u, and finally:</p><formula xml:id="formula_35">A(w + Bu + BAw) = (? ? 1 + ??)u = ?(? ? 1)u = ?Aw</formula><p>Which recovers exactly <ref type="formula" target="#formula_0">(11)</ref>: ? is indeed an eigenvalue of J <ref type="figure">(A, B)</ref>.</p><p>A.1. Momentum ResNets in the limit ? ? ? 0 -Proof of Proposition 2</p><p>Proof. We take T = 1 without loss of generality. We are going to use the implicit function theorem. Note that x ? is solution of (6) if and only if (</p><formula xml:id="formula_36">x ? , v ? =? ? ) is solution of ? = v, x(0) = x 0 ?v = f (x, ?) ? v, v(0) = v 0 . Consider for u = (x, v) ? (x 0 , v 0 ) + C ? 0 ([0, 1], R d ) 2 ?(u, ?) = x 0 ? x + t 0 v, t 0 (f (x, ?) ? v) ? ?v + ?v 0 ,</formula><p>so that x ? is solution of (6) if and only if u ? = (x ? , v ? =? ? ) satisfies ?(u ? , ?) = 0. Let u * = (x * ,? * ). One has ?(u * , 0) = 0. ? is differentiable everywhere, and at (u * , 0) we have <ref type="figure">0)</ref> is continuous, and it is invertible with continuous inverse because it is linear and continuous, and because</p><formula xml:id="formula_37">? u ?(u * , 0)(x, v) = ( t 0 v) ? x, t 0 (? x f (x * , ?).x ? v) . ? u ?(u * ,</formula><formula xml:id="formula_38">? u ?(u * , 0)(x, v) = 0 if and only if ?t ? [0, 1], x(t) = t 0 v ?t ? [0, 1], v(t) = ? x f (x * (t), ?(t)).x(t) which is equivalent to ? ? ? ? ?? = ?f (x * , ?).x x(0) = 0 v =?,</formula><p>which is equivalent, because this equation is linear to (x, v) = (0, 0). Using the implicit function theorem, we know that there exists two neighbourhoods U ? R and V ? (x 0 , v 0 ) + C ? 0 ([0, 1], R d ) 2 of 0 and u * and a continuous function</p><formula xml:id="formula_39">? : U ? V such that ?(u, ?) ? U ? V, ?(u, ?) = 0 ? u = ?(?)</formula><p>This in particular ensures that x ? converges uniformly to x * as ? goes to 0 A.2. Momentum ResNets are more general than neural ODEs -Proof of Proposition 3</p><p>Proof. If x satisfies (5) we get by derivation tha?</p><formula xml:id="formula_40">x = ? x f (x, ?)f (x, ?) + ? ? f (x, ?)? Then, if we definef (x, ?) = ?[? x f (x, ?)f (x, ?) + ? ? f (x, ?)?] + f (x, ?)</formula><p>, we get that x is also solution of the second-order model ?? +? =f (x, ?) with (x(0),?(0)) = (x 0 , f (x 0 , ? 0 )).</p><p>A.3. Solution of (7) -Proof of Proposition 4 (7) writes</p><formula xml:id="formula_41">? = v, x(0) = x 0 v = ?x?v ? , v(0) = 0.</formula><p>For which the solution at time t writes</p><formula xml:id="formula_42">x(t) v(t) = exp 0 Id d t ?t ? ? Id d t ? .</formula><p>x 0 0 .</p><p>The calculation of this exponential gives</p><formula xml:id="formula_43">x(t) = e ? t 2? +? n=0 1 (2n)! ( ? ? + Id d 4? 2 ) n t 2n + +? n=0 1 2?(2n + 1)! ( ? ? + Id d 4? 2 ) n t 2n+1 x 0 .</formula><p>Note that it can be checked directly that this expression satisfies (7) by derivations. At time 1 this effectively gives</p><formula xml:id="formula_44">x(1) = ? ? (?)x 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Representable mappings for a Momentum ResNet with linear residual functions -Proof of Theorem 1</head><p>In what follows, we denote by f ? the function of matrices defined by</p><formula xml:id="formula_45">f ? (?) = ? ? (?? ? I 4? ) = e ? 1 2? +? n=0 1 (2n)! + 1 2?(2n + 1)! ? n .</formula><p>Because ? ? (R d?d ) = f ? (R d?d ), we choose to work on f ? .</p><p>We first need to prove that f ? is surjective on C.</p><formula xml:id="formula_46">A.4.1. SURJECTIVITY ON C OF f ? Lemma 1 (Surjectivity of f ? ). For ? &gt; 0, f ? is surjective on C.</formula><p>Proof. Consider</p><formula xml:id="formula_47">F ? : C ?? C z ?? e ? 1 2? (cosh(z) + 1 2?z sinh(z)).</formula><p>For z ? C, we have f ? (z 2 ) = F ? (z), and because z ? z 2 is surjective on C, it is sufficient to prove that F ? is surjective on C. Suppose by contradiction that there exists w ? C such that ?z ? C, exp ( 1 2? )F ? (z) = w. Then exp ( 1 2? )F ? ? w is an entire function <ref type="bibr" target="#b31">(Levin, 1996)</ref> of order 1 with no zeros. Using Hadamard's factorization theorem <ref type="bibr" target="#b8">(Conway, 2012)</ref>, this implies that there exists a, b ? C such that ?z ? C,</p><formula xml:id="formula_48">cosh(z) + sinh(z) 2?z ? w = exp (az + b).</formula><p>However, since F ? is an even function one has that ?z ? C exp (az + b) = exp (?az + b) so that ?z ? C, 2az ? 2i?Z. Necessarily, a = 0, which is absurd because F ? is not constant.</p><p>We first prove Theorem 1 in the diagonalizable case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2. THEOREM 1 IN THE DIAGONALIZABLE CASE</head><p>Proof. Necessity Suppose that D can be represented by a second-order model <ref type="formula" target="#formula_15">(7)</ref>. This means that there exists a real matrix X such that D = f ? (X) with X real and</p><formula xml:id="formula_49">f ? (X) = e ? 1 2? ( +? n=0 a ? n X n )</formula><p>with a ? n = 1 (2n)! + 1 2?(2n + 1)! .</p><p>X commutes with D so that there exists P ? GL d (C) such that P ?1 DP is diagonal and P ?1 XP is triangular. Because f ? (P ?1 XP ) = P ?1 DP , we have that ?? ? Sp(D), there exists z ? Sp(X) such that ? = f ? (z). Because ? &lt; ? ? , necessarily, z ? C ? R. In addition, ? = f ? (z) =? = f ? (z). Because X is real, each z ? Sp(X) must be associated with z in P ?1 XP . Thus, ? appears in pairs in P ?1 DP .</p><p>Sufficiency Now, suppose that ?? ? Sp(D) with ? &lt; ? ? , ? is of even multiplicity order. We are going to exhibit a X real such that D = f ? (X). Thanks to Lemma 1, we have that f ? is surjective. Let ? ? Sp(D).</p><p>? If ? ? R and ? &lt; ? ? or ? ? C ? R then there exists z ? C ? R by Lemma 1 such that ? = f ? (z).</p><p>? If ? ? R and ? ? ? ? , then because f ? is continuous and goes to infinity when x ? R goes to infinity, there exists</p><p>x ? R such that ? = f ? (x).</p><p>In addition, there exist (? 1 , ...,</p><formula xml:id="formula_50">? k ) ? (C ? R) k ? [??, ? ? [ k , (? 1 , ..., ? p ) ? [? ? , +?] p such that D = Q ?1 ?Q, with Q ? GL d (R), and ? = ? ? ? ? ? ? ? ? ? ? ? P ?1 1 D ?1 P 1 0 2 ? ? ? ? ? ? ? ? ? 0 2 0 2 . . . ? ? ? ? ? ? ? ? ? 0 2 . . . . . . P ?1 k D ? k P k 0 2 ? ? ? 0 2 0 ? ? ? ? ? ? ? 1 ? ? ? 0 0 ? ? ? ? ? ? 0 . . . 0 0 ? ? ? ? ? ? ? ? ? ? ? ? ? p ? ? ? ? ? ? ? ? ? ? ? ? R d?d</formula><p>with P j ? GL 2 (C) and D ?j = ? j 0 0? j .</p><p>Let (z 1 , ..., z k ) ? (C ? R) k and (x 1 , ..., x p ) ? R p be such that f ? (z j ) = ? j and f ? (x j ) = ? j . For 1 ? j ? k, one has P ?1 j D zj P j ? R 2?2 . Indeed, writing ? j = a j + ib j with a j , b j ? R, the fact that P ?1 j D ?j P j ? R 2?2 implies that</p><formula xml:id="formula_51">i 1 0 0 ?1 ? iR 2?2 . Writing z j = u j + iv j with u j , v j ? R, we get that P ?1 j D zj P j ? R 2?2 . Then X = Q ? ? ? ? ? ? ? ? ? ? ? P ?1 1 D z1 P 1 0 2 ? ? ? ? ? ? ? ? ? 0 2 0 2 . . . ? ? ? ? ? ? ? ? ? 0 2 . . . . . . P ?1 k D z k P k 0 2 ? ? ? 0 2 0 ? ? ? ? ? ? x 1 ? ? ? 0 0 ? ? ? ? ? ? 0 . . . 0 0 ? ? ? ? ? ? ? ? ? ? ? ? x p ? ? ? ? ? ? ? ? ? ? ? Q ?1 ? R d?d</formula><p>is such that f ? (X) = D, and D is represented by a second-order model <ref type="formula" target="#formula_15">(7)</ref>.</p><p>We now state and demonstrate the general version of Theorem 1.</p><p>First, we need to demonstrate properties of the complex derivatives of the entire function f ? .</p><formula xml:id="formula_52">A.4.3. THE ENTIRE FUNCTION f ? HAS A DERIVATIVE WITH NO-ZEROS ON C ? R.</formula><p>Lemma 2 (On the zeros of f ? ). ?z ? C ? R we have f ? (z) = 0.</p><p>Proof. One has</p><formula xml:id="formula_53">G ? (z) = e ? 1 2? (cos(z) + 1 2?z sin(z)) = f ? (?z 2 )</formula><p>so that G ? (z) = ?2zf ? (?z 2 ) and it is sufficient to prove that the zeros of G ? are all real.</p><p>We first show that G ? belongs to the Laguerre-P?lya class <ref type="bibr" target="#b9">(Craven &amp; Csordas, 2002)</ref>. The Laguerre-P?lya class is the set of entire functions that are the uniform limits on compact sets of C of polynomials with only real zeros. To show that G ? belongs to the Laguerre-P?lya class, it is sufficient to show <ref type="bibr">(Dryanov &amp; Rahman, 1999, p. 22</ref>) that:</p><p>? The zeros of G ? are all real.</p><p>? If (z n ) n?N denotes the sequence of real zeros of G ? , one has 1 |zn| 2 &lt; ?. ? G ? is of order 1.</p><p>First, the zeros of G ? are all real, as demonstrated in <ref type="bibr" target="#b47">Runckel (1969)</ref>. Second, if (z n ) n?N denotes the sequence of real zeros of G ? , one has z n ? n? + ? 2 as n ? ? ?, so that 1 |zn| 2 &lt; ?. Third, G ? is of order 1. Thus, we have that G ? is indeed in the Laguerre-P?lya class.</p><p>This class being stable under differentiation, we get that G ? also belongs to the Laguerre-P?lya class. So that the roots of G ? are all real, and hence those of f ? as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.4. THEOREM 1 IN THE GENERAL CASE</head><p>When ? = 0, we have in the general case the following from <ref type="bibr" target="#b10">Culver (1966)</ref>:</p><p>Let A ? R d?d . Then A can be represented by a first-order model (8) if and only if A is not singular and each Jordan block of A corresponding to an eigen value ? &lt; 0 occurs an even number of time.</p><p>We now state and demonstrate the equivalent of this result for second order models (7). Theorem 2 (Representable mappings for a Momentum ResNet with linear residual functions -General case). Let A ? R d?d .</p><p>If A can be represented by a second-order model <ref type="formula" target="#formula_15">(7)</ref>, then each Jordan block of A corresponding to an eigen value ? &lt; ? ? occurs an even number of time.</p><p>Reciprocally, if each Jordan block of A corresponding to an eigen value ? ? ? ? occurs an even number of time, then A can be represented by a second-order model.</p><p>Proof. We refer to the arguments from <ref type="bibr" target="#b10">Culver (1966)</ref> and use results from <ref type="bibr" target="#b15">Gantmacher (1959)</ref> for the proof.</p><p>Suppose that A can be represented by a second-order model <ref type="formula" target="#formula_15">(7)</ref>. This means that there exists X ? R d?d such that A = f ? (X ). The fact that X is real implies that its Jordan blocks are:</p><formula xml:id="formula_54">(? ? z k ) a k , z k ? R (? ? z k ) b k and (? ?z k ) b k , z k ? C ? R.</formula><p>Let ? k = f ? (z k ) be an eigenvalue of A such that ? k &lt; ? ? . Necessarily, z k ? C ? R, and f ? (z k ) = 0 thanks to Lemma 2. We then use Theroem 9 from <ref type="bibr">Gantmacher (1959) (p. 158)</ref> to get that the Jordan blocks of A corresponding to ? k are</p><formula xml:id="formula_55">(? ? f ? (z k )) b k and (? ? f ? (z k )) b k .</formula><p>Since f ? (z k ) = f ? (z k ) = ? k , we can conclude that the Jordan blocks of A corresponding ? k &lt; ? ? occur an even number of time.</p><p>Now, suppose that each Jordan block of A corresponding to an eigen value ? ? ? ? occurs an even number of times. Let ? k be an eigenvalue of A.</p><formula xml:id="formula_56">? If ? k ? C ? R we can write, because f ? is surjective (proved in Lemma 1), ? k = f ? (z k ) with z k ? C ? R. Necessarily,</formula><p>because A is real, the Jordan blocks of A corresponding to ? k have to be associated to those corresponding to? k . In addition, thanks to Lemma 2, f ? (z k ) = 0</p><formula xml:id="formula_57">? If ? k &lt; ? ? , we can write, because f ? is surjective, ? k = f ? (z k ) = f ? (z k ) with z k ? C ? R. In addition, f ? (z k ) = 0. ? If ? k &gt; ? ? , then there exists z k ? R such that ? k = f ? (z k ) and f ? (z k ) = 0 because, if x ? is such that f ? (x ? ) = ? ? , we have that f ? &gt; 0 on ]x ? , +?[. ? If ? k = ? ? , there exists z k ? R such that ? k = f ? (z k ). Necessarily, f ? (z k ) = 0 but f ? (z k ) = 0.</formula><p>This shows that the Jordan blocks of A are necessarily of the form</p><formula xml:id="formula_58">(? ? f ? (z k )) b k and (? ? f ? (z k )) b k , z k ? C ? R (? ? f ? (z k )) a k , z k ? R, f ? (z k ) = ? ? (? ? ? ? ) c k and (? ? ? ? ) c k .</formula><p>Let Y ? R d?d be such that its Jordan blocks are of the form</p><formula xml:id="formula_59">(? ? z k ) b k and (? ?z k ) b k , z k ? C ? R, f ? (z k ) = 0 (? ? z k ) a k , z k ? R, f ? (z k ) = ? ? , f ? (z k ) = 0 (? ? z k ) 2c k , z k ? R, f ? (z k ) = ? ? .</formula><p>Then again by the use of Theorem 7 from <ref type="bibr" target="#b15">Gantmacher (1959)</ref> </p><formula xml:id="formula_60">(p. 158), because if f ? (z k ) = ? ? with z k ? R, f ? (z k ) = 0, we have that f ? (Y ) is similar to A. Thus A writes A = P ?1 f ? (Y )P = f ? (P ?1 Y P ) with P ? GL d (R)</formula><p>. Then, X = P ?1 Y P satisfies X ? R d?d and f ? (X) = A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional theoretical results</head><p>B.1. On the convergence of the solution of a second order model when ? ? ? Proposition 5 (Convergence of the solution when ? ? ? +?). We let x * (resp. x ? ) be the solution of? = f (x, ?) (resp. x + 1 ?? = f (x, ?)) on [0, T ], with initial conditions x * (0) = x ? (0) = x 0 and? * (0) =? ? (0) = v 0 . Then x ? converges uniformly to x * as ? ? ? +?.</p><formula xml:id="formula_61">Proof. The equation? + 1 ?? = f (x, ?) with x ? (0) = x 0 ,? ? (0) = v 0 writes in phase space (x, v) ? = v, x(0) = x 0 v = f (x, ?) ? v ? , v(0) = v 0 .</formula><p>It then follows from the Cauchy-Lipschitz Theorem with parameters <ref type="bibr" target="#b42">(Perko, 2013</ref>, Theorem 2, Chapter 2) that the solutions of this system are continuous in the parameter 1 ? . That is x ? converges uniformly to x * as ? ? ? +?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Universality of Momentum ResNets</head><p>Proposition 6 (When v 0 is free any mapping can be represented). Consider h : R d ? ? R d , and the OD?</p><formula xml:id="formula_62">x +? = 0 (x(0),?(0)) = (x 0 , h(x 0 ) ? x 0 1 ? 1/e ) Then ? 1 (x 0 ) = h(x 0 ).</formula><p>Proof. This is because the solution is ? t (x 0 ) = x 0 ? v 0 (e ?t ? 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Non-universality of Momentum ResNets when v 0 = 0</head><p>Proposition 7 (When v 0 = 0 there are mappings that cannot be learned if the equation is autonomous.). When d = 1, consider the autonomous ODE</p><formula xml:id="formula_63">?? +? = f (x) (x(0),?(0)) = (x 0 , 0)<label>(13)</label></formula><p>If there exists x 0 ? R+ * such that h(x 0 ) ? ?x 0 and x 0 ? h(?x 0 ) then h cannot be represented by <ref type="formula" target="#formula_0">(13)</ref>.</p><p>This in particular proves that x ? ?x for ? ? ?1 cannot be represented by this ODE with initial conditions (x 0 , 0).</p><p>Proof. Consider such an x 0 and h. Since ? 1 (x 0 ) = h(x 0 ) ? ?x 0 , that ? 0 (x 0 ) = x 0 and that t ? ? t (x 0 ) is continuous, we know that there exists t 0 ? [0, 1] such that ? t0 (x 0 ) = ?x 0 . We denote x(t) = ? t (x 0 ), solution of</p><formula xml:id="formula_64">x + 1 ?? = f (x)</formula><p>Since d = 1, one can write f as a derivative: f = ?E . The energy E m = 1 2? 2 + E satisfies:</p><formula xml:id="formula_65">E m = ? 1 ?? 2 So that E m (t 0 ) ? E m (0) = ? 1 ? t0 0? 2</formula><p>In other words:</p><formula xml:id="formula_66">1 2 v(t 0 ) 2 + 1 ? t0 0? 2 + E(?x 0 ) = E(x 0 )</formula><p>So that E(?x 0 ) ? E(x 0 ) We now apply the exact same argument to the solution starting at x 1 = ?x 0 . Since x 0 ? h(?x 0 ) = h(x 1 ) there exists t 1 ? [0, 1] such that ? t1 (x 1 ) = x 0 . So that:</p><formula xml:id="formula_67">1 2 v(t 1 ) 2 + 1 ? t1 0? 2 + E(x 0 ) = E(?x 0 ) So that E(x 0 ) ? E(?x 0 ). We get that E(x 0 ) = E(?x 0 )</formula><p>This implies that? = 0 on [0, t 0 ], so that the first solution is constant and x 0 = ?x 0 which is absurd because x 0 ? R * .</p><p>B.4. When v 0 = 0 there are mappings that can be represented by a second-order model but not by a first-order one.</p><p>Proposition 8. There exits f such that the solution of?</p><formula xml:id="formula_68">+ 1 ?? = f (x)</formula><p>with initial condition (x 0 , 0) at time 1 is</p><formula xml:id="formula_69">x(1) = ?x 0 ? exp(? 1 2? )</formula><p>Proof. Consider the ODE?</p><formula xml:id="formula_70">+ 1 ?? = (?? 2 ? 1 4? 2 )x<label>(14)</label></formula><p>with initial condition (x 0 , 0) The solution of this ODE is</p><formula xml:id="formula_71">x(t) = x 0 e ? t 2? (cos(?t) + 1 2?? sin(?t))</formula><p>which at time 1 gives:</p><formula xml:id="formula_72">x(1) = ?x 0 e ? 1 2?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Orientation preservation of first-order ODEs</head><p>Proposition 9 (The homeomorphisms represented by (5) are orientation preserving.). If K ? R d is a compact set and h : K ? ? R d is a homeomorphism represented by <ref type="formula" target="#formula_9">(5)</ref>, then h is in the connected component of the identity function on K for the . ? topology.</p><p>We first prove the following:</p><formula xml:id="formula_73">Lemma 3. Consider K ? R d a compact set. Suppose that ?x ? K, ? t (x) is defined for all t ? [0, 1]. Then C = {? t (x) | x ? K, t ? [0, 1]} is compact as well.</formula><p>Proof. We consider (? tn (x n )) n?N a sequence in C. Since K ? [0, 1] is compact, we can extract sub sequences (t ?(n) ) n?N , (x ?(n) ) n?N that converge respectively to t 0 and x 0 . We denote them (t n ) n?N and (x n ) n?N again for simplicity of the notations. We have that:</p><formula xml:id="formula_74">? tn (x n ) ? ? t (x) ? ? tn (x n ) ? ? tn (x) + ? tn (x) ? ? t (x) .</formula><p>Thanks to Gronwall's lemma, we have</p><formula xml:id="formula_75">? tn (x n ) ? ? tn (x) ? x n ? x exp (kt n ), where k is f 's Lipschitz constant. So that ? tn (x n ) ? ? tn (x) ? ? 0 as n ? ? ?. In addition, it is obvious that ? tn (x) ? ? t (x) ? ? 0 as n ? ? ?. We conclude that ? tn (x n ) ? ? ? t (x) ? C,</formula><p>so that C is compact.</p><p>Proof. Let's denote by H the set of homeomorphisms defined on K. The application</p><formula xml:id="formula_76">? : [0, 1] ? ? H defined by ?(t) = ? t is continuous. Indeed, we have for any x 0 in R d that ? t+? (x 0 ) ? ? t (x 0 ) = t+? t f (? s (x 0 ))ds ? ?M f ,</formula><p>where M f bounds the continuous function f on C defined in lemma 3. Since M f does not depend on x 0 , we have that</p><formula xml:id="formula_77">? t+? ? ? t ? ? ? 0</formula><p>as ? ? ? 0, which proves that ? is continuous. Since ?(0) = Id K , we get that ?t ? [0, 1], ? t is connected to Id K .</p><p>B.6. On the linear mappings represented by autonomous first order ODEs in dimension 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider the autonomous ODE?</head><formula xml:id="formula_78">= f (x),<label>(15)</label></formula><p>Theorem 3 (Linearity). Suppose d = 1. If (15) represents a linear mapping x ? ax at time 1, we have that f is linear.</p><p>Proof. If a = 1, consider some x 0 ? R. Since ? 1 (x 0 ) = x 0 = ? 0 (x 0 ), there exists, by Rolle's Theorem a t 0 ? [0, 1] such that?(t 0 ) = 0. Then f (x(t 0 )) = 0. But since the constant solution y = x(t 0 ) then solves? = f (y), y(0) = x(t 0 ), we get by the unicity of the solutions that x(t 0 ) = y(0) = x(1) = y(1 ? t 0 ) = x 0 . So that f (x 0 ) = f (x(t 0 )) = 0. Since this is true for all x 0 , we get that f = 0. We now consider the case where a = 1 and a &gt; 0. Consider some x 0 ? R * . If f (x 0 ) = 0, then the solution constant to x 0 solves (3), and thus cannot reach ax 0 at time 1 because a = 1. Thus, f (x 0 ) = 0 if x 0 = 0. Second, if the trajectory starting at x 0 ? R * crosses 0 and f (0) = 0, then by the same argument we know that x 0 = 0, which is absurd. So that, ?x 0 ? R * , ?t ? [0, 1], f (? t (x 0 )) = 0 . We can thus rewrite (3) a?</p><formula xml:id="formula_79">x f (x) = 1.<label>(16)</label></formula><p>Consider F a primitive of 1 f . Integrating <ref type="formula" target="#formula_0">(16)</ref> F (x(t))?(t)dt = 1.</p><p>In other words, ?x ? R * : F (ax) = F (x) + 1.</p><p>We derive this equation and get: af (x) = f (ax).</p><p>This proves that f (0) = 0. We now suppose that a &gt; 1. We also have that a n f ( x a n ) = f (x).</p><p>But when n ? ? ?, f ( x a n ) = x a n f (0) + o( 1 a n ) so that f (x) = f (0)x and f is linear. The case a &lt; 1 treats similarly by changing a n to a ?n . B.7. There are mappings that are connected to the identity that cannot be represented by a first order autonomous ODE</p><p>In bigger dimension, we can exhibit a matrix in GL + d (R) (and hence connected to the identity) that cannot be represented by the autonomous ODE (15). Proposition 10 (A non-representable matrix). Consider the matrix</p><formula xml:id="formula_80">A = ?1 0 0 ?? ,</formula><p>where ? &gt; 0 and ? = 1. Then A ? GL + 2 (R) ? GL 2 (R) 2 and A cannot be represented by (15).</p><p>Proof. The fact that A ? GL + 2 (R) ? GL 2 (R) 2 is because A has two single negative eigenvalues, and because det(A) = ? &gt; 0. We consider the point (0, 1). At time 1, it has to be in (0, ??). Because the trajectory are continuous, there exists 0 &lt; t 0 &lt; 1 such that the trajectory is at (x, 0) at time t 0 , and thus at (?x, 0) at time t 0 + 1, and again at (x, 0) at time t 0 + 2. However, the particle is at (0, ? 2 ) at time 2. All of this is true because the equation is autonomous. Now, we showed that trajectories starting at (0, 1) and (0, ? 2 ) would intersect at time t 0 at (x, 0), which is absurd. <ref type="figure">Figure 11</ref> illustrates the paradox.</p><p>(0,1)</p><p>. (x,0) (0, ? ?) (?x,0) (0,? 2 ) . .</p><p>. . <ref type="figure">Figure 11</ref>. Illustration of Proposition 10. The points starting at (0, 1) and (0, ? 2 ) are distinct but their associated trajectories would have to intersect in (x, 0), which is impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Exact multiplication</head><p>Algorithm 1 Exactly reversible multiplication by a ratio, from <ref type="bibr" target="#b35">Maclaurin et al. (2015)</ref> 1: Input: Information buffer i, value c, ratio n/d 2: i = i ? d 3: i = i + (c mod d) 4: c = c ? d 5: c = c ? n 6: c = c + (i mod n) 7: i = i ? n 8: return updated buffer i, updated value c</p><p>We here present the algorithm from <ref type="bibr" target="#b35">Maclaurin et al. (2015)</ref>. In their paper, the authors represent ? as a rational number, ? = n d ? Q. The information is lost during the integer division of v n by d in (2). The store this information, it is sufficient to store the remainder r of this integer division. r is stored in an "information buffer" i. To update i, one has to left-shift the bits in i by multiplying it by n before adding r. The entire procedure is illustrated in Algorithm 1 from <ref type="bibr" target="#b35">Maclaurin et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment details</head><p>In all our image experiments, we use Nvidia Tesla V100 GPUs.</p><p>For our experiments on CIFAR-10 and 100, we used a batch-size of 128 and we employed SGD with a momentum of 0.9. The training was done over 220 epochs. The initial learning rate was 0.01 and was decayed by a factor 10 at epoch 180. A constant weight decay was set to 5 ? 10 ?4 . Standard inputs preprocessing as proposed in Pytorch <ref type="bibr" target="#b40">(Paszke et al., 2017)</ref> was performed.</p><p>For our experiments on ImageNet, we used a batch-size of 256 and we employed SGD with a momentum of 0.9. The training was done over 100 epochs. The initial learning rate was 0.1 and was decayed by a factor 10 every 30 epochs. A constant weight decay was set to 10 ?4 . Standard inputs preprocessing as proposed in Pytorch <ref type="bibr" target="#b40">(Paszke et al., 2017)</ref> was performed: normalization, random croping of size 224 ? 224 pixels, random horizontal flip.</p><p>For our experiments in the continuous framework, we adapted the code made available by <ref type="bibr" target="#b6">Chen et al. (2018)</ref> to work on the CIFAR-10 data set and to solve second order ODEs. We used a batch-size of 128, and used SGD with a momentum of 0.9. The initial learning rate was set to 0.1 and reduced by a factor 10 at iteration 60. The training was done over 120 epochs.</p><p>For the learning to optimize experiment, we generate a random Gaussian matrix D of size 16 ? 32. The columns are then normalized to unit variance. We train the networks by stochastic gradient descent for 10000 iterations, with a batch-size of 1000 and a learning rate of 0.001. The samples y q are generated as follows: we first sample a random Gaussian vector y q , and then we use y q =? q D ?q ? , which ensures that every sample verify D y q ? = 1. This way, we know that the solution x * is zero if and only if ? ? 1. The regularization is set to ? = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Backpropagation for Momentum ResNets</head><p>In order to backpropagate the gradient of some loss in a Momentum ResNet, we need to formulate an explicit version of (2). Indeed, (2) writes explicitly v n+1 = ?v n + (1 ? ?)f (x n , ? n )</p><p>x n+1 = x n + (?v n + (1 ? ?)f (x n , ? n )).</p><p>Writing z = (x, v), the backpropagation for Momentum ResNets then writes, for some loss L</p><formula xml:id="formula_82">? z k?1 L = I + (1 ? ?)? x f (x k?1 , ? k?1 ) ?I (1 ? ?)? x f (x k?1 , ? k?1 ) ?I T ? z k L ? ? k?1 L = (1 ? ?) ? ? f (x k?1 , ? k?1 ) ? ? f (x k?1 , ? k?1 ) T ? z k L.</formula><p>We implement these formula to obtain a custom Jacobian-vector product in Pytorch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Overview of the four different paradigms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Separation of four nested rings using a ResNet (upper row) and a Momentum ResNet (lower row). From left to right, each figure represents the point clouds transformed at layer 3k. The ResNet fails whereas the Momentum ResNet succeeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Upper row: Memory used (using a profiler) for a ResNet and a Momentum ResNet on one training epoch, as a function of the batch size. Lower row: Final test accuracy as a function of the memory used (per epoch) for training Momentum ResNets-101 on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Accuracy as a function of time on hymenoptera when finetuning a ResNet-152 and a Momentum ResNet-152 with batch sizes of 2 and 4, respectively, as permitted by memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>, we getF (ax 0 ) ? F (x 0 ) = 1 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F. 1 .Figure 12 .</head><label>112</label><figDesc>Learning curves on CIFAR-10We here show the learning curves when training a ResNet-101 and a Momentum ResNet-101 on CIFAR-10. v 0 = f (x 0 )) Test error and test loss as a function of depth on CIFAR-10 with a ResNet-101 and two Momentum ResNets-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of reversible residual architectures</figDesc><table><row><cell>N eu r.O D E</cell><cell>i-R es N et</cell><cell>i-R ev N et</cell><cell>R ev N et</cell><cell>M om .N et</cell></row><row><cell>Closed-form inversion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Same parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unconstrained training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Test accuracy for CIFAR over 10 runs for each model Momentum ResNet, v 0 = f (x 0 ) 95.18 ? 0.06 76.38 ? 0.42 ResNet 95.15 ? 0.12 76.86 ? 0.25</figDesc><table><row><cell>Model</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>Momentum ResNet, v 0 = 0</cell><cell>95.1 ? 0.13</cell><cell>76.39 ? 0.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Test Accuracy? train = ? test , full training ? train = 1, no refit ? train = 1, refit for 20 epochs Test Accuracy ? train = ? test , full training ? train = 0, no refit ? train = 0, refit for 20 epochs</figDesc><table><row><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ResNet Momentum ResNet (v0 = 0)</cell><cell></cell></row><row><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test error</cell><cell>40% 50%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell></row><row><cell></cell><cell cols="4">Momentum at test time ? test</cell><cell></cell><cell></cell><cell cols="4">Momentum at test time ? test</cell><cell></cell><cell></cell><cell></cell><cell>Iterations</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267-288, 1996. Touvron, H., Vedaldi, A., Douze, M., and Jegou, H. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch?-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems.</figDesc><table><row><cell>Curran Associates, Inc.,</cell></row><row><cell>2019.</cell></row><row><cell>Verma, A. An introduction to automatic differentiation.</cell></row><row><cell>Current Science, pp. 804-807, 2000.</cell></row><row><cell>Wang, L., Ye, J., Zhao, Y., Wu, W., Li, A., Song, S. L.,</cell></row><row><cell>Xu, Z., and Kraska, T. Superneurons: Dynamic gpu</cell></row><row><cell>memory management for training deep neural networks.</cell></row><row><cell>In Proceedings of the 23rd ACM SIGPLAN Symposium</cell></row><row><cell>on Principles and Practice of Parallel Programming, pp.</cell></row><row><cell>41-53, 2018.</cell></row><row><cell>Weinan, E. A proposal on machine learning via dynamical</cell></row><row><cell>systems. Communications in Mathematics and Statistics,</cell></row><row><cell>5(1):1-11, 2017.</cell></row><row><cell>Weinan, E., Han, J., and Li, Q. A mean-field optimal control</cell></row><row><cell>formulation of deep learning. Research in the Mathemat-</cell></row><row><cell>ical Sciences, 6(1):10, 2019.</cell></row><row><cell>Zhang, H., Gao, X., Unterman, J., and Arodz, T. Approxi-</cell></row><row><cell>mation capabilities of neural odes and invertible residual</cell></row><row><cell>networks. In International Conference on Machine Learn-</cell></row><row><cell>ing, pp. 11086-11095. PMLR, 2020.</cell></row><row><cell>Zhang, T., Yao, Z., Gholami, A., Keutzer, K., Gonza-</cell></row><row><cell>lez, J., Biros, G., and Mahoney, M. Anodev2: A cou-</cell></row><row><cell>pled neural ode evolution framework. arXiv preprint</cell></row><row><cell>arXiv:1906.04596, 2019.</cell></row><row><cell>Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired</cell></row><row><cell>image-to-image translation using cycle-consistent adver-</cell></row><row><cell>sarial networks. In Proceedings of the IEEE international</cell></row><row><cell>conference on computer vision, pp. 2223-2232, 2017.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.kaggle.com/ajayrana/hymenoptera-data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was granted access to the HPC resources of IDRIS under the allocation 2020-[AD011012073] made by GENCI. This work was supported in part by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). This work was supported in part by the European Research Council (ERC project NORIA). The authors would like to thank David Duvenaud and Dougal Maclaurin for their helpful feedbacks. M. S. thanks Pierre Rizkallah and Pierre Roussillon for fruitful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The image of the exponential map and some applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-A</forename><surname>Rohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Joint Conference on Mathematics and Computer Science MaCS</title>
		<meeting>8th Joint Conference on Mathematics and Computer Science MaCS<address><addrLine>Komarno, Slovakia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational graphs and rounding error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic differentiation in machine learning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Radul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reversible architectures for arbitrarily deep residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentumnet: Fast and convergent iterative neural network for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Functions of one complex variable II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Conway</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">159</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterated laguerre and tur?n inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csordas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inequal. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the existence and uniqueness of the real logarithm of a matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Culver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An iterative thresholding algorithm for linear inverse problems with a sparsity constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defrise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Mol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1413" to="1457" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation by entire functions belonging to the laguerre-polya class</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dryanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods and Applications of Analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="38" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The theory of matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Gantmacher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="volume">I</biblScope>
			<pubPlace>Chelsea, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Anode: Unconditionally accurate memory-efficient gradients for neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10298</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Vishwanathan, S., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conferenceon machine learning</title>
		<meeting>the 27th international conferenceon machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Evaluating derivatives: principles and techniques of algorithmic differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daulbaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katrutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09222</idno>
		<title level="m">Towards understanding normalization in neural odes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Geometric numerical integration: structure-preserving algorithms for ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hairer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lubich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wanner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dense sets of diagonalizable matrices. Proceedings of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hartfiel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="1669" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<title level="m">Deep invertible networks. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Big</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<title level="m">General visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lectures on entire functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">150</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimization algorithm inspired deep neural network structure design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep learning via dynamical systems: An approximation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10382</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3276" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradientbased hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training deep and recurrent networks with hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="479" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dissecting neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3952" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Momentumrnn: Integrating momentum into recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06919</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norcliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Simidjievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07220</idno>
		<title level="m">On second order behaviour in augmented neural odes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Differential equations and dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mathematical theory of optimal processes. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Pontryagin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02389</idno>
		<title level="m">Continuous-in-depth neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zeros of entire functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Runckel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="343" to="362" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Coupled oscillatory recurrent neural network (co{rnn}): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Stochastic training of residual networks: a differential equation viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00174</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: Full training or fine tuning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NIPS 2019)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Universal approximation property of neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Teshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tojo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02414</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
