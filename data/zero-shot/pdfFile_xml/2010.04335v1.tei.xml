<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanshu</forename><surname>Kumar</surname></persName>
							<email>kpriyanshu256@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology (Indian School of Mines</orgName>
								<address>
									<settlement>Dhanbad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadarsh</forename><surname>Singh</surname></persName>
							<email>aadarshsingh191198@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology (Indian School of Mines</orgName>
								<address>
									<settlement>Dhanbad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We experiment with COVID-Twitter-BERT and RoBERTa models to identify informative COVID-19 tweets. We further experiment with adversarial training to make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains a F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task 2 and ranks 1 st on the leaderboard. The ensemble of the models trained using adversarial training also produces similar result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since 2006, Twitter has been a popular social network where people express their thoughts and opinions on various topics. The enforcement of lockdown in various parts of the world, due to COVID-19, led to an increased usage of social media. Thus, the world has witnessed a plethora of tweets since the beginning of COVID-19. People have tweeted about many issues regarding the pandemic, mostly about the increasing infection rate, carelessness and incapability of governance and authorities to handle the increasing rate of cases. In addition, various preventive measures have also been conveyed through tweets.</p><p>Although there have been more than 600 million English tweets on COVID-19 <ref type="bibr" target="#b3">(Lamsal, 2020)</ref> , only a few of them are informative enough to be used by various monitoring systems to update their databases. Manual identification of these informative tweets can be tedious and erroneous. Hence, there is a dire need to develop systems in the form of machine learning models that can help us in filtering informative tweets.</p><p>In this paper, we present our approaches for the shared task "Identification of informative COVID-19 English Tweets" organized under the Workshop on Noisy User-generated Text (W-NUT). Our method makes use of ensembles consisting of Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> pretrained on COVID-19 tweets <ref type="bibr" target="#b8">(M?ller et al., 2020)</ref> and Robustly Optimized BERT Pretraining Approach (RoBERTa) <ref type="bibr" target="#b4">(Liu et al., 2019)</ref>. We also experiment with adversarial training so as to create models that generalise well and are robust.</p><p>The rest of the paper is organized as follows: Related work has been discussed in Section 2, followed by a brief description of the data used in Section 3. The proposed methods and experimental settings 1 have been elaborated in Section 4, 5 and 6. Section 7 and 8 contains the results and error analysis respectively. Section 9 concludes the paper and also includes possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been much research to identify informative tweets during times of emergency and disaster. <ref type="bibr" target="#b9">Neppalli et al. (2018)</ref> explored the performance of traditional machine learning algorithms and deep learning approaches for identifying informative tweets during disaster. They created manual features using the content of tweets for machine learning approaches and also tried out features obtained from Convolutional and Recurrent networks for deep learning approaches. A multi-modal approach for classifying informative tweets during disaster was proposed by <ref type="bibr" target="#b6">Madichetty and Sridevi (2020)</ref>. They combined the features obtained from text by a Convolutional Neural Network and the VGG-16 features obtained from the image accompanying the tweet, using late fusion for better performance than models using only text or only image. <ref type="bibr" target="#b13">Roy et al. (2020)</ref> proposed a classification and summarisation approach to identify informative tweets during the Fani cyclone (which occurred in 2019, affecting large parts of South-East Asia). They trained a Support Vector Machine (SVM) on linguistic features and Parts of Speech (POS) tags from tweets to identify informative tweets. To summarise the informative tweets, they experimented with Latent Semantic Analysis (LSA) and Luhn summarisation techniques. <ref type="bibr" target="#b18">Zahera et al. (2019)</ref> experimented with the transformer based architecture BERT to classify disaster related tweets into multi-label information types. They preprocessed the tweets of TREC-IS dataset before feeding them into BERT to produce significantly better results than the median score. In addition, they also experimented with Focal loss instead of binary crossentropy loss.</p><p>A new dataset for identifying informative tweet was released by <ref type="bibr" target="#b0">Aggarwal (2019)</ref>. The results of various machine learning algorithms using GloVe <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> word embeddings, syntactic information in the form of tf-idf vectors and BERT embeddings were also presented in the work.</p><p>Due to the abundance of tweets related to COVID-19, many works have been done to analyze the content, intent and effect of such tweets. <ref type="bibr" target="#b15">Singh et al. (2020)</ref> presented an analysis of COVID-19 tweets on the grounds of location, content and misinformation spread. A comprehensive study about the content of misinformative COVID-19 tweets and other aspects associated with them have been done by <ref type="bibr" target="#b14">Shahi et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>The dataset <ref type="bibr" target="#b10">(Nguyen et al., 2020)</ref> provided to the participants of the shared task contains 10,000 English COVID-19 tweets, out of which 4719 are labeled as INFORMATIVE and 5281 are labeled as UNINFORMATIVE. The tweets were annotated by 3 independent annotators and an inter-annotator agreement score of Fleiss' Kappa at 0.818 was obtained. The dataset contains the tweet ID, the tweet and the corresponding label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Preprocessing</head><p>Twitter data contains a lot of noise. Therefore, preprocessing on Twitter data will help the pretrained models in better performance. We perform the following data preprocessing steps, most of which have been inspired from M?ller et al. <ref type="formula">(2020) :</ref> 1. Unescape HTML tags 2. Remove unnecessary spaces, tabs and newlines 3. Replacing the the mentioned hyperlinks in the tweets (depicted as HTTPURL), with URL. A simple explanation for this could be that "URL" is a more commonly used expression of hyperlinks than HTTPURL.</p><p>4. Using the Python emoji 2 library to demojise the emojis i.e. replace them with a short textual description.</p><p>The user handles were already replaced by @USER in the tweets, hence no processing was required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Models</head><p>We experiment with the following models and techniques:</p><p>1. COVID-Twitter-BERT: BERT is based on the Transformer architecture <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. It consists of multi-attention heads which apply a sequence-to-sequence transformation on the input text sequence. For its training, BERT makes use of the following objectives: (a) learn to predict a masked token using the left and right context of the text sequence (Masked Language Model) (b) learn to predict whether two sentences occur in continuation or not (Next Sentence Prediction) <ref type="bibr" target="#b8">M?ller et al. (2020)</ref> pretrain the large version of BERT (BERT-Large) on COVID-19 related tweets posted by users between January 12 and April 16, 2020. This version of BERT has a better understanding of the given data as compared to BERT-large, which is pretrained on texts from Wikipedia. Hence, COVID-Twitter-BERT will be even more beneficial for the task when fine tuned.</p><p>2. RoBERTa Large: RoBERTa has the same architecture as BERT, but is different from BERT on the grounds of pretraining, which helps in better optimisation and performance. RoBERTa is pretrained on a larger dataset as compared to BERT, uses a larger batch size and replaces the Next Sentence Prediction objective. It also uses dynamic masking pattern as a better alternative to the static masking pattern used in BERT, i.e. RoBERTa duplicates the data and masks those differently each time, whereas BERT will mask the data only once.</p><p>3. Adversarial Training: With time, adversarial training is gaining popularity in Natural Language Processing (NLP) as well. In the field of Computer Vision, adversarial training is done by perturbing the input images slightly and minimising the adversarial loss. In NLP, the nature of input being discrete, small perturbations are done on the word embeddings.</p><p>Adversarial training not only increases the robustness of models but also helps in better generalisation. Both properties are beneficial and desirable for identification of informative tweets.</p><p>Although many approaches for adversarial training in NLP have been developed, we experiment with the approach proposed by Miyato et al. <ref type="formula">(2016)</ref> with a slight modification. In their approach, first the word embeddings are normalized. The gradients are then computed using the data and the required perturbations are created using the obtained gradients.</p><p>Let the sequence of (normalized) word embedding vectors of a text be t. The model parameters are represented by ?. The probability of the text belonging to class y is given by p(y|t; ?). The adversarial perturbations z adv are computed as follows: g = ? t log p(y|t; ?)</p><formula xml:id="formula_0">z adv = ? g/ g 2</formula><p>where is a hyper-parameter controlling the size of the perturbations. The adversarial loss is defined as :</p><formula xml:id="formula_1">L adv (?) = ? 1 N N n=1</formula><p>log p(y n |t n + z adv,n ; ?)</p><p>By using the gradients calculated from the above loss, the weights of the model are updated (the non-perturbed word embeddings of the model are updated). The slight modification in our experiments is that we do not normalize our pretrained word embedding of the model, since it might change the semantic meaning of the pretrained word embeddings. We perform adversarial training on both COVID-Twitter-BERT and RoBERTa Large models using = 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ensembling:</head><p>The ensembling of the predictions for our submissions is done at two levels-(a) Fold level i.e. the predictions obtained by the models trained using the different folds (during cross validation) are averaged.</p><p>(b) Model level i.e. the fold level averaged predictions of the two different models are ensembled using averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Settings</head><p>We concatenate the training and the validation data and perform a 5-fold stratified cross validation to train our models. Each fold is trained for 5 epochs using early stopping with patience of 3 and tolerance of 1e-3. The models are optimised using AdamW <ref type="bibr" target="#b5">(Loshchilov and Hutter, 2017)</ref> with a learning rate of 2e-5 and a batch size of 16. The models have been implemented using Pytorch <ref type="bibr" target="#b11">(Paszke et al., 2019)</ref> and Huggingface's Transformers <ref type="bibr" target="#b17">(Wolf et al., 2019)</ref> library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We evaluate the performance of the models and their ensemble using cross-validation (CV). We also tabulate the models' performance on the test set as evaluated by the F1 score on the positive class.</p><p>The ensembling done at two levels increases the robustness of the results. The out-of-folds predictions were used to find the optimal threshold of the submissions, 0.498 for the ensemble without adversarial training and 0.487 for the ensemble with adversarial training. We also compare our results with the fastText-baseline <ref type="bibr" target="#b2">(Joulin et al., 2016)</ref> by the organizers. <ref type="table">Table 1</ref> shows the results of our experiments.</p><p>The ensemble of COVID-Twitter-BERT and RoBERTa Large performs the best on the test data and also obtains the 1 st rank on the leaderboard. Owing to its pretraining, COVID-Twitter-BERT performs better than RoBERTa Large. The adversarial training of the models is also found to boost the scores. The ensemble of adversarial models produces results similar to the ensemble of models trained without adversarial training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis</head><p>We perform our analysis on the out-of-folds predictions from our two ensembles -without adversarial training and with adversarial training.</p><p>We calculate the binary cross entropy loss for all samples and then examine some of the top common mis-classifications by both the ensembles <ref type="table">( Table  2)</ref>.</p><p>We observe that there are instances where our models are mistaken. A possible reason might be the variance in gold-labels of samples from annotator-to-annotator. This acts as noise in the labels of the data which is used to train the normal ensemble models. On the other hand, adversarial training adds some noise to the samples. Hence, the models in the adversarial ensemble have been trained on data which has a combination of existing noise and externally added noise. We believe that because of this difference in noise, the models in the two ensembles must be behaving in different ways. To inspect this, we examine the number of samples which have been inferred incorrectly by one ensemble and correctly by the other.</p><p>We observe that the normal ensemble misclassified a total of 277 samples whereas, only 260 samples were misclassified by the adversarial ensemble. Moreover, out of the 277 examples that were misclassified by the normal ensemble, 102 were correctly predicted by the adversarial ensemble. On the other hand, only 85 out of the 260 samples misclassified by the adversarial ensemble, were correctly predicted by the normal ensemble. Thus, it is evident that the models in both the ensembles have learnt different patterns from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We explored the performance of COVID-Twitter-BERT and RoBERTa-Large at identifying COVID-19 English tweets that are informative in nature. Their ensemble achieves the state-of-the-art performance. Adversarial training is found to improve our model further. For future work, we can pretrain other Transformer-based models on COVID-19 tweets. Data augmentation techniques can help us generate more data for training models. We can also experiment with combining models trained with and without adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tweet Label</head><p>Election Judge Hospitalized After Primary Dies Of Coronavirus #RIP #Sem-perFi fought for his life while -@USER got her hair done #LetThatSinkIn "face of Chicago" thinks she's more important than Chicagoans welfare #Light-footLiedPeopleDied HTTPURL UNINFORMATIVE Austin area nursing home residents who test positive for COVID-19 but do not need to be in the hospital will soon be moving to one of two new "isolation facilities," one in Travis County, one in Williamson County: HTTPURL @USER INFORMATIVE LOCAL NEWS SHOUTOUT: Have a family member or close friend with a Michigan connection who has died from COVID-19 and would like to share their story with @USER Please contact Georgea Kovanis at gkovanis@USER INFORMATIVE BREAKING: Southern AB #coronavirus case rumored to be Steve Busey, the younger brother of movie star @USER According to our sources, Steve works in the oil &amp;amp; gas industry and is a huge @USER fan. #COVID-19 HTTPURL UNINFORMATIVE <ref type="table">Table 2</ref>: Some common highly misclassified samples.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code available at https://github.com/ kpriyanshu256/WNUT-2020-Task-2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pypi.org/project/emoji/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Google Colab for providing free GPU services for experimentation purposes.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification approaches to identify informative tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piush</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop Associated with RANLP 2019</title>
		<meeting>the Student Research Workshop Associated with RANLP 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Corona virus (covid-19) tweets dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamsal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Dataport</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreenivasulu</forename><surname>Madichetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridevi</surname></persName>
		</author>
		<title level="m">Classifying informative and non-informative tweets from the twitter by adapting image features during disaster. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Salath?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Per</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kummervold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07503</idno>
		<title level="m">Covid-twitter-bert: A natural language processing model to analyse covid-19 content on twitter</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks versus naive bayes classifiers for identifying informative tweets during disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Venkata Kishore Neppalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCRAM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><forename type="middle">Hoang</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linh</forename><forename type="middle">The</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Noisy User-generated Text</title>
		<meeting>the 6th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification and summarization for informative tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Matam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Students&apos; Conference on Electrical, Electronics and Computer Science (SCEECS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Gautam Kishore Shahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Majchrzak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05710</idno>
		<title level="m">An exploratory study of covid-19 misinformation on twitter</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A first look at covid-19 information and misinformation sharing on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shweta</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leticia</forename><surname>Bode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceren</forename><surname>Budak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangqing</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kornraphop</forename><surname>Kawintiranon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colton</forename><surname>Padden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Vanarsdall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Vraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13907</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">Transformers: State-of-theart natural language processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-tuned bert model for multi-label tweets classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><forename type="middle">A</forename><surname>Zahera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rricha</forename><surname>Elgendy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ahmed</forename><surname>Jalota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
