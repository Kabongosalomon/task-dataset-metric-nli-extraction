<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NGC: A Unified Framework for Learning with Open-World Noisy Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Fan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wei</surname></persName>
							<email>weit@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
							<email>jianwen.jjw@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojie</forename><surname>Mao</surname></persName>
							<email>chaojie.mcj@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
							<email>mingqian.tmq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NGC: A Unified Framework for Learning with Open-World Noisy Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existence of noisy data is prevalent in both the training and testing phases of machine learning systems, which inevitably leads to the degradation of model performance.</p><p>There have been plenty of works concentrated on learning with in-distribution (IND) noisy labels in the last decade, i.e., some training samples are assigned incorrect labels that do not correspond to their true classes. Nonetheless, in real application scenarios, it is necessary to consider the influence of out-of-distribution (OOD) samples, i.e., samples that do not belong to any known classes, which has not been sufficiently explored yet. To remedy this, we study a new problem setup, namely Learning with Open-world Noisy Data (LOND). The goal of LOND is to simultaneously learn a classifier and an OOD detector from datasets with mixed IND and OOD noise. In this paper, we propose a new graph-based framework, namely Noisy Graph Cleaning (NGC), which collects clean samples by leveraging geometric structure of data and model predictive confidence. Without any additional training effort, NGC can detect and reject the OOD samples based on the learned class prototypes directly in testing phase. We conduct experiments on multiple benchmarks with different types of noise and the results demonstrate the superior performance of our method against state of the arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have gained popularity in a variety of applications. Despite their success, DNNs often rely on the availability of large-scale labeled training datasets. In practice, data annotation inevitably introduces label noise, and it is extremely expensive and timeconsuming to clean up the corrupted labels. The existence * Equal contribution. ? Corresponding author. This work was supported by Alibaba Group through Alibaba Innovative Research Program and the National Natural Science Foundation of China (61772262). of label noise can be problematic for overparameterized deep networks, as they may overfit to label noise even on randomly-assigned labels <ref type="bibr" target="#b53">[54]</ref>. Therefore, mitigating the effects of noisy labels becomes a critical issue. When learning with noisy labels (LNL), plenty of promising methods have been proposed to improve the generalization <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b46">47]</ref>. Many existing methods work by analyzing output predictions to identify mislabeled samples <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> or reweighting samples to alleviate the influence of noisy labels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1]</ref>. Note that, these methods are particularly designed to deal with in-distribution (IND) label noise. Some other works also consider the existence of out-of-distribution (OOD) noise in training datasets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>. Their basic assumption is that clean samples are clustered together while OOD samples are widely scattered in the feature space.</p><p>Although significant performance improvement is achieved, most existing LNL works only take account of OOD samples in training phase, while the existence of OOD samples in testing phase is neglected, which is crucial for machine learning systems in real applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>. In this paper, we study this practical problem, i.e., the ex- istence of both IND and OOD noise in training phase, as well as the presence of OOD samples in testing phase. We name this new setup as learning with open-world noisy data (LOND). An illustration of the LOND setup can be found in <ref type="figure" target="#fig_0">Figure 1</ref>. A straightforward approach to address LOND is to combine LNL methods with OOD detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref>. However, we empirically find that such direct combinations lead to unsatisfactory results as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Therefore, obtaining models that can handle IND and OOD noise in both training and testing phases remains challenging.</p><p>To address the LOND problem, we present Noisy Graph Cleaning (NGC), a unified framework for learning with open-world noisy data. Different from previous LNL methods that utilize either model predictions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> or neighborhood information <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref>, where the interaction between model predictions and geometric structure of data is neglected, NGC simultaneously takes advantage of output confidence and the geometric structure. With the help of graph structure, we find that the confidence-based strategy can break the connectivity between clean and noisy samples, which significantly facilitates the geometry-based strategy. In specific, NGC iteratively constructs the nearest neighbor graph using latent representations of training samples. Given the graph structure, NGC corrects IND noisy labels by aggregating information from neighborhoods through soft pseudo-label propagation. Then, to remove the OOD and remaining obstinate IND noise, we present subgraph selection. It first degrades the connectivity between clean and noisy samples by removing samples with low-confidence predictions. Then, subgraphs corresponding to the largest connected component are constructed for each class. Moreover, NGC employs the devised contrastive losses <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> to refine the representations from both instance-level and subgraph-level, which in return benefits label correction and subgraph selection. At test time, NGC can readily detect and reject OOD samples by calculating distances to learned class prototypes.</p><p>The main contributions of this work are:</p><p>1. We study a new problem, that is, the training set contains both IND and OOD noise and the test set contains OOD samples, which is practical in real applications.</p><p>2. We propose a new graph-based noisy label learning framework, NGC, which corrects IND noisy labels and sieves out OOD samples by utilizing the confidence of model predictions and geometric structure of data. Without any additional training effort, NGC can detect and reject OOD samples at testing time.</p><p>3. We evaluate NGC on multiple benchmark datasets under various noise types as well as real-world tasks.</p><p>Experimental results demonstrate the superiority of NGC over the state-of-the-art methods.</p><p>The rest of the paper is organized as follows. First, we introduce some related work. Then, we present the studied learning problem and the proposed framework. Furthermore, we experimentally analyze the proposed method. Finally, we conclude this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning from Noisy Labels is a heavily studied problem. Many methods attempt to rectify the loss function, which can be categorized into two types. The first type treats samples equally and rectifies the loss by either removing or relabeling noisy samples <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b31">32]</ref>. For example, AUM <ref type="bibr" target="#b34">[35]</ref> designs a margin-based method for detecting noisy samples by observing that clean samples have a larger margin than noisy samples. TopoFilter <ref type="bibr" target="#b44">[45]</ref> assumes that clean data is clustered together while noisy samples are isolated. Joint-Optim <ref type="bibr" target="#b38">[39]</ref> and PENCIL <ref type="bibr" target="#b50">[51]</ref> treat labels as learnable variables, which are jointly optimized along with model parameters. Another type of method learns to reweight samples with higher weights for clean data points <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>. Instead of using a fixed weight for all samples, M-correction <ref type="bibr" target="#b0">[1]</ref> uses dynamic hard and soft bootstrapping loss to dynamically reweight training samples. Some recent works resort to early-learning regularization <ref type="bibr" target="#b27">[28]</ref> and data augmentation <ref type="bibr" target="#b32">[33]</ref> to handle noisy labels.</p><p>The above methods only consider IND label noise in training datasets. Recently, some works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> propose to handle both IND and OOD noise in training datasets. For instance, ILON <ref type="bibr" target="#b40">[41]</ref> discriminates noise samples by density estimating. MoPro <ref type="bibr" target="#b23">[24]</ref> and ProtoMix <ref type="bibr" target="#b22">[23]</ref> identify IND and OOD noise according to predictive confidence. However, these approaches cannot be directly applied for detecting OOD at test time, and the performance of simply combining with existing OOD detection methods is not satisfactory. In this work, we introduce a new framework that simultaneously learns a classifier and an OOD detector from training data with both IND and OOD noise.</p><p>OOD Detection aims to identify test data points that are far from the training distribution. According to whether requiring labels during training time, OOD detection methods can be categorized into supervised learning methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b10">11]</ref> and unsupervised learning meth-ods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref>. For example, ODIN <ref type="bibr" target="#b26">[27]</ref> separates IND and OOD samples by using temperature scaling and adding perturbations to the input. Lee et al. <ref type="bibr" target="#b18">[19]</ref> obtains the class conditional Gaussian distributions and calculates confidence score based on Mahalanobis distance. Recently, SSD <ref type="bibr" target="#b37">[38]</ref> uses self-supervised learning to extract latent feature representations and Mahalanobis distance to compute the membership score between test data points and IND samples.</p><p>Compared with supervised detectors, NGC does not assume the availability of clean datasets which are often difficult to obtain in many real-world applications <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. Instead, NGC can detect OOD examples by training on noisylabeled datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning with Open-World Noisy Data</head><p>In this section, we first introduce the studied problem setup and an overview of the proposed noisy graph cleaning framework. Then, we present the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><formula xml:id="formula_0">Given a training dataset D train = {x i , y i } N i=1</formula><p>, where x i is an instance feature representation and y i ? C = {1, . . . , K} is the class label assigned to it. In D train , we assume that the instance-label pair (x i , y i ), 1 ? i ? N consists of three types. Denote y * i as the ground-truth label of x i , a correctly-labeled sample whose assigned label matches the ground-truth label, i.e., y i = y * i . An IND mislabeled sample has an assigned label that does not match the ground-truth label, but the input matches one of the classes in C, i.e., y i = y * i and y * i ? C. An OOD mislabeled sample is one where the input does not match the assigned label and other known classes, i.e., y i = y * i and y * i / ? C. In inference, there are two types of test samples. An IND sample is one where x is taken from the distribution of one of the known classes, i.e., y * i ? C. An OOD sample is the one taken from unknown class distributions, i.e., y * i / ? C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">An Overview of the Proposed Framework</head><p>To address the LOND problem, we present a graphbased framework, named Noisy Graph Cleaning (NGC), which can exploit the relationships among data and learn robust representations from reliable data. Initially, a k-NN graph is constructed, where samples are represented as vertices (nodes) in the graph with edges represent similarities between samples. Since labels of samples may be mislabeled, we refer to the resulting graph as noisy graph. Then, NGC accomplishes noisy graph cleaning in two steps. First, to cope with IND noise, NGC refines noisy labels using the proposed soft pseudo-label propagation based on the smoothness assumption <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b12">13]</ref>. Second, since OOD samples do not belong to any IND classes, soft pseudo-label propagation is not able to correct their labels. We propose to collect a subset of clean samples to guide the learning of the network. To achieve this goal, a two-stage subgraph selection method is introduced, i.e., confidence-based and geometry-based selection. The confidence-based strategy breaks the edges between nodes with clean labels and noisy labels by removing samples with low-confidence predictions. Then the geometry-based strategy selects nodes that are likely to be clean. <ref type="figure" target="#fig_3">Figure 3</ref> provides an illustration of the proposed method. We observe that these two selection strategies are indispensable and single application of each one leads to inferior performance. Based on that, we employ devised instance-level and subgraph-level contrastive losses to learn robust representations, which in return can benefit the construction of graph and the subgraph selection. In each training iteration, the graph is re-constructed and noise correction as well as subgraph selection are performed. Then, the selected clean samples are used for the training of DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph-based Noise Correction</head><p>The goal of noise correction is to propagate labels on the undirected graph G = V, E by leveraging similarities between data. V and E denote the set of graph vertices and edges, respectively. In graph G, the similarities between vertices are encoded by a weight matrix W . For scalability, we adopt the k-NN matrix, which is obtained by:</p><formula xml:id="formula_1">W ij := z i z j ? + , if i = j ? z i ? NN k (z j ) 0, otherwise<label>(1)</label></formula><p>Here, ? is a parameter simply set as ? = 1 in our experiments. z i is the latent representation for x i and NN k denotes the k nearest neighbors. To capture high-order graph information, researchers have designed models on the assumption that labels vary smoothly over the edges of the graph <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. In this work, we propose to propagate soft pseudo-labels obtained from the network. Denote Y = [y 1 , ? ? ? , y N ] ? R N ?K as the initial label matrix. We set y i to the one-hot label vector of x i if x i is selected as a clean sample by our method introduced in Section 3.4, otherwise we use model prediction aggregated by temporal ensemble <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> to initialize it. Let D be the diagonal degree matrix for W with entry d ii = j W ij , we obtain the refined soft pseudo-labels? = [? 1 , ? ? ? ,? N ] ? R N ?K by solving the following minimization problem:</p><formula xml:id="formula_2">J(? ) := ? 2 N i,j=1 W ij ? i ? d ii ?? j d jj 2 +(1??) Y ?? 2 F</formula><p>(2) In Eq. (2), all nodes propagate pseudo-labels to their neighbors according to edge weights. ? is used to trade-off between information from neighborhoods and vertices themselves and we simply set it to 0.5 in all experiments. This  minimization problem can be solved by using conjugate gradient as <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b12">13]</ref>. After obtaining refined soft pseudolabels, it is common to transform? into hard pseudo-labels to guide the training. Specifically, in iteration t, the hard pseudo-label for the i-th data point is generated by taking the largest prediction score as? i = arg max k?</p><formula xml:id="formula_3">(t) ik , wher? y (t) ik represents the k-th element in? (t) i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Subgraph Selection</head><p>When training DNNs with noisy labels, it is observed that clean samples of the same class are usually clustered together in the latent feature space, while noisy samples are pushed away from these clusters <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45]</ref>. This inspires us to find the connected component with the same class label in the graph for each class. Unfortunately, OOD samples can be similar to some clean samples, leading to undesirable edges in the graph such that nodes corresponding to OOD samples are included in the largest connected component (LCC). To remedy this, we introduce confidence-based selection to remove edges associated with low-confidence nodes because these edges are unreliable. After that, the geometry-based selection is employed to obtain the LCC in subgraphs of each class.</p><p>Confidence-based Sample Selection.</p><p>Since lowconfidence nodes are more likely to connect OOD nodes to the clusters of clean nodes, we use a sufficiently high threshold ? ? [0, 1] to select a reliable subset of nodes:</p><formula xml:id="formula_4">g i = 1, if? (t) iyi &gt; 1 K I max k? (t) ik &gt; ? , otherwise<label>(3)</label></formula><p>where g i is a binary indicator representing the conservation of node v i ? V when g i = 1 and the removal of node v i when g i = 0. Note that we have another conditio? Y (t) iyi &gt; 1 K which is complementary to the high-confidence condition inspired by previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>. The reason is that the network may not produce confident predictions in the early phase of training, while it has been observed to first fit the training data with clean labels <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28]</ref>. Therefore, we incline to treat label y i as clean if its corresponding prediction score is higher than uniform probability 1 K , and we set? i = y i . Then we refine graph G based on</p><formula xml:id="formula_5">the indicator g as? = V \ {v | ?v ? V, g v = 0} and E = E \ {e | ?e = e 1 , e 2 ? E, g e1 + g e2 &lt; 2}.</formula><p>In this way, low-confidence nodes and their corresponding edges are removed from graph G and the resulting graph is denoted byG = ? ,? . In the modified graphG, the connectivity between nodes are more reliable, which facilitates to the geometry-based selection.</p><p>Geometry-based Sample Selection. In graphG, we expect that nodes with same labels are connected. Since nodes with noisy labels locate far away from clean ones, more than one connected component may exist for each class. Therefore, we selected the LCC for robustness. Specifically, for the k-th class, graph nodes that possess labels of other classes, i.e.,? i = k, ?i ? [N ], and their adjacent edges G are removed. We denote this as the class-specific subgraph for class k asG(k). LetG(k) lcc be the set of nodes in the LCC ofG(k), we obtain a subset of clean samples by S = K k=1G (k) lcc . Note that a connected component ofG(k) is a subgraph in which any two vertices are connected by edges, and which is not connected to any other vertex in the rest of the graph. In other words, we consider data points belonging to the LCC of the class-specific subgraphs for each class to be clean, since small connected components may contain noisy samples. In practice, we implement disjoint-set data structures to compute the components effectively.</p><p>In summary, we identify clean samples by using both predictive confidence and the geometric structure of data:</p><formula xml:id="formula_6">g i = I[i ? S], if? (t) iyi &gt; 1 K I max k? (t) ik &gt; ? ? I[i ? S], otherwise<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Subgraph-level Contrastive Learning</head><p>It is noted that exploring the similarities between samples is essentially based on meaningful feature representations. To this end, we take advantage of contrastive learning, which has been successfully used to learn good representations in many tasks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>. The basic idea of contrastive learning is to pull together two embeddings of the same samples, while pushing apart embeddings of other samples. Formally, the instance-level contrastive loss is obtained as follows.</p><formula xml:id="formula_7">L inst = ? i?I log exp z i ? z j(i) /? 1 a?A(i) exp (z i ? z a /? 1 )<label>(5)</label></formula><p>Here z i = Proj (Enc (x i )) ? R D P denotes the l 2 normalized feature representation with dimension D P , and ? 1 is a scalar temperature parameter. I denotes the set of training samples, I is another augmented set, and A(i) = (I\{i}) ? I . Different augmentation strategies can be used on I and I as <ref type="bibr" target="#b22">[23]</ref>. We use j(i) to denote the index of the other augmented sample of x i . However, direct optimization of the instance-level contrastive objective in Eq. <ref type="formula" target="#formula_7">(5)</ref> is ineffective, which does not leverage the label information and the geometry of data. To this end, we design a subgraph-level contrastive loss:</p><formula xml:id="formula_8">L subgraph = i?I ?1 |P (i)| p?P (i) log exp (z i ? z p /? 2 ) a?A(i) exp (z i ? z a /? 2 ) (6) Here P (i) = {p ? A(i) :? p =? i ? g p + g i = 2}</formula><p>, and |P (i)| is its cardinality. In the calculation of |P (i)|, g i = 1 indicates that only selected clean samples by NGC are used for training. ? 2 is another temperature parameter. For each class, samples belonging to the corresponding LCC are pulled together by optimizing Eq. (6). In return, it benefits the clean data selection because more samples of the same class are connected in the k-NN graph.</p><p>Considering the above definitions and denoting L ce as conventional cross-entropy loss, the overall training objective is written as follows.</p><formula xml:id="formula_9">L = L ce + ? 1 L inst + ? 2 L subgraph ,<label>(7)</label></formula><p>where hyperparameters ? 1 and ? 2 are simply set to 1 in all experiments. We adopt DNN model as feature extractor Enc(?) and a linear layer as projector Proj(?) to generate latent feature representation z i . Another linear layer following the feature extractor is used as classifier. Finally, we train the network by minimizing the total loss in Eq. (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">OOD Detection</head><p>By far, NGC is able to learn classifiers from data with mixed IND and OOD noise. To fully achieve the goal of LOND, the framework must account for the presence of OOD samples at test time. This motivates us to design a principled way to detect OOD samples by measuring the class-conditional probability. Specifically, given a feature representation learned from NGC, the class-conditional probability is computed based on the similarity between the latent representation of input x and the class prototypes</p><formula xml:id="formula_10">{c k } K k=1 ,</formula><p>where c k is the normalized mean embedding for selected clean samples of class k, and can be obtained by:</p><formula xml:id="formula_11">c k = Normalize( 1 i?I k g i i?I k g i z i ),<label>(8)</label></formula><p>where I k denotes the set of samples for which the corresponding pseudo-labels? i = k, ?i ? [N ]. Then, the maximum class-wise similarity is computed as follows.</p><formula xml:id="formula_12">s (x) := max k?[K] sim (z, c k ) .<label>(9)</label></formula><p>Here z = Proj (Enc (x)) and sim stands for any similarity measure. In practice, we measure cosine similarity to compute s(x). When detecting OOD samples, the lower s (x) is, the more likely it is to be an OOD sample. To make hard decisions, the probability threshold ? is used. That is, a testing point x is deemed as OOD if and only if s(x) &lt; ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we investigate the performance of the proposed NGC on multiple datasets with various label noises. Specifically, we introduce our experiments in three aspects as shown in <ref type="table" target="#tab_0">Table 1</ref>. We verify the effectiveness of our method in the proposed LOND task and learning with closed-world noisy labels (LCNL) as well as learning from real-world noisy dataset (LRND) tasks in order. Implementation details. For all CIFAR experiments, we train PreAct ResNet-18 network using SGD optimizer with momentum 0.9 and weight decay 5 ? 10 ?4 . The initial learning rate is set to 0.15 and cosine decay schedule is used. The batch size is set to 512 and the dimension of projector layer is set to 64. For CIFAR-10 experiments, we use k = 30 for sym. noise and k = 10 for asym. noise, warmup with cross-entropy loss for 5 epochs. For CIFAR-100 experiments, we set k = 200 and warmup for 30 epochs. The network is trained for 300 epochs. Mixup <ref type="bibr" target="#b54">[55]</ref> and Aug-Mix <ref type="bibr" target="#b11">[12]</ref> are used as data augmentation. We provide detailed experimental settings in the supplementary material.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning with Open-World Noisy Data</head><p>To investigate the effectiveness of NGC, we test it under mixed IND and OOD label noise. In this setup, we report both classification and OOD detection performance to show that NGC can learn a good classifier and OOD detector simultaneously. We use CIFAR-10 and CIFAR-100 as IND datasets, and TinyImageNet and Places-365 as the OOD datasets. We first add 50% symmetric IND noise. Then, additional samples are randomly selected from the OOD datasets to form the training dataset. It is noted that the CIFAR-100 dataset is also used as one of the OOD datasets when CIFAR-10 is treated as the IND dataset.</p><p>First, we present the classification performance in <ref type="table" target="#tab_1">Table 2</ref>. We compare NGC with the cross-entropy baseline and three recent methods for LNL, i.e., ILON <ref type="bibr" target="#b40">[41]</ref>, RoG <ref type="bibr" target="#b19">[20]</ref> and DivideMix <ref type="bibr" target="#b20">[21]</ref>. ILON reweights samples based on the outlier measurement. RoG uses an ensemble of generative classifiers built from features extracted from multiple layers of the pretrained model. DivideMix is the state-of-the-art method for LNL. We report the results of DivideMix without ensemble for a fair comparison. The number of OOD samples in training datasets is set to either 10k or 20k. We can see that NGC and DivideMix significantly outperform the other three methods. On CIFAR-10, NGC achieves better or on par performance compared with DivideMix. On CIFAR-100, NGC obtains an average performance gain of ?4%. This demonstrates the superiority of NGC in classification.</p><p>Next, we present the OOD detection performance using AUROC in <ref type="table" target="#tab_2">Table 3</ref> following <ref type="bibr" target="#b9">[10]</ref> and open-set classification performance <ref type="bibr" target="#b1">[2]</ref> using F-measure in <ref type="table" target="#tab_3">Table 4</ref> as the metric. Since different OOD detectors need particularly tuned probability thresholds ?, for fair comparison, we search the best ? for all methods. Noted that LOND has not been studied before, we hence combine one of the best LNL methods DivideMix with leading OOD detectors including <ref type="table">Table 5</ref>: Test accuracy (%) under controlled IND label noise compared with state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets. We run our method three times with different random seeds and report the mean and the standard deviation. Results for baseline methods are copied from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>   <ref type="bibr" target="#b26">[27]</ref> and Mahalanobis distance (MD) <ref type="bibr" target="#b18">[19]</ref> for comparisons. We also compare with recent OOD detection methods, Rot <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> and SSD <ref type="bibr" target="#b37">[38]</ref>, which cannot be simply combined with DivideMix and need separate training. From the results, it can be seen that most comparison methods perform significantly worse than NGC. In terms of AUROC, NGC obtains performance gains over 17.2% on CIFAR-10 and 1.27% on CIFAR-100. Regarding Fmeasure, NGC outperforms other methods by at least 14% on CIFAR-10 and 3.5% on CIFAR-100. In supplementary material, we conduct comprehensive comparisons with another recent method for LNL, i.e., ProtoMix <ref type="bibr" target="#b22">[23]</ref>, due to limited space. We also provide further analysis to show that our method is robust to the selection of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning with Closed-World Noisy Labels</head><p>In addition to the LOND task, we test NGC in the conventional closed-world noisy label setup. We conduct experiments under controlled IND noise using the CIFAR-10 and CIFAR-100 datasets. To validate the efficacy of NGC, we compare it with many existing methods, including Meta-Learning <ref type="bibr" target="#b21">[22]</ref>, P-correction <ref type="bibr" target="#b50">[51]</ref>, M-correction <ref type="bibr" target="#b0">[1]</ref>, Di-videMix <ref type="bibr" target="#b20">[21]</ref>, and ProtoMix <ref type="bibr" target="#b22">[23]</ref>. Following commonly used LNL setups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>, we run algorithms under asymmetric noise and symmetric noise with different noise levels. The noise level for symmetric noise ranges from 20% to 90% where it consists of randomly selecting labels for a percentage of the training data using all possible labels (i.e., the true label could be randomly retained). The noise level for asymmetric noise is set to 40%.</p><p>As <ref type="table">Table 5</ref> shown, in most cases, our method outperforms recent methods particularly designed for closedworld noisy label problems. This indicates the superiority and robustness of NGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning from Real-World Noisy Dataset</head><p>We test the performance of our method on real-world dataset WebVision <ref type="bibr" target="#b24">[25]</ref> which contains noisy-labeled images collected from Flickr and Google. Similar to previous work <ref type="bibr" target="#b20">[21]</ref>, we perform experiments on the first 50 classes. We report comparison results in <ref type="table" target="#tab_5">Table 6</ref>, measuring top-1 and top-5 accuracy on WebVision validation set and Im-ageNet ILSVRC12 validation set. NGC consistently outperforms competing methods in most cases, which verifies the efficacy of our method on real-world noisy label task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies and Discussion</head><p>To better understand NGC, we examine the impact of each component of NGC in <ref type="table" target="#tab_6">Table 7</ref>. It can be observed that all components contribute to the efficacy of NGC. In particular, the two strategies in subgraph selection and the  subgraph-level contrastive learning serve as the most important parts in our framework, without which the performance deteriorates severely. The observations validate that confidence-based (CS) and geometry-based selection (GS) can exploit neighborhood information from graph structure effectively. As a result, the test accuracy and OOD detection performance also improve as shown in <ref type="figure" target="#fig_7">Figure 5a</ref>, demonstrating the good generalization ability of our method. In supplementary material, we also demonstrate the robustness of our method to hyperparameters, i.e., ? in Eq. (3) and k which is used to construct the k-NN graph. Discussion on subgraph selection. To further examine the effect of the two subgraph selection strategies, we investigate the impact of each one for selecting clean samples. In <ref type="figure" target="#fig_5">Figure 4a</ref> and <ref type="figure" target="#fig_5">Figure 4b</ref>, we can see that the noise rate in the selected data by performing each strategy alone is significantly larger than the combined strategy. <ref type="figure" target="#fig_5">Figure 4c</ref> and <ref type="figure" target="#fig_5">Figure 4d</ref> further show that both IND and OOD noise can be drastically removed by the combined strategy, while merely using one of them has little effect. This is because the confidence-based selection can degrade the connectivity between clean samples and noisy samples such that samples in the largest connected component are clean. Moreover, we divide the nodes into three parts: clean data, IND noise and OOD noise, and analyze the average degrees of nodes in each part after performing confidence-based selection. As demonstrated in <ref type="figure" target="#fig_7">Figure 5b</ref>, we find that as the training process progresses, the average node degree of OOD noisy samples is decreasing, while the average degree of clean samples is increasing. This further validates that confidence-based strategy facilitates the selection of the largest connected component in geometry-based strategy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we study a realistic problem where the training dataset contains both IND and OOD noise, and the presence of OOD samples at test time. To address this problem, we introduce a noisy graph cleaning framework that simultaneously performs noise correction and clean data selection based on prediction confidence and geometric structure of data in latent feature space. NGC outperforms many existing methods on different datasets with varying degrees of noise. Our work may motivate researchers in two directions: learning from IND and OOD noisy data is worth further exploration due to its broad range of applications and OOD detection from weakly-labeled datasets is promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Experimental Details</head><p>In this section, we introduce the experiment details. We first introduce the out-of-distribution (OOD) datasets used in our experiments. Then, we present the experimental settings of our method. Finally, we provide details about the evaluation metrics used for evaluating the classification and OOD detection performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Out-of-Distribution Datasets</head><p>We use the OOD datasets below in our experiments:</p><p>? TinyImageNet. The Tiny ImageNet dataset contains 50,000 training images from 200 different classes, which are drawn from the original 1,000 classes of Im-ageNet. We randomly choose samples from training set and resize each image to 32 ? 32.</p><p>? Places-365. The Places-365 dataset has 365 scene categories and there are 900 images per category in the test set. The OOD samples are randomly chosen from test set of Places-365 and resize to 32 ? 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experimental Setup</head><p>For all CIFAR experiments, we train PreAct ResNet-18 network for 300 epochs using SGD with the momentum 0.9 and weight decay 5 ? 10 ?4 . The initial learning rate is set to 0.15 and cosine decay schedule is used. The batch size is set to 512. The dimension of projector layer is set to 64. The temperature parameter is fixed as ? 1 = 0.3 and ? 2 = 1.0. For CIFAR-10 experiments, we use k = 30 for sym. noise and k = 10 for asym. noise, warmup with cross-entropy loss without other components for 5 epoch. For all CIFAR-100 experiments, we use k = 200, warmup for 30 epoch for CIFAR-100 datasets. For parameter ?, in LOND task, we use 0.8 for all experiments, and in closed-world noisy label task, we set it to 0.7 for CIFAR-10 and 0.6 for CIFAR-100.</p><p>For Webvision-50 dataset, most of hyperparameters are the same with CIFAR experiments except we set k = 100, ? = 0.8. We train the inception-resnet v2 model using SGD following prior works. The initial learning rate is set to 0.2 and the batch size is 256. We train the network for 80 epochs and the warmup stage lasts 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Evaluation Metrics</head><p>We use the following three performance metrics to evaluate the performance.</p><p>? Classification Accuracy. The top-1 classification accuracy is calculated as the mean accuracy over all known (IND) classes. Predictions of data are obtained as the classes with the highest softmax probabilities.</p><p>? AUROC. AUROC is the Area Under the Receiver Operating Characteristic curve and can be calculated by the area under the TPR against FPR curve.</p><p>? F-measure. The F-measure (F) is calculated as 2 times the product of precision (p) and recall (r) divided by the sum of p and r:</p><formula xml:id="formula_13">F = 2 ? p ? r p + r .<label>(10)</label></formula><p>p is calculated as true positive over the sum of T p and false positive:</p><formula xml:id="formula_14">p = T p T p + F p .<label>(11)</label></formula><p>r is calculated as T p over the sum of T p and false negative:</p><formula xml:id="formula_15">r = T p T p + F n .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Additional Experimental Results</head><p>In this section, we first show the visualization results of feature representation and subgraph selection, which demonstrate the validity of our methods. Then we present the effectiveness of graph-based noise correction. We also analyze the sensitivity of hyperparameters. In addition, the performance of model ensemble and the impact of AugMix on WebVision-50 is provided. Finally, we compare NGC with recent related work, ProtoMix [23] on LOND task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Visualization Results</head><p>Visualization of learned representation. We visualize the learned feature representations of our method and Di-videMix via t-SNE in <ref type="figure">Figure 7</ref>. CIFAR-10 with 50% sym. noise is used as IND dataset and 20k OOD samples are added in each experiment. We use CIFAR-100, TinyIm-ageNet, and Places-365 as OOD datasets for each experiment, respectively. The points in brown represent OOD samples, while samples with other colors are from CIFAR-10. <ref type="figure">Figures 7a to 7c</ref> show the learned representations of DivideMix, which are extracted from the last layer of the model. For comparison, <ref type="figure">Figures 7d to 7f</ref> visualize the output of the projector Proj in our method. It can be observed that our method can learn more meaningful representations and separate OOD samples from IND samples effectively.</p><p>Visualization of subgraph selection. To further justify the efficacy of the proposed subgraph selection, we visualize the k-NN graph obtained at different training iterations in <ref type="figure" target="#fig_11">Figure 8</ref>. CIFAR-10 with 50% sym. noise is used as IND dataset and 20k CIFAR-100 data are added as OOD samples. We draw all the samples with pseudo-label 1. In these graphs, we use green points to represent samples removed by confidence-based selection while black points are samples removed by geometry-based selection. Points in yellow represent clean data selected by our method. The edges included in the largest connected component are in  red. At different training iterations, we visualize the constructed k-NN graph (top row) and the refined graph (bottom row) by performing our confidence-based selection.</p><p>As the training progresses, the feature representations of IND and OOD samples are gradually separated. Moreover, it can be seen that confidence-based selection signifi-  cantly degrades the connectivity between clean samples and OOD samples, which can be further beneficial to geometrybased selection. As a consequence, samples retained by geometry-based selection distribute more and more compact in feature space. This observation justifies the validity of subgraph selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Effectiveness of Noise Correction</head><p>We demonstrate the effectiveness of graph-based noise correction on CIFAR-10 and CIFAR-100 datasets with 50% symmetric noise. As shown in <ref type="figure">Figure 6a</ref>, As the training progresses, the noise rate continues decreasing. Our method reduces noise rate from 50% to 4.24% for CIFAR-10 and 14.67% for CIFAR-100. This validates our noise correction methods can correct noisy labels effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Hyperparameter Sensitivity Analysis</head><p>Analysis of ? and k. We investigate the impact of ? for confidence-based selection and k which is used to construct the k-NN graph. The results are shown in <ref type="figure">Figure 6b</ref> and 6c. We vary ? from 0.6 to 0.9, and the test accuracy increases from 70% to 72%, showing that a small confidence threshold results in more label noise being included. AUROC increases from 72.08 to 92.23, this is because a higher threshold ? can filter out more OOD noisy samples, which can be further beneficial for representation learning and the calculation of prototypes. As for the parameter k, we choose its value from {50, 100, 150, 200}. It can be seen that NGC achieves similar performance with different values except k = 50. The reason is that when k is too small, the k-NN graph is very sparse, resulting in fewer data points being obtained from the largest connected component, hence only a few clean samples are selected for training.</p><p>Analysis of ?. We report F-measure under best threshold ? in <ref type="table" target="#tab_7">Table 8</ref>. Even with fixed ? from 0.5 to 0.7, our method is robust enough and outperforms other methods with their best values of ? in most cases. Here we report results for ? = 0.5 and ? = 0.7. We also report the standard deviation of best ?, which shows the stability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Performance of Model Ensemble</head><p>Since model ensemble has shown to be useful when dealing with noisy data, we ensemble the outputs of two networks during testing phase and report the results in <ref type="table" target="#tab_8">Table 9</ref>. The complete DivideMix (DM) is used for comparison. Results show that our method outperforms DivideMix in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Impact of AugMix on WebVision-50</head><p>To better reveal the superiority of our method, we conduct ablation studies for AugMix on WebVision-50 dataset. The results are reported in <ref type="table" target="#tab_0">Table 10</ref>. First, it can be seen that AugMix does help enhance the performance. Second, without applying AugMix, our method consistently outperforms strong baselines, i.e., ELR and DivideMix. The results further demonstrate the effectiveness of our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Comparison with ProtoMix</head><p>As one of the most recent related works, ProtoMix [23] employs unsupervised contrastive loss and mixup prototypical contrastive loss to learn robust representations, which can address different types of noisy data. We report the comparison results of NGC and ProtoMix on LOND task in <ref type="table" target="#tab_0">Table 11</ref>. For all experiments, we inject 50% symmetric IND noise. 20k and 10k OOD samples are randomly selected and added into training set and test set, respectively. Although ProtoMix is not designed to detect OOD examples at test time, it is natural to achieve this by measuring the similarity between test examples and class prototypes, as shown in Eq. (9) in the main text. From the results, we can observe that NGC achieves better or comparable results in test accuracy. Regarding AUROC and F-measure, NGC consistently outperforms ProtoMix in all cases. Recall that, ProtoMix identifies IND and OOD noise according to predictive confidence, which means samples with high predictive confidence are determined as clean. As a result, many noisy samples are likely to be misidentified as DNNs gradually fit the training data. NGC overcomes this problem by exploiting the geometric structure of data. For each class, confident samples that clustered together are further selected by calculating the largest connected component. Our belief is that clean samples of the same class should distribute closely to each other, while noisy samples are pushed away. By first performing confidence-based selection, it breaks the connection between noisy and clean samples in the graph, which facilitates our geometry-based selection. Consequently, NGC excludes more noisy samples from training and achieves better performance. Appendix C. Pseudo-code of Our Proposed Method Algorithm 1 lists the pseudo-code of NGC. For a better understanding of the proposed method, we illustrate the whole process in <ref type="figure" target="#fig_12">Figure 9</ref>.</p><p>Algorithm 1 Noisy Graph Cleaning Procedure (one epoch) <ref type="bibr">1:</ref> Input: training dataset {(x i , y i ) N i=1 }, k-NN parameter k, confidence threshold ?. 2: Construct the k-NN graph G on training samples. <ref type="bibr">3:</ref> Refine soft pseudo-label? i for each sample x i by performing graph-based noise correction on G. 4: If max k?ik &lt; ? and? iyi ? 1 K , remove the point x i and its adjacent edges from the graph. 5: The resulting graph is denoted byG. <ref type="bibr">6:</ref> Initialize the set of clean data S = ?. 7: for k = 1 ? ? ? K do <ref type="bibr">8:</ref> Remove points that do not belong to class k from graphG, i.e.,? i = k, ?i ? [N ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>The resulting graph is denoted byG(k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Determine the connected components ofG(k) by disjoint-set data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Remove small connected components of the graphG(k), that is, only the largest connected component is retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>The resulting graph is denoted byG(k) lcc . Points inG(k) lcc are treated as clean samples. <ref type="bibr">13:</ref> Update clean data set S = S ?G(k) lcc . 14: end for 15: Calculate cross-entropy loss and subgraph-level contrastive loss on S. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A demonstration of the LOND setup. We use green boxes to represent clean samples while yellow and red boxes are IND and OOD noisy samples, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>DivideMixFigure 2 :</head><label>2</label><figDesc>arXiv:2108.11035v1 [cs.LG] 25 Aug 2021 CIFAR-10 (w/ 50% sym. noise); OOD dataset: CIFAR-100 (20k in training set and 10k in test set) Performance on testing dataset with extra OOD samples. MSP, ODIN, MD are combined with DivideMix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of graph-based noise correction and subgraph selection in binary classification case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>IND dataset OOD dataset DM MSP ODIN MD Ours CIFAR-10 CIFAR-100 0.632 0.698 0.681 0.635 0.838 TinyImageNet 0.638 0.726 0.707 0.702 0.875 Places-365 0.637 0.717 0.705 0.651 0.887 CIFAR-100 TinyImageNet 0.516 0.687 0.705 0.526 0.773 Places-365 0.519 0.685 0.696 0.541 0.731</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of subgraph selection under 50% IND noise (CIFAR-100) and 20k OOD noise (Places-365).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Visualization for convergence of our method and node degrees under 50% IND noise (CIFAR-100) and 20k OOD (Places-365).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Experimental results. (a) Effectiveness of noise correction. Both CIFAR-10 and CIFAR-100 datasets are under 50% sym. noise. (b-c) Analysis of the impact of hyperparameters under 50% IND noise (CIFAR-100), 20k and 10k OOD noise (Places-365) in training set and test set, respectively. ? is for confidence-based selection and k is for k-NN graph. (a) CIFAR-100 as OOD (DivideMix) (b) TinyImageNet as OOD (DivideMix) (c) Places-365 as OOD (DivideMix) (d) CIFAR-100 as OOD (Ours) (e) TinyImageNet as OOD (Ours) (f) Places-365 as OOD (Ours) t-SNE visualization of learned feature representation. CIFAR-10 with 50% sym. noise is used as IND dataset and 20k OOD samples are added for all experiments. The OOD samples are represented by brown points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>t-SNE visualization of the proposed subgraph selection at different training iterations. CIFAR-10 with 50% sym. noise is used as IND dataset and 20k CIFAR-100 data are added as OOD samples. We draw all samples with pseudo-label 1. Green points represent samples removed by confidence-based selection and black points are samples removed by geometrybased selection. Points in yellow represent clean data selected by our method. Edges in the largest connected component are colored red. We visualize the constructed k-NN graph (top row) and the refined graph (bottom row) by performing our confidence-based selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " Y e C p D p T L 3 q P X X 7 0 p b m N w q y F 8 Y R 0 = " &gt; A A A C 4 H i c j V G 7 S s R A F D 0 b 3 + 9 V y 2 2 C i 2 C 1 J C J q u W h j Y b G C q w u u y m Q c d T A v J h N B w h Z 2 d m L r D 9 j q 1 4 h / o H / h n T E L P h C d k O T c c + 8 5 M 3 d u k I Y y 0 5 7 3 U n E G B o e G R 0 b H x i c m p 6 Z n q r N z e 1 m S K y 7 a P A k T 1 Q l Y J k I Z i 7 a W O h S d V A k W B a H Y D y 4 2 T X 7 / U q h M J v G u v k r F Y c T O Y n k q O d N E H V d r 3 Y j p c 8 7 C Y r t 3 V N h A R Y W M M 9 3 r H V f r X s O z y / 0 J / B L U U a 5 W U n 1 G F y d I w J E j g k A M T T g E Q 0 b P A X x 4 S I k 7 R E G c I i R t X q C H c d L m V C W o g h F 7 Q d 8 z i g 5 K N q b Y e G Z W z W m X k F 5 F S h e L p E m o T h E 2 u 7 k 2 n 1 t n w / 7 m X V h P c 7 Y r + g e l V 0 S s x j m x f + n 6 l f / V m V 4 0 T r F u e 5 D U U 2 o Z 0 x 0 v X X J 7 K + b k 7 q e u N D m k x B l 8 Q n l F m F t l / 5 5 d q 8 l s 7 + Z u m c 2 / 2 k r D m p i X t T n e z C l p w P 7 3 c f 4 E e 8 s N f 7 X h 7 6 z U m x v l q E d R w w K W a J 5 r a G I L L b T J + x o P e M S T E z g 3 z q 1 z 9 1 H q V E r N P L 4 s 5 / 4 d a 9 W b q w = = &lt; / l a t e x i t &gt; L inst &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I 8 D W g B Q + P 4 I v e u 5 H e r 2 G m 2 R m e i w = " &gt; A A A C 5 H i c j V H L S s N A F D 2 N 7 3 f V p Q u D R X B V E h F 1 W X T j w k U F a w v 1 w W S c t s G 8 m E w E C V 2 6 c y d u / Q G 3 + i 3 i H + h f e G e M o B b R C U n O P f e e M 3 P n e k n g p 8 p x X k r W 0 P D I 6 N j 4 x O T U 9 M z s X H l + 4 S i N M 8 l F g 8 d B L F s e S 0 X g R 6 K h f B W I V i I F C 7 1 A N L 2 L X Z 1 v X g q Z + n F 0 q K 4 S c R K y b u R 3 f M 4 U U W f l 5 e O Q q R 5 n Q b 7 f P 8 1 N I M M 8 z b y u Z E m v 3 z 8 r V 5 y q Y 5 Y 9 C N w C V F C s e l x + x j H O E Y M j Q w i B C I p w A I a U n j Z c O E i I O 0 F O n C T k m 7 x A H 5 O k z a h K U A U j 9 o K + X Y r a B R t R r D 1 T o + a 0 S 0 C v J K W N V d L E V C c J 6 9 1 s k 8 + M s 2 Z / 8 8 6 N p z 7 b F f 2 9 w i s k V q F H 7 F + 6 z 8 r / 6 n Q v C h 1 s m x 5 8 6 i k x j O 6 O F y 6 Z u R V 9 c v t L V 4 o c E u I 0 P q e 8 J M y N 8 v O e b a N J T e / 6 b p n J v 5 p K z e q Y F 7 U Z 3 v Q p a c D u z 3 E O g q P 1 q r t Z d Q 8 2 K r W d Y t T j W M I K 1 m i e W 6 h h D 3 U 0 y P s a D 3 j E k 9 W x b q x b 6 + 6 j 1 C o V m k V 8 W 9 b 9 O w s n n X E = &lt; / l a t e x i t &gt; L subgraph An illustration of proposed framework in binary classification case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Three types of tasks considered in our experiments. Setup IND noise in Dtrain OOD in Dtrain OOD in Dtest</figDesc><table><row><cell>LOND</cell></row><row><cell>LCNL</cell></row><row><cell>LRND</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) under mixed IND and OOD noise compared with state-of-the-art LNL methods. 50% sym. IND noise is injected into dataset. We run methods three times with different seeds and report the mean and the standard deviation.</figDesc><table><row><cell>IND dataset</cell><cell>OOD dataset</cell><cell># OOD</cell><cell>CE</cell><cell>RoG [20]</cell><cell>ILON [41]</cell><cell>DivideMix [21]</cell><cell>Ours</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>10k 20k</cell><cell>53.36?0.92 50.73?0.80</cell><cell>63.01?0.46 62.56?1.76</cell><cell>75.17?1.50 74.85?1.61</cell><cell>92.73?0.27 92.26?0.13</cell><cell>93.69 ?0.09 92.31 ?0.29</cell></row><row><cell>CIFAR-10</cell><cell>TinyImageNet</cell><cell>10k 20k</cell><cell>51.85?1.09 52.32?1.41</cell><cell>61.69?1.18 63.15?1.13</cell><cell>75.93?1.13 74.63?0.74</cell><cell>94.08 ?0.18 93.83 ?0.08</cell><cell>93.73?0.36 93.54?0.21</cell></row><row><cell></cell><cell>Places-365</cell><cell>10k 20k</cell><cell>54.06?0.53 55.30?1.31</cell><cell>64.21?0.27 63.52?1.73</cell><cell>76.17?0.90 76.36?1.26</cell><cell>93.81?0.33 93.59?0.07</cell><cell>94.18 ?0.09 93.67 ?0.22</cell></row><row><cell>CIFAR-100</cell><cell>TinyImageNet</cell><cell>10k 20k</cell><cell>37.01?0.40 34.55 ?0.55</cell><cell>52.65?0.30 50.40 ?0.44</cell><cell>51.43?0.29 50.14?0.66</cell><cell>70.38?0.09 69.89?0.25</cell><cell>74.57 ?0.23 73.49 ?0.11</cell></row><row><cell></cell><cell>Places-365</cell><cell>10k 20k</cell><cell>37.53?0.54 34.54?0.18</cell><cell>52.43?0.03 50.32?0.29</cell><cell>50.74?0.65 49.87?0.46</cell><cell>70.01?0.11 69.84?0.15</cell><cell>74.89 ?0.21 73.44 ?0.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>IND dataset</cell><cell>OOD dataset</cell><cell>MSP[10] +</cell><cell>ODIN[27]</cell></row></table><note>AUROC (%) comparison with state-of-the-art OOD detectors. 50% sym. IND noise is injected into training dataset. 20k and 10k OOD samples are added into training set and test set, respectively.+ indicates supervised detection methods.+ MD[19] + Rot[6] Rot[11] + SSD[38] SSD[38] +</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>F-measure comparison with DivideMix (DM) combined with OOD detection methods. 50% sym. IND noise is injected into training set, 20k and 10k OOD sam- ples are added into training set and test set, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Ours 95.88?0.13 94.54?0.35 91.59?0.31 80.46?1.97 90.55?0.29 79.31?0.35 75.91?0.39 62.70?0.37 29.76?0.85</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noise type</cell><cell></cell><cell>Sym.</cell><cell></cell><cell></cell><cell>Asym.</cell><cell></cell><cell>Sym.</cell><cell></cell><cell></cell></row><row><cell>Noise level</cell><cell>20%</cell><cell>50%</cell><cell>80%</cell><cell>90%</cell><cell>40%</cell><cell>20%</cell><cell>50%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>Cross-Entropy</cell><cell>82.7</cell><cell>57.9</cell><cell>26.1</cell><cell>16.8</cell><cell>85.0</cell><cell>61.8</cell><cell>37.3</cell><cell>8.8</cell><cell>3.5</cell></row><row><cell>F-correction [34]</cell><cell>83.1</cell><cell>59.4</cell><cell>26.2</cell><cell>18.8</cell><cell>87.2</cell><cell>61.4</cell><cell>37.3</cell><cell>9.0</cell><cell>3.4</cell></row><row><cell>Co-teaching+ [53]</cell><cell>88.2</cell><cell>84.1</cell><cell>45.5</cell><cell>30.1</cell><cell>-</cell><cell>64.1</cell><cell>45.3</cell><cell>15.5</cell><cell>8.8</cell></row><row><cell>Mixup [55]</cell><cell>92.3</cell><cell>77.6</cell><cell>46.7</cell><cell>43.9</cell><cell>-</cell><cell>66.0</cell><cell>46.6</cell><cell>17.6</cell><cell>8.1</cell></row><row><cell>P-correction [51]</cell><cell>92.0</cell><cell>88.7</cell><cell>76.5</cell><cell>58.2</cell><cell>88.5</cell><cell>68.1</cell><cell>56.4</cell><cell>20.7</cell><cell>8.8</cell></row><row><cell>Meta-Learning [22]</cell><cell>92.0</cell><cell>88.8</cell><cell>76.1</cell><cell>58.3</cell><cell>89.2</cell><cell>67.7</cell><cell>58.0</cell><cell>40.1</cell><cell>14.3</cell></row><row><cell>M-correction [1]</cell><cell>93.8</cell><cell>91.9</cell><cell>86.6</cell><cell>68.7</cell><cell>87.4</cell><cell>73.4</cell><cell>65.4</cell><cell>47.6</cell><cell>20.5</cell></row><row><cell>DivideMix [21]</cell><cell>95.0</cell><cell>93.7</cell><cell>92.4</cell><cell>74.2</cell><cell>91.4</cell><cell>74.8</cell><cell>72.1</cell><cell>57.6</cell><cell>29.2</cell></row><row><cell>ProtoMix [23]</cell><cell>95.8</cell><cell>94.3</cell><cell>92.4</cell><cell>75.0</cell><cell>91.9</cell><cell>79.1</cell><cell>74.8</cell><cell>57.7</cell><cell>29.3</cell></row></table><note>MSP [10], ODIN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Accuracy (%) on WebVision-50 and ILSVRC2012 validation sets. Results of baselines are from<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">WebVision</cell><cell cols="2">ILSVRC12</cell></row><row><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>F-correction [34]</cell><cell>61.12</cell><cell>82.68</cell><cell>57.36</cell><cell>82.36</cell></row><row><cell>Decoupling [31]</cell><cell>62.54</cell><cell>84.74</cell><cell>58.26</cell><cell>82.26</cell></row><row><cell>D2L [30]</cell><cell>62.68</cell><cell>84.00</cell><cell>57.80</cell><cell>81.36</cell></row><row><cell>MentorNet [14]</cell><cell>63.00</cell><cell>81.40</cell><cell>57.80</cell><cell>79.92</cell></row><row><cell>Co-teaching [8]</cell><cell>63.58</cell><cell>85.20</cell><cell>61.48</cell><cell>84.70</cell></row><row><cell>Iterative-CV [3]</cell><cell>65.24</cell><cell>85.34</cell><cell>61.60</cell><cell>84.98</cell></row><row><cell>DivideMix [21]</cell><cell>77.32</cell><cell>91.64</cell><cell>75.20</cell><cell>90.84</cell></row><row><cell>ELR+ [28]</cell><cell>77.78</cell><cell>91.68</cell><cell>70.29</cell><cell>89.76</cell></row><row><cell>Ours</cell><cell>79.16</cell><cell>91.84</cell><cell>74.44</cell><cell>91.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study. GNC denotes graph-based noise correction. CS denotes confidence-based selection and GS denotes graph-based selection. For experiments whose noise type is OOD, Places-365 is used as OOD dataset and 50% sym. IND noise is injected into training set. L inst 92.45 94.02 82.67 71.38 73.30 51.59 w/o L subgraph 70.39 85.12 79.17 55.12 58.06 41.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell>Noise type</cell><cell cols="4">OOD Sym. Asym. OOD</cell><cell>Sym.</cell></row><row><cell>Noise level</cell><cell>20k</cell><cell>50%</cell><cell>40%</cell><cell>20k</cell><cell>50% 80%</cell></row><row><cell>w/o GNC</cell><cell cols="5">92.13 94.32 85.85 72.85 74.20 55.56</cell></row><row><cell>w/o CS</cell><cell cols="5">87.20 92.44 89.68 63.78 73.22 37.82</cell></row><row><cell>w/o GS</cell><cell cols="5">86.55 85.59 81.17 65.34 67.18 35.16</cell></row><row><cell cols="6">w/o 42</cell></row><row><cell>w/o mixup</cell><cell cols="5">89.51 90.73 84.24 66.93 68.06 42.59</cell></row><row><cell>w/o AugMix</cell><cell cols="5">93.62 94.53 89.39 71.49 75.18 61.75</cell></row><row><cell>Ours</cell><cell cols="5">93.67 94.54 90.55 73.44 75.91 62.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>F-measure (threshold ?). IND dataset is with 50% symmetric noise, 20k and 10k OOD samples are added into training set and test set, respectively. Bold: best; Underlined: 2nd &amp; 3rd.</figDesc><table><row><cell cols="2">IND OOD</cell><cell>MSP</cell><cell>ODIN</cell><cell>MD</cell><cell cols="2">Ours Ours ?=0.50 Ours ?=0.70</cell></row><row><cell></cell><cell cols="6">C-100 0.698(0.81) 0.681(0.83) 0.635(0.36) 0.838 (0.55) 0.835(0.50) 0.788(0.70)</cell></row><row><cell>C-10</cell><cell cols="6">TIN 0.726(0.83) 0.707(0.85) 0.702(0.33) 0.875 (0.54) 0.873(0.50) 0.802(0.70)</cell></row><row><cell></cell><cell cols="6">P-365 0.717(0.81) 0.705(0.14) 0.651(0.40) 0.887 (0.56) 0.882(0.50) 0.827(0.70)</cell></row><row><cell>C-100</cell><cell cols="6">TIN 0.687(0.41) 0.705(0.02) 0.526(0.38) 0.773 (0.67) 0.743(0.50) 0.770(0.70)</cell></row><row><cell></cell><cell cols="6">P-365 0.685(0.37) 0.696(0.01) 0.541(0.39) 0.731 (0.70) 0.687(0.50) 0.731(0.70)</cell></row><row><cell cols="3">? (stand. dev.) 0.21</cell><cell>0.39</cell><cell>0.02</cell><cell>0.07</cell><cell>0.00</cell><cell>0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy (%) using model ensemble. + indicates ensemble models. 92.4 74.2 91.4 74.8 72.1 57.6 29.2 Ours 95.88 94.54 91.59 80.46 90.55 78.98 75.91 62.70 29.76 DM + 95.7 94.4 92.9 75.4 92.1 76.9 74.2 59.6 31.0 Ours + 96.27 95.09 92.20 83.75 91.70 81.08 77.16 64.00 34.18</figDesc><table><row><cell>Data</cell><cell>CIFAR-10</cell><cell></cell><cell>CIFAR-100</cell></row><row><cell>Type</cell><cell>Sym.</cell><cell>Asym.</cell><cell>Sym.</cell></row><row><cell cols="4">Ratio 20% 50% 80% 90% 40% 20% 50% 80% 90%</cell></row><row><cell>DM</cell><cell>95.0 93.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Ablation study for AugMix on WebVision-50. + indicates ensemble models.</figDesc><table><row><cell>Method</cell><cell cols="2">WebVision</cell><cell cols="2">ILSVRC12</cell></row><row><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>Ours (w/ AugMix)</cell><cell>79.16</cell><cell>91.84</cell><cell>74.44</cell><cell>91.04</cell></row><row><cell>ELR</cell><cell>76.26</cell><cell>91.26</cell><cell>68.71</cell><cell>87.84</cell></row><row><cell>Ours (w/o AugMix)</cell><cell>77.56</cell><cell>91.36</cell><cell>72.92</cell><cell>91.32</cell></row><row><cell>DivideMix +</cell><cell>77.32</cell><cell>91.64</cell><cell>75.20</cell><cell>90.84</cell></row><row><cell>ELR +</cell><cell>77.78</cell><cell>91.68</cell><cell>70.29</cell><cell>89.76</cell></row><row><cell>Ours + (w/o AugMix)</cell><cell>79.08</cell><cell>91.80</cell><cell>75.12</cell><cell>91.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Performance comparison of ProtoMix and NGC (Ours) on LOND task. 50% symmetric IND noise is injected into training set, 20k and 10k OOD samples are added into training set and test set, respectively. 92.51 / 92.31 84.64 / 90.37 0.783 / 0.838 TIN 93.12 / 93.54 93.47 / 94.18 0.862 / 0.875 P-365 92.76 / 93.67 94.14 / 94.31 0.868 / 0.887 C-100 TIN 72.80 / 73.49 78.58 / 94.24 0.653 / 0.773 P-365 72.05 / 73.44 75.19 / 91.20 0.624 / 0.731</figDesc><table><row><cell>IND OOD</cell><cell>Accuracy</cell><cell>AUROC</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>ProtoMix / NGC</cell><cell></cell></row><row><cell>C-100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>C-10</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective classification for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4878" to="4887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9781" to="9791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep selflearning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15637" to="15648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NLNL: negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting outof-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3763" to="3772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning from noisy data with robust representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mopro: Webly supervised learning with momentum prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lightweight label propagation for large-scale network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Ming</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2071" to="2082" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="20331" to="20342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SELF: learning to filter noisy labels with self-ensembling</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Augmentation strategies for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>H?llerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8022" to="8031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17044" to="17056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evidentialmix: Learning with combined open-set and closed-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3607" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SSD: A unified framework for self-supervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Co-mining: Deep face recognition with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9357" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning safe multi-label prediction for weakly labeled data. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan-Zhe</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="703" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Does tail label help for large-scale multi-label learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction Neural Networks Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2315" to="2324" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A topological filter for learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21382" to="21393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Part-dependent label noise: Towards instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7597" to="7610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Webly supervised image classification with self-contained confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="779" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dual T: reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7260" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised out-ofdistribution detection by maximum classifier discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9517" to="9525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9294" to="9303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Thomas Navin Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Semisupervised learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
