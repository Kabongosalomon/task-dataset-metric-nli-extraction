<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced 3D Human Pose Estimation from Videos by using Attention-Based Neural Network with Dilated Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">?</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ? Sen-Ching</forename><surname>Cheung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
						</author>
						<title level="a" type="main">Enhanced 3D Human Pose Estimation from Videos by using Attention-Based Neural Network with Dilated Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D Human Pose ? Motion Reconstruction ? Monocular Capture ? Performance-driven Retargeting ? Attention ? Multi-scale Dilation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The attention mechanism provides a sequential prediction framework for learning spatial models with enhanced implicit temporal consistency. In this work, we show a systematic design (from 2D to 3D) for how conventional networks and other forms of constraints can be incorporated into the attention framework for learning long-range dependencies for the task of pose estimation. The contribution of this paper is to provide a systematic approach for designing and training of attention-based models for the end-to-end pose estimation, with the flexibility and scalability of arbitrary video sequences as input. We achieve this by adapting temporal receptive field via a multi-scale structure of dilated convolutions. Besides, the proposed architecture can be easily adapted to a causal model enabling real-time performance. Any off-the-shelf 2D pose estimation systems, e.g. Mocap libraries, can be easily integrated in an ad-hoc fashion. Our method achieves the state-of-the-art performance and outperforms existing methods by reducing the mean per joint position error to 33.4 mm 1 on Human3.6M dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We introduced attention mechanism for the task of articulated 3D pose reconstruction from videos in the recent work <ref type="bibr">(Liu et al., 2020b)</ref>, which exploits the temporal contexts of long-range dependencies across frames. The ability to adaptively identify important frames or tensors output from each deep net layer and combine them with the advantages afforded by convolutional architectures allows for globally op-timal inference through simultaneous processing . The concept of "attention" is to learn optimized global alignment between pairwise data and has gained recent success in the integration with deep networks for processing mono/multimodal data, such as text-to-speech matching <ref type="bibr" target="#b10">(Chorowski et al., 2015)</ref>, neural machine translation <ref type="bibr" target="#b2">(Bahdanau et al., 2016)</ref> and 2D human pose estimation <ref type="bibr" target="#b11">(Chu et al., 2017)</ref>. In this paper, we extend our original attention model further by integrating it with deep networks in both 2D and 3D domain, leading to improved estimation while preserving natural temporal coherence in videos.</p><p>Articulated 3D human pose estimation from unconstrained single images or videos is considered as an ill-posed problem due to the nonlinearity of human dynamics, occlusions, and the high-dimensional variability introduced in the wild. Traditional approaches such as multi-view capture <ref type="bibr" target="#b0">(Amin et al., 2013)</ref>, marker based systems <ref type="bibr" target="#b27">(Mandery et al., 2015)</ref> and multi-modal sensing <ref type="bibr" target="#b32">(Palmero et al., 2016)</ref> require a laborious setup process and are not practical for applications in the less controlled environment. Recent efforts of using deep architectures have significantly advanced the state-ofthe-art in 3D pose reasoning <ref type="bibr" target="#b42">(Toshev and Szegedy, 2014;</ref><ref type="bibr" target="#b29">Neverova et al., 2014)</ref>. The end-to-end learning process alleviates the need of using tailor-made features or spatial constraints, thereby minimizing the characteristic errors such as double-counting image evidence <ref type="bibr">(Ferrari et al., 2009)</ref>. While vast and powerful deep models on 3D pose prediction are emerging (from convolutional neural network (CNN) <ref type="bibr" target="#b34">(Pavlakos et al., 2017;</ref><ref type="bibr" target="#b41">Tekin et al., 2016;</ref><ref type="bibr" target="#b21">Li et al., 2015)</ref> to generative adversarial networks (GAN) <ref type="bibr" target="#b45">(Yang et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019)</ref>), many of these approaches focus on a single image inference, which is inclined to jittery motion or inexact body configuration. To resolve this, temporal information is taken into account for better motion consistency. Existing works can be generally classified into two categories: direct 3D estimation and 2D-to-3D estimation <ref type="bibr" target="#b54">(Zhou et al., 2016b;</ref><ref type="bibr" target="#b7">Chen et al., 2016)</ref>. The former explores the possibility of jointly extracting both 2D and 3D poses in a holistic arXiv:2103.03170v1 [cs.CV] 4 Mar 2021 manner <ref type="bibr" target="#b34">(Pavlakos et al., 2017;</ref><ref type="bibr" target="#b43">Varol et al., 2017)</ref>; while the latter decouples the estimation into two steps: 2D body part detection and 3D correspondence inference <ref type="bibr" target="#b5">(Chen and Ramanan, 2017;</ref><ref type="bibr" target="#b3">Bogo et al., 2016;</ref><ref type="bibr" target="#b54">Zhou et al., 2016b)</ref>. We refer readers to the recent survey for more details of their respective advantages <ref type="bibr" target="#b28">(Martinez et al., 2017)</ref>.</p><p>Our approach falls under the category of 2D-to-3D estimation with three key contributions:</p><p>1. Development of a systematic approach for designing and training of attention-based models for pose estimation in three levels: 2D joints attention, 3D-to-2D projection attention, and 3D pose attention. 2. Learning of implicit dependencies in large temporal receptive fields via a multi-scale structure of dilated convolutions. 3. Design of a systematic architecture for the integration of the attention-based model and dilation convolutional structure to enhance 3D pose inference to facilitate performance driven animation applications.</p><p>Experimental evaluations show that the resulting system can reach almost the same level of estimation accuracy under both causal or non-causal conditions, making it very attractive for real-time or consumer-level applications. To date, state-of-the-art results on video-based 2Dto-3D estimation can be achieved by a semi-supervised approach <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref> or a layer normalized LSTM approach <ref type="bibr" target="#b16">(Hossain et al., 2018)</ref>. Our model can further improve the performance in both quantitative accuracy and qualitative evaluation. The simple requirement of our framework makes it well suited for interactive applications like computer games, virtual communication, and avatar animation re-targeting from videos. Given a video with continuous body movements and 3D avatars as input, we transfer the captured pose and motion from the subject video to a target character. In <ref type="figure">Fig. 1</ref>, we show an example of how the solution can be employed in performance-based animations from videos. In this example, we create six 3D avatars with different shapes and appearances and take six different videos as input. There are not any constraints (e.g., camera intrinsic and extrinsic parameters, pose complexities, or background environment settings) about these input videos, which can be downloaded from any online sources, such as YouTube. By using the proposed technique, it enables automated body pose extraction from the video streams and applies motion re-targeting to the corresponding characters in the scene. The green arrows at the top of <ref type="figure">Fig. 1</ref> indicates associated video for each character. The subsequent frames demonstrate the result of automatic motion transferring from the video to the 3D characters. <ref type="figure">Fig. 1</ref>: An application that shows 3D avatars re-targeting from 2D video streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Articulated pose estimation from an unconstrained video has been studied for decades. Early work relies on graphical or restrictive models to account for the high degree of freedom and dependencies among body parts, such as tree structures <ref type="bibr" target="#b1">(Andriluka et al., 2009;</ref><ref type="bibr" target="#b46">Yang and Ramanan, 2011;</ref><ref type="bibr" target="#b0">Amin et al., 2013)</ref>, and pictorial structures <ref type="bibr" target="#b1">(Andriluka et al., 2009</ref>). These methods often introduce a large number of parameters that require careful and manual tuning using techniques such as piecewise approximation. The performance of graphical model based approaches have been surpassed by convolutional neural networks (CNNs) <ref type="bibr" target="#b38">(Sarafianos et al., 2016;</ref><ref type="bibr" target="#b34">Pavlakos et al., 2017)</ref>, which can learn an automated representation that disentangles the dependencies among output variables without a tailor-made solver.</p><p>For the last few years, various CNN based architectures have been proposed. For example, <ref type="bibr" target="#b41">(Tekin et al., 2016)</ref> trains an auto-encoder to project human joint positions to a high dimensional space to enforce structural constraints. <ref type="bibr">(Park Fig. 2</ref>: An example of a 4-layer architecture for attention-based temporal convolutional neural network (ATCN). In this architecture, all the kernel sizes are 3. In practice, different layers can have different kernel sizes. et al., 2016) estimates the 3D pose by propagating the 2D classification results to the 3D pose regressors inside a neural network <ref type="bibr" target="#b33">(Park et al., 2016)</ref>. A kinematic object model composing of bones and joints is introduced in <ref type="bibr" target="#b53">(Zhou et al., 2016a)</ref> to guarantee the geometric validity of the estimated human body. A comprehensive list of convolutional systems can be found in the survey presented in <ref type="bibr" target="#b38">(Sarafianos et al., 2016)</ref>.</p><p>Our contribution to this rich body of works lies in the introduction of an attention based mechanism to the body pose estimation problem. The traditional concept of "attention" is to provide an optimal matching strategy that globally aligns pairwise data from the same domain, e.g., word-to-word or phrase-to-phrase alignment in sentences <ref type="bibr" target="#b47">(Yao et al., 2013)</ref>, or across different modalities, e.g., text-to-speech <ref type="bibr" target="#b10">(Chorowski et al., 2015)</ref> and text-to-image <ref type="bibr" target="#b45">(Xu et al., 2015)</ref> in domain transformation. Prior work on attention in deep learning (DL) mostly addresses long short-term memory networks (LSTMs) <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997)</ref> and recently it has gained popularity in training neural networks <ref type="bibr" target="#b48">(Yin et al., 2016)</ref>. Recent research indicates that certain convolutional architectures can reach state-of-the-art accuracy in audio synthesis, word-level language modeling, and machine translation <ref type="bibr" target="#b18">Kalchbrenner et al., 2016;</ref><ref type="bibr">Dauphin et al., 2017)</ref>. Compared to the language modeling architecture of <ref type="bibr">(Dauphin et al., 2017)</ref>, temporal convolutional networks (TCNs) (Bai et al., 2018) do not use gating mechanisms and have much longer memory. Our 3D human pose estimation and reconstruction network integrates the attention units and multi-scale dilation units to the TCN architecture.</p><p>As mentioned earlier, there are recent works that take multiple frames with 2D detection as the input for 3D prediction such as the LSTM-based method <ref type="bibr" target="#b16">(Hossain et al., 2018)</ref> and a TCN based approach with semi-supervised training <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref>. For the LSTM-based system, the frames have to be processed sequentially based on time steps, while we propose to process all of the frames in parallel for 3D pose estimation. Another objective should be that any estimation failure of one frame would not affect the other frames. In our proposed work, we also employ some similarity to the TCN-based approach as in <ref type="bibr" target="#b35">(Pavllo et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020;</ref><ref type="bibr" target="#b23">Liu et al., 2020a)</ref> along with the usage of a voting mechanism to select important frames for prediction. In addition, we incorporate the following three distinct features in our proposed method:</p><p>(i) Instead of making a "hard" decision on a subset of frames, we use a "soft" decision by considering all the frames.</p><p>(ii) Along with the "soft" decisions to the input frames, we apply all the immediate outputs from every layer through the network, thereby expanding the scope of selection to cover both raw frames and generated features.</p><p>(iii) We use a multi-scale dilated convolution that enables us to have a broad range of frame selection without increasing the number of neural net layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Attention-based Approach</head><p>In this section, we present an overview of the proposed system for 3D pose estimation from a 2D video stream and show how our attention model guides the network to adap-tively identify significant portion of each deep neural net layer's output resulting in an enhanced estimation. <ref type="figure">Fig. 2</ref> (right) depicts the overall architecture of our attentionbased neural network. It takes a sequence of n frames with 2D joint positions as the input and outputs the estimated 3D pose for the target frame as labeled. The framework involves two types of processing modules: the Temporal Attention module (indicated by the long green bars) and the Kernel Attention module (indicated by the gray squares). The kernel attention module can be further categorized as TCN Units (in dark grey color) and Feature Aggregation (in light grey color) <ref type="bibr" target="#b13">(He et al., 2016)</ref>. By viewing the graphical model vertically from the top, one can notice the two attention modules distribute in an interlacing pattern that a row of kernel attention modules situate right below a temporal attention module. We regard these two adjacent modules as one layer, which has the same notion as a neural net layer. According to the functionalities, the layers can be grouped as top layer, middle layers, and bottom layer. Note that the top layer only has TCN units for the kernel module, while the bottom layer only has a feature aggregation to deliver the result. It is also worth mentioning that the number of middle layers can be varied depending on the receptive field setting, which will be discussed in section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Attention</head><p>The goal of the temporal attention module is to provide a contribution metric for the output tensors. Each attention module produces a set of scalars, {? (l) 0 , ? (l) 1 , . . . }, weighing the significance of different tensors within a layer:</p><formula xml:id="formula_0">W (l) ? T (l) ? = ? (l) 0 ? T (l) 0 , . . . , ? (l) ? l ?1 ? T (l) ? l ?1<label>(1)</label></formula><p>where l and ? l indicate the layer index and the number of tensors output from the l (th) layer. We use T (l) u to denote the u th tensor output from the l th layer. The bold format of W?T is a compacted vector. Note for the top layer, the input to the TCN units is just the 2D joints. The choice for computing their attention scores can be flexible. A commonly used scheme is the multilayer perceptron strategy for optimal feature set selection <ref type="bibr" target="#b37">(Ruck et al., 1990)</ref>. Empirically, we achieve desirable result by simply computing the normalized cross-correlation (ncc) that measures the positive cosine similarity between P i and P t on their 2D joint positions <ref type="bibr" target="#b49">(Yoo and Han, 2009)</ref>:</p><formula xml:id="formula_1">W (0) = [ncc(P 0 , P t ), . . . , ncc(P n?1 , P t )] T<label>(2)</label></formula><p>where P 0 , . . . , P n?1 are the 2D joint positions. t indicates the target frame index. The output W (0) is forwarded to the attention matrix ? t (l) to produce tensor weights for the subsequent layers.</p><formula xml:id="formula_2">W (l) = sig ? t (l)T W (l?1) , for l ? [1, L ? 2]<label>(3)</label></formula><p>where sig(?) is the sigmoid activation function. We require the dimension of ? t (l) ? R F ?F matching the number of output tensors between layers l ? 1 and l, s.t. F = ? l?1 and F = ? l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Kernel Attention</head><p>Similar to the temporal attention that determines a tensor weight distribution W (l) within layer l, the kernel attention module assigns a channel weight distribution within a tensor, denoted as W <ref type="bibr">(l)</ref> . <ref type="figure">Fig. 2</ref> (right) depicts the steps on how an updated tensor T</p><formula xml:id="formula_3">(l) f inal is generated through the weight adjustment. Given an input tensor T (l) ? R C?F , we gener- ate M new tensors T (l) m using M TCN units with different dilation rates.</formula><p>These M tensors are fused together through elementwise summation:</p><formula xml:id="formula_4">T (l) = M m=1 T (l)</formula><p>m , which is fed into a global average pooling (GAP) layer to generate channelwise statistics T (l) c ? R C?1 . The channel number C is acquired through a TCN unit as discussed in the ablation study. The output T (l) c is forwarded to a fully-connected layer to learn the relationship among features of different kernel sizes: T</p><formula xml:id="formula_5">(l) r = ? r (l) T (l)</formula><p>c . The role of matrix ? r (l) ? R r?C is to reduce the channel dimension to r. Guided by the compacted feature descriptor T (l) r , M vectors are generated (indicated by the yellow cuboids) through a second fully-connected layer across channels. Their kernel attention weights are computed by a softmax function:</p><formula xml:id="formula_6">W (l) ? = W (l) 1 , ..., W (l) M W (l) m = e ?m (l) T (l) r M m=1 e ?m (l) T (l) r<label>(4)</label></formula><p>where ? m (l) ? R C?r are the kernel attention parameters and M m=1 W (l) m = 1. Based on the weight distribution, we finally obtain the output tensor:</p><formula xml:id="formula_7">T (l) f inal ? = M m=1 W (l) m ? T (l) m<label>(5)</label></formula><p>The channel update procedure can be further decomposed as: As the level index increases, the receptive field over frames (layer index = 0) or tensors (layer index ? 0) increases.</p><formula xml:id="formula_8">W (l) m ? T (l) m = ? (l) 1 ? T (l) 1 , . . . , ? (l) C ? T (l) C<label>(6)</label></formula><p>This shares the same format as the tensor distribution process (equation 1) in the temporal attention module but focuses on the channel distribution. The temporal attention parameters ? t (l) and kernel attention parameters</p><formula xml:id="formula_9">? r (l) , ? m (l) for l ? [1, L ? 2]</formula><p>are learned through mini-batch stochastic gradient descent (SGD) in the same manner as the TCN unit training <ref type="bibr" target="#b4">(Bottou, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Integration with Dilated Convolutions</head><p>For the proposed attention model, a large receptive field is crucial to learn long range temporal relationships across frames, thereby enhancing the estimation consistency. However, with more frames feeding into the network, the number of neural layers increases together with more training parameters. To avoid vanishing gradients or other superfluous layers problems <ref type="bibr" target="#b28">(Martinez et al., 2017)</ref>, we devise a multi-scale dilation (MDC) strategy by integrating dilated convolutions. <ref type="figure" target="#fig_0">Fig. 3</ref> shows our dilated network architecture. For visualization purpose, we project the network into an xyz space. The xy plane has the same configuration as the network in <ref type="figure">Fig. 2</ref>, with the combination of temporal and kernel attention modules along the x direction, and layers layout along the y direction. As an extension, we place the dilated convolution units (DCUs) along the z direction. This z-axis is labeled as levels to differ from the layer concept along the y direction. As the level index increases, the receptive field grows with increasing dilation size while reducing the number of DCUs. unit, the numbers represent the unit configuration, e.g. K3D9, 1024 means kernel size is 3, dilation rate is 9, and tensor depth or number of channels is 1024. M3D3, 1024 means TCN units are 3, dilation rate is 9, and tensor depth or number of channels is 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>This section discusses our system implementation as well as the evaluation results compared to the state-of-the-art techniques by using the standard pose estimation protocols on public datasets. We first describe the configuration and timings for each functional module, as well as the timings for the run-time algorithm. Ablation studies of the system are conducted by analyzing each component and discuss their performance and limitations. Then we evaluate the estimation accuracy compared to other approaches as well as the ground truth. Finally we demonstrate the robustness and flexibility of the proposed approach on videos in the wild with various environment complexities and unknown camera settings. Our model is generic and runs on novel users without requiring any offline training or manual preprocessing steps. More extensive evaluation can be found at our lab website 2 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Configuration and Computational Complexity</head><p>To investigate the practical feasibility of the proposed approach, we implemented three prototypes with different layer L and dilation level V combinations: All the prototypes are implemented in native Python (Pytorch 1.0) and tested on a NVIDIA TITAN RTX GPU without parallel optimization. Despite the difference in layers and levels, all the prototypes present similar convergence rate in training and testing, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. With data augmentation, the L6 ? V 4 setting demonstrates the best Mean Per Joint Position Error (MPJPE) performance with approximately 16 hrs training on 1.6M frames. The optimizer is Ranger <ref type="bibr" target="#b50">(Zhang et al., 2019;</ref><ref type="bibr" target="#b24">Liu et al., 2019)</ref>, and the learning rate is 0.001 with decay=0.05 for 80 epoch, the batch size is 1024 and dropout is 0.2. For real-time inference, it can reach 3000 FPS.</p><formula xml:id="formula_10">L4 ? V 2 ? N 27, L5 ? V 3 ? N 81, and L6 ? V 4 ? N 243,</formula><p>Table 1 compares our model with TCN based semi supervised approach <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref>, and the layer normalized LSTM approach <ref type="bibr" target="#b16">(Hossain et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets and Evaluation Protocols</head><p>Our quantitative evaluation is conducted on two most commonly used datasets: Human3.6M <ref type="bibr" target="#b17">(Ionescu et al., 2013)</ref> and HumanEva <ref type="bibr" target="#b39">(Sigal et al., 2010)</ref>. We also applied our approach to some challenging YouTube videos, which include fast motion activities and low-resolution frames. It would be extremely difficult to obtain meaningful 2D detection for those challenging videos collected in the wild. For the Human3.6M, we follow the same training and validation schemes as in the previous works <ref type="bibr" target="#b28">(Martinez et al., 2017;</ref><ref type="bibr" target="#b45">Yang et al., 2018;</ref><ref type="bibr" target="#b16">Hossain et al., 2018;</ref><ref type="bibr" target="#b35">Pavllo et al., 2019)</ref>. Specifically, subjects S1, S5, S6, S7, and S8 are used for training, and subjects S9 and S11 are used for testing. In the same manner, we conducted training/testing on the HumanEva (a comparatively smaller dataset) with the "Walk" and "Jog" actions performed by subjects S1, S2, and S3.</p><p>For both datasets, we use the standard evaluation metrics MPJPE and P-MPJPE to measure the offset between the estimation result and ground-truth (GT) relative to the root node in millimeters <ref type="bibr" target="#b17">(Ionescu et al., 2013)</ref>. Two protocols are involved in the experiment: Protocol 1 computes the mean Euclidean distance across all the joints after aligning the root joints (i.e., pelvis) between the predicted and ground-truth poses, referred as MPJPE <ref type="bibr">(Fang et al., 2018;</ref><ref type="bibr" target="#b19">Lee et al., 2018;</ref><ref type="bibr" target="#b34">Pavlakos et al., 2017;</ref><ref type="bibr" target="#b26">Luvizon et al., 2018)</ref>. Protocol 2 applies additional similarity transformation Procrustes analysis <ref type="bibr" target="#b20">(Lepetit et al., 2005)</ref> to the predicted pose as an enhancement and it is called P-MPJPE <ref type="bibr" target="#b28">(Martinez et al., 2017;</ref><ref type="bibr" target="#b16">Hossain et al., 2018;</ref><ref type="bibr" target="#b45">Yang et al., 2018;</ref><ref type="bibr" target="#b35">Pavllo et al., 2019)</ref>. In contrast to protocol 1, this evaluation can be more robust to individual joint prediction failure due to the rigid alignment. It is worth mentioning that some researchers also use another protocol by performing a scale alignment on the predicted pose and it is named as N-MPJPE <ref type="bibr" target="#b36">(Rhodin et al., 2018)</ref>. Since it has a similar goal as protocol 2 with relatively less transformation, the error usually drops between the outputs produced by protocols 1 &amp; 2. As such, the accuracy performance should be sufficiently evaluated by using these two protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>To verify the impact and performance of each component in the network, we conducted ablation experiments on the Human3.6M dataset under P rotocol#1.</p><p>TCN Unit Channels: we first investigated how the channel number C affects the performance between TCN units and temporal attention models. In our test, we used both the CPN and GT as the 2D input. Starting with a receptive field of n = 3 ? 3 ? 3 = 27, as we increase the channels (C ? 512), the MPJPE drops down significantly. However, the MPJPE changes slowly when C grows between 512 and 1024, and remains almost stable afterwards. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, with the CPN input, a marginal improvement is yielded from MPJPE 49.9mm at C = 1024 to 49.6mm at C = 2048. A similar curve shape can be observed for the GT input. Considering the computation load with more parameters introduced, we chose C = 1024 in our experiments. Kernel Attention: <ref type="table" target="#tab_2">Table 2</ref> shows how the setting of different parameters inside the Kernel Attention module impacts the performance under P rotocol#1. The left three columns list the main variables. For validation purposes, we divide the configuration into three groups in row-wise. Within each group, we assign different values in one variable while keeping the other two fixed. The items in bold represent the best individual setting for each group. Empirically, we chose the combination of M = 3, G = 8, and r = 128 as the optimal setting (labeled in box). Note, we select G = 8 instead of the individual best assignment G = 2, which introduces a larger number of parameters with negligible MPJPE improvement.  In <ref type="table" target="#tab_4">Table 3</ref>, we discuss the choice of different types of receptive fields and how it affects the network performance. The first column shows various layer configurations, which generate different receptive fields, ranging from n = 27 to n = 1029. To validate the impact of n, we fix the other parameters, i.e. M = 3, G = 8, r = 128. Note that for a network with smaller number of layers (e.g. L = 3), a larger receptive field may reduce the error more effectively. For example, increasing the receptive field from n = 3?3?3 = 27 to n = 3 ? 3 ? 7 = 147, the MPJPE drops from 40.6 to 36.8 . However, for a deeper network, a larger receptive field may not be always optimal, e.g. when n = 1029, MPJPE = 37.0. Empirically, we obtained the best performance with the setting of n = 243 and L = 5, as indicated in the last row.</p><p>Multi-Scale Dilation: To evaluate the impact of the dilation component on the network, we tested the system with and without dilation and compared their individual outcomes. In the same way, the GT and CPN 2D detectors are used as input and being tested on the Human3.6M dataset under P rotocol#1. <ref type="table">Table 4</ref> demonstrates the integration of attention, and multi-scale dilation components surpass their individual performance with the minimum MPJPE for all the three prototypes. We also found the attention model makes an increasingly significant contribution as the layer number grows. This is because more layers lead to a larger receptive field, allowing the multi-scale dilation to capture longterm dependency across frames. The effect is more noticeable when fast motion or self-occlusion present in videos.   <ref type="table">Table 4</ref>: Ablation study on different components in our method. The evaluation is performed on Human3.6M under P rotocol#1 with MPJPE (mm).</p><p>Step by step performance enhancement: Here we list all the steps and additional modules used to obtain the performance. The step-by-step gains brought by each component are illustrated in <ref type="table" target="#tab_6">Table 5</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with State-of-the-Art</head><p>In this subsection, we systematically analyze the performance of our proposed method by comparing it with state-of-theart. To fairly evaluate the accuracy, we use the same training and testing datasets as others. <ref type="table" target="#tab_2">Tables 6 -8 demonstrate the  comparison by following Protocol 1 and 2. Tables 6 and 7</ref> illustrate the Human3.6M results and <ref type="table" target="#tab_10">Table 8</ref> illustrates the results of HumanEva. The results of each method are displayed in row-wise. Each column indicates a different pose scene, e.g., walking, eating, etc. We highlight the best and second best results in each column in bold and underline formats respectively. The last column of each    To further demonstrate the efficacy, we evaluated the performance and advantage of our approach in three aspects:</p><p>1. Joint-wise: Analyzing the accuracy of individual joint measurement with MPJPE comparison 2. Frame-wise: Tracking the average MPJPE of all the joints across frames 3. Re-targeting-wise: Applying motion-retargeting by transferring the estimated pose to a 3D avatar</p><p>We compare our approach with three state-of-the-art techniques, which represent the best reported results on monocular video-based 2D-to-3D estimation to date: the deep feedforward 2D-to-3D network <ref type="bibr" target="#b28">(Martinez et al., 2017)</ref>, the layernormalized LSTM based algorithm <ref type="bibr" target="#b16">(Hossain et al., 2018)</ref>, and the dilated temporal convolution with semi-supervised training <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref>. <ref type="figure">Fig. 8</ref> demonstrates the jointwise MPJPE for a selected frame from the WalkDog S11 data. The top row shows the input 2D color image and its corresponding estimated 3D poses by other methods. The histograms in the second row show the quantified measurement on each joint, e.g., R-Knee, Nose, Neck. Note that our approach, indicated by the green bar, achieves minimum MPJPE among all the other methods in most of the joints. To further validate the accuracy, we trace these individual joints across frames in the corresponding video sequence and measure their MPJPE in the temporal space. <ref type="figure">Fig. 7</ref> plots the MPJPE curves over time (around 1400 frames) on two selected joints: the left ankle from Walking S9 and the left elbow from Smoking S9. Compared to the recent works <ref type="bibr" target="#b28">(Martinez et al., 2017;</ref><ref type="bibr" target="#b16">Hossain et al., 2018;</ref><ref type="bibr" target="#b35">Pavllo et al., 2019)</ref>, our results yield low errors consistently through learning the long-range dependencies using the multi-scale dilation convolution.</p><p>In light of possible biases and uncertainties that individual joint may introduce, we perform frame-wise analysis by taking the average MPJPE of all the joint estimation in each frame and measure how it changes through a video sequence. <ref type="figure" target="#fig_7">Fig. 10</ref> shows the testing results on two scenes of the Human3D dataset: smoking S9 and photo S9. For each scene, the top row presents the estimated 3D pose results from the same frame produced by different methods. Though it is hard to see the difference from the single frame, from the MPJPE (the green number on the top-left corner of each pose result), our attention-based model delivers the best result. In the second row of each scene, We trace these average joint errors across all of the frames in the corresponding video sequence. Our results maintain low MPJPE compared to the other methods.</p><p>To visually demonstrate the significance of the estimation improvement, we apply animation retargeting to a 3D avatar by synthesizing the captured motion from the same frame of the Walking S9 and Posing S9 sequences as shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. With the support of additional mesh surface driven by the pose, it helps magnify the degree of body part arrangement that enhance the contrast of estimation. From the side-by-side comparisons, one can easily see the difference between the rendered results against the ground truth. Specifically, the shadows of the legs and the right hand are differently rendered due to the erroneous pose estimated using the method in <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref> while ours stay more aligned to the ground truth. The quantified MPJPE for each joint estimation is shown in the correponding histograms right below it. <ref type="figure" target="#fig_8">Fig. 11</ref> shows more retargeting results on the same dataset for different frames. The zoom-in views illustrate the details of the animated characters of different pose configurations. For the Posing S9 in the first row, our results bear the closest similarity as the ground truth with the right arm of the character naturally hanging down the side of the body, while others present more distinct arm gesture. The second row of <ref type="figure" target="#fig_8">Fig. 11</ref> demonstrates the improvement of our approach on leg movement prediction with optimistic estimate on the two legs relative positions. Note that this is just one selected frame from the walking sequence, which is a common body activity involving the alternate of left and right legs in a repetitive manner. Accurate and consistent part detection is crucial to deliver smooth motion sequences without any jittering effect in 3D pose reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Detection</head><p>We investigated the impact of 2D pose detection on our 3D pose estimation performance by exploring several widely adopted 2D detectors. Firstly, we utilized the pre-trained Stacked Hourglass network (SH) <ref type="bibr" target="#b30">(Newell et al., 2016)</ref> on the MPII dataset to extract 2D keypoint locations within the groundtruth bounding boxes. We also applied the results of fine-tuned SH model on the Human3.6M dataset developed by <ref type="bibr" target="#b28">(Martinez et al., 2017)</ref>. Researchers also investigated automated methods with detected bounding boxes for 2D human pose detection, such as Simple baselines for human pose estimation <ref type="bibr" target="#b44">(Xiao et al., 2018)</ref>, Deep high-resolution representation for human pose estimation (HRnet) <ref type="bibr" target="#b40">(Sun et al., 2019)</ref> or Cascaded Pyramid Network (CPN) <ref type="bibr" target="#b5">(Chen and Ramanan, 2017)</ref> together with Mask R-CNN  and ResNet-101-FPN <ref type="bibr" target="#b22">(Lin et al., 2017)</ref> as the backbone. We applied the pre-trained SH, fine-tuned SH, and fine-tuned CPN models <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref> as the 2D detectors for performing a fair comparison, as shown in <ref type="table" target="#tab_12">Table 9</ref>.</p><p>The big difference between the pre-trained and fine-tuned models are the 2D human joints estimation accuracy and number of joints. Based on the results of our experiment, our network can learn different joint label information. MPII has 16 joints which missed the neck/nose joint in the Hu-man3.6M dataset. Although COCO dataset has the same   joint number, the order of the labels of joints is different from Human3.6M. To get a more accurate 3D joints position result, we utilize a fine-tuned model to get the corresponding 2D joints on Human3.6M. Furthermore, in the second part of <ref type="table" target="#tab_8">Table 6</ref>, we show the results with ground-truth (GT) 2D input. For both cases, our attention model demonstrates a clear advantage by utilizing the temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Attention Results</head><p>To facilitate real-time performance for potential interactive applications, we also investigate a causal attention based network that estimates the target pose by only processing the current frame and its previous frames. The architecture of the causal attention model is shown in <ref type="figure" target="#fig_9">Fig. 12</ref>. The architecture is similar to the one described in <ref type="figure">Fig. 2</ref>, but here we only consider the left half of the input video sequence. The number of input frames can also be determined by the number of layers of the model, but it shifts to the N ?1 2 previous frames, where N is the corresponding number of frames in the fullmodel illustrated in <ref type="figure">Fig. 2</ref> . For example, for the configuration of L4 ? V 2 ? N 27, 27 causal frames are fed into the network (included the target frame); while L5 ? V 3 ? N 81 requires 81 causal frames as the input. Similarly, to verify the performance, we implemented three different prototypes according to the number of layers and levels, as shown in <ref type="table" target="#tab_14">Table 10</ref>. Horizontally, each row indicates a different prototype of the causal model. Vertically, each column indicates a different 2D detector. We provide a side-by-side comparison with the results in the recent CVPR paper on the same problem with various 2D detectors <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref>. Even our causal model only considers casual input frames compared to the TCN based semi-supervised approach in <ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref>, the results of our method (ATCN + MDC) demonstrate higher accuracy consistently. In particular, more noticeable improvements are achieved as the number of input frames increases. The result of real-time processing using causal model is shown in <ref type="figure" target="#fig_0">Fig. 13</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performance on Videos in-the-wild</head><p>To evaluate the performance on videos in-the-wild, we validated our approach on both public datasets and online videos with the former emphasizing quantitative validation while the later demonstrating qualitative performance. While there exists limited datasets with accurate 3D pose in the wild, we adopt some of the standard activities with outdoor scene  <ref type="table">Table 11</ref>: P rotocol#2 measurement on the estimation results from the simulated scenes. Training and testing on ground truth 2d joint locations plus different levels of additive gaussian noise. simulation to quantitatively evaluate the performance and compare with other approaches. In contrast to static background and cameras capture setting, outdoor has more dynamic and unrestricted environment with frequent occlusion and high variation in background/foreground objects appearance. <ref type="figure" target="#fig_11">Fig. 14</ref> shows several outdoor simulations on the standard activities with snow, fog, and occlusion effects (each column). The corresponding pose estimation results by different approaches are shown in each of the following rows. <ref type="table">Table 11</ref> provides the quantitative measurement on their output. In a similar manner, joint-wise analysis is conducted on a selected joint from the Human3.6M scene with the generated noises. One can see our approach consistently yields less MPJPE over the frames as shown in <ref type="figure" target="#fig_2">Fig. 15</ref>. To quantitatively demonstrate the robustness and efficacy, various videos in the wild are collected online and added with extra noises, e.g. snow or fog effect. <ref type="figure" target="#fig_13">Fig. 17</ref> shows satisfactory results are achieved, given the additional noises. For example, in the foggy scene (row 5 and 6), the target person is almost occluded by the thick fog. Thanks to the attention model that successfully extracts temporal information from neighbor frames, the full 3D pose is correctly recovered.</p><p>To further demonstrate the temporal consistency, we gather online video sequences from YouTube and predict the 3D poses directly from these videos in the wild. <ref type="figure">Fig. 18</ref> demonstrates the results of this experiment on various activities. Even though the input videos are either of low resolution or with fast motions, our approach is still able to estimate the 3D pose with satisfactory output. For example, for the dancing scenes (rows 1-2 and rows 9-10) and the skating scene (rows 5-6), given the presence of fast body movement and self-occlusion, the estimations are accurate enough to provide the corresponding 3D positions for each frame. To further verify the robustness, different sports activities with novel body poses (rows 3-4, rows 7-8, and rows 11-12) are processed. Our algorithm can faithfully capture and reproduce these pose details without requiring any additional offline training or manual preprocessing steps. In particular, for the challenging scene in rows 3-4, the target person has relatively casual dress with partial leg occlusion by the top costume The generated 3D pose from our attention model are visually plausible and resemble the user's body motion very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>In this paper, we present a novel and practical approach for 3D human pose estimation and reconstruction in unconstrained videos. In order to enhance temporal coherency, we integrate an attentional mechanism to the temporal convolutional neural network to guide the network towards learning informative representation. Moreover, we introduced a multi-scale dilation convolution, which is capable of capturing several levels of temporal receptive fields, achieving (a) Heavy occlusion.</p><p>(b) Instantaneous movement. <ref type="figure" target="#fig_4">Fig. 16</ref>: Unresolved cases: there were a few failed frames from the tested wild videos, where severe occlusion and fast motion presented. long-range dependencies among frames. Extensive experiments on benchmark demonstrates that our approach improves upon the state-of-the-art and offers an effective, alternative framework to address the 3D human pose estimation problem. The implementation is straightforward and can adaptive corporate with standard convolution neural networks. For the input data, any off-the-shelf 2D pose estimation systems,e.g. Mocap libraries, can be easily integrated in an adhoc fashion.</p><p>Though our results outperform the state-of-the-art on public datasets, there are still some specific limitation remaining unresolved. Two examples are shown in <ref type="figure" target="#fig_4">Fig. 16</ref>. For example, when the performer's arms are crossing under the fans, it causes heavy occlusion with missing joints detection, thereby resulting in poor pose estimation, indicated in <ref type="figure" target="#fig_4">Fig. 16(a)</ref>. In <ref type="figure" target="#fig_4">Fig. 16(b)</ref>, when the leg has a very fast movement, our temporal system categorizes it as an outlier position rather than using them to contribute the pose inference. Another limitation is on the inference accuracy for some multi-person human scenarios due to the limited training data on labeled multi-person 3D pose video datasets. However, if using the top-down 2D pose detecting algorithm with pose tracking, it would be possible to reconstruct multi-person 3D pose from a video. The tracking error may affect the temporal attention performance. Our future direction will explore a more generic framework that integrates the proposed attention model and person re-identification solution to handle instantaneous body part movements and heavy occlusions caused by multiple people. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>The model of temporal dilated convolution network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Prototype 1: Layer 4 ? Level 2 (L4?V2) (b) Prototype 2: Layer 5 ? Level 3 (L5?V3) Fig. 4: Architectures of input/output data flows across different dilated convolution units. Inside each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Convergence characteristics for training and testing on three prototypes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where the last term N indicates the corresponding input frame number.Fig. 4provides a deeper insight on unit configuration of the prototypes: L4?V 2?N 27 and L5?V 3?N 81. By dropping the x-axis fromFig. 3, it only displays the level and layer distribute in a 2D view. For simplicity, we use a black/gray rectangle shape to denote the group of TCN units within a layer. At level 0 , the TCN units are placed by layers along the yaxis corresponding to the ones depicted inFig. 3. From level 1, along the positive z-axis, different scaled dilated convolution units are placed. As the level index grows, the number of dilated units decreases due to the increasing receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The impact of channel number on MPJPE. CPN: cascaded pyramid network and GT: ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Protocol 1: joint error analysis across frames in Human3.6M Walking S9 left ankle.(b) Protocol 1: joint error analysis across frames in Human3.6M Smoking S9 left elbow.Fig. 7: Joint-wise analysis across frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Comparison results: (top): side-by-side views of motion retargeting results on a 3D avatar; the source is from frame 857 of walking S9 and frame 475 posing S9 in Human3.6M. (bottom): the average joint error comparison across all the frames of the video walking S9 (Pavllo et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Frame-wise comparasion with state-of-the-art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Comparison with state-of-the-art results on motion re-targeting model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 :</head><label>12</label><figDesc>An example of a 4-layer architecture for causal attention-based temporal convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :</head><label>13</label><figDesc>3D reconstruction results from different angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 :</head><label>14</label><figDesc>Samples of synthesized outdoor environment on the Human3.6M dataset and their 3D pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Protocol 1: joint error analysis after adding noise across frames in Human3.6M Walking S9 left ankle.(b) Protocol 1: joint error analysis after adding noise across frames in Human3.6M Smoking S9 left elbow.Fig. 15: Joint-wise analysis and comparison on the outdoor simulated scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 :</head><label>17</label><figDesc>Qualitative results on gathered in the wild videos: original frame sequence with added noises and the recovered 3D poses. motion. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 668-683 Dauphin YN, Fan A, Auli M, Grangier D (2017) Language modeling with gated convolutional networks. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70, JMLR. org, pp 933-941 Fang HS, Xu Y, Wang W, Liu X, Zhu SC (2018) Learning pose grammar to encode human body configuration for 3d pose estimation. Thirty-Second AAAI Conference on Artificial Intelligence Ferrari V, Marin-Jimenez M, Zisserman A (2009) Pose search: Retrieving people using their pose. IEEE Conference on Computer Vision and Pattern Recognition p 1-8 Fig. 18: Qualitative results on gathered Youtube videos: original frame sequence and the recovered 3D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Ablation study on different parameters in our kernel attention model. Here, we use receptive field n = 3?3?3? 3 ? 3 = 243. The evaluation is performed on Human3.6M under P rotocol#1 with MPJPE (mm).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on different receptive fields in our kernel attention model. The evaluation is performed on Hu-man3.6M under P rotocol#1 with MPJPE (mm).</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell cols="3">n = 27 n = 81 n = 243</cell></row><row><cell>Attention model (CPN)</cell><cell></cell><cell>49.1</cell><cell>47.2</cell><cell>45.7</cell></row><row><cell cols="2">Multi-Scale Dilation model (CPN)</cell><cell>50.3</cell><cell>49.8</cell><cell>49.1</cell></row><row><cell cols="2">Attention and Dilation (CPN)</cell><cell>49.0</cell><cell>46.5</cell><cell>45.1</cell></row><row><cell>Attention model (GT)</cell><cell></cell><cell>39.5</cell><cell>37.8</cell><cell>35.5</cell></row><row><cell cols="2">Multi-Scale Dilation model (GT)</cell><cell>39.2</cell><cell>37.2</cell><cell>35.3</cell></row><row><cell cols="2">Attention and Dilation (GT)</cell><cell>38.9</cell><cell>36.2</cell><cell>33.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Model</cell><cell cols="2">Method CPN</cell><cell>GT</cell></row><row><cell>Baseline 27 frames</cell><cell></cell><cell>51.2</cell><cell>40.6</cell></row><row><cell cols="2">+ Receptive field(243 frames)</cell><cell>49.2</cell><cell>37.8</cell></row><row><cell>+ Attention</cell><cell></cell><cell>47.9</cell><cell>35.5</cell></row><row><cell>+ Dilation</cell><cell></cell><cell>47.2</cell><cell>34.7</cell></row><row><cell>+ Project on 2D</cell><cell></cell><cell>47.0</cell><cell>34.5</cell></row><row><cell>+ 2D pose enhance</cell><cell></cell><cell>46.5</cell><cell>-</cell></row><row><cell>+ Data augment</cell><cell></cell><cell>44.8</cell><cell>33.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Ablation study on different components in our method. The evaluation is performed on Human3.6M under P rotocol#1 with MPJPE (mm).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>table showsthe average results across all the different pose scenes. Note that our model outperforms all the existing approaches by reaching a minimum average error of 48.6mm in MPJPE and 37.7mm in P-MPJPE. Admittedly, for some pose scenes, e.g., Phone, Eat, our method does not achieve the best performance. This could be due to the nature of the particular activities, for example, if the less noticeable motion or only upper-body movement are involved, limited information is fed into the attention layers to learn tensor distributions.</figDesc><table><row><cell>Method</cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Pur.</cell><cell>Sit</cell><cell cols="6">SitD. Smoke Wait WalkD. Walk WalkT.</cell><cell>Avg</cell></row><row><cell cols="2">(Martinez et al., 2017) 51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell cols="3">55.2 58.1 74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>(Fang et al., 2018)</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell cols="3">53.4 55.7 72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>(Yang et al., 2018)</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.0</cell><cell>62.1</cell><cell>65.4</cell><cell cols="3">49.8 52.7 69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>(Pavlakos et al., 2017)</cell><cell>48.5</cell><cell>54.4</cell><cell>54.4</cell><cell>52.0</cell><cell>59.4</cell><cell>65.3</cell><cell cols="3">49.9 52.9 65.8</cell><cell>71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell>(?)</cell><cell>49.2</cell><cell>51.6</cell><cell>47.6</cell><cell>50.5</cell><cell>51.8</cell><cell>60.3</cell><cell cols="3">48.5 51.7 61.5</cell><cell>70.9</cell><cell>53.7</cell><cell>48.9</cell><cell>57.9</cell><cell>44.4</cell><cell>48.9</cell><cell>53.2</cell></row><row><cell>(Hossain et al., 2018)</cell><cell>48.4</cell><cell>50.7</cell><cell>57.2</cell><cell>55.2</cell><cell>63.1</cell><cell>72.6</cell><cell cols="3">53.0 51.7 66.1</cell><cell>80.9</cell><cell>59.0</cell><cell>57.3</cell><cell>62.4</cell><cell>46.6</cell><cell>49.6</cell><cell>58.3</cell></row><row><cell>(Lee et al., 2018)</cell><cell>40.2</cell><cell>49.2</cell><cell>47.8</cell><cell>52.6</cell><cell>50.1</cell><cell>75.0</cell><cell cols="3">50.2 43.0 55.8</cell><cell>73.9</cell><cell>54.1</cell><cell>55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>(Dabral et al., 2018)</cell><cell>44.8</cell><cell>50.4</cell><cell>44.7</cell><cell>49.0</cell><cell>52.9</cell><cell>61.4</cell><cell cols="3">43.5 45.5 63.1</cell><cell>87.3</cell><cell>51.7</cell><cell>48.5</cell><cell>52.2</cell><cell>37.6</cell><cell>41.9</cell><cell>52.1</cell></row><row><cell>(Zhao et al., 2019a)</cell><cell>47.3</cell><cell>60.7</cell><cell>51.4</cell><cell>60.5</cell><cell>61.1</cell><cell>49.9</cell><cell cols="3">47.3 68.1 86.2</cell><cell>55.0</cell><cell>67.8</cell><cell>61.0</cell><cell>42.1</cell><cell>60.6</cell><cell>45.3</cell><cell>57.6</cell></row><row><cell>(Pavllo et al., 2019)</cell><cell>45.2</cell><cell>46.7</cell><cell>43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1</cell><cell cols="3">44.6 44.3 57.3</cell><cell>65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>(Cheng et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.8 +</cell></row><row><cell>Ours (n=243 CPN)</cell><cell>41.8</cell><cell>44.0</cell><cell>41.1</cell><cell>43.9</cell><cell>47.4</cell><cell>54.1</cell><cell cols="3">42.1 42.2 55.3</cell><cell>63.6</cell><cell>45.3</cell><cell>42.7</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>44.8</cell></row><row><cell cols="2">(Martinez et al., 2017) 37.7</cell><cell>44.4</cell><cell>40.3</cell><cell>42.1</cell><cell>48.2</cell><cell>54.9</cell><cell cols="3">44.4 42.1 54.6</cell><cell>58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>(Hossain et al., 2018)</cell><cell>35.2</cell><cell>40.8</cell><cell>37.2</cell><cell>37.4</cell><cell>43.2</cell><cell>44.0</cell><cell cols="3">38.9 35.6 42.3</cell><cell>44.6</cell><cell>39.7</cell><cell>39.7</cell><cell>40.2</cell><cell>32.8</cell><cell>35.5</cell><cell>39.2</cell></row><row><cell>(Lee et al., 2018)</cell><cell>32.1</cell><cell>36.6</cell><cell>34.4</cell><cell>37.8</cell><cell>44.5</cell><cell>49.9</cell><cell cols="3">40.9 36.2 44.1</cell><cell>45.6</cell><cell>35.3</cell><cell>35.9</cell><cell>37.6</cell><cell>30.3</cell><cell>35.5</cell><cell>38.4</cell></row><row><cell>(Zhao et al., 2019a)</cell><cell>37.8</cell><cell>49.4</cell><cell>37.6</cell><cell>40.9</cell><cell>45.1</cell><cell>41.4</cell><cell cols="3">40.1 48.3 50.1</cell><cell>42.2</cell><cell>53.5</cell><cell>44.3</cell><cell>40.5</cell><cell>47.3</cell><cell>39.0</cell><cell>43.8</cell></row><row><cell>(Pavllo et al., 2019)</cell><cell>35.2</cell><cell>40.2</cell><cell>32.7</cell><cell>35.7</cell><cell>38.2</cell><cell>45.5</cell><cell cols="3">40.6 36.1 48.8</cell><cell>47.3</cell><cell>37.8</cell><cell>39.7</cell><cell>38.7</cell><cell>27.8</cell><cell>29.5</cell><cell>37.8</cell></row><row><cell>Ours (n=243 GT)</cell><cell>33.0</cell><cell>35.7</cell><cell>31.7</cell><cell>32.4</cell><cell>32.1</cell><cell>36.5</cell><cell cols="3">37.2 32.6 40.7</cell><cell>41.4</cell><cell>32.6</cell><cell>33.1</cell><cell>30.9</cell><cell>24.9</cell><cell>25.9</cell><cell>33.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">However, if one considers all the scenarios, our overall per-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">formance demonstrates higher accuracy than other methods</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">by a fair margin. In particular, under protocol 1, our model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">reduces the best-reported error rate of MPJPE (Pavllo et al.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">2019) by approximate 3% using ground truth 2D pose as the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>input.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>truth, (+) -the result without data</cell></row></table><note>Protocol 1: Reconstruction Error on Human3.6M. Top-half: input 2D joints are acquired by detection; bottom-half: input 2D joints with ground-truth. (CPN) -cascaded pyramid network, (GT) -ground-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Protocol 2: Reconstruction Error on Human3.6M with similarity transformation.Fig. 8: Individual joint MPJPE comparison with state-of-the-art.</figDesc><table><row><cell></cell><cell></cell><cell>Walk</cell><cell></cell><cell></cell><cell>Jog</cell><cell></cell></row><row><cell></cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg</cell></row><row><cell cols="2">(Pavlakos et al., 2017) (MA) 22.3</cell><cell>19.5</cell><cell cols="5">29.7 28.9 21.9 23.8 24.35</cell></row><row><cell>(Martinez et al., 2017) (SA)</cell><cell>19.7</cell><cell>17.4</cell><cell cols="4">46.8 26.9 18.2 18.6</cell><cell>24.6</cell></row><row><cell>(Lee et al., 2018) (MA)</cell><cell>18.6</cell><cell>19.9</cell><cell cols="4">30.5 25.7 16.8 17.7</cell><cell>21.5</cell></row><row><cell>(Pavllo et al., 2019)(MA)</cell><cell>13.4</cell><cell>10.2</cell><cell cols="4">27.2 17.1 13.1 13.8</cell><cell>15.8</cell></row><row><cell>Ours (n=27 MA)</cell><cell>13.1</cell><cell>9.8</cell><cell cols="4">26.8 16.9 12.8 13.3</cell><cell>15.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Protocol 2: Reconstruction Error on HumanEva.</cell></row><row><cell>(MA) -multi-action model, (SA) -single action model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Performance impacted by 2D detectors under Pro-</cell></row><row><cell>tocol 1 and Protocol 2. (PT) -pre-trained, (FT) -fine-tuned,</cell></row><row><cell>SH -stacked hourglass.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Bottom-table: Causal sequence processing performance in terms of the different 2D detectors under P rotocol#1. PT -pre-trained, FT -fine-tuned, GT -groundtruth, SH -stacked hourglass, CPN -cascaded pyramid network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>NoiseICCV<ref type="bibr" target="#b28">(Martinez et al., 2017)</ref> ECCV<ref type="bibr" target="#b16">(Hossain et al., 2018)</ref> CVPR<ref type="bibr" target="#b35">(Pavllo et al., 2019)</ref> Ours</figDesc><table><row><cell>GT/GT</cell><cell>37.1</cell><cell>31.7</cell><cell>28.1</cell><cell>26.1</cell></row><row><cell>GT/GT + N (0, 5)</cell><cell>46.7</cell><cell>37.5</cell><cell>30.9</cell><cell>28.3</cell></row><row><cell>GT/GT + N (0, 10)</cell><cell>52.8</cell><cell>49.4</cell><cell>39.3</cell><cell>36.7</cell></row><row><cell>GT/GT + N (0, 15)</cell><cell>60.0</cell><cell>61.8</cell><cell>50.3</cell><cell>42.5</cell></row><row><cell>GT/GT + N (0, 20)</cell><cell>70.2</cell><cell>73.7</cell><cell>62.2</cell><cell>56.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Ruixu Liu et al.   </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiview pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y ; Iclr</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>arXiv:180301271</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural machine translation by jointly learning to align and translate</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMP-STAT&apos;2010</title>
		<meeting>COMP-STAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Anatomy-aware 3d human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno>arXiv:200210322</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial learning of structure-aware fully convolutional networks for landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Learning 3d human pose from structure and</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask r-cnn. International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory. neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xxx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oord</forename><surname>Avd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>arXiv:161010099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular model-based 3d tracking of rigid objects: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="89" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gast-net: Graph attention spatio-temporal convolutional networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rojas</surname></persName>
		</author>
		<idno>arXiv:200314179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>arXiv:190803265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Asari V (2020b) Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheung</forename><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The kit whole-body human motion database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mandery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Terlemez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vahrenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiscale deep learning for gesture detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops pp</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oord</forename><surname>Avd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>arXiv:160903499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-modal rgb-depth-thermal human body segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Palmero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clap?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bahnsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?gelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="239" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<title level="m">3d human pose estimation using convolutional neural networks with 2d pose information. European Conference on Computer Vision (ECCV) Workshops p</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature selection using a multilayer perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabrisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Network Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:190209212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="991" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y ;</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3d human pose estimation in the wild by adversarial learning. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semimarkov phrase-based monolingual alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing pp</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="590" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast normalized cross-correlation. Circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">819</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:190708610</idno>
		<title level="m">Lookahead optimizer: k steps forward, 1 step back</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Deep kinematic pose regression. European Conference on Computer Vision (ECCV) Workshops p</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
