<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">?cole normale sup?rieure</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">?cole normale sup?rieure</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL Research University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Courant Institute</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><forename type="middle">Ai</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">VICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.</p><p>Published as a conference paper at ICLR 2022 Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. In BMVC, 2018. 7</p><p>Rong-En Fan, Kaiyour own latent:</p><p>A new approach to self-supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-supervised representation learning has made significant progress over the last years, almost reaching the performance of supervised baselines on many downstream tasks <ref type="bibr" target="#b1">Bachman et al. (2019)</ref>; <ref type="bibr" target="#b18">Misra &amp; Maaten (2020)</ref>; <ref type="bibr" target="#b9">He et al. (2020)</ref>; <ref type="bibr" target="#b23">Tian et al. (2020)</ref>; <ref type="bibr" target="#b6">Caron et al. (2020)</ref>; <ref type="bibr" target="#b21">Grill et al. (2020)</ref>; <ref type="bibr" target="#b9">Chen &amp; He (2020)</ref>; <ref type="bibr">Gidaris et al. (2021)</ref>; <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>. Several recent approaches rely on a joint embedding architecture in which two networks are trained to produce similar embeddings for different views of the same image. A popular instance is the Siamese network architecture <ref type="bibr" target="#b3">Bromley et al. (1994)</ref>, where the two networks share the same weights. The main challenge with joint embedding architectures is to prevent a collapse in which the two branches ignore the inputs and produce identical and constant output vectors. There are two main approaches to preventing collapse: contrastive methods and information maximization methods. Contrastive <ref type="bibr" target="#b3">Bromley et al. (1994)</ref>; <ref type="bibr" target="#b11">Chopra et al. (2005)</ref>; He et al. <ref type="bibr">(2020)</ref>; <ref type="bibr" target="#b1">Hjelm et al. (2019)</ref>; <ref type="bibr" target="#b7">Chen et al. (2020a)</ref> methods tend to be costly, require large batch sizes or memory banks, and use a loss that explicitly pushes the embeddings of dissimilar images away from each other. They often require a mining procedure to search for offending dissimilar samples from a memory bank <ref type="bibr" target="#b9">He et al. (2020)</ref> or from the current batch <ref type="bibr" target="#b7">Chen et al. (2020a)</ref>. Quantization-based approaches <ref type="bibr" target="#b6">Caron et al. (2020;</ref> force the embeddings of different samples to belong to different clusters on the unit sphere. Collapse is prevented by ensuring that the assignment of samples to clusters is as uniform as possible. A similarity term encourages the cluster assignment score vectors from the two branches to be similar. More recently, a few methods have appeared that do not rely on contrastive samples or vector quantization, yet produce high-quality representations, for example <ref type="bibr">BYOL Grill et al. (2020)</ref> and <ref type="bibr">SimSiam Chen &amp; He (2020)</ref>. They exploit several tricks: batch-wise or feature-wise normalization, a "momentum encoder" in which the parameter vector of one branch is a low-pass-filtered version of the parameter vector of the other branch <ref type="bibr" target="#b21">Grill et al. (2020)</ref>; <ref type="bibr" target="#b21">Richemond et al. (2020)</ref>, or a stop-gradient operation in one of the branches <ref type="bibr" target="#b9">Chen &amp; He (2020)</ref>. The dynamics of learning in these methods, and how they avoid collapse, is not fully understood, although theoretical and empirical studies point to the crucial importance of batch-wise or feature-wise normalization <ref type="bibr" target="#b21">Richemond et al. (2020)</ref>; <ref type="bibr" target="#b24">Tian et al. (2021)</ref>. Finally, an 1 arXiv:2105.04906v3 [cs.CV] 28 Jan 2022</p><p>Published as a conference paper at ICLR 2022</p><formula xml:id="formula_0">( ) ( ) ( ?) ( , ?) I t ~ T t !~ T X Z Y ( ? ) ( ?) X? Z? Y? ? (+ ?? )+ T t,t' ! , ? !" ? # , ?? #" I X, X' Y, Y' Z, Z'</formula><p>: maintain variance : bring covariance to zero : minimize distance : distribution of transformations : random transformations : encoders : expanders : batch of images : batches of views : batches of representations : batches of embeddings <ref type="figure">Figure 1</ref>: VICReg: joint embedding architecture with variance, invariance and covariance regularization. Given a batch of images I, two batches of different views X and X are produced and are then encoded into representations Y and Y . The representations are fed to an expander producing the embeddings Z and Z . The distance between two embeddings from the same image is minimized, the variance of each embedding variable over a batch is maintained above a threshold, and the covariance between pairs of embedding variables over a batch are attracted to zero, decorrelating the variables from each other. Although the two branches do not require identical architectures nor share weights, in most of our experiments, they are Siamese with shared weights: the encoders are ResNet-50 backbones with output dimension 2048. The expanders have 3 fully-connected layers of size 8192.</p><p>alternative class of collapse prevention methods relies on maximizing the information content of the embedding <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>; <ref type="bibr" target="#b15">Ermolov et al. (2021)</ref>. These methods prevent informational collapse by decorrelating every pair of variables of the embedding vectors. This indirectly maximizes the information content of the embedding vectors. The Barlow Twins method drives the normalized cross-correlation matrix of the two embeddings towards the identity <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>, while the Whitening-MSE method whitens and spreads out the embedding vectors on the unit sphere <ref type="bibr" target="#b15">Ermolov et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VICREG: INTUITION</head><p>We introduce VICReg (Variance-Invariance-Covariance Regularization), a self-supervised method for training joint embedding architectures based on the principle of preserving the information content of the embeddings. The basic idea is to use a loss function with three terms:</p><p>? Invariance: the mean square distance between the embedding vectors.</p><p>? Variance: a hinge loss to maintain the standard deviation (over a batch) of each variable of the embedding above a given threshold. This term forces the embedding vectors of samples within a batch to be different.</p><p>? Covariance: a term that attracts the covariances (over a batch) between every pair of (centered) embedding variables towards zero. This term decorrelates the variables of each embedding and prevents an informational collapse in which the variables would vary together or be highly correlated.</p><p>Variance and Covariance terms are applied to both branches of the architecture separately, thereby preserving the information content of each embedding at a certain level and preventing informational collapse independently for the two branches. The main contribution of this paper is the Variance preservation term, which explicitly prevents a collapse due to a shrinkage of the embedding vectors towards zero. The Covariance criterion is borrowed from the Barlow Twins method and prevents informational collapse due to redundancy between the embedding variables <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>. VICReg is more generally applicable than most of the aforementioned methods because of fewer constraints on the architecture. In particular, VICReg:</p><p>? does not require that the weights of the two branches be shared, not that the architectures be identical, nor that the inputs be of the same nature;  <ref type="bibr" target="#b15">Ermolov et al. (2021)</ref>. One of the most interesting feature of VICReg is the fact that the two branches are not required to share the same parameters, architecture, or input modality. This opens the door to the use of non-contrastive self-supervised joint-embedding for multi-modal signals, such as video and audio. We demonstrate the effectiveness of the proposed approach by evaluating the representations learned with VICReg on several downstream image recognition tasks including linear head and semi-supervised evaluation protocols for image classification on ImageNet <ref type="bibr" target="#b13">Deng et al. (2009)</ref>, and other classification, detection, instance segmentation, and retrieval tasks. Furthermore, we show that incorporating variance preservation into other self-supervised joint-embedding methods yields better training stability and performance improvement on downstream tasks. More generally, we show that VICReg is an explicit and effective, yet simple method for preventing collapse in self-supervised joint-embedding learning.  <ref type="bibr">(2015)</ref>. These methods train a student network to predict the representations of a teacher network, for which the weights are a running average of the student network's weights <ref type="bibr" target="#b21">Grill et al. (2020)</ref>, or are shared with the student network, but no gradient is back-propagated through the teacher <ref type="bibr" target="#b9">Chen &amp; He (2020)</ref>. These methods are effective, but there is no clear understanding of why and how they avoid collapse. Alternatively, the images can be represented as bags of word over a dictionary of visual features, which effectively prevents collapse. In OBoW <ref type="bibr">Gidaris et al. (2020)</ref> and <ref type="bibr">Gidaris et al. (2021)</ref> the dictionary is obtained by off-line or on-line clustering. By contrast, our method explicitly prevents collapse in the two branches independently, which removes the requirement for shared weights and identical architecture, opening the door to the application of joint-embedding SSL to multi-modal signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Information maximization methods. A principle to prevent collapse is to maximize the information content of the embeddings. Two such methods were recently proposed: W-MSE <ref type="bibr" target="#b15">Ermolov et al. (2021)</ref> and Barlow Twins <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>. In W-MSE, an extra module transforms the embeddings into the eigenspace of their covariance matrix (whitening or Karhunen-Lo?ve transform), and forces the vectors thereby obtained to be uniformly distributed on the unit sphere. In Barlow Twins, a loss term attempts to make the normalized cross-correlation matrix of the embedding vectors from the two branches to be close to the identity. Both methods attempt to produce embedding variables that are decorrelated from each other, thus preventing an informational collapse in which the variables carry redundant information. Because all variables are normalized over a batch, there is no incentive for them to shrink nor expand. This seems to sufficient to prevent collapse. Our method borrows the decorrelation mechanism of Barlow Twins. But it includes an explicit variance-preservation term for each variable of the two embeddings and thus does not require any normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VICREG: DETAILED DESCRIPTION</head><p>VICReg follows recent trends in self-supervised learning <ref type="bibr" target="#b6">Caron et al. (2020)</ref>; Grill et al. (2020); Chen &amp; He (2020); <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>; <ref type="bibr" target="#b7">Chen et al. (2020a)</ref> and is based on a joint embedding architecture. Contrary to many previous approaches, our architecture may be completely symmetric or completely asymmetric with no shared structure or parameters between the two branches. In most of our experiments, we use a Siamese net architecture in which the two branches are identical and share weights. Each branch consists of an encoder f ? that outputs the representations (used for downstream tasks), followed by an expander h ? that maps the representations into an embedding space where the loss function will be computed. The role of the expander is twofold: (1) eliminate the information by which the two representations differ, (2) expand the dimension in a non-linear fashion so that decorrelating the embedding variables will reduce the dependencies (not just the correlations) between the variables of the representation vector. The loss function uses a term s that learns invariance to data transformations and is regularized with a variance term v that prevents norm collapse and a covariance term c that prevents informational collapse by decorrelating the different dimensions of the vectors. After pretraining, the expander is discarded and the representations of the encoder are used for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">METHOD</head><p>Given an image i sampled from a dataset D, two transformations t and t are sampled from a distribution T to produce two different views x = t(i) and x = t (i) of i. These transformations are random crops of the image, followed by color distortions. The distribution T is described in Appendix C. The views x and x are first encoded by f ? into their representations y = f ? (x) and y = f ? (x ), which are then mapped by the expander h ? onto the embeddings z = h ? (y) and z = h ? (y ). The loss is computed at the embedding level on z and z .</p><p>We describe here the variance, invariance and covariance terms that compose our loss function. The images are processed in batches, and we denote Z = [z 1 , . . . , z n ] and Z = [z 1 , . . . , z n ] the two batches composed of n vectors of dimension d, of embeddings coming out of the two branches of the siamese architecture. We denote by z j the vector composed of each value at dimension j in all vectors in Z. We define the variance regularization term v as a hinge function on the standard deviation of the embeddings along the batch dimension:</p><formula xml:id="formula_1">v(Z) = 1 d d j=1 max(0, ? ? S(z j , )),<label>(1)</label></formula><p>where S is the regularized standard deviation defined by:</p><formula xml:id="formula_2">S(x, ) = Var(x) + ,<label>(2)</label></formula><p>? is a constant target value for the standard deviation, fixed to 1 in our experiments, is a small scalar preventing numerical instabilities. This criterion encourages the variance inside the current batch to be equal to ? along each dimension, preventing collapse with all the inputs mapped on the same vector. Using the standard deviation and not directly the variance is crucial. Indeed, if we take S(x) = Var(x) in the hinge function, the gradient of S with respect to x becomes close to 0 when x is close tox. In this case, the gradient of v also becomes close to 0 and the embeddings collapse. We define the covariance matrix of Z as:</p><formula xml:id="formula_3">C(Z) = 1 n ? 1 n i=1 (z i ?z)(z i ?z) T , wherez = 1 n n i=1 z i .<label>(3)</label></formula><p>Inspired by Barlow Twins <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>, we can then define the covariance regularization term c as the sum of the squared off-diagonal coefficients of C(Z), with a factor 1/d that scales the criterion as a function of the dimension:</p><formula xml:id="formula_4">c(Z) = 1 d i =j [C(Z)] 2 i,j .<label>(4)</label></formula><p>This term encourages the off-diagonal coefficients of C(Z) to be close to 0, decorrelating the different dimensions of the embeddings and preventing them from encoding similar information. Decorrelation at the embedding level ultimately has a decorrelation effect at the representation level, which is a non trivial phenomenon that we study in Appendix D. We finally define the invariance criterion s between Z and Z as the mean-squared euclidean distance between each pair of vectors, without any normalization:</p><formula xml:id="formula_5">s(Z, Z ) = 1 n i z i ? z i 2 2 .<label>(5)</label></formula><p>The overall loss function is a weighted average of the invariance, variance and covariance terms:</p><formula xml:id="formula_6">(Z, Z ) = ?s(Z, Z ) + ?[v(Z) + v(Z )] + ?[c(Z) + c(Z )],<label>(6)</label></formula><p>where ?, ? and ? are hyper-parameters controlling the importance of each term in the loss. In our experiments, we set ? = 1 and perform a grid search on the values of ? and ? with the base condition ? = ? &gt; 1. The overall objective function taken on all images over an unlabelled dataset D is given by:</p><formula xml:id="formula_7">L = I?D t,t ?T (Z I , Z I ),<label>(7)</label></formula><p>where Z I and Z I are the batches of embeddings corresponding to the batch of images I transformed by t and t . The objective is minimized for several epochs, over the encoder parameters ? and expander parameters ?. We illustrate the architecture and loss function of VICReg in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMPLEMENTATION DETAILS</head><p>Implementation details for pretraining with VICReg on the 1000-classes ImagetNet dataset without labels are as follows. Coefficients ? and ? are 25 and ? is 1 in Eq. (6), and is 0.0001 in Eq. (1). We give more details on how we choose the coefficients of the loss function in Appendix D.4. The encoder network f ? is a standard ResNet-50 backbone He et al. <ref type="formula" target="#formula_1">(2016)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In this section, we evaluate the representations obtained after self-supervised pretraining of a ResNet-50 He et al. <ref type="formula" target="#formula_1">(2016)</ref> backbone with VICReg during 1000 epochs, on the training set of ImageNet, using the training protocol described in section 4. We also pretrain on pairs of image and text data and evaluate on retrieval tasks on the MS-COCO dataset.  <ref type="bibr" target="#b13">Deng et al. (2009)</ref> linear evaluation protocol, we train a linear classifier on top of the frozen representations of the ResNet-50 backbone pretrained with VICReg. We also evaluate the performance of the backbone when fine-tuned with a linear classifier on a subset of ImageNet's training set using 1% or 10% of the labels, using the split of <ref type="bibr" target="#b7">Chen et al. (2020a)</ref>. We give implementation details about the optimization procedure for these tasks in Appendix C. We have applied the training procedure described in section 4 with three different random initialization. The numbers reported in <ref type="table" target="#tab_2">Table 1</ref> for VICReg are the mean scores, and we have observed that the difference between worse and best run is lower than 0.1% accuracy for linear classification, which shows that VICReg is a very stable algorithm. Lack of time has prevented us from doing the same for the semi-supervised classification experiments, and the experiments of section 5.2 and 6, but we expect similar conclusion to hold. We compare in <ref type="table" target="#tab_2">Table 1</ref> our results on both tasks against other methods on the validation set of ImageNet. The performance of VICReg is on par with the state of the art without using the negative pairs of SimCLR, the clusters of SwAV, the bag-of-words representations of OBoW, or any asymmetric networks architectural tricks such as the momentum encoder of BYOL and the stop-gradient operation of SimSiam. The performance is comparable to that of Barlow Twins, which shows that VICReg's more explicit way of constraining the variance and comparing views has the same power than maximizing cross-correlations between pairs of twin dimensions. The main advantage of VICReg is the modularity of its objective function and the applicability to multi-modal setups.  <ref type="formula" target="#formula_1">2017)</ref> with a R50-FPN backbone. We report the performance in <ref type="table" target="#tab_3">Table 2</ref>,  VICReg performs on par with most concurrent methods, and better than Barlow Twins, across all classification tasks, but is slightly behind the top-3 on detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRANSFER TO OTHER DOWNSTREAM TASKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MULTI-MODAL PRETRAINING ON MS-COCO</head><p>One fundamental difference of VICReg compared to Barlow Twins is the way the branches are regularized. In VICReg, both branches are regularized independently, as the covariance term is applied on each branch separately, which works better in the scenarios where the branches are completely different, have different types of architecture and process different types of data. Indeed, the statistics of the output of the two branches can be very different, and the amount of regularization required for each may vary a lot. In Barlow Twins, the regularization is applied on the cross-correlation matrix, which favors the scenarios where the branches produce outputs with similar statistics. We demonstrate the capabilities of VICReg in a multi-modal experiment where we pretrain on pairs of images and corresponding captions on the MS-COCO dataset. We regularize each branch with a different coefficient, which is not possible with Barlow Twins, and we show that VICReg outperforms Barlow Twins on image and text retrieval downstream tasks. <ref type="table" target="#tab_4">Table 3</ref> reports the performance of VICReg against the contrastive loss proposed by VSE++ Faghri et al. <ref type="formula" target="#formula_1">(2018)</ref>, and against Barlow Twins, in the identical setting proposed in Faghri et al. <ref type="bibr">(2018)</ref>. VICReg outperforms the two by a significant margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANALYSIS</head><p>In this section we study how the different components of our method contribute to its performance, as well as how they interact with components from other self-supervised methods. We also evaluate different scenarios where the branches have different weights and architecture. All reported results are obtained on the linear evaluation protocol, using a ResNet-50 backbone if not mentioned otherwise, and 100 epochs of pretraining, which gives results consistent with those obtained with 1000 epochs of pretraining. The optimization setting used for each experiment is described in Appendix C.</p><p>Asymmetric networks. We study the impact of different components used in asymmetric architectures and the effects of adding variance and covariance regularization, in terms of performance and training stability. Starting from a simple symmetric architecture with an encoder and an expander without batch normalization, which correspond to VICReg without batch normalization in the expander, we progressively add batch normalization in the inner layers of the expander, a predictor, a stop-gradient operation and a momentum encoder. We use the training protocol and architecture of SimSiam Chen &amp; He (2020) when a stop-gradient is used and the training protocol and architecture of BYOL Grill et al. (2020) when a momentum encoder is used. The predictor as used in SimSiam and BYOL is a learnable module g ? that predicts the embedding of a view given the embedding of the other view of the same image. If z and z are the embeddings of two views of an image, then p = g ? (z) and p = g ? (z ) are the predictions of each view. The invariance loss function of Eq. <ref type="formula" target="#formula_5">(5)</ref> is now computed between a batch of embeddings Z = [z 1 , . . . , z n ] and the corresponding batch of predictions P = [p 1 , . . . , p n ], then symmetrized:</p><formula xml:id="formula_8">s(Z, Z , P, P ) = 1 2n i D(z i ? p i ) + 1 2n i D(z i ? p i ),<label>(8)</label></formula><p>where D is a distance function that depends on the method used. BYOL uses the mean square error between l 2 -normalized vectors, SimSiam uses the negative cosine similarity loss and VICReg uses the mean square error without l 2 -normalization. The variance and covariance terms are regularizing the output Z and Z of the expander, which we empirically found to work better than regularizing the output of the predictor. We compare different settings in <ref type="table" target="#tab_5">Table 4</ref>, based on the default data augmentation, optimization and architecture settings of the original BYOL, SimSiam and VICReg methods. In all settings, the absence of BN indicates that BN is also removed in the predictor when one is used.</p><p>We analyse first the impact of variance regularization (VR) in the different settings. When using VR, adding a predictor (PR) to VICReg does not lead to a significant change of the performance, which indicates that PR is redundant with VR. In comparison, without VR, the representations collapse, and both stop-gradient (SG) and PR are necessary. Batch normalization in the inner layers of the expander (BN) in VICReg leads to a 1.0% increase in the performance, which is not a big improvement considering that SG and PR without BN is performing very poorly at 35.1%. Finally, incorporating VR with SG or ME further improves the performance by small margins of respectively 0.2% and 0.9%, which might be explained by the fact that these architectural tricks that prevent collapse are not perfectly maintaining the variance of the representations, i.e. very slow collapse is happening with these methods. We explain this intuition by studying the evolution of the standard deviation of the representations during pretraining for BYOL and SimSiam in Appendix D.</p><p>We then analyse the impact of adding additional covariance regularization (CR) in the different settings, along with variance regularization. We found that optimization with SG and CR is hard, even if our analysis of the average correlation coefficient of the representations during pretraining in Appendix D shows that both fulfill the same objective.</p><p>The performance of BYOL and SimSiam slightly drops compared to VR only, except when PR is removed, where SG becomes useless. BN is still useful and improves the performance by 1.3%. Finally with CR, PR does not harm the performance and even improves it by a very small margin. VICReg+PR with 1000 epochs of pretraining exactly matches the score of VICReg (73.2% on linear classification).</p><p>Weight sharing. Contrary to most self-supervised learning approaches based on Siamese architectures, VICReg has several unique properties: (1) weights do not need to be shared between the branches, each branch's weights are updated independently of the other branch's weights;</p><p>(2) the branches are regularized independently, the variance and covariance terms are computed on each branch individually; (3) no predictor is necessary unlike with methods where one branch predicts outputs of the other branch. We compare the robustness of VICReg against other methods in different scenarios where the weights of the branches can be shared (SW), not shared (DW), and where the encoders can have different architectures (DA). Among other self-supervised methods, SimCLR and Barlow Twins are the only ones that can handle these scenarios. The asymmetric methods that are based on a discrepancy between the branches requires either the architecture or the weights to be shared between the branches. The performance drops by 2.1% with VICReg and 4.5% with Barlow Twins, between the shared weights scenario (SW) and the different weight scenario (DW). The difference between VICReg and Barlow Twins is also significant in scenarios with different architectures, in particular VICReg performs better than Barlow Twins by 2.8% with ResNet-50/ResNet-101 and better by 2.3% with ResNet-50/ViT-S <ref type="bibr">Dosovitskiy et al. (2021)</ref>. This shows that VICReg is more robust than Barlow Twins in these kind of scenarios. The performance of SimCLR remains stable across scenarios, but is significantly worse than the performance of VICReg. Importantly, the ability of VICReg to function with different parameters, architectures, and input modalities for the branches widens the applicability to joint-embedding SSL to many applications, including multi-modal signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We introduced VICReg, a simple approach to self-supervised learning based on a triple objective: learning invariance to different views with a invariance term, avoiding collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a covariance regularization term. VICReg achieves results on par with the state of the art on many downstream tasks, but is not subject to the same limitations as most other methods, particularly because it does not require the embedding branches to be identical or even similar. We compare here VICReg with other methods in terms of methodology, and we discuss the mechanisms used by these methods to avoid collapse and to learn representations, and how they relate to VICReg. We synthesize and illustrate the differences between these methods in <ref type="figure" target="#fig_4">Figure 2</ref>.</p><p>Relation to Barlow Twins <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>. VICReg uses the same decorrelation mechanism as Barlow Twins, which consists in penalizing the off-diagonal terms of a covariance matrix computed on the embeddings. However, Barlow Twins uses the cross-correlation matrix where each entry in the matrix is a cross-correlation between two vectors z i and z j , from the two branches of the siamese architecture. Instead of using cross-correlations, we simply use the covariance matrix of each branch individually, and the variance term of VICReg allows us to get rid of standardization. Indeed, Barlow Twins forces the correlations between pairs of vectors z i and z i from the same dimension i to be 1. Without normalization, this target value of 1 becomes arbitrary and the vectors take values in a wider range. Moreover, there is an undesirable phenomenon happening in Barlow Twins, the embeddings before standardization can shrink and become constant to numerical precision, which could cause numerical instabilities. In practice, this is solved by adding a constant scalar in the denominator of standardization of the embeddings. Without normalization, VICReg naturally avoids this edge case.</p><p>Relation to W- <ref type="bibr">MSE Ermolov et al. (2021)</ref>. The whitening operation of W-MSE consists in computing the inverse covariance matrix of the embeddings and use its square root as a whitening operator on the embeddings. Using this operator has two downsides. First, matrix inversion is a very costly and potentially unstable operation. VICReg does not need to inverse the covariance matrix. Second, as mentioned in <ref type="bibr" target="#b15">Ermolov et al. (2021)</ref> the whitening operator is constructed over several consecutive iteration batches and therefore might have a high variance, which biases the estimation of the meansquared error. This issue is overcome in practice by a batch slicing strategy, where the whitening operator is computed over randomly constructed sub-batches. VICReg does not apply any operator on the embeddings, but instead regularizes the variance and covariance of the embeddings using an additional constraint.</p><p>Relation to BYOL and SimSiam Grill et al. (2020); <ref type="bibr" target="#b9">Chen &amp; He (2020)</ref>. The core components that avoid collapse in BYOL and SimSiam are the average moving weights and the stop-gradient operation on one side of their asymmetric architecture, which play the role of the repulsive term used in other methods. Our experiments in Appendix D.8 show that in addition to preventing collapse, these components also have a decorrelation effect. In addition, we have conducted the following experiment: We compute the correlation matrix of the final representations obtained with SimSiam, BYOL, VICReg and VICReg without covariance regularization. We measure the average correlation coefficient and observe that this coefficient is much smaller for SimSiam, BYOL and VICReg, compared to VICReg without covariance regularization. We observe in <ref type="figure" target="#fig_10">Figure 5</ref> that even without covariance regularization, SimSiam and BYOL naturally minimize the average correlation coefficient of the representations. VICReg replaces the moving average weights and the stop-gradient operation, which are architectural trick that require some dependency between the branches, by an explicit constraint on the variance and the covariance of both embeddings separately, which achieves the same goal of decorrelating the representations and avoiding collapse, while being clearer, more interpretable, and working with independent branches.  <ref type="bibr">(2021)</ref>. Contrastive and clustering based self-supervised algorithms rely on direct comparisons between elements of negative pairs. In the case of SimCLR, the negative pairs involve embeddings mined from the current batch, and large batch sizes are required. Despite the fact that SwAV computes clusters using elements in the current batch, it does not seem to have the same dependency on batch size. However, it still requires a lot of prototype vectors for negative comparisons between embeddings and codes. VICReg eliminates the negative comparisons and replace them by an explicit constraint on the variance of the embeddings, which efficiently plays the role of a negative term between the vectors. SwAV can also be interpreted as a distillation method, where a teacher network produces quantized vectors, used as target for a student network. Ensuring an equal partition of the quantized vectors in different bins or clusters effectively prevents collapse. OBOW can also be interpreted under the same framework. The embeddings are bag-of-words over a vocabulary of visual features, and collapse is avoided by the underlying quantization operation.  <ref type="formula" target="#formula_2">(2020)</ref>; <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref>. Two random crops from the input image are sampled and resized to 224 ? 224, followed by random horizontal flip, color jittering of brightness, contrast, saturation and hue, Gaussian blur and random grayscale. Each crop is normalized in each color channel using the ImageNet mean and standard deviation pixel values. In more details, the exact set of augmentations is based on BYOL Grill et al. (2020) data augmentation pipeline but is symmetrised. The following operations are performed sequentially to produce each view:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to</head><formula xml:id="formula_9">(Z, Z?) (a) VICReg (Z) X Y Z X? Y? Z? (Z?) (Z) (Z?) (b) Barlow Twins X Y Z X? Y? Z? (Z, Z?) B-norm (Z, Z?) ? !" ! ? # ?? #" ? # ? # ! ! X Y Z X? Y? Z? (Z, Z?) Batch slicing ? # ? # ! ! (c) W-MSE F-norm + PCA Batch slicing B-norm F-norm + PCA (Z, Z?) (d) BYOL X Y Z X? Y? Z? ! ? # ? # ! ema ! ! ema F-norm $ P F-norm (e) SimSiam X Y Z X? Y? Z? ? ! ? ! " " X Y Z X? Y? Z? InfoNCE(Z, Z?) ? ! ? ! " " (f) SimCLR F-norm F-norm X Y Z X? Y? Z? (Z, Z?) ? ! ? ! " " (g) SwAV Quantization F-norm F-norm X Y Z X? Y # Z? ce(Z, Z?) BoW " (h) OBoW</formula><p>? Random cropping with an area uniformly sampled with size ratio between 0.08 to 1.0, followed by resizing to size 224 ? 224. RandomResizedCrop(224, scale=(0.08, 0.1)) in PyTorch. ? Random horizontal flip with probability 0.5.</p><p>? Color jittering of brightness, contrast, saturation and hue, with probability 0.8.</p><p>ColorJitter(0.4, 0.4, 0.2, 0.1) in PyTorch. ? Grayscale with probability 0.2.</p><p>? Gaussian blur with probability 0.5 and kernel size 23.</p><p>? Solarization with probability 0.1.</p><p>? color normalization with mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 IMAGENET EVALUATION</head><p>Linear evaluation. We follow standard procedure and train a linear classifier on top of the frozen representations of a ResNet-50 pretrained with VICReg. We use the SGD optimizer with a learning rate of 0.02, a weight decay of 10 ?6 , a batch size of 256, and train for 100 epochs. The learning rate follows a cosine decay. The training data augmentation pipeline is composed of random cropping and resize of ratio 0.2 to 1.0 with size 224 ? 224, and random horizontal flips. During evaluation the validation images are simply center cropped and resized to 224 ? 224.</p><p>Semi-supervised evaluation. We train a linear classifier and fine-tune the representations using 1 and 10% of the labels. We use the SGD optimizer with no weight decay and a batch size of 256, and train for 20 epochs. We perform a grid search on the values of the encoder and linear head learning rates. In the 10% of labels case, we use a learning rate of 0.01 for the encoder and 0.1 for the linear head. In the 1% of labels case we use 0.03 for the encoder and 0.08 for the linear head. The two learning rates follow a cosine decay schedule. The training data and validation augmentation pipelines are identical to the linear evaluation data augmentation pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 TRANSFER LEARNING</head><p>We use the VISSL library Goyal et al.  R-CNN C-4 backbone for 24K iterations with a batch size of 16. The backbone is initialized with our pretrained ResNet-50 backbone. We use a learning rate of 0.1, divided by 10 at iteration 18K and 22K, a linear warmup with slope of 0.333 for 1000 iterations, and a region proposal network loss weight of 0.2. For COCO we use Mask R-CNN FPN backbone for 90K iterations with a batch size of 16, a learning rate of 0.04, divided by 10 at iteration 60K and 80K and with 50 warmup iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 ANALYSIS</head><p>We give here implementation details on the results of <ref type="table" target="#tab_5">Table 4</ref> with BYOL and SimSiam, as well as the default setup for VICReg with 100 epochs of pretraining, used in all our ablations included in Appendix D. For both BYOL and SimSiam experiments, the variance criterion has coefficient ? = 1 and the covariance criterion has coefficient ? = 0.01, the data augmentation pipeline and the architectures of the expander and predictor exactly follow the pipeline and architectures described in their paper. The linear evaluation setup of each methods follows closely the setup described in the original papers.</p><p>BYOL setup. We use our own BYOL implementation in PyTorch, which outperforms the original implementation for 100 epochs of pretraining (69.3% accuracy on the linear evaluation protocol against 66.5% for the original implementation) and matches its performance for 1000 epochs of pretraining. We use the LARS optimizer <ref type="bibr" target="#b33">You et al. (2017)</ref>, with a learning rate of base_lr * batch_size/256 where base_lr = 0.45, and batch_size = 4096, a weight decay of 10 ?6 , an eta value of 0.001 and a momentum of 0.9, for 100 epoch of pretraining with 10 epochs of warmup. The learning rate follows a cosine decay schedule. The initial value of the exponential moving average factor is 0.99 and follows a cosine decay schedule.</p><p>SimSiam setup. We use our own implementation of SimSiam, which reproduces exactly the performance reported in the paper <ref type="bibr" target="#b9">Chen &amp; He (2020)</ref>. We use SGD with a learning rate of base_lr * batch_size/256 where base_lr = 0.05, batch_size = 2048, with a weight decay of 0.0001 and a momentum of 0.9 for 100 epochs of pretraining and 10 epochs of warmup. The learning rate of the encoder and the expander follow a cosine decay schedule while the learning rate of the predictor is kept fixed.</p><p>VICReg setup. The setting of VICReg's experiments is identical to the setting described in section 4.2, except that the number of pretraining epochs is 100 and the base learning rate is 0.3. The base learning rates used for the batch size study are 0.8, 0.5 and 0.4 for batch size 128, 256 and 512 respectively, and 0.3 for all other batch sizes. When a predictor is used, it has a similar architecture as the expander described in section 4.2, but with 2 layers instead of 3, which gives better results in practice. <ref type="table" target="#tab_10">Table 9</ref> reports the performance of VICReg on linear classification with large ResNet architectures. We focus on the wider family of ResNet <ref type="bibr" target="#b34">Zagoruyko &amp; Komodakis (2016)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 OTHER RESNET ARCHITECTURES</head><p>and multiple by 2 or 4 the number of filters in every convolutional layer, which also has the effect of multiplying the dimensionality of the representations. Second, as originally proposed in <ref type="bibr" target="#b34">Zagoruyko &amp; Komodakis (2016)</ref>, we only multiply the number of filters in the bottleneck layers, which does not increases the dimensionality of the representations. We call this architecture Narrow ResNet (with prefix N-in <ref type="table" target="#tab_10">Table 9</ref>). The main observation we make is the dependency of VICReg on the dimensionality of the representation. Using the narrow architecture, the performance of VICReg, jumps from 73.2% top-1 accuracy on linear classification with a ResNet-50, to 74.7% with Narrow ResNet-50 (x2), which is a 1.5% improvement and 76.0% with Narrow ResNet-50 (x4), which is a 2.8% improvement. We observe a similar trend going from ResNet-50 to , which is a 2.3% improvement but the performance completely saturates with , which is a 0.1% improvement over . <ref type="table" target="#tab_2">Table 10</ref> reports the performance of VICReg on semi-supervised classification with large ResNet architectures. VICReg combined with a ResNet-50 (x2) outperforms the current state-of-the-art methods BYOL and SimCLR, using this encoder architecture. Our largest We demonstrate the ability of VICReg to function in a setting where the branches have different architectures by pretraining on the ESC-50 audio dataset <ref type="bibr" target="#b19">Piczak (2015)</ref>, which is an environmental sound classification dataset with 50 classes. We jointly embedded a raw audio time-series representation on one branch, with its corresponding time-frequency representation on the other branch. We use the standard split of ESC-50 <ref type="bibr" target="#b19">Piczak (2015)</ref>, composed of 1600 training audio samples and 400 validation sample. The raw audio encoder is a 1-dimensional ResNet-18 with output dimension 384. The time-frequency image representation is the mel spectrogram with 1 channel of the raw audio, that we normalize between 0 and 1, and that is processed be a ResNet-18 with output dimension of 512. We use the AdamW optimizer with learning rate 0.0005 for 100 epochs of pretraining. <ref type="table" target="#tab_7">Table 6</ref> reports the performance of a linear classifier trained one the frozen representations obtained with VICReg and Barlow Twins to a simple supervised baseline where we train a ResNet-18 on the time-frequency representation in a supervised way. VICReg performs better by 5.7% than our supervised baseline, and better by 3.0% than Barlow Twins. We give more details in Appendix ??. Current best approaches that report around 95% accuracy on this task uses tricks such as heavy data augmentation or pretraining on larger audio and video datasets. With this experiment, our purpose is not to push the state of the art on ESC-50, but merely to demonstrate the applicability of VICReg to settings with multiple architectures and input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 K-NEAREST-NEIGHBORS</head><p>Following recent protocols <ref type="bibr" target="#b6">Caron et al. (2020)</ref>; <ref type="bibr" target="#b27">Wu et al. (2018)</ref>; <ref type="bibr" target="#b37">Zhuang et al. (2019)</ref>, we evaluate the learnt representations using K-nearest-neighbors classifiers built on the training set of ImageNet and evaluated on the validation set of ImageNet. We report the results with K=20 and K=200 in <ref type="table" target="#tab_2">Table 11</ref>. VICReg performs slightly lower than other methods in the 20-NN case but remains competitive in the 200-NN case. These results with K-NN classifiers demonstrate the potential applicability of VICReg to downstream tasks based on nearest neighbors search, such as content retrieval in images or videos.</p><p>D.4 LOSS FUNCTION COEFFICIENTS. <ref type="table" target="#tab_8">Table 7</ref> reports the performance for various values of the loss term coefficients in Eq. (6). Without variance regularization the representations immediately collapse to a single vector and the covariance term, which has no repulsive effect preventing collapse, has no impact. The invariance term is absolutely necessary and without it the network can not learn any good representations. By simply using the invariance term and variance regularization, which is a very simple baseline, VICReg still reaches an accuracy of 57.5%. These results show that variance and covariance regularizations have complementary effects, and that both are required.</p><p>On ImageNet, we choose the final coefficients the following way. First, we have empirically found that using very different values for ? and ?, or taking ? = ? with ? &gt; ? leads to unstable training. On the other hand taking ? = ? and picking ? &lt; ? leads to stable convergence, with the exact value picked for mu having very limited influence on the final linear classification accuracy. We have found  that setting lambda = mu = 25 and nu = 1 works best (by a small margin) for Imagenet but we have also obtained excellent results on MNIST and Cifar-10 and 100 using these exact same values. We could easily have tuned these parameters by cross-validation on the validation sets of these two smaller datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 NORMALIZATIONS</head><p>VICReg is the first self-supervised method for joint-embedding architectures we are aware of that does not require normalization. Contrary to SimSiam, W-MSE, SwAV and BYOL, and others, the embedding vectors are not projected on the unit sphere. Contrary to Barlow Twins, they are not standardized (equivalent to batch normalization without the adaptive parameters). <ref type="table" target="#tab_9">Table 8</ref> shows that the best settings do not involve any normalization of the embeddings, whether it is batch-wise or feature-wise (as in l 2 normalization). Whenever the embeddings are standardized (lines 3 and 5 in the table) the covariance matrix of Eq.</p><p>(3) becomes the normalized auto-correlation matrix with coefficients between -1 and 1. This hurts the accuracy by 0.2%. We observe that when unconstrained, the coefficients in the covariance matrix take values in a wider range, which seems to facilitate the training process. Standardization is still an important component that helps stabilize the training when used in the hidden layers of the expander, and the performance drops by 1.2% when it is removed. Projecting the embeddings on the unit sphere implicitly constrains their standard deviation along the batch dimension to be 1/ ? d, where d is the dimension of the vectors. We change the invariance term of Eq. (5) to be the mean square error between l 2 -normalized vectors, and the target ? in the variance term of Eq. (1) is set to 1/ ? d instead of 1, forcing the standard deviation to get closer to 1/ ? d, and the vectors to be spread out on the unit sphere. This puts a lot more constraints on the network and the performance drops by 3.5%.     VICReg borrows the decorrelation mechanism of Barlow Twins <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref> and we observe that it therefore has the same dependency on the dimensionality of the expander network. <ref type="table" target="#tab_2">Table 12</ref> reports the impact of the width and depth of the expander network. The dimensionality corresponds the number of hidden and output units in the expander network during pretraining. As the dimensionality increases, the performance dramatically increases from 55.9% top-1 accuracy on linear evaluation with a dimensionality of 256, to 68.8% with dimensionality 16384. The performance tends to saturate as the difference between dimensionality 8192 and 16384 is only of 0.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 BATCH SIZE</head><p>Contrastive methods suffer from the need of a lot of negative examples which can translate into the need for very large batch sizes <ref type="bibr" target="#b7">Chen et al. (2020a)</ref>. <ref type="table" target="#tab_2">Table 13</ref> reports the performance on linear classification when the size of the batch varies between 128 and 4096. For each value of batch size, we perform a grid search on the base learning rate described in Appendix C.4. We observe a 0.7% and 1.2% drop in accuracy with small batch size of 256 and 128 which is comparable with the robustness to batch size of Barlow Twins <ref type="bibr" target="#b35">Zbontar et al. (2021)</ref> and SimSiam <ref type="bibr" target="#b9">Chen &amp; He (2020)</ref>, and a 0.8% drop with a batch size of 4096, which is reasonable and allows our method to be very easily parallelized on multiple GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.8 COMBINATION WITH BYOL AND SIMSIAM</head><p>BYOL <ref type="bibr" target="#b21">Grill et al. (2020)</ref> and SimSiam Chen &amp; He (2020) rely on a effective but difficult to interpret mechanism for preventing collapse, which may lead to instabilities during the training. We incorporate our variance regularization loss into BYOL and SimSiam and show that it helps stabilize the training and offers a small performance improvement. For both methods, the results are obtained using our own implementation and the exact same data augmentation and optimization settings as in their original paper. The variance and covariance regularization losses are incorporated with a factor of ? = 1 for variance and ? = 0.01 for covariance. We report in <ref type="figure" target="#fig_8">Figure 3</ref> the improvement obtained over these methods on the linear evaluation protocol for different number of pre-training epochs. For BYOL the improvement is of 0.9% with 100 epochs and becomes less significant as the number of pre-training epochs increases with a 0.2% improvement with 1000 epochs. This indicates that variance regularization makes BYOL converge faster. In SimSiam the improvement is not as significant. We plot in <ref type="figure" target="#fig_9">Figure 4</ref> the evolution of the standard deviation computed along each dimension and averaged across the dimensions of the representation and the embeddings, during BYOL and SimSiam pretraining. For both methods, the standard deviation computed on the embeddings perfectly matches 1/ ? d where d is the dimension of the embeddings, which indicates that the embeddings are perfectly spread-out across the unit sphere. This translates in an increased standard deviation at the representation level, which seems to be correlated to the performance improvement. We finally study in <ref type="figure" target="#fig_10">Figure 5</ref> the evolution of the average correlation coefficient, during pretraining of BYOL and SimSiam, with and without variance and covariance regularization. The average correlation coefficient is computed by averaging the off-diagonal coefficients of the </p><formula xml:id="formula_11">1 2d(d ? 1) i =j C(Y ) 2 i,j + C(Y ) 2 i,j ,<label>(9)</label></formula><p>where Y and Y are the standardized representations and C is defined in Eq. (3). In BYOL this coefficient is much lower using covariance regularization, which translate in a small improvement of the performance, according to <ref type="table" target="#tab_5">Table 4</ref>. We do not observe the same improvement in SimSiam, both in terms of correlation coefficient, and in terms of performance on linear classification. The average correlation coefficient is correlated with the performance, which motivates the fact that decorrelation and redundancy reduction are core mechanisms for learning self-supervised representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E RUNNING TIME</head><p>We report in <ref type="table" target="#tab_2">Table 14</ref>, the running time of VICReg in comparison with other methods. All methods are run by us on 32 Tesla V100 GPUs. Each method offers a different trade-off between running time, memory and performance. SwAV is a very fast algorithm which use less memory and run faster than the other methods but with a lower performance, multi-crop helps the performance at the cost of additional compute and memory usage. BYOL has the highest memory requirement, which is due to the need of storing the target network weights. Finally, Barlow Twins and VICReg offer an interesting trade-off, consuming less memory than BYOL and SwAV with multi-crop, and running faster than SwAV with multi-crop, but with a slightly worse performance. The difference of 1h running time between Barlow Twins and VICReg is probably due to implementation details not related to the method.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>with 2048 output units. The expander h ? is composed of two fully-connected layers with batch normalization (BN)Ioffe &amp;  Szegedy (2015)  and ReLU, and a third linear layer. The sizes of all 3 layers were set to 8192. As with Barlow Twins, performance improves when the size of the expander layers is larger than the dimension of the representation. The impact of the expander dimension on performance is studied in Appendix D. The training protocol follows those of BYOL and Barlow Twins: LARS optimizer<ref type="bibr" target="#b33">You et al. (2017);</ref> Goyal et al. (2017)  run for 1000 epochs with a weight decay of 10 ?6 and a learning rate lr = batch_size/256 ? base_lr, where batch_size is set to 2048 by default and base_lr is a base learning rate set to 0.2. The learning rate follows a cosine decay schedule<ref type="bibr" target="#b17">Loshchilov &amp; Hutter (2017)</ref>, starting from 0 with 10 warmup epochs and with final value of 0.002.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Following</head><label></label><figDesc>the setup from Misra &amp; Maaten (2020), we train a linear classifier on top of the frozen representations learnt by our pretrained ResNet-50 backbone on a variety of different datasets: the Places205 Zhou et al. (2014) scene classification dataset, the VOC07 Everingham et al. (2010) multi-label image classification dataset and the iNaturalist2018 Horn et al. (2018) fine-grained image classification dataset. We then evaluate the quality of the representations by transferring to other vision tasks including VOC07+12 Everingham et al. (2010) object detection using Faster R-CNN Ren et al. (2015) with a R50-C4 backbone, and COCO Lin et al. (2014) instance segmentation using Mask-R-CNN He et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>VICReg pytorch pseudocode. # f: encoder network, lambda, mu, nu: coefficients of the invariance, variance and covariance losses, N: batch size , D: dimension of the representations # mse_loss: Mean square error loss function, off_diagonal: off-diagonal elements of a matrix, relu: ReLU activation function for x in loader: # load a batch with N samples # two randomly augmented versions of x x_a, x_b = augment(x) # compute representations z_a = f(x_a) # N x D z_b = f(x_b) # N x D # invariance loss sim_loss = mse_loss(z_a, z_b) # variance loss std_z_a = torch.sqrt(z_a.var(dim=0) + 1e-04) std_z_b = torch.sqrt(z_b.var(dim=0) + 1e-04) std_loss = torch.mean(relu(1 -std_z_a)) + torch.mean( relu(1 -std_z_b)) # covariance loss z_a = z_a -z_a.mean(dim=0) z_b = z_b -z_b.mean(dim=0) cov_z_a = (z_a.T @ z_a) / (N -1) cov_z_b = (z_b.T @ z_b) / (N -1) cov_loss = off_diagonal(cov_z_a).pow_(2).sum() / D + off_diagonal(cov_z_b).pow_(2).sum() / D # loss loss = lambda * sim_loss + mu * std_loss + nu * cov_loss # optimization step loss.backward() optimizer.step() B RELATION TO OTHER SELF-SUPERVISED METHODS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>SimCLR, SwAV and OBoW Caron et al. (2020); Chen et al. (2020a); Gidaris et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Conceptual comparison between different self-supervised methods. The inputs X and X are fed to an encoder f with weights ?. The representations Y and Y are further processed by a network h with weights ?. h can be a projector (narrowing trapeze) that reduces the dimensionality of the representations, or an expander (widening trapeze) that increases their dimensionality. A criterion is finally applied on the embeddings Z and Z . VICReg (a) works when both branches have encoders f and f with different architectures and sets of weights ? and ? . Each branch's variance and covariance are regularized by regularizers v and c, and the distance between both branches is minimized with a mean-squared error loss s. Barlow Twins (b) uses a loss c to decorrelate pairs of different dimensions in the batch-wise normalized (B-Norm) embeddings, and learns invariance with a loss i that makes similar dimensions highly correlated. W-MSE (c) uses a batch slicing operation that shuffles batches into small sub-batches, and apply PCA as a whitening operation on the featurewise normalized (F-Norm) embeddings of each sub-batch. BYOL (d) has an asymmetric architecture where the weights ? m of one encoder are an exponential moving average (ema) of the other encoder's weights ?. A predictor g with weights ? is used in the branch with learnable weights. SimSiam (e) uses a predictor on one branch and a stop-gradient operation (sg) on the other one. SimCLR (f) uses the InfoNCE contrastive loss where all the feature-wise normalized embeddings are compared between them inside a batch. Samples from distorted versions of the same input are brought close to each other, while other samples are pushed away. SwAV (g) quantizes the feature-wise normalized embeddings of a branch and use it as target for the other one. OBoW (h) uses bag-of-words (BoW) representations and a cross-entropy loss to compare the BoW generated by a teacher network from the feature maps Y F of the encoder, to the BoW predicted by a student network. Green blocks: parametric functions; yellow boxes: non-parametric functions; blue boxes: objective functions.C ADDITIONAL IMPLEMENTATION DETAILS C.1 DATA AUGMENTATION We follow the image augmentation protocol first introduced in SimCLR Chen et al. (2020a) and now commonly used by similar approaches based on siamese networks Caron et al. (2020); Grill et al. (2020); Chen &amp; He</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(2021) for linear classification tasks and the detectron2 library Wu et al. (2019) for object detection and segmentation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Linear classification. We follow standard protocols<ref type="bibr" target="#b18">Misra &amp; Maaten (2020)</ref>;<ref type="bibr" target="#b6">Caron et al. (2020)</ref>;<ref type="bibr" target="#b35">Zbontar et al. (2021)</ref> and train linear models on top of the frozen representations. For VOC07<ref type="bibr" target="#b16">Everingham et al. (2010)</ref>, we train a linear SVM with LIBLINEAR Fan et al.(2008). The images are center cropped and resized to 224 ? 224, and the C values are computed with cross-validation. For Places205<ref type="bibr" target="#b36">Zhou et al. (2014)</ref> we use SGD with a learning rate of 0.003, a weight decay of 0.0001, a momentum of 0.9 and a batch size of 256, for 28 epochs. The learning rate is divided by 10 at epochs 4,<ref type="bibr" target="#b9">8 and</ref> 12. For Inaturalist2018 Horn et al. (2018), we use SGD with a learning rate of 0.005, a weight decay of 0.0001, a momentum of 0.9 and a batch size of 256, for 84 epochs. The learning rate is divided by 10 at epochs 24, 48 and 72.Object detection and instance segmentation.Following the setup of He et al. (2020); Zbontar et al. (2021), we use the trainval split of VOC07+12 with 16K images for training and a Faster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>and aggregated ResNet Xie et al. (2017), and we consider two ways of widening a standard ResNet. First, we follow standard practice in recent self-supervised learning work Caron et al. (2020); Grill et al. (2020); Chen et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Incorporating variance regularization in BYOL and SimSiam. Top-1 accuracy on the linear evaluation protocol for different number of pretraining epochs. For both methods pre-training follows the optimization and data augmentation protocol of their original paper but is based on our implementation. Var indicates variance regularization correlation matrix of the representations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Standard deviation of the features during BYOL and SimSiam pretraining. Evolution of the average standard deviation of each dimension of the features with and without variance regularization (Var). left: the standard deviation is measured on the representations, right: the standard deviation is measured on the embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Average correlation coefficient of the features during BYOL and SimSiam pretraining. Evolution of the average correlation coefficient measured by averaging the off-diagonal terms of the correlation matrix of the representations with BYOL, BYOL with variance-covariance regularization (BYOL VarCov), SimSiam, and SimSiam with variance-covariance regularization (SimSiam VarCov).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? does not require a memory bank, nor contrastive samples, nor a large batch size; ? does not require batch-wise nor feature-wise normalization; and ? does not require vector quantization nor a predictor module.</figDesc><table><row><cell>Other methods require asymmetric stop gradient operations, as in SimSiam Chen &amp; He (2020),</cell></row><row><cell>weight sharing between the two branches as in classical Siamese nets, or weight sharing through</cell></row><row><cell>exponential moving average dampening with stop gradient in one branch, as in BYOL and MoCo He</cell></row><row><cell>et al. (2020); Grill et al. (2020); Chen et al. (2020c), large batches of contrastive samples, as in</cell></row><row><cell>SimCLR Chen et al. (2020a), or batch-wise and/or feature-wise normalization Caron et al. (2020);</cell></row><row><cell>Grill et al. (2020); Chen &amp; He (2020); Zbontar et al. (2021);</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on ImageNet. Evaluation of the representations obtained with a ResNet-50 backbone pretrained with VICReg on: (1) linear classification on top of the frozen representations from ImageNet; (2) semi-supervised classification on top of the fine-tuned representations from 1% and 10% of ImageNet samples. We report Top-1 and Top-5 accuracies (in %). Top-3 best self-supervised methods are underlined.</figDesc><table><row><cell>Linear</cell><cell>Semi-supervised</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Transfer learning on downstream tasks. Evaluation of the representations from a ResNet-50 backbone pretrained with VICReg on: (1) linear classification tasks on top of frozen representations, we report Top-1 accuracy (in %) for Places205<ref type="bibr" target="#b36">Zhou et al. (2014) and</ref> iNat18 Horn et al. (2018), and mAP for VOC07<ref type="bibr" target="#b16">Everingham et al. (2010)</ref>; (2) object detection with fine-tunning, we report AP 50 for VOC07+12 using Faster R-CNN with C4 backbone<ref type="bibr" target="#b20">Ren et al. (2015)</ref>; (3) object detection and instance segmentation, we report AP for COCOLin et al. (2014) using Mask R-CNN with FPN  backbone He et al. (2017. We use ? to denote the experiments run by us. Top-3 best self-supervised methods are underlined.</figDesc><table><row><cell></cell><cell cols="3">Linear Classification</cell><cell></cell><cell>Object Detection</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Places205 VOC07 iNat18</cell><cell cols="3">VOC07+12 COCO det COCO seg</cell></row><row><cell>Supervised</cell><cell>53.2</cell><cell>87.5</cell><cell>46.7</cell><cell>81.3</cell><cell>39.0</cell><cell>35.4</cell></row><row><cell>MoCo He et al. (2020)</cell><cell>46.9</cell><cell>79.8</cell><cell>31.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PIRL Misra &amp; Maaten (2020)</cell><cell>49.8</cell><cell>81.1</cell><cell>34.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR Chen et al. (2020a)</cell><cell>52.5</cell><cell>85.5</cell><cell>37.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MoCo v2 Chen et al. (2020c)</cell><cell>51.8</cell><cell>86.4</cell><cell>38.6</cell><cell>82.5</cell><cell>39.8</cell><cell>36.1</cell></row><row><cell>SimSiam Chen &amp; He (2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.4</cell><cell>-</cell><cell>-</cell></row><row><cell>BYOL Grill et al. (2020)</cell><cell>54.0</cell><cell>86.6</cell><cell>47.6</cell><cell>-</cell><cell>40.4  ?</cell><cell>37.0  ?</cell></row><row><cell>SwAV (m-c) Caron et al. (2020)</cell><cell>56.7</cell><cell>88.9</cell><cell>48.6</cell><cell>82.6</cell><cell>41.6</cell><cell>37.8</cell></row><row><cell>OBoW Gidaris et al. (2021)</cell><cell>56.8</cell><cell>89.3</cell><cell>-</cell><cell>82.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Barlow Twins Grill et al. (2020)</cell><cell>54.1</cell><cell>86.2</cell><cell>46.5</cell><cell>82.6</cell><cell>40.0  ?</cell><cell>36.7  ?</cell></row><row><cell>VICReg (ours)</cell><cell>54.3</cell><cell>86.6</cell><cell>47.0</cell><cell>82.4</cell><cell>39.4</cell><cell>36.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on MS-COCO 5K retrieval tasks. Comparison of VICReg with the contrastive loss of VSE++ Faghri et al. (2018), and with Barlow Twins, pretrain on the training set of MS-COCO.In all settings, the encoder for text is a word embedding followed by a GRU layer, the encoder for images is a ResNet-152.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Image-to-text</cell><cell></cell><cell></cell><cell>Text-to-Image</cell><cell></cell></row><row><cell></cell><cell>R@1</cell><cell>R@5</cell><cell>R@10</cell><cell>R@1</cell><cell>R@5</cell><cell>R@10</cell></row><row><cell>Contrastive (VSE++)</cell><cell>30.3</cell><cell>59.4</cell><cell>72.4</cell><cell>41.3</cell><cell>71.1</cell><cell>81.2</cell></row><row><cell>Barlow Twins</cell><cell>31.4</cell><cell>60,4</cell><cell>75.1</cell><cell>42.9</cell><cell>74.0</cell><cell>83.5</cell></row><row><cell>VICReg</cell><cell>33.6</cell><cell>62.7</cell><cell>77.9</cell><cell>45.2</cell><cell>76.1</cell><cell>84.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of incorporating variance and covariance regularization in different methods. Top-1 ImageNet accuracy with the linear evaluation protocol after 100 pretraining epochs. For all methods, pretraining follows the architecture, the optimization and the data augmentation protocol of the original method using our reimplementation. ME: Momentum Encoder. SG: stop-gradient. PR: predictor. BN: Batch normalization layers after input and inner linear layers in the expander.No Reg: No additional regularization. Var Reg: Variance regularization. Var/Cov Reg: Variance and Covariance regularization. Unmodified original setups are marked by a ?.</figDesc><table><row><cell>Method</cell><cell>ME</cell><cell>SG</cell><cell>PR</cell><cell>BN</cell><cell>No Reg</cell><cell>Var Reg</cell><cell>Var/Cov Reg</cell></row><row><cell>BYOL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.3  ?</cell><cell>70.2</cell><cell>69.5</cell></row><row><cell>SimSiam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.9  ?</cell><cell>68.1</cell><cell>67.6</cell></row><row><cell>SimSiam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.1</cell><cell>67.3</cell><cell>67.1</cell></row><row><cell>SimSiam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>collapse</cell><cell>56.8</cell><cell>66.1</cell></row><row><cell>VICReg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>collapse</cell><cell>56.2</cell><cell>67.3</cell></row><row><cell>VICReg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>collapse</cell><cell>57.1</cell><cell>68.7</cell></row><row><cell>VICReg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>collapse</cell><cell>57.5</cell><cell>68.6  ?</cell></row><row><cell>VICReg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>collapse</cell><cell>56.5</cell><cell>67.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Impact of sharing weights or not between branches. Top-1 accuracy on linear classification with 100 pretraining epochs. The encoder and expander of both branches can share the same architecture and share their weights (SW), share the same architecture with different weights (DW), or have different architectures (DA). The encoders can be ResNet-50, ResNet-101 or ViT-S.</figDesc><table><row><cell></cell><cell cols="4">SW R50 DW R50 DA R50/R101 DA R50/ViT-S</cell></row><row><cell>BYOL</cell><cell>69.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR</cell><cell>64.4</cell><cell>63.1</cell><cell>63.9</cell><cell>63.5</cell></row><row><cell>Barlow Twins</cell><cell>68.7</cell><cell>64.2</cell><cell>65.3</cell><cell>63.9</cell></row><row><cell>VICReg</cell><cell>68.6</cell><cell>66.5</cell><cell>68.1</cell><cell>66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Evaluation on ESC-50. Evaluation of the representations obtained with a ResNet-18 backbone pretrained with VICReg on ESC-50<ref type="bibr" target="#b19">Piczak (2015)</ref> by processing jointly a raw audio time-series and its corresponding time-frequency representation. The supervised baseline corresponds to a ResNet-18 trained on the time-frequency representation in a supervised way. We report Top-1 accuracy on the validation set (in %).</figDesc><table><row><cell>Method</cell><cell>Top-1</cell></row><row><cell>Supervised baseline</cell><cell>72.7</cell></row><row><cell>Barlow Twins</cell><cell>75.4</cell></row><row><cell>VICReg</cell><cell>78.4</cell></row><row><cell cols="2">model ResNet-200 (x2) performs lower than BYOL when 1% of the labels are used but is on par</cell></row><row><cell cols="2">with 10% of the labels. These results demonstrate the capabilities of VICReg to scale up when large</cell></row><row><cell>architectures are used.</cell><cell></cell></row><row><cell cols="2">D.2 PRETRAINING AND EVALUATION ON ESC-50 AUDIO CLASSIFICATION</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Impact of variance-covariance regularization. Inv: a invariance loss is used, ? &gt; 0, Var: variance regularization, ? &gt; 0, Cov: covariance regularization, ? &gt; 0, in Eq. (6).</figDesc><table><row><cell>Method</cell><cell cols="2">? ? ? Top-1</cell></row><row><cell>Inv</cell><cell cols="2">1 0 0 collapse</cell></row><row><cell>Inv + Cov</cell><cell cols="2">25 0 1 collapse</cell></row><row><cell>Inv + Cov</cell><cell cols="2">0 25 1 collapse</cell></row><row><cell>Inv + Var</cell><cell>1 1 0</cell><cell>57.5</cell></row><row><cell cols="3">Inv + Var + Cov (VICReg) 1 1 1 collapse</cell></row><row><cell></cell><cell cols="2">1 10 1 collapse</cell></row><row><cell></cell><cell cols="2">10 1 1 collapse</cell></row><row><cell></cell><cell>5 5 1</cell><cell>68.1</cell></row><row><cell></cell><cell>10 10 1</cell><cell>68.2</cell></row><row><cell></cell><cell>25 25 1</cell><cell>68.6</cell></row><row><cell></cell><cell>50 50 1</cell><cell>68.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Impact of normalization. Std: variables are centered and divided by their standard deviation over the batch. This is applied or not to the embedding and the expander hidden layers. l 2 : the embedding vectors are l 2 -normalized.</figDesc><table><row><cell cols="3">Representation Embedding Top-1</cell></row><row><cell>Std</cell><cell>None</cell><cell>68.6</cell></row><row><cell>Std</cell><cell>Std</cell><cell>68.4</cell></row><row><cell>None</cell><cell>Std</cell><cell>67.4</cell></row><row><cell>Std</cell><cell>None</cell><cell>67.2</cell></row><row><cell>None</cell><cell>l2</cell><cell>65.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Linear classification with large architectures. Top-1 accuracy comparison between different methods using various encoder architectures. For all VICReg results, the output dimensionality of the expander is 8192. N-R stands for Narrow ResNet, where only the bottleneck convolutional layers are widen.</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>Param.</cell><cell cols="3">Repr. Top-1 Top-5</cell></row><row><cell cols="2">SimCLR Chen et al. (2020a) R50 (x2)</cell><cell>93M</cell><cell>4096</cell><cell>74.2</cell><cell>92.0</cell></row><row><cell></cell><cell>R50 (x4)</cell><cell>375M</cell><cell>8192</cell><cell>76.5</cell><cell>93.2</cell></row><row><cell>SwAV Caron et al. (2020)</cell><cell>R50 (x2)</cell><cell>93M</cell><cell>4096</cell><cell>77.3</cell><cell>-</cell></row><row><cell></cell><cell>R50 (x4)</cell><cell>375M</cell><cell>8192</cell><cell>77.9</cell><cell>-</cell></row><row><cell></cell><cell>R50 (x5)</cell><cell cols="2">586M 10240</cell><cell>78.5</cell><cell>-</cell></row><row><cell>BYOL Grill et al. (2020)</cell><cell>R50 (x2)</cell><cell>93M</cell><cell>4096</cell><cell>77.4</cell><cell>93.6</cell></row><row><cell></cell><cell>R50 (x4)</cell><cell>375M</cell><cell>8192</cell><cell>78.6</cell><cell>94.2</cell></row><row><cell></cell><cell>R200 (x2)</cell><cell>250M</cell><cell>4096</cell><cell>79.6</cell><cell>94.8</cell></row><row><cell>VICReg (ours)</cell><cell>N-R50 (x2)</cell><cell>66M</cell><cell>2048</cell><cell>74.7</cell><cell>91.9</cell></row><row><cell></cell><cell>N-R50 (x4)</cell><cell>221M</cell><cell>2048</cell><cell>76.0</cell><cell>92.4</cell></row><row><cell></cell><cell>R50 (x2)</cell><cell>93M</cell><cell>4096</cell><cell>75.5</cell><cell>92.1</cell></row><row><cell></cell><cell>R50 (x4)</cell><cell>375M</cell><cell>8192</cell><cell>75.6</cell><cell>92.2</cell></row><row><cell></cell><cell>RNXT101-32-16</cell><cell>191M</cell><cell>2048</cell><cell>76.1</cell><cell>92.3</cell></row><row><cell></cell><cell>R200 (x2)</cell><cell>250M</cell><cell>4096</cell><cell>77.3</cell><cell>93.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Semi-supervised classification with large architectures. Top-1 accuracy comparison between different methods using various encoder architectures. For all VICReg results, the output dimensionality of the expander is 8192.</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>Param. Repr.</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1% 10% 1% 10 %</cell></row><row><cell cols="2">SimCLR Chen et al. (2020a) R50 (x2)</cell><cell cols="3">93M 4096 58.5 71.7 83.0 91.2</cell></row><row><cell></cell><cell>R50 (x4)</cell><cell cols="3">375M 8192 63.0 74.4 85.8 92.6</cell></row><row><cell>BYOL Grill et al. (2020)</cell><cell>R50 (x2)</cell><cell cols="3">93M 4096 62.2 73.5 84.1 91.7</cell></row><row><cell></cell><cell>R50 (x4)</cell><cell cols="3">375M 8192 69.1 75.7 87.9 92.5</cell></row><row><cell></cell><cell>R200 (x2)</cell><cell cols="3">250M 4096 71.2 77.7 89.5 93.7</cell></row><row><cell>VICReg (ours)</cell><cell>R50 (x2)</cell><cell cols="3">93M 4096 62.6 73.9 84.5 91.8</cell></row><row><cell></cell><cell>R200 (x2)</cell><cell cols="3">250M 4096 68.8 77.3 88.2 93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>K-NN classifiers on ImageNet. Top-1 accuracy with 20 and 200 nearest neighbors.</figDesc><table><row><cell>Method</cell><cell cols="2">20-NN 200-NN</cell></row><row><cell>NPID Wu et al. (2018)</cell><cell>-</cell><cell>46.5</cell></row><row><cell>LA Zhuang et al. (2019)</cell><cell>-</cell><cell>49.4</cell></row><row><cell>PCL Li et al. (2021)</cell><cell>54.5</cell><cell>-</cell></row><row><cell>BYOL Grill et al. (2020)</cell><cell>66.7</cell><cell>64.9</cell></row><row><cell>SwAV Caron et al. (2020)</cell><cell>65.7</cell><cell>62.7</cell></row><row><cell>Barlow Twins Zbontar et al. (2021)</cell><cell>64.8</cell><cell>62.9</cell></row><row><cell>VICReg</cell><cell>64.5</cell><cell>62.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Impact of expander dimensionality. Top-1 accuracy on the linear evaluation protocol with 100 pretraining epochs.</figDesc><table><row><cell>Dimensionality</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16834</cell></row><row><cell>Top-1</cell><cell>55.9</cell><cell>59.2</cell><cell>62.4</cell><cell>65.1</cell><cell>67.3</cell><cell>68.6</cell><cell>68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Impact of batch size. Top-1 accuracy on the linear evaluation protocol with 100 pretraining epochs.</figDesc><table><row><cell>Batch size</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell></row><row><cell>Top-1</cell><cell>67.3</cell><cell>67.9</cell><cell>68.2</cell><cell>68.3</cell><cell>68.6</cell><cell>67.8</cell></row><row><cell cols="3">D.6 EXPANDER NETWORK ARCHITECTURE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Running time and peak memory. Comparison between different methods, the training is distributed on 32 Tesla V100 GPUs, the running time is measured over 100 epochs and the peak memory is measured on a single GPU. We report top-1 accuracy (%) on linear classification on top of the frozen representations.</figDesc><table><row><cell>Method</cell><cell cols="3">time / 100 epochs peak memory / GPU Top-1 accuracy (%)</cell></row><row><cell>SwAV</cell><cell>9h</cell><cell>9.5G</cell><cell>71.8</cell></row><row><cell>SwAV (w/ multi-crop)</cell><cell>13h</cell><cell>12.9G</cell><cell>75.3</cell></row><row><cell>BYOL</cell><cell>10h</cell><cell>14.6G</cell><cell>74.3</cell></row><row><cell>Barlow Twins</cell><cell>12h</cell><cell>11.3G</cell><cell>73.2</cell></row><row><cell>VICReg</cell><cell>11h</cell><cell>11.3G</cell><cell>73.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. Jean Ponce was supported in part by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton/ENS Chair in Artificial Intelligence and the Inria/NYU collaboration. Adrien Bardes was supported in part by a FAIR/Prairie CIFRE PhD Fellowship. The authors wish to thank Jure Zbontar for the BYOL implementation, St?phane Deny for useful comments on the paper, and Li Jing, Yubei Chen, Mikael Henaff, Pascal Vincent and Geoffrey Zweig for useful discussions. We thank Quentin Duval and the VISSL team for help obtaining the results of table 2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Sackinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2020b. 3</idno>
		<editor>NeurIPS</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>ICLR, 2021. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Whitening for selfsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Winn</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sgdr: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10241</idno>
		<title level="m">Byol works even without batch statistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849v4</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding self-supervised learning dynamics without contrastive pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06810</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno>arxiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
