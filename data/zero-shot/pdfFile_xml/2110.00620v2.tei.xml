<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPEC: Seeing People in the Wild with an Estimated Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao</forename><forename type="middle">P</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Tesch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>M?ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tue.mpg.deotmar.hilliges@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPEC: Seeing People in the Wild with an Estimated Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Input image (b) SOTA method (c) Our results Focal length = 755 px Camera pitch = 33.4 o Camera roll = -0.6 o Figure 1: State-of-the-art methods for 3D human pose and shape estimation from images (such as HMR trained with EFT [27] data) struggle with imagery containing perspective effects. (b) In part, this is due to the use of the standard weak perspective camera. (c) SPEC learns to estimate perspective camera parameters and uses these to regress more accurate 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Due to the lack of camera parameter information for inthe-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. Code and datasets are available for research purposes at https://spec.is.tue.mpg.de/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Max Planck Institute for Intelligent Systems, T?bingen, Germany 2 ETH Zurich {mkocabas,chuang2,jtesch,lea.mueller,black}@tue.mpg.de otmar.hilliges@inf.ethz.ch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Due to the lack of camera parameter information for inthe-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. Code and datasets are available for research purposes at https://spec.is.tue.mpg.de/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D human pose and shape (HPS) from a single RGB image is a core challenge in computer vision and has many applications in robotics, computer graphics, and AR/VR. Reconstructing a high dimensional 3D structure from 2D observations is ill-posed by nature. To overcome this, much attention has been given to structured prediction <ref type="bibr">[17,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b37">66]</ref> and incorporating shape and pose priors <ref type="bibr" target="#b21">[50,</ref><ref type="bibr" target="#b44">73]</ref> to guide estimation. Weakly-supervised training of HPS regressors leverages 2D-pose datasets <ref type="bibr">[1,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b12">41]</ref> and requires various forms of regularization <ref type="bibr" target="#b0">[29,</ref><ref type="bibr" target="#b7">36,</ref><ref type="bibr" target="#b46">75]</ref>. Data for full 3D supervision often relies on controlled lab settings <ref type="bibr">[22,</ref><ref type="bibr" target="#b32">61]</ref>, synthetic images <ref type="bibr" target="#b36">[65]</ref>, or, more recently, in-the-wild capture of reference data <ref type="bibr" target="#b17">[46,</ref><ref type="bibr" target="#b38">67]</ref>.</p><p>Despite rapid progress, we observe that most state-ofthe-art (SOTA) methods <ref type="bibr">[5,</ref><ref type="bibr">7,</ref><ref type="bibr">16,</ref><ref type="bibr">25,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b0">29,</ref><ref type="bibr" target="#b1">30,</ref><ref type="bibr" target="#b4">33,</ref><ref type="bibr" target="#b7">36,</ref><ref type="bibr" target="#b8">37,</ref><ref type="bibr" target="#b21">50,</ref><ref type="bibr" target="#b26">55,</ref><ref type="bibr" target="#b33">62,</ref><ref type="bibr" target="#b48">77,</ref><ref type="bibr" target="#b49">78]</ref> make several simplifying assumptions about the image formation process itself. First, they all apply a weak perspective or orthographic projection assumption; resulting in a simplified camera model with only three parameters which capture the camera translation relative to the body. Moreover, some <ref type="bibr" target="#b4">[33,</ref><ref type="bibr" target="#b7">36,</ref><ref type="bibr" target="#b21">50]</ref> set the focal length to a predefined large constant for every input image. Finally, they all assume zero camera rotation, which entangles body rotation and camera rotation, making it extremely hard to correctly estimate the body orientation in 3D. These 1 assumptions are valid for images where bodies are roughly perpendicular to the principal axis and are located far away from the camera. However, in most real world images of people, perspective effects are clearly evident, e.g. foreshortening in selfies. Ignoring perspective projection leads to errors in pose, shape, and global orientation (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>To overcome these limitations in existing methods, we present SPEC (Seeing People in the wild with Estimated Cameras), the first 3D human pose and shape estimation framework that leverages cues present in the image to extract perspective camera information and exploits this to better reconstruct 3D human bodies from images in the wild. SPEC consists of two parts: camera calibration and body reconstruction. We make contributions to each.</p><p>One might hope that embedded EXIF information would be sufficient to address this problem. However, many images lack EXIF information, some applications strip this off, and even if present, converting the stored focal length in millimeters to pixels requires knowing specifics of the image sensor. Given the huge variety of cameras on the market, exploiting this is a non-trivial task. Furthermore, this does not give information about the camera rotation.</p><p>Instead, we estimate the camera directly from the RGB image. Recent work [20, <ref type="bibr" target="#b2">31,</ref><ref type="bibr" target="#b40">69,</ref><ref type="bibr" target="#b52">81]</ref> casts this ill-posed regression problem as a classification task. However, training such methods with their losses, e.g. cross-entropy and KLdivergence, ignores the natural notion of distance or ordering of the original target space. To address this, we propose a new loss, Softargmax-L 2 , to preserve distance during loss calculation. Moreover, we observe that HPS accuracy is quite sensitive to underestimation of focal length and less sensitive to overestimates as also noted by <ref type="bibr" target="#b3">[32,</ref><ref type="bibr" target="#b47">76]</ref>. Therefore, we modify Softargmax-L 2 to be asymmetric such that less penalty is applied when the focal length is overestimated. These novel losses help us to train a better regressor for direct camera calibration, which we term CamCalib.</p><p>We integrate the regressed camera parameters into two 3D-body-reconstruction paradigms: (1) an optimizationbased approach, SMPLify-X <ref type="bibr" target="#b21">[50]</ref>, and (2) a regressionbased one similar to HMR or SPIN <ref type="bibr" target="#b0">[29,</ref><ref type="bibr" target="#b7">36]</ref>. Since SMPLify-X estimates a 3D body my minimizing the difference between projected 3D joints and observed 2D joints, improving the the projective geometry improves the estimated body.</p><p>In the case of direct HPS regression from pixels, the estimated camera is employed in two ways: (1) in the reprojection loss similar to the one in SMPLify-X and (2) as conditioning for the network by appending the camera parameters to the CNN image features. This second contribution is a key novelty of SPEC, which enables us to disentangle camera and body orientation. SOTA methods <ref type="bibr" target="#b4">[33,</ref><ref type="bibr" target="#b5">34,</ref><ref type="bibr" target="#b7">36,</ref><ref type="bibr" target="#b48">77]</ref>, cannot do this because the body is estimated in camera space, entangling body orientation and camera rotation.</p><p>Training such a body regressor requires in-the-wild images annotated with both 3D human bodies and the camera parameters. Since existing 3D human body datasets <ref type="bibr">[22,</ref><ref type="bibr" target="#b16">45,</ref><ref type="bibr" target="#b38">67]</ref> contain little variation in camera parameters, we create two new datasets with rich camera variety. First, we create a photorealistic synthetic dataset which has accurate ground-truth human and camera annotations (SPEC-SYN) using ideas from <ref type="bibr" target="#b20">[49]</ref>. This dataset is used both for testing and training. Second, we collect a crowdsourced dataset following the Mimic-The-Pose framework <ref type="bibr" target="#b19">[48]</ref> (SPEC-MTP). We ask web participants to calibrate their camera and take videos from different angles while mimicking a predefined pose. Then, we obtain pseudo ground-truth labels by fitting the SMPL model to the provided videos while exploiting the predefined pose as a prior. Through extensive experiments and analysis using these new datasets, alongside an existing in-the-wild dataset (3DPW <ref type="bibr" target="#b38">[67]</ref>), we show that going beyond the weak-perspective/orthographic assumption improves human pose and shape estimation results.</p><p>In summary, our contributions are: (1) We propose a single-view, camera-aware, 3D human body estimation framework that estimates perspective camera parameters from in-the-wild images directly and reconstructs the 3D body without relying on weak-perspective assumptions or offline calibration. (2) We train a neural network to regress the perspective camera parameters given one RGB image, using two novel losses: Softargmax-L 2 and the asymmetric variant to improve the calibration accuracy. (3) Using the estimated camera parameters helps to reconstruct a better 3D body with the optimization-based SMPLify-X algorithm. (4) Conditioning on camera information helps a direct regression approach based on HMR <ref type="bibr" target="#b0">[29]</ref> learn to regress better poses. (5) We present two different datasets with ground-truth camera and human body parameters: (i) a photorealistic synthetic dataset, SPEC-SYN, and (ii) a crowdsourced dataset, SPEC-MTP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review work that captures/reconstructs 3D humans under calibrated-camera settings, then focus on our goal: in-the-wild 3D human body reconstruction from monocular RGB. We discuss how prior art simplifies the camera model to make the problem tractable and also discuss relevant methods of camera calibration from one RGB image. 3D HPS Estimation with Calibrated Cameras. To capture human motions in 3D, early work exploits calibrated and synchronized multi-camera settings. They can be largely classified to "bottom up" approaches <ref type="bibr">[4,</ref><ref type="bibr">6,</ref><ref type="bibr">9,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b31">60,</ref><ref type="bibr" target="#b51">80]</ref> that assemble 3D body poses from multi-view image evidence (keypoints, silhouettes), and "top down" approaches <ref type="bibr">[3,</ref><ref type="bibr">11,</ref><ref type="bibr">13,</ref><ref type="bibr">21,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b37">66]</ref> that deform a pre-defined 3D human template according to detected image features in each view. Powered by CNNs, learning-based methods <ref type="bibr">[19,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b23">52,</ref><ref type="bibr" target="#b24">53,</ref><ref type="bibr" target="#b35">64]</ref> gain robustness by training keypoint detection and multi-view pose reconstruction end-to-end. Some monocular approaches <ref type="bibr" target="#b16">[45,</ref><ref type="bibr" target="#b17">46,</ref><ref type="bibr" target="#b22">51,</ref><ref type="bibr" target="#b30">59]</ref> are trained with direct supervision from multi-view data, while others <ref type="bibr">[17,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b6">35,</ref><ref type="bibr" target="#b25">54]</ref> enforce multi-view consistency as weak, or self, supervision. In either way, known intrinsic or extrinsic parameters are always assumed. Yu et al. <ref type="bibr" target="#b47">[76]</ref> propose perspective crop layers, which crop the image around a person according to camera parameters and image location, effectively removing some effect of camera geometry.</p><p>All these methods require offline calibration and have a risk of overfitting to the cameras used in training and are typically limited to controlled settings. Single-view HPS Estimation with Unknown Cameras. In early work, Liebowitz and Carlsson <ref type="bibr" target="#b11">[40]</ref> use the repetitive structure of a moving person as a cue for camera calibration. Since then, numerous methods reconstruct 3D human bodies given single-view images or videos in uncontrolled settings. Closely related to structure-from-motion and bundle adjustment, [2, 10, 38, 71] take videos as input and jointly estimate cameras and reconstruct human bodies; <ref type="bibr">[18,</ref><ref type="bibr" target="#b13">42]</ref> further ground the bodies in 3D scenes.</p><p>We focus on the more general scenario in which the input is a single image. SOTA methods use parametric body models [28, <ref type="bibr" target="#b14">43,</ref><ref type="bibr" target="#b21">50,</ref><ref type="bibr" target="#b45">74]</ref> and estimate the parameters either by fitting to detected image features [5, <ref type="bibr" target="#b21">50,</ref><ref type="bibr" target="#b41">70]</ref> or by regressing directly from pixels with deep neural networks <ref type="bibr">[7,</ref><ref type="bibr">16,</ref><ref type="bibr">25,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b0">29,</ref><ref type="bibr" target="#b7">36,</ref><ref type="bibr" target="#b26">55,</ref><ref type="bibr" target="#b27">56,</ref><ref type="bibr" target="#b33">62,</ref><ref type="bibr" target="#b46">75,</ref><ref type="bibr" target="#b48">77,</ref><ref type="bibr" target="#b49">78]</ref>. All these approaches, including non-parametric approaches <ref type="bibr" target="#b8">[37,</ref><ref type="bibr" target="#b28">57,</ref><ref type="bibr" target="#b29">58,</ref><ref type="bibr" target="#b50">79]</ref>, assume weak perspective/orthographic projection or pre-define the focal length as a large constant for all images. Additionally, they all assume zero camera rotation, which entangles body rotation and camera rotation. As a result, these camera models have only three parameters, capturing the camera translation relative to the body.</p><p>Kissos et al. <ref type="bibr" target="#b3">[32]</ref> identify this problem and show that replacing focal length with a constant closer to ground truth, i.e. f = 5000 ? 2200, improves results. Wang et al. <ref type="bibr" target="#b39">[68]</ref> demonstrate that jointly estimating camera viewpoints and 3D human poses improves cross-dataset generalization. To show this, they train a 3D pose estimation model on available 3D human pose datasets in a supervised way. However, these datasets are limited in terms of camera viewpoint and focal length diversity, background, and number of subjects.</p><p>In contrast to the above methods, SPEC generalizes to inthe-wild settings, varied camera intrinsics and viewpoints. Single-image Camera Calibration. Recent work [20, <ref type="bibr" target="#b2">31,</ref><ref type="bibr" target="#b40">69,</ref><ref type="bibr" target="#b52">81]</ref> directly estimates camera parameters from a single image. Zhu et al. <ref type="bibr" target="#b52">[81]</ref> also recover the height of some scene objects, e.g. people and cars, together with the camera geometry. They estimate 2D human poses but not 3D bodies. To estimate camera rotations and fields of view, these methods train a neural network to leverage geometric cues in the image without calibration boards or body keypoints. They discretize the continuous space of rotation into bins, casting the problem as a classification task and applying crossentropy <ref type="bibr" target="#b40">[69]</ref> or KL-divergence <ref type="bibr">[20,</ref><ref type="bibr" target="#b52">81]</ref> losses. These losses unfortunately ignore the ordering in target spaces. We devise new losses to retain the concept of distance in the original space, leading to better estimated cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>A pinhole camera maps a 3D point X ? R 3 to an image pixel x ? R 2 through x = K(R c X + t c ), where K ? R 3?3 is the intrinsic matrix storing focal length f x , f y and the principal point (o x , o y ). We follow previous work and omit skew, radial and tangential distortion. Extrinsic parameters are R c ? SO(3) and t c = (t c x , t c y , t c z ) ? R 3 , representing camera rotation and translation in the world coordinate frame, respectively.</p><p>We estimate a parameteric human body model, SMPL <ref type="bibr" target="#b14">[43,</ref><ref type="bibr" target="#b21">50]</ref>, that deforms a predefined human surface M according to body pose ? and shape ?. When both body translation t b and body orientation R b are zero, the mesh is located near the origin of world coordinates, facing the z + direction and y + is the up-vector, as visualized in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>.</p><p>Existing approaches <ref type="bibr">[5,</ref><ref type="bibr">7,</ref><ref type="bibr">16,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b0">29,</ref><ref type="bibr" target="#b4">33,</ref><ref type="bibr" target="#b21">50,</ref><ref type="bibr" target="#b33">62,</ref><ref type="bibr" target="#b48">77]</ref> assume zero camera rotation, R c = I, and estimate the camera translation t c in two ways: (1) by fitting the body joint coordinates to 2D keypoints [5, <ref type="bibr" target="#b21">50,</ref><ref type="bibr" target="#b49">78]</ref> or (2) by predicting weak perspective camera parameters (s, t c x , t c y ) with a neural network, where the scale parameter s is converted to t c z <ref type="bibr" target="#b0">[29,</ref><ref type="bibr" target="#b1">30,</ref><ref type="bibr" target="#b4">33,</ref><ref type="bibr" target="#b7">36]</ref>. See Sup. Mat. for details of this conversion. The underlying assumption here is weak perspective projection. It is assumed that the camera is placed very far from the person, such that depth variations in the z coordinate of the person are negligible compared to the distance from the camera. This is violated regularly in natural images where it is common that the distance from the camera  to the body is no more than the height of the body itself.</p><p>For intrinsic parameters, [5, <ref type="bibr" target="#b4">33,</ref><ref type="bibr" target="#b7">36,</ref><ref type="bibr" target="#b21">50]</ref> set the focal length as a large constant f x = f y = f ? 5000 to meet the weak-perspective assumption and set the principal point (o x , o y ) at the center of the resized cropped image around the person, while <ref type="bibr" target="#b0">[29,</ref><ref type="bibr" target="#b48">77,</ref><ref type="bibr" target="#b49">78]</ref> directly apply weak perspective projection x = sX + t c . Despite the differences in modeling projection and translation, one common feature of these simplified cameras is that they only have three unknowns: (s, t c x , t c y ) or equivalently (t c x , t c y , t c z ). We refer to them collectively as IWP-cam in this paper, standing for Identity rotation and Weak Perspective.</p><p>Note that both camera variables (R c , t c ) and body variables (R b , t b ) are expressed in world coordinates, not in camera space. Given just one single-view image, the network/optimizer can change both (R c , t c ) and (R b , t b ) to explain the image observations. IWP-cam addresses this by assuming R c = I and t b = 0 to solve only for camera translation t c and body orientation R b , or more precisely, the body orientation in the camera space: <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. This approach is a key reason why most methods evaluate accuracy after Procrustes alignment of the estimated body to the ground truth.</p><formula xml:id="formula_0">R b c = R c R b . See</formula><p>IWP-cam works well when the weak-perspective assumption holds. However, images captured by cameras with significant pitch and smaller focal lengths, such as those in <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure">Fig. 5</ref>, have foreshortening distortion that breaks this assumption because changes in the z coordinate of the 3D body are no longer negligible compared to the distance from the camera. IWP-cam expects HPS methods to absorb this camera pitch ? into the relative body orientation R b c , but in practice, due to the mismatch in focal length, the optimization often unnecessarily changes body pose; e.g. the wrong arm and leg poses in <ref type="figure" target="#fig_0">Fig. 1</ref></p><formula xml:id="formula_1">(b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Camera Calibration from a Single Image</head><p>Inspired by single-view camera calibration and metrology [20, 31, 69, 81], we estimate camera rotation R c and focal length f from a single RGB image. By lifting the zero camera rotation constraint, i.e. R c = I, and directly estimating R c , we bypass the camera-relative body orientation R b c , and thus disentangle camera rotations from body orientations. Doing so allows us to handle perspective/foreshortening distortion while keeping a consistent xz-plane aligned ground plane located at [0, y, 0]. Utilizing better focal length also improves the pose estimation quality by leveraging accurate perspective projection.</p><p>Specifically, camera rotation is parameterized by three angles: pitch ?, roll ? and yaw. Since focal length in pixels has an unbounded range and it changes whenever one resizes images, we estimate vertical field of view (vfov) ? in radians and convert it to focal length f y via:</p><formula xml:id="formula_2">f y = 1 2 h tan( 1 2 ?) ,<label>(1)</label></formula><p>where h is the image height in pixels. We follow <ref type="bibr" target="#b52">[81]</ref> to assume zero camera yaw and f x = f y = f . Our camera thus has three more parameters -pitch, ?, roll, ?, and vertical field of view, ?, in addition to the original (s, t c x , t c y ). Since the three new parameters are all in radians, we choose to learn them with one camera calibration model termed Cam-Calib. We place the camera at the origin so t c = [0, 0, 0], as depicted in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, and leave the estimation of body translation t b to the downstream body estimator.</p><p>Many human body reconstruction networks take only a cropped image patch around the person as input. In contrast, CamCalib takes the uncropped full-frame image to predict pitch ?, roll ?, and vfov ?, which are the same for all subjects in the image. We argue that this is beneficial because the full image contains rich cues that facilitate camera calibration. For example, there are abundant geometric cues, e.g. vanishing points and lines, available to help determine camera rotations and field of view in the original image. Following [20, <ref type="bibr" target="#b2">31,</ref><ref type="bibr" target="#b52">81]</ref>, we use a CNN as the backbone for CamCalib and discretize the spaces of pitch ?, roll ?, and vfov ? into B bins, converting the regression problem into a B-way classification problem. However, instead of crossentropy <ref type="bibr" target="#b40">[69]</ref> or KL-divergence [20, 81] losses, we aggregate the predicted probability mass using a softargmax operation, i.e. computing the expectation value of the prediction, and measure its difference to the ground truth with an L 2 loss which we term Softargmax-L 2 . Thus, we avoid the difficulty of regressing in a continuous target space while retaining the notion of distance in the loss. The detailed formulation of Softargmax-L 2 is provided in the Sup. Mat.</p><p>Furthermore, as pointed out by <ref type="bibr" target="#b3">[32,</ref><ref type="bibr" target="#b47">76]</ref> and shown in the Sup. Mat., predicting larger focal lengths than the ground truth (or equivalently smaller fov) is less harmful to the reconstructed 3D poses than predicting smaller focal lengths (larger fov). We therefore apply an asymmetric loss for vfov ?. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, predictions? larger than the ground truth ? yield higher penalty through a standard L 2 loss, while the penalty for smaller predictions? saturates via a Geman-McClure function <ref type="bibr">[14]</ref>. We verify the benefits of all these design choices in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization approach: SMPLify-X-cam</head><p>Next, we showcase how using the estimated camera parameters help human body estimation in an optimization-  <ref type="figure">Figure 4</ref>: SPEC overview. CamCalib takes the whole input image as input and predicts camera pitch ?, roll ?, and vertical field of view ?. These parameters are then used to construct camera rotation R c and intrinsics K. Horizon line (green) is rendered following <ref type="bibr" target="#b52">[81]</ref> to indicate camera rotations. SPEC takes a cropped bounding box as input and extracts image features using a CNN backbone. Predicted camera parameters from CamCalib are concatenated with image features to estimate SMPL body parameters ?, ? along with the body translation t b . Camera parameters are also taken into account when computing a loss between the projected 3D joints? 2D and ground truth. based approach. To this end, we modify the SMPLify-X method <ref type="bibr" target="#b21">[50]</ref>. Given an image, CamCalib predicts camera pitch ?, roll ?, and vfov ?. We convert them into camera</p><formula xml:id="formula_3">rotation R c = R(?)R(?) and intrinsics, K, storing f = f x = f y and the principal point (o x , o y ) = (w/2, h/2),</formula><p>where f y is computed from Eq. 1 and w, h are the image width and height in pixels. Then, we estimate the 2D keypoints J 2D using an off-the-shelf 2D keypoint detector <ref type="bibr">[8]</ref> and define the the SMPLify-X-cam energy function as:</p><formula xml:id="formula_4">E(?, ?, R c , K, t b ) = E J + E ? + E ? ,<label>(2)</label></formula><p>where ?, ? are SMPL shape and pose parameters, t b is SMPL body translation, E ? and E ? are pose and shape prior terms, and E J is the data term. We modify the original SMPLify-X method to take perspective camera parameters into account in the data-term E J . We obtain SMPL 3D joint locations using a pretrained joint regressor W b? J 3D = W M(?, ?). E J measures the difference between the detected J 2D and the estimated? 3D , projected on the image by the estimated camera parameters:</p><formula xml:id="formula_5">E J = ?? 3D ? J 2D 2 2 , where ? = K[R c | ? t b ]. (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning-based approach: SPEC</head><p>To evaluate the effect of estimated camera parameters on a regression-based method, we take a simple and widelyused method, HMR <ref type="bibr" target="#b0">[29]</ref>, as a backbone, which employs a 2D reprojection loss during training that uses the estimated IWP-cam. We incorporate our estimated camera parameters in two ways: (1) by using them as R c and K during the projection of 3D joints like in Eq. 3 and (2) by conditioning the fully connected layers, which estimate SMPL parameters, with R c and ?. <ref type="figure">Figure 4</ref> gives an overview of SPEC.</p><p>Given an image, we first estimate the camera pitch ?, roll ?, and vfov ? using CamCalib and then convert them into R c and K as explained in Sec. 3.3. For human body regression, we take a cropped bounding-box image as input and extract image features using a backbone CNN. These image features are concatenated with R c and ? and fed to an iterative regressor <ref type="bibr" target="#b0">[29]</ref> to regress SMPL pose ? and shape ? along with body translation t b . By doing so, SPEC learns to disentangle the SMPL body's global orientation R b from the camera rotation R c . See Sup. Mat. for the details of t b . Then, we obtain SMPL 3D joint location? J 3D = W M(?, ?) and 2D projection? 2D = ?? 3D as in Eq. 3. Overall, our total loss for each training sample is:</p><formula xml:id="formula_6">L = ? 3D L 3D + ? 2D L 2D + ? SM P L L SM P L where L 3D = ? 3D ? J 3D 2 F , (4) L 2D = ? 2D ? J 2D 2 F , (5) L SMPL = ? ? ? 2 2 + ? ? ? 2 2 ,<label>(6)</label></formula><p>where? represents the prediction for the corresponding variable. ?'s are scalar coefficients to balance the loss terms. Note that we define both 3D and 2D losses on the body joints. This is because the 3D ground truth we use for training is not always reliable and thus the 2D joints provide important additional image cues, which can be exploited particularly well when using the correct camera geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We focus the evaluation on CamCalib and SPEC; for evaluation of SMPLify-X-cam, see Sup. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Pano360 dataset. Previous work <ref type="bibr">[20,</ref><ref type="bibr" target="#b52">81]</ref>, uses the SUN360 <ref type="bibr" target="#b43">[72]</ref> dataset to train camera calibration networks, which is unfortunately no longer available due to licensing issues. Therefore, we have curated a new dataset of equirectangular panorama images called Pano360. The Pano360 dataset consists of 35K panoramic images of which 34K are from Flickr and 1K rendered from photorealistic 3D scenes. Following previous work <ref type="bibr">[20,</ref><ref type="bibr" target="#b52">81]</ref>, we randomly sample the camera pitch, roll, yaw, and vertical field of view to generate 400K training and 15K validation images. We use these to train our CamCalib model. SPEC-SYN. Existing datasets, used to train HPS regressors, contain limited camera variation. Hence, they are not ideal for training and evaluating the effects of camera estimation on HPS. Therefore, we created a photorealistic synthetic dataset, inspired by AGORA <ref type="bibr" target="#b20">[49]</ref>, to train and evaluate our model. It has high-quality textured human 3D scans and provides reference SMPL(-X) parameters for them. We place these scans in 5 different large high-quality photorealistic 3D scenes, enabling the generation of many unique views. We randomly sample camera viewpoints, ? ? U(?30 ? , 15 ? ) and ? ? N (0 ? , 2.8 ? ), and focal lengths, ? ? U(70 ? , 130 ? ), to add diversity. In total, we generate 22191 images with 71982 ground truth bodies for training, and 3783 images with 12071 bodies for testing.</p><p>SPEC-MTP. To evaluate calibrated HPS (CHPS) methods on real data, we collect a new dataset with highquality pseudo ground truth using Amazon Mechanical Turk (AMT). Following the idea of the MTP dataset <ref type="bibr" target="#b19">[48]</ref>, we ask AMT workers to mimic 10 poses for which we have 3D ground truth. While the person maintains a pose, a second person records a video from different viewpoints. In addition, the worker calibrates the camera and provides their height and weight. We extend SMPLify-XC <ref type="bibr" target="#b19">[48]</ref> and use the calibrated camera to fit the SMPL-X model to multiple video frames. See Sup. Mat. for details. In total, we collect 64 videos of 7 subjects (4 male, 3 female) and extract 3284 images at a frame rate of 1 fps.</p><p>Other datasets. To train 3D CHPS estimation, we use the 3DPW <ref type="bibr" target="#b38">[67]</ref>, COCO <ref type="bibr" target="#b12">[41]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b16">[45]</ref>, and Hu-man3.6M [22] datasets. We evaluate SPEC using separate test data: 3DPW-test, SPEC-SYN, and SPEC-MTP. Since there are no ground truth 3D body and camera annotations for COCO, we use CamCalib to estimate camera parameters and SMPLify-X-cam to obtain pseudo 3D body ground truth, using EFT [27] annotations as the initialization.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics</head><formula xml:id="formula_7">? cam 3D ? J 3D and, (2) R c ?1? cam 3D ? J 3D ,</formula><p>where R c is the estimated camera rotation by CamCalib. By reporting (2), we do not assume known camera rotations for any method and compare them all in world coordinates. This also illustrates the effect of using CamCalib with prior work. We discuss these metrics in greater detail and report MPJPE &amp; PVE in the Sup. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>CamCalib. We follow the implementation of [20, <ref type="bibr" target="#b52">81]</ref> but use ResNet-50 as the backbone and predict pitch ?, roll ?, and vfov ? with separate fully-connected layers. Each parameter has B = 256 bins and we apply Softargmaxbiased-L 2 for ?, Softargmax-L 2 for ? and ?. The model is trained with images of varied resolutions for 30 epochs. The Pano360 dataset is used for training and evaluation.</p><p>SPEC. Similar to the original HMR <ref type="bibr" target="#b0">[29]</ref>, we use a ResNet-50 backbone, followed by fully-connected layers that iteratively regress SMPL parameters. We do not apply HMR's adversarial discriminator since we use psuedoground-truth 3D training data. The Adam optimizer with a learning rate of 5e ?5 is used. For the first 150 training epochs we use the COCO and SPEC-SYN datasets, and then incorporate MPI-INF-3DHP and Human3.6M. Total training takes around 175 epochs, ?4-5 days.</p><p>Note that CamCalib and SPEC are trained separately. Training them jointly is not possible because we lack an in-the-wild dataset that has both ground-truth bodies and diverse camera focal lengths and views. SPEC-MTP meets these requirements but is small so we us it for evaluation. During inference, CamCalib and SPEC run jointly. <ref type="table" target="#tab_1">Table 1</ref> reports the mean angular error in camera pitch ?, roll ?, and vfov ? for different camera calibration methods on the Pano360 test set. For reference, we inlcude the open-source implementation of ScaleNet <ref type="bibr" target="#b52">[81]</ref>, which uses a different backbone than CamCalib. We also train ScaleNet with our backbone on Pano360 (CamCalib (KL loss) in <ref type="table" target="#tab_1">Table 1</ref>). We evaluate the effect of different loss functions, i.e. KL-divergence, Softargmax-L 2 , and Softargmaxbiased-L 2 , and define the final CamCalib network to be the best performing version (Softargmax-biased-L 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Single-image camera calibration results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">SPEC evaluation</head><p>Tables 6, 7, and 8 show the results of recent SOTA methods on the SPEC-MTP, SPEC-SYN and 3DPW datasets.</p><p>The correct metric. We believe W-MPJPE is the metric that best reflects performance in real-world applications, so we report W-MPJPE, PA-MPJPE, and W-PVE. MPJPE is also reported and discussed in Sup. Mat. For W-MPJPE and W-PVE, we report both definitions from Sec. 4.2, i.e. (1)/(2) in the tables. Note that SPEC has the same error under both metrics. PA-MPJPE has only one entry because Procrustes alignment removes the effect of camera rotation (and more); this effectively hides the fact that SOTA methods do not estimate global pose well.</p><p>Comparison to the state-of-the-art. To compute the performance of SOTA methods, we use their open source implementations. We use HMR * as our IWP-cam baseline, which is HMR <ref type="bibr" target="#b0">[29]</ref> trained with the same datasets as SPEC, i.e. COCO, SPEC-SYN, MPI-INF-3DHP, and Human3.6M. Again, we do not use HMR's discriminator since we train with ground-truth or pseudo ground-truth 3D labels. For I2L-MeshNet <ref type="bibr" target="#b18">[47]</ref>, we use the SMPL output of this method instead of non-parametric mesh to be able to report W-PVE and denote this with ? .</p><p>Since W-MPJPE and W-PVE measure the error w.r.t. the body in world coordinates, these measures reveal the performance improvement of SPEC over the SOTA when the camera deviates from the IWP-cam assumption. Compared a dataset like 3DPW, SPEC-MTP and SPEC-SYN have significantly more variety in focal lengths and viewpoints as shown in <ref type="figure">Fig. 5</ref>. As a result, SPEC yields a larger improvement in W-MPJPE and W-PVE over the SOTA for these datasets. Using explicit camera information is a key driver of this improvement. The improvement in PA-MPJPE is less significant, suggesting that the largest improvements come from estimating the body in world coordinates rather than better articulated pose. This is valuable in many applications, e.g. human-scene interaction, where bodies and objects are often reconstructed from distinct methods but should reside in a common space. <ref type="figure">Figure 6</ref> analyzes W-MPJPE on SPEC-SYN for different camera viewpoints and focal lengths. SPEC results are  We use the implementations provided by the authors to obtain results. HMR * means that we train HMR using the same data as SPEC for fair comparison. ? means we use the SMPL output of this method instead of the non-parametric mesh to be able to report W-PVE. All numbers are in mm.    <ref type="table">Table 5</ref>: Ablation studies with SPEC-SYN. c: using the image center as camera center. f and R c : using CamCalib estimated focal length and camera rotation, respectively. similar across different camera settings, while the HMR * is less robust to values that deviate from its assumptions. Ablation experiments. We ablate different camera parameters and components of SPEC to investigate their effect on performance. We report results on the SPEC-SYN test data <ref type="table">(Table 5</ref>) because it has challenging cameras; for ablation results on 3DPW please see Sup. Mat.</p><p>We use HMR * as our baseline and use the same datasets and training configurations for all methods. To obtain the predicted 2D joints,? 2D , HMR * uses the bounding-box center as the principal point. We start by changing it to the image center, i.e. o x = w/2, o y = h/2 denoted as "HMR * + c". This ensures a better projective geometry than using the bounding-box center as the image center and already improves results. Next, we replace the fixed focal length of 5000 with the values estimated by CamCalib, denoted as "HMR * + c + f ". "HMR * + c + f + R c " uses R c instead of the identity matrix as the camera rotation during projection. Finally, SPEC uses c, f , and R c both during projection and as a conditioning input to the final stage of HMR * predictions. Overall, improving the camera model improves W-MPJPE and W-PVE. Conditioning the network on the camera parameters helps, but we suspect that better camera conditioning schemes can be employed to make the network more aware of the camera geometry.</p><p>Qualitative results. <ref type="figure">Figure 5</ref> shows representative results. HMR * assumes IWP-cam and yields incorrect body poses (legs in top row) and incoherent body orientations (middle &amp; bottom); SPEC predicts overall more globally coherent bodies as can be seen in side-view images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we demonstrate that a) camera geometry can be estimated from images and b) can effectively be leveraged to improve 3D HPS accuracy. Existing methods make simplifying assumptions about the camera: weakperspective projection, large constant focal length, and zero camera rotation. To go beyond these simple assumptions, we introduce SPEC, the first 3D HPS method that regresses a perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. Using the estimated camera parameters improves both SOTA camera regression methods and HPS regression methods. We introduce two new datasets, i.e. SPEC-MTP and SPEC-SYN, with accurate camera and 3D body annotations to showcase the effect of SPEC through ablation studies and comparison with the SOTA and to foster future research in this area. In this supplementary document, we provide more information that is not covered in the main text, ranging from technical details in the method, visual examples of the proposed datasets, more qualitative results, more ablation studies as well as more analysis and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Formulation of per-body translation t b</head><p>For each body in the image, besides SMPL parameters ?, ?, SPEC also estimates camera parameters (s, tx, ty), which is defined w.r.t. the bounding box (bbox) of the subject. Similar to <ref type="bibr">[25,</ref><ref type="bibr" target="#b3">32]</ref>, we perform a coordinate transformation to obtain the final t b vector w.r.t. the original full image following:</p><formula xml:id="formula_8">t b x = tx + 2(cx ? w/2) s ? w bbox , t b y = ty + 2(cy ? h/2) s ? h bbox , and t b z = 2 ? f h bbox ? s ,<label>(7)</label></formula><p>where (cx, cy) is the bbox center, w, h are originial image sizes, w bbox , h bbox are bbox sizes, and focal length f is estimated by CamCalib. As explained in the main text, Eq. 3, t b is used during</p><formula xml:id="formula_9">perspective projection ? = K[R c | ? t b ].</formula><p>A.2. Softargmax-L2 and Softargmax-biased-L2</p><p>As described in the main manuscript, we follow <ref type="bibr" target="#b52">[81]</ref> to discretize the spaces of pitch ?, roll ? and vertical field of view </p><p>This differentiable operation has been commonly-used in human joint detection <ref type="bibr" target="#b15">[44,</ref><ref type="bibr" target="#b34">63]</ref> to determine the peak location in a likelihood heat map, in contrast to the non-differentiable argmax operation.</p><p>For pitch ? and roll ? angles, we apply the standard L2 loss between the prediction and the ground truth. To encourage underestimation of vfov ? more than overestimation, we design an asymmetric loss as depicted in <ref type="figure" target="#fig_3">Fig. 3</ref> in the main text; formally:</p><formula xml:id="formula_11">L(?) = (???) 2 (???) 2 +1 , if? &lt;= ? (? ? ?) 2 , otherwise.<label>(9)</label></formula><p>We verify the benefits of these design choices in <ref type="table" target="#tab_1">Table 1</ref> in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Virtual ground plane</head><p>In all qualitative results, we visualize a virtual ground plane with a checkerboard, which is parallel to the xz-plane and therefore parameterized as [0, y, 0]. We define y = min (M(?, ?)[:, 2]), i.e. we place the ground plane just below the SMPL mesh.</p><p>This simple parameterization is feasible because we disentangle the camera rotation R c from the body orientation R b . For SOTA methods that apply the IWP-cam model, the virtual ground planes are often tilted. As a result, it requires further processing to estimate the up-vector, or conversely, the direction of gravity, making it non-trivial to integrate the reconstructed bodies for some downstream applications, e.g. scene understanding, character animation, physics simulation. SPEC, on the other hand, reconstructs bodies in the world coordinate frame with a consistent up vector [0, 1, 0], which is more physically plausible when visualized together with the ground plane. See Sup. Mat. video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. SMPLify-X-Cam</head><p>To integrate the estimated camera parameters from CamCalib into an optimization-based method, we use the original implementation of SMPLify-X <ref type="bibr" target="#b21">[50]</ref> with slight modifications. We replace the IWP-cam with the estimated K and R c as described in Sec. 3.3 of the main text. Additionally, we initialize the optimization with the output of HMR-EFT [27], instead of starting from a mean pose and a mean shape. We use the Adam optimizer with a step size of 10 ?2 for 100 steps for both the first and second stage of optimization. The results of SMPLify-X-cam are evaluated in Sec. C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. SPEC-MTP Dataset</head><p>M?ller et al. <ref type="bibr" target="#b19">[48]</ref> propose a "Mimic The Pose" (MTP) task to collect datasets of natural images with high-quality pseudo ground truth body parameters from Amazon Mechanical Turk (AMT). Each image shows a person mimicking a posed SMPL-X mesh presented to them on AMT. M?ller et al. devise a three-stage optimization routine, SMPLify-XC, to fit the SMPL-X model to each image. It is based on the default SMPLify-X <ref type="bibr" target="#b21">[50]</ref> but considers additionally the pose? and self-contactC of the presented SMPL-X mesh to constrain the optimization. The body parts in self-contact are identified by finding vertices that are close in Euclidean and far in geodesic space. Please see <ref type="bibr" target="#b19">[48]</ref> for a detailed definition of self-contact.</p><p>We follow the MTP approach but add two distinctions in order to obtain ground truth camera parameters: (1) Instead of getting a single picture for each pose, we ask AMT subjects to record a video showing the pose from multiple viewpoints, similar to Mannequin-Challenge style <ref type="bibr" target="#b10">[39]</ref>.</p><p>(2) We also ask them to print out a calibration pattern and record a video of the grid following a detailed protocol. In addition, we ask them to measure the size of the grid and take a picture of the grid and a ruler to verify the measured values.</p><p>To fit SMPL-X pose ? and shape ?, as well as camera pitch ?, roll ?, yaw ?, and camera translation t c to the collected MTP videos, we extend SMPLify-XC and introduce SMPLify-XC-Cam. We follow the three-stage optimization routine. In the first stage, body pose ? is initialized as the poses of the presented mesh, ? =?, and stays fixed in this stage. The objective is:</p><formula xml:id="formula_12">E(?, ?, ?, ?, t c ) = ?M EM + F i=1 EJ i .<label>(10)</label></formula><p>EM denotes the SMPLify-XC shape loss, that takes the ground truth height and weight of a person into account.</p><formula xml:id="formula_13">EJ i = ?i? ? M(?, ?), R c i , K, t c i ? J2D i 2 2 (11)</formula><p>is the 2D re-projection error of a single frame i with detected 2D joints J2D i , and camera rotation R c i and translation t c i . ?i and? are per joint confidence and weight, respectively. F is the number of frames per video, extracted at one frame per second.</p><p>In the second and the third stage, we freeze body shape ? and refine the pose ? and camera parameters by minimizing:</p><formula xml:id="formula_14">E(?, ?, ?, ?, t c ) =?m h Em h + ??E? + ?C EC + ?SES + F i=1 EJ i .<label>(12)</label></formula><p>Em h , E?, EC , ES denote the hand and presented pose priors, the presented contact loss and the general contact loss as defined in <ref type="bibr" target="#b19">[48]</ref>, respectively. <ref type="figure" target="#fig_9">Fig. 7</ref> shows several examples of SPEC-MTP frames and the computed SMPL-X fit. SPEC-MTP is used only for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. SPEC-SYN Dataset</head><p>We obtain the 3D scans and SMPL-X fits to those scans from the AGORA dataset <ref type="bibr" target="#b20">[49]</ref>. This includes many high-quality 3D scans of clothed people with accurate SMPL-X ground truth shape and pose. We convert the SMPL-X model to the SMPL format. We then put these scans in 3D scenes and use Unreal engine <ref type="bibr">[12]</ref> to generate photorealistic images with diverse fields of view (fov) and camera rotations. <ref type="figure" target="#fig_10">Fig. 8</ref> shows several examples, with SMPL fits overlaid on the images. One can observe some perspective distortion at the image boundary in the 3rd and 4th rows, indicating large fov (small focal length); the first and the last row show examples with high camera pitch angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Pano360 Dataset</head><p>To generate training dataset from equirectangular panorama images, we follow the strategy of Zhu et al. <ref type="bibr" target="#b52">[81]</ref>. We crop images from panorama images with random viewpoints and focal lengths. <ref type="figure" target="#fig_11">Fig. 9</ref> shows a sample panorama image along with the cropped images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Training Datasets</head><p>In addition to SPEC-SYN dataset which is described in main text, we use datasets explained below for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP</head><p>[45] is a multi-view indoor 3D human pose estimation dataset. 3D annotations are captured via a commercial markerless mocap software, therefore it is less accurate than some of the 3D datasets e.g. Human3.6M <ref type="bibr">[22]</ref>. We use all of the training subjects S1 to S8 which makes 90K images in total. Human3.6M [22] is an indoor, multi-view 3D human pose estimation dataset. Following previous methods, for training, we use 5 subjects (S1, S5, S6, S7, S8) which means 292K images. COCO <ref type="bibr" target="#b12">[41]</ref> dataset is a 2D keypoint dataset. In addition to 2D keypoint annotations, we utilize SMPLift-X-cam and Cam-Calib method to obtain SMPL and camera parameters annotations. We initialize the SMPLify-X-cam with SMPL fits provided by EFT [27] method.</p><p>Training Dataset Ratios. To obtain the final best performing model, we follow EFT [27] and SPIN <ref type="bibr" target="#b7">[36]</ref> which use fixed data sampling ratios for each batch. We first train SPEC with 50% SPEC-SYN, 50% COCO for 175K steps. Then, we continue training with 20% Human3.6M, 20% MPI-INF-3DHP, with 50% SPEC-SYN, and 50% COCO for around 50K steps until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. CamCalib Qualitative Results</head><p>In <ref type="figure" target="#fig_0">Fig. 10</ref>, we show the qualitative results of CamCalib. We follow <ref type="bibr" target="#b52">[81]</ref> to visualize the estimated camera rotation by drawing the estimated horizons (red dashed lines). If the camera is pointing down (pitch angle ? &gt; 0), the horizon should appear in the upper half of the image. Tilting to the left or right indicates the camera roll. CamCalib estimates reasonable camera parameters for most examples. We also show the failure cases in <ref type="figure" target="#fig_0">Fig. 11</ref>. We observe that they are all portrait images in which background contain little information for estimating camera parameters. We remark that despite no rich information in the background, human bodies still provide useful cues for the calibration purpose and we leave this to the future work. <ref type="table">Table 6</ref> to 8 summarize the performance of SPEC in comparison to SOTA methods on three datasets: SPEC-MTP, SPEC-SYN, and 3DPW. In addition to the three metrics, W-MPJPE, PA-MPJPE and W-PVE that are already reported in the main paper, we also include MPJPE and PVE here. The two versions of W-MPJPE and W-PVE are defined in Sec. 4.2 in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. SPEC MPJPE/PVE Results and Discussions</head><p>First, we observe that SPEC yields better "pure body pose" according to the improved PA-MPJPE. Moving onward, a 3DHPS method should learn not only to reconstruct body poses and shapes but also to place and orient them properly in the space. To this end, MPJPE and PVE are often considered stricter than the Procrustesaligned counterparts as they measure additionally discrepancies in rotation. SPEC also outperforms SOTA methods <ref type="bibr" target="#b7">[36,</ref><ref type="bibr" target="#b8">37,</ref><ref type="bibr" target="#b18">47,</ref><ref type="bibr" target="#b26">55]</ref> in MPJPE/PVE, but yield on-par or slightly worse results than HMR * , which is an IWP-cam baseline trained under the identical setting as SPEC.</p><p>Note that MPJPE and PVE are typically computed in the camera space. One needs to transform the ground truth bodies using the camera extrinsics provided by the datasets, and thus the error       SMPLify-X ( f = 5000) SMPLify-X (CamCalib K) <ref type="figure" target="#fig_0">Figure 12</ref>: Sensitivity of SMPLify-X to focal length perturbation on SPEC-SYN dataset. Using CamCalib estimated f yields better results. Body reconstruction accuracy is less sensitive to larger focal lengths. Therefore, we propose Softargmax-biased-L 2 loss.  <ref type="table">Table 6</ref> caption.</p><p>encodes dataset-specific camera information. However, existing benchmarks, e.g., MPI-INF-3DHP, Human3.6M and 3DPW are often captured with little variation in camera parameters. As a results, MPJPE and PVE cannot clearly reflect the performance of a IWP-cam method for in-the-wild scenarios where camera types and viewpoints are diverse and unknown. We advocate W-MPJPE and W-PVE because they also measure discrepancies in rotation, but unlike MPJPE/PVE, they are computed in world coordinates, assuming no access to camera information. In W-MPJPE and W-PVE, SPEC again outperforms SOTA methods <ref type="bibr" target="#b7">[36,</ref><ref type="bibr" target="#b8">37,</ref><ref type="bibr" target="#b18">47,</ref><ref type="bibr" target="#b26">55]</ref> and yield consistently better results than HMR * on datasets captured with diverse camera parameters -SPEC-MTP and SPEC-SYN. Even on 3DPW which is captured with single focal length value, SPEC attains improved or on-par results than HMR * . <ref type="table" target="#tab_8">Table 9</ref> and 10 summarize the performance of SMPLify-X-cam in comparison to the baseline SMPLify-X method. The same 2D keypoint detections from [8] are used for all the reported methods. We compare the default SMPLify-X with f = 5000 <ref type="bibr" target="#b21">[50]</ref>, the setting considered by Kissos et al. <ref type="bibr" target="#b3">[32]</ref> where f = 2200, and the setting which uses CamCalib estimated K and R c . We observe a consistent improvement in W-MPJPE and W-PVE when R c is used. This is due to correct global orientation reconstruction w.r.t. world coordinates. On SPEC-SYN, using K improves the PA-MPJPE due to more accurate projective geometry. 3DPW is captured with a single camera where f = 1962 so it is not a good dataset with which to evaluate the effect of focal length. Even in this case, SMPLify-X-cam improves the results over the default setting and the PA-MPJPE result is on par with results using f = 2200. The f = 2200 approximation is already close to the single focal length used in 3DPW dataset f = 1962, consequently it does well. As a reference, the average CamCalib focal length error is 360 and 246 pixels on 3DPW and SPEC-SYN datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. SMPLify-X-cam Results</head><p>To further analyze the impact of focal length on body reconstructions, we run SMPLify-X on SPEC-SYN with focal lengths perturbed from the real ground truth values and plot the W-MPJPE trend in <ref type="figure" target="#fig_0">Fig. 12 (blue curve)</ref>. We see that the quality of HPS are sensitive to underestimated focal lengths and less sensitive to overestimation, as also reported in <ref type="bibr" target="#b3">[32,</ref><ref type="bibr" target="#b47">76]</ref>. In addition, we visualize default SMPLify-X (f = 5000) and SMPLify-X (CamCalib K) according to the corresponding W-MPJPE in <ref type="table" target="#tab_8">Table 9</ref>. One can see that in average, with the focal lengths from CamCalib, body reconstructions are closer to the low-error basin of small perturbations, i.e. closer to the true focal lengths. Note that, the averaged focal length in SPEC-SYN is 840 pixels, so the averaged error of 246 pixels (29%) confirms again that SMPLify-X-cam is relatively robust when the estimated focal length ranges between 0.7 and 1.3 times the true one.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Ablation study on SPEC with 3DPW</head><p>The <ref type="table">Table 5</ref> in the main text provides an ablation study on SPEC using the SPEC-SYN dataset, which dissects the improve-ment over the baseline HMR * in various aspects: using the original image center as the principal point, using the CamCalib estimated focal length, using the estimated rotations, and lastly conditioning the network with the estimated cameras (SPEC). We repeat this here on the common 3DPW benchmark as in <ref type="table" target="#tab_1">Table 11</ref>. Despite that it is not a suitable dataset to analyze the impact of each camera parameters, we still observe that appending the estimated cameras to the image feature leads to improvement in five metrics (c.f. the last two rows), so does using the estimated focal length (HMR * + c + f vs. HMR * + c).</p><p>C.6. Comparison to Wang et al. <ref type="bibr" target="#b39">[68]</ref> Wang et al. <ref type="bibr" target="#b39">[68]</ref> train their methods on 3DPW training set. We also train SPEC on 3DPW to make a comparison to their method. Results are denoted in <ref type="table" target="#tab_1">Table 12</ref>. SPEC outperforms Wang et al. <ref type="bibr" target="#b39">[68]</ref> in terms of PA-MPJPE metric, but performs poorly in MPJPE. This is due to the use of estimated camera parameters in SPEC's evaluation. We argue that SPEC would perform better with W-MPJPE metric, however a comparison is not possible since the code of Wang et al. <ref type="bibr" target="#b39">[68]</ref> is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7. Qualitative results of SPEC</head><p>In <ref type="figure" target="#fig_0">Fig. 13</ref>, we show several qualitative results from SPEC. One can observe that SPEC yields on-par or more physically plausible reconstructed bodies than the baseline that is trained with the identical setting. For more clear illustration, please see the 360 ? visualizations in Sup. Mat. video.</p><p>The failure cases of SPEC are shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> We observe that some examples share similar traits as those in <ref type="figure" target="#fig_0">Fig. 10</ref>: portrait images with limited background information. The error of SPEC can be partially attributed to the error from CamCalib. Other scenarios include rarely-seen viewpoints or poses that are not observed in the training data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Input image (b) SOTA method (c) Our results Focal length = 755 px Camera pitch = 33.4 o Camera roll = -0.6 o State-of-the-art methods for 3D human pose and shape estimation from images (such as HMR trained with EFT [27] data) struggle with imagery containing perspective effects. (b) In part, this is due to the use of the standard weak perspective camera. (c) SPEC learns to estimate perspective camera parameters and uses these to regress more accurate 3D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of IWP-cam and SPEC. R c and t c are camera rotation and translation. R b and t b are body orientation and translation. All are defined in world coordinate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Softargmaxbiased-L 2 penalizes underestimates of vfov less than over estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The mean per joint position error (MPJPE), Procrustesaligned mean per joint position error (PA-MPJPE), and per vertex error (PVE) are the most commonly-used evaluation metrics in the literature. PA-MPJPE exists as a met-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Qualitative results. Top &amp; middle: SPEC-MTP; bottom: SPEC-SYN. We also provide failure cases in Sup. Mat. Breakdown of W-MPJPE per camera focal length and pitch range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(vfov) ? into B = 256 bins but avoid casting it as a pure classification problem. To this end, we propose Softargmax-L2 loss and the biased variant. Let ? = [?1, . . . ?i, . . . ?B], ? = [?1, . . . ?i, . . . ?B], and ? = [?1, . . . ?i, . . . ?B] denote the center values of each of the bins, and let p ? = [p ? 1 , . . . p ? i , . . . p ? B ], p ? = p ? 1 , . . . p ? i , . . . p ? B , and p ? = [p ? 1 , . . . p ? i , . . . p ? B] denote the probability mass from the fully-connected layers of each head respectively. We compute the expectation value of the probability mass as the prediction:?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>annotation overlay (c) SPEC-MTP annotation sideviews</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>SPEC-MTP benchmark samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>SPEC-SYN dataset samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Pano360 dataset. Random viewpoints (b) from a single equirectangular panorama image (a). Horizon annotations are shown in red line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>CamCalib qualitative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>CamCalib failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>SPEC qualitative results. SPEC failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Methods vfov ? ? ? pitch ? ? ? roll ? ? ?</figDesc><table><row><cell>ScaleNet [81]</cell><cell>5.68</cell><cell>2.61</cell><cell>1.41</cell></row><row><cell>CamCalib (KL Loss)</cell><cell>3.53</cell><cell>2.32</cell><cell>1.15</cell></row><row><cell>CamCalib (Softargmax-L 2 )</cell><cell>3.34</cell><cell>2.06</cell><cell>1.11</cell></row><row><cell>CamCalib (Softargmax-biased-L 2 )</cell><cell>3.24</cell><cell>1.94</cell><cell>1.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Regressing camera parameters. CamCalib methods are trained and tested on the Pano360 dataset. ScaleNet<ref type="bibr" target="#b52">[81]</ref> results use the authors' implementation. ric specifically because current HPS methods that use IWPcam reconstruct bodies in camera coordinates? cam 3D . Procrustes alignment "hides many sins" in that it removes the rotation of the body caused by an unknown camera pose. See Sup. Mat. for details of how PA-MPJPE and MPJPE are computed.Instead, we propose variants of MPJPE and PVE that compute the error in world coordinates without the need of camera information and dub them W-MPJPE and W-PVE. Since SPEC disentangles camera and body rotations, the predictions reside in world coordinates? world</figDesc><table><row><cell>3D</cell><cell>and W-</cell></row></table><note>3D ? J 3D . For existing SOTA methods, we report two versions of W-MPJPE: (1)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of SOTA methods on SPEC-MTP dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="2">W-MPJPE PA-MPJPE</cell><cell>W-PVE</cell></row><row><cell>GraphCMR [37]</cell><cell>137.8 / 129.4</cell><cell cols="2">69.1 158.4 / 152.1</cell></row><row><cell>SPIN [36]</cell><cell>122.2 / 116.6</cell><cell cols="2">59.0 140.9 / 135.8</cell></row><row><cell cols="2">Partial Humans [55] 139.4 / 132.9</cell><cell cols="2">76.9 160.1 / 152.7</cell></row><row><cell>I2L-MeshNet  ? [47]</cell><cell>133.3 / 119.6</cell><cell cols="2">60.0 154.5 / 141.2</cell></row><row><cell>HMR  *  [29]</cell><cell>119.2 / 104.0</cell><cell cols="2">53.7 136.2 / 120.6</cell></row><row><cell>SPEC</cell><cell>106.4 / 106.4</cell><cell cols="2">53.2 127.4 / 127.4</cell></row></table><note>Results of SOTA methods on SPEC-SYN. See Ta- ble 6 caption.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of SOTA methods on 3DPW test set. SeeTable 6caption.</figDesc><table><row><cell>Methods</cell><cell cols="2">W-MPJPE PA-MPJPE</cell><cell>W-PVE</cell></row><row><cell>HMR  *</cell><cell>128.7 / 96.4</cell><cell cols="2">55.9 144.2 / 111.8</cell></row><row><cell>HMR  *  + c</cell><cell>120.4 / 84.2</cell><cell>54.0</cell><cell>135.3 / 98.8</cell></row><row><cell>HMR  *  + c + f</cell><cell>118.3 / 85.1</cell><cell>54.0</cell><cell>132.8 / 99.7</cell></row><row><cell cols="2">HMR  77.2 / 77.2</cell><cell>55.3</cell><cell>93.8 / 93.8</cell></row><row><cell>SPEC</cell><cell>74.9 / 74.9</cell><cell>54.5</cell><cell>90.5 / 90.5</cell></row></table><note>* + c + f + Rc</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>[ 1 ]</head><label>1</label><figDesc>Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In IEEE Conference on Computer Vision and Pattern Recognition, 2014. 1 [2] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Kristen Grauman, Gregory Shakhnarovich, and Trevor Dar-rell. Inferring 3D structure with a statistical image-based shape model. In International Conference on Computer Vision, 2003. 2 [16] Riza Alp Guler and Iasonas Kokkinos. HoloPose: Holistic 3D human reconstruction in-the-wild. In IEEE Conference on Computer Vision and Pattern Recognition, June 2019.</figDesc><table><row><cell>Ex-ploiting temporal context for 3D human pose estimation in the wild. In IEEE Conference on Computer Vision and Pat-tern Recognition, 2019. 3 [3] Alexandru O Balan, Leonid Sigal, Michael J Black, James E Davis, and Horst W Haussecker. Detailed human shape and pose from images. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2007. 2 [4] Vasileios Belagiannis, Sikandar Amin, Mykhaylo Andriluka, Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation. In IEEE Con-ference on Computer Vision and Pattern Recognition, 2014. 2 [5] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In European Conference on Computer Vision, 2016. 1, 3, 4 [6] Magnus Burenius, Josephine Sullivan, and Stefan Carlsson. 3d pictorial structures for multiple view articulated pose esti-mation. In IEEE Conference on Computer Vision and Pattern Recognition, 2013. 2 [7] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-itrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention. In European Conference on Computer Vision, 2020. 1, 3 [8] MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://github.com/ open-mmlab/mmpose, 2020. 5, 8 [9] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estima-tion from multiple views. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2 [10] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xi-aowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, 2020. 3 [11] Ahmed Elhayek, Edilson de Aguiar, Arjun Jain, Jonathan Tompson, Leonid Pishchulin, Micha Andriluka, Chris Bre-gler, Bernt Schiele, and Christian Theobalt. Efficient convnet-based marker-less motion capture in general scenes with a low number of cameras. In IEEE Conference on Com-puter Vision and Pattern Recognition, 2015. 2 [12] Epic Games. Unreal engine. https://www. unrealengine.com, 2021. 2 [13] Juergen Gall, Carsten Stoll, Edilson De Aguiar, Christian Theobalt, Bodo Rosenhahn, and Hans-Peter Seidel. Motion capture using joint skeleton tracking and surface estimation. In IEEE Conference on Computer Vision and Pattern Recog-nition, 2009. 2 [14] Stuart Geman and Donald E. McClure. Statistical methods for tomographic image reconstruction. Bulletin of the Inter-national Statistical Institute, LII-4:5-21, 1987. 4 [15] 1, 3 [17] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-ard Pons-Moll, and Christian Theobalt. DeepCap: Monocu-lar human performance capture using weak supervision. In IEEE Conference on Computer Vision and Pattern Recogni-tion. IEEE, jun 2020. 1, 3 [18] Nils Hasler, Bodo Rosenhahn, Thorsten Thormahlen, Michael Wand, J?rgen Gall, and Hans-Peter Seidel. Mark-erless motion capture with unsynchronized moving cameras. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 224-231. IEEE, 2009. 3 [19] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. Epipolar transformers. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 3 [20] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisen-mann, Matthew Fisher, Emiliano Gambaretto, Sunil Hadap, and Jean-Fran?ois Lalonde. A perceptual measure for deep single image camera calibration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2, 3, 4, 6 [21] Chun-Hao Huang, Benjamin Allain, Edmond Boyer, Jean-Sebastien Franco, Federico Tombari, Nassir Navab, and Slo-bodan Ilic. Tracking-by-detection of 3d human shapes: from surfaces to volumes. IEEE Transaction on Pattern Analysis and Machine Intelligence, 2017. 1, 2 [22] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predic-tive methods for 3D human sensing in natural environments. In IEEE Transaction on Pattern Analysis and Machine Intel-ligence, 2014. 1, 2, 6 [23] Umar Iqbal, Pavlo Molchanov, and Jan Kautz. Weakly-supervised 3d human pose learning via multi-view images in the wild. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 3 [24] Karim Iskakov, Egor Burkov, Victor Lempitsky, and Yury Malkov. Learnable triangulation of human pose. In Interna-tional Conference on Computer Vision, 2019. 3 [25] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis. Coherent reconstruction of multiple humans from a single image. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 1, 3 [26] Sam Johnson and Mark Everingham. Learning effective hu-man pose estimation from inaccurate annotation. In IEEE Conference on Computer Vision and Pattern Recognition, 2011. 1 [27] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-emplar fine-tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation. arXiv preprint arXiv:2004.03686, 2020. 1, 3, 6, 2 [28] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-ture: A 3D deformation model for tracking faces, hands, and bodies. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8320-8329, 2018. 2, 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>HPS optimization with an estimated camera. SMPLify-X-cam on the SPEC-SYN dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SMPLify-X with varied f</cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell></row><row><cell>W-MPJPE (mm)</cell><cell>160 180</cell><cell></cell><cell></cell></row><row><cell></cell><cell>140</cell><cell></cell><cell></cell></row><row><cell></cell><cell>?100</cell><cell>?50 Focal length perturbation (%) 0 50</cell><cell>100</cell><cell>150</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Results of SOTA methods on 3DPW test set. See</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Ablation experiments on SPEC with 3DPW validation set. c: using the image center as camera center; f and R c : using CamCalib estimated focal length and camera rotation, respectively. All numbers are in mm.</figDesc><table><row><cell>Methods</cell><cell cols="2">MPJPE PA-MPJPE</cell></row><row><cell>Want et al. [68]</cell><cell>89.7</cell><cell>65.2</cell></row><row><cell>SPEC w. 3DPW</cell><cell>96.4</cell><cell>52.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Comparison to Wang et al.<ref type="bibr" target="#b39">[68]</ref>. Here both methods are trained with 3DPW training set for a fair comparison.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PoseNet: A convolutional network for real-time 6-DOF camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond weak perspective for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imry</forename><surname>Kissos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Oks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2, 3, 4, 1</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PARE: Part attention regressor for 3D human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3D human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SMPLy benchmarking 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Br?gier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Combaluzier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4521" to="4530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Uncalibrated motion capture exploiting articulated structure constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liebowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="171" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">4d human body capture from egocentric video via 3d scene grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13341,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graphics (Proc. SIG-GRAPH Asia)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3DVision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3DVision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On self contact and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">T</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Full-body awareness from partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Frankmocap: Fast monocular 3d hand and body motion capture by regression and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-view matching (mvm): Facilitating multi-person 3d pose estimation learning with action-frozen people video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeji</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In arXiv preprint, 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Articulated mesh animation from multi-view silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovan</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Horizon lines in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MonoClothCap: Towards temporally coherent clothing capture from monocular rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3DVision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing scene viewpoint using panoramic place representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GHUM &amp; GHUML: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">GHUM &amp; GHUML: Generative 3D human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6183" to="6192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3d pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PCLs: Geometry-aware neural reconstruction of 3d pose with perspective crop layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cen-terHMR: a bottom-up single-shot method for multi-person 3d mesh recovery from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Yili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Tao</surname></persName>
		</author>
		<idno>arxiv:2008.12272</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">4D association graph for realtime multi-person motion capture using multiple video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Single view metrology in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Hold-Geoffroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-Mpjpe Pa-Mpjpe W-Pve</forename><surname>Methods</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smplify-X-Cam (camcalib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+ R C</surname></persName>
		</author>
		<idno>115.2 / 115.2 73.5 135.0 / 135.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mpjpe W-Mpjpe Pa-Mpjpe W-Pve</forename><surname>Methods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pve</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">We use the implementations provided by the authors to obtain results. HMR * means that we train HMR using the same data as SPEC for fair comparison. ? means we use the SMPL output of this method instead of the non-parametric mesh to be able to report W-PVE. All numbers are in mm</title>
		<ptr target="MethodsMPJPEW-MPJPEPA-MPJPEW-PVEPVE" />
	</analytic>
	<monogr>
		<title level="m">Results of SOTA methods on SPEC-MTP dataset</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">Results of SOTA methods on SPEC-SYN. See Table 6 caption. Methods MPJPE W-MPJPE PA-MPJPE W-PVE PVE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-Mpjpe Pa-Mpjpe W-Pve</forename><surname>Methods</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">HPS optimization with an estimated camera. SMPLify-X-cam on the 3DPW validation set. Methods W-MPJPE PA-MPJPE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
