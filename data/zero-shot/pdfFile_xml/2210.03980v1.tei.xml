<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zheng</surname></persName>
							<email>junhaozheng47@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxian</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
							<email>qianlima@scut.edu.cn*</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continual Learning for Named Entity Recognition (CL-NER) aims to learn a growing number of entity types over time from a stream of data. However, simply learning Other-Class in the same way as new entity types amplifies the catastrophic forgetting and leads to a substantial performance drop. The main cause behind this is that Other-Class samples usually contain old entity types, and the old knowledge in these Other-Class samples is not preserved properly. Thanks to the causal inference, we identify that the forgetting is caused by the missing causal effect from the old data. To this end, we propose a unified causal framework to retrieve the causality from both new entity types and Other-Class. Furthermore, we apply curriculum learning to mitigate the impact of label noise and introduce a self-adaptive weight for balancing the causal effects between new entity types and Other-Class. Experimental results on three benchmark datasets show that our method outperforms the state-ofthe-art method by a large margin. Moreover, our method can be combined with the existing state-of-the-art methods to improve the performance in CL-NER. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) is a vital task in various NLP applications <ref type="bibr" target="#b16">(Ma and Hovy, 2016)</ref>. Traditional NER aims at extracting entities from unstructured text and classifying them into a fixed set of entity types (e.g., Person, Location, Organization, etc). However, in many real-world scenarios, the training data are streamed, and the NER systems are required to recognize new entity types to support new functionalities, which can be formulated into the paradigm of continual learning (CL, a.k.a. incremental learning or lifelong learning) * *Corresponding author 1 Our codes are publicly available at https://github.com/zzz47zzz/CFNER <ref type="figure">Figure 1</ref>: An illustration of Other-class in CL-NER. Suppose that a model learns four entity types in CoNLL2003 sequentially. "LOC": Location; "MISC": Miscellaneous; "ORG": Organisation; "PER": Person. <ref type="bibr" target="#b31">(Thrun, 1998;</ref><ref type="bibr" target="#b21">Parisi et al., 2019)</ref>. For instance, voice assistants such as Siri or Alexa are often required to extract new entity types (e.g. Song, Band) for grasping new intents (e.g. GetMusic) <ref type="bibr">(Monaikul et al., 2021)</ref>.</p><p>However, as is well known, continual learning faces a serious challenge called catastrophic forgetting in learning new knowledge <ref type="bibr" target="#b17">(McCloskey and Cohen, 1989;</ref><ref type="bibr" target="#b26">Robins, 1995;</ref><ref type="bibr" target="#b8">Goodfellow et al., 2013;</ref><ref type="bibr" target="#b14">Kirkpatrick et al., 2017)</ref>. More specifically, simply fine-tuning a NER system on new data usually leads to a substantial performance drop on previous data. In contrast, a child can naturally learn new concepts (e.g., Song and Band) without forgetting the learned concepts (e.g., Person and Location). Therefore, continual learning for NER (CL-NER) is a ubiquitous issue and a big challenge in achieving human-level intelligence.</p><p>In the standard setting of continual learning, only new entity types are recognized by the model in each CL step. For CL-NER, the new dataset contains not only new entity types but also Other-class tokens which do not belong to any new entity types. For instance, about 89% tokens belongs to Otherclass in OntoNotes5 <ref type="bibr" target="#b12">(Hovy et al., 2006)</ref>. Unlike accuracy-oriented tasks such as the image/text classification, NER inevitably introduces a vast number of Other-class samples in training data. As a result, the model strongly biases towards Other-class <ref type="bibr" target="#b15">(Li et al., 2020)</ref>. Even worse, the meaning of Otherclass varies along with the continual learning pro- <ref type="figure">Figure 2</ref>: An illustration of the impact of Other-class samples on OntoNotes5. We consider two scenarios with different extra annotation levels on Other-class samples: (1) annotate all recognized entity types on the data in the current CL step (Current); (2) no extra annotations on Other-class samples (None). cess. For example, "Europe" is tagged as Location if and only if the entity type Location is learned in the current CL step. Otherwise, the token "Europe" will be tagged as Other-class. An illustration is given in <ref type="figure">Figure 1</ref> to demonstrate Other-class in CL-NER. In a nutshell, the continually changing meaning of Other-class as well as the imbalance between the entity and Other-class tokens amplify the forgetting problem in CL-NER. <ref type="figure">Figure 2</ref> is an illustration of the impact of Otherclass samples. We divide the training set into 18 disjoint splits, and each split corresponds to one entity type to learn. Then, we only retain the labels of the corresponding entity type in each split while the other tokens are tagged as Other-class. Next, the NER model learns 18 entity types one after another, as in CL. To eliminate the impact of forgetting, we assume that all recognized training data can be stored. <ref type="figure">Figure 2</ref> shows two scenarios where Other-class samples are additionally annotated with ground-truth labels or not. Results show that ignoring the different meanings of Otherclasses affects the performance dramatically. The main cause is that Other-class contains old entities. From another perspective, the old entities in Other-class are similar to the reserved samples of old classes in the data replay strategy <ref type="bibr" target="#b25">(Rebuffi et al., 2017)</ref>. Therefore, we raise a question: how can we learn from Other-class samples for anti-forgetting in CL-NER?</p><p>In this study, we address this question with a Causal Framework for CL-NER (CFNER) based on causal inference <ref type="bibr" target="#b7">(Glymour et al., 2016;</ref><ref type="bibr" target="#b29">Sch?lkopf, 2022)</ref>. Through causal lenses, we determine that the crux of CL-NER lies in establishing causal links from the old data to new entity types and Other-class. To achieve this, we utilize the old model (i.e., the NER model trained on old entity types) to recognize old entities in Other-class samples and distillate causal effects <ref type="bibr" target="#b7">(Glymour et al., 2016)</ref> from both new entity types and Other-class simultaneously. In this way, the causality of Otherclass can be learned to preserve old knowledge, while the different meanings of Other-classes can be captured dynamically. In addition, we design a curriculum learning <ref type="bibr" target="#b2">(Bengio et al., 2009</ref>) strategy to enhance the causal effect from Other-class by mitigating the label noise generated by the old model. Moreover, we introduce a self-adaptive weight to dynamically balance the causal effects from Other-class and new entity types. Extensive experiments on three benchmark NER datasets, i.e., <ref type="bibr">OntoNotes5, i2b2 (Murphy et al., 2010</ref><ref type="bibr">) and CoNLL2003 (Sang and De Meulder, 2003</ref>, validate the effectiveness of the proposed method. The experimental results show that our method outperforms the previous state-of-the-art method in CL-NER significantly. The main contributions are summarized as follows:</p><p>? We frame CL-NER into a causal graph <ref type="bibr" target="#b23">(Pearl, 2009</ref>) and propose a unified causal framework to retrieve the causalities from both Otherclass and new entity types.</p><p>? We are the first to distillate causal effects from Other-class for anti-forgetting in CL, and we propose a curriculum learning strategy and a self-adaptive weight to enhance the causal effect in Other-class.</p><p>? Through extensive experiments, we show that our method achieves the state-of-the-art performance in CL-NER and can be implemented as a plug-and-play module to further improve the performances of other CL methods.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Continual Learning for NER</head><p>Despite the fast development of CL in computer vision, most of these methods <ref type="bibr" target="#b6">(Douillard et al., 2020;</ref><ref type="bibr" target="#b25">Rebuffi et al., 2017;</ref><ref type="bibr" target="#b11">Hou et al., 2019)</ref> are devised for accuracy-oriented tasks such as image classification and fail to preserve the old knowledge in Other-class samples. In our experiment, we find that simply applying these methods to CL-NER does not lead to satisfactory performances. In CL-NER, a straightforward solution for learning old knowledge from Other-class samples is self-training <ref type="bibr" target="#b27">(Rosenberg et al., 2005;</ref><ref type="bibr" target="#b4">De Lange et al., 2019)</ref>. In each CL step, the old model is used to annotate the Other-class samples in the new dataset. Next, a new NER model is trained to recognize both old and new entity types in the dataset. The main disadvantage of self-training is that the errors caused by wrong predictions of the old model are propagated to the new model <ref type="bibr">(Monaikul et al., 2021)</ref>. <ref type="bibr">Monaikul et al. (2021)</ref> proposed a method based on knowledge distillation <ref type="bibr" target="#b10">(Hinton et al., 2015)</ref> called ExtendNER where the old model acts as a teacher and the new model acts as a student. Compared with self-training, this distillation-based method takes the uncertainty of the old model's predictions into consideration and reaches the state-of-the-art performance in CL-NER.</p><p>Recently, <ref type="bibr" target="#b3">Das et al. (2022)</ref> alleviates the problem of Other-tokens in few-shot NER by contrastive learning and pretraining techniques. Unlike them, our method explicitly alleviates the problem brought by Other-Class tokens through a causal framework in CL-NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Inference</head><p>Causal inference <ref type="bibr" target="#b7">(Glymour et al., 2016;</ref><ref type="bibr" target="#b29">Sch?lkopf, 2022)</ref> has been recently introduced to various computer vision and NLP tasks, such as semantic segmentation , long-tailed classification <ref type="bibr" target="#b30">(Tang et al., 2020;</ref><ref type="bibr" target="#b20">Nan et al., 2021)</ref>, distantly supervised NER  and neural dialogue generation <ref type="bibr" target="#b35">(Zhu et al., 2020)</ref>. <ref type="bibr" target="#b13">Hu et al. (2021)</ref> first applied causal inference in CL and pointed out that the vanishing old data effect leads to forgetting. Inspired by the causal view in <ref type="bibr" target="#b13">(Hu et al., 2021)</ref>, we mitigate the forgetting problem in CL-NER by mining the old knowledge in Other-class samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Causal Views on (Anti-) Forgetting</head><p>In this section, we explain the (anti-) forgetting in CL from a causal perspective. First, we model the causalities among data, feature, and prediction at any consecutive CL step with a causal graph <ref type="bibr" target="#b23">(Pearl, 2009)</ref> to identify the forgetting problem. The causal graph is a directed acyclic graph whose nodes are variables, and directed edges are causalities between nodes. Next, we introduce how causal effects are utilized for anti-forgetting. <ref type="figure">Figure 3a</ref> shows the causal graph of CL-NER when no anti-forgetting techniques are used. Specifically, we denote the old data as S; the new data as D; the feature of new data extracted from the old and new model as X 0 and X; the prediction of new data as? (i.e., the probability distribution (scores)). The causality between notes is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Graph</head><p>(1) D ? X ?? : D ? X represents that the feature X is extracted by the backbone model (e.g., BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>), and X ?? indicates that the prediction? is obtained by using the feature X with the classifier (e.g., a fully-connected layer); (2) S ? X 0 ? D: these links represent that the old feature representation of new data X 0 is determined by the new data D and the old model trained on old data S. <ref type="figure">Figure 3a</ref> shows that the forgetting happens because there are no causal links between S and? . More explanations about the forgetting in CL-NER are demonstrated in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Colliding Effects</head><p>In order to build cause paths from S to? , a naive solution is to store (a fraction of) old data, resulting in a causal link S ? D is built. However, storing old data contradicts the scenario of CL to some extent. To deal with this dilemma, <ref type="bibr" target="#b13">Hu et al. (2021)</ref> proposed to add a causal path S ? D between old and new data by using Colliding Effect <ref type="bibr" target="#b7">(Glymour et al., 2016)</ref>. Consequently, S and D will be correlated to each other when we control the collider X 0 . Here is an intuitive example: a causal graph sprinkler ? pavement ? weather represents the pavement's condition (wet/dry) is determined by both the weather (rainy/sunny) and the sprinkler (on/off). Typically, the weather and the sprinkler are independent of each other. However, if we observe that the pavement is wet and know that the sprinkler is off, we can infer that the weather is likely to be rainy, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Causal Framework for CL-NER</head><p>In this section, we frame CL-NER into a causal graph and identify that learning the causality in Other-class is crucial for CL-NER. Based on the characteristic of CL-NER, we propose a unified causal framework to retrieve the causalities from both Other-class and new entity types. We are the first to distillate causal effects from Other-class for anti-forgetting in CL. Furthermore, we introduce  a curriculum-learning-based strategy and a selfadaptive weight to allow the model to better learn the causalities from Other-class samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>In the i-th CL step, given an NER model M i which is trained on a set of entities E i = {e 1 , e 2 ? ? ? , e n i }, the target of CL-NER is to learn the best NER model M i+1 to identify the extended entity set E i+1 = {e 1 , e 2 ? ? ? , e n i+1 } with a training set annotated only with E N ew i = {e n i +1 , e n i +2 , ? ? ? , e n i+1 }.</p><p>Suppose the model consists of a backbone network for feature extraction and a classifier for classification. As a common practice, M i+1 is first initialized by the parameter of M i , and then the dimensions of the classifier are extended to adapt to new entity types. Then, M i will guide the learning process of M i+1 through knowledge distilla-tion <ref type="bibr">(Monaikul et al., 2021)</ref> or regularization terms <ref type="bibr" target="#b6">(Douillard et al., 2020)</ref> to preserve old knowledge. Our method is based on knowledge distillation where the old model M i acts as a teacher and the new model M i+1 acts as a student. Our method further distillates causal effects in the process of knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distilling Colliding Effects in CL-NER</head><p>Based on the causal graph in <ref type="figure">Figure 3a</ref>, we figure out that the crux of CL lies in building causal paths between old data and prediction on the new model. If we utilize colliding effects, the causal path between old and new data can be built without storing old data.</p><p>To distillate the colliding effects, we first need to find tokens in new data which have the same feature representation X 0 in the old feature space, i.e., condition on X 0 . However, it is almost im-  possible to find such matched tokens since features are sparse in high dimensional space <ref type="bibr" target="#b0">(Altman and Krzywinski, 2018)</ref>. Following <ref type="bibr" target="#b13">Hu et al. (2021)</ref>, we approximate the colliding effect using K-Nearest Neighbor (KNN) strategy. Specifically, we select a token as anchor token and search the k-nearest neighbor tokens whose features bear a resemblance to the anchor token's feature in the old feature space. Next, when calculating the prediction of the anchor token, we use matched tokens for joint prediction. Note that in backpropagation, only the gradient of the anchor token is computed. <ref type="figure">Figure  4</ref> shows a demonstration for distilling colliding effects.</p><p>Although Other-class tokens usually do not directly guide the model to recognize new entity types, they contain tokens from old entity types, which allow models to recall what they have learned. Naturally, we use the old model to recognize the Other-class tokens which actually belong to old entity types. Since these Other-class tokens belong to the predefined entity types, we call them as Defined-Other-Class tokens, and we call the rest tokens in Other-class as Undefined-Other-Class tokens.</p><p>Based on the characteristics of NER, we extend the causal graph in <ref type="figure">Figure 3a</ref> to <ref type="figure">Figure 3b</ref>. The key adjustment is that the node of new data is split into two nodes, including new entity tokens D E and Defined-Other-Class tokens D O . Then, we apply the colliding effects on D E and D O respectively, resulting in that D E and D O collide with S on nodes X E 0 and X O 0 in the old feature space. In this way, we build two causal paths from old data S to new predictions? . In the causal graph, we ignore the token from Undefined-Other-Class since they do not help models learn new entity types or review old knowledge. Moreover, we expect the model to update instead of preserving the knowledge about Other-class in each CL step. Here, we consider two paths separately because the colliding effects are distilled from different kinds of data and calculated in different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A causal framework for CL-NER</head><p>Formally, we define the total causal effects Effect as follow:</p><formula xml:id="formula_0">Effect = Effect E + Effect O (1) = ?? i CE(Y i , Y i ) ? ? j KL(Y j , Y j ), (2) s.t. D i ? D E , D j ? D O .</formula><p>In Eq. <ref type="formula">(1)</ref>, Effect E and Effect O denote the colliding effect of new entity types and Defined-Other-Class. In Eq.</p><p>(2), CE(?, ?) and KL(?, ?) represent the crossentropy and KL divergence loss, and D i ,D j are the i-th,j-th token in new data. In cross-entropy loss, Y i is the ground-truth entity type of the i-th token in new data. In KL divergence loss, Y j is the soft label of the j-th token given by the old model over old entity types. In both losses, Y represents the weighted average of prediction scores over anchor and matched tokens. When calculating KL(?, ?), we follow the common practice in knowledge distillation to introduce temperature parameters T t , T s for teacher (old model) and student model (new model) respectively. Here, we omit T t , T s in KL(?, ?) for notation simplicity. The weighted average scores of the i-th token is calculated as follow:</p><formula xml:id="formula_1">Y i = W i?i + ? K k=1 W ik?ik (3) s.t. W i ? W i1 ? W i2 ? ? ? ? ? W iK W i + ? K k=1 W ik = 1,</formula><p>where the i-th token is the anchor token and K matched tokens are selected according to the KNN strategy. We sort the K matched tokens in ascending order according to the distance to the anchor token in the old feature space.? i ,? ik are the prediction scores of the i-th token and its k-th matched token, and W i , W ik are the weight for? i ,? ik , respectively. The weight constraints ensure that the token closer to the colliding feature has a more significant effect. Until now, we calculate the effects in D E and D O and ignore the Undefined-Other-Class. Following <ref type="bibr">Monaikul et al. (2021)</ref>, we apply the standard knowledge distillation to allow new models to learn from old models. To this end, we just need to rewrite Effect O in Eq.(1) as follow:</p><formula xml:id="formula_2">Effect O = ?? j KL(Y j , Y j ) ? ? n KL(? n , Y n ) (4) s.t. D j ? D O , D n ? D U O ,</formula><p>where D U O is the data belong to the Undefined-Other-Class. The Eq.(4) can be seen as calculating samples from D O and D U O in the same way, except that samples from D U O have no matched tokens. We summarize the proposed causal framework in <ref type="figure" target="#fig_1">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mitigating Label Noise in Effect O</head><p>In our method, we use the old model to predict the labels of Defined-Other-Class tokens. However, it inevitably generates label noise when calculating Effect O . To address this problem, we adopt curriculum learning to mitigate the label noise in the proposed method. Curriculum learning has been widely used for handling label noises <ref type="bibr" target="#b9">(Guo et al., 2018)</ref> in computer vision. <ref type="bibr" target="#b1">Arazo et al. (2019)</ref> empirically find that networks tend to fit correct samples before noisy samples. Motivated by this, we introduce a confidence threshold ? (? ? [0, 1]) to encourage the model to learn first from clean Other-class samples and then noisier ones. Specifically, when calculating Effect O , we only select Defined-Other-Class tokens whose predictive confidences are larger than ? for distilling colliding effects while others are for knowledge distillation. The value of ? changes along with the training process and the value of ? in the i-th epoch is calculated as follow:</p><formula xml:id="formula_3">? i = ? 1 + i?1 m?1 (? m ? ? 1 ), 1 ? i ? m ? m , i &gt; m,<label>(5)</label></formula><p>where m, ? 1 and ? m are the predefined hyperparameters and ? m should be smaller than ? 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5</head><p>Balancing Effect E and Effect O <ref type="figure" target="#fig_1">Figure 5</ref> shows that the total causal effect Effect consists of Effect E and Effect O , where Effect E is for learning new entities while Effect O is for reviewing old knowledge. With the learning process of CL, the need to preserve old knowledge varies <ref type="bibr" target="#b11">(Hou et al., 2019)</ref>. For example, more efforts should be made to preserve old knowledge when there are 15 old classes and 1 new class v.s. 5 old classes and 1 new class. In response to this, we introduce a self-adaptive weight for balancing Effect E and Effect O :</p><formula xml:id="formula_4">? = ? base C O /C N<label>(6)</label></formula><p>where ? base is the initial weight and C O ,C N are the numbers of old and new entity types respectively. In this way, the causal effects from new entity types and Other-class are dynamically balanced when the ratio of old classes to new classes changes. Finally, the objective of the proposed method is given as follow:</p><formula xml:id="formula_5">max Effect = Effect E + ? * Effect O<label>(7)</label></formula><p>5 Experiments  <ref type="bibr" target="#b28">Meulder, 2003)</ref>. To ensure that each entity type has enough samples for training, we filter out the entity types which contain less than 50 training samples. We summarize the statistics of the datasets in <ref type="table" target="#tab_7">Table 5</ref> in Appendix F. Following <ref type="bibr">Monaikul et al. (2021)</ref>, we split the training set into disjoint slices, and in each slice, we only retain the labels which belong to the entity types to learn while setting other labels to Other-class. Different from <ref type="bibr">Monaikul et al. (2021)</ref>, we adopt a greedy sampling strategy to partition <ref type="table">Table 1</ref>: Comparisons with state-of-the-art methods on I2B2 and OntoNotes5. The average results as well as standard derivations are provided. Mi-F1: micro-F1; Ma-F1: macro-F1; Forget: Forgetting; : higher is better; : lower is better. The best F1 results are bold.  the training set to better simulate the real-world scenario. Specifically, the sampling algorithm encourages that the samples of each entity type are mainly distributed in the slice to learn. We provide more explanations and the detailed algorithm in Appendix B.</p><formula xml:id="formula_6">FG-1-PG-1 FG-2-PG-2 FG-8-PG-1 FG-8-PG-2 Dataset Method Mi-F1 Ma-F1 Mi-F1 Ma-F1 Mi-F1 Ma-F1 Mi-F1</formula><p>Training. We use bert-base-cased <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> as the backbone model and a fully-connected layer for classification. Following previous work in CL <ref type="bibr" target="#b13">(Hu et al., 2021)</ref>, we predefine a fixed order of classes (alphabetically in this study) and train models with the corresponding data slice sequentially. Specifically, F G entity types are used to train a model as the initial model, and every P G entity types are used for training in each CL step (denoted as F G-a-P G-b). For evaluation, we only retain the new entity types' labels while setting other labels to Other-class in the validation set. In each CL step, we select the model with the best validation performance for testing and the next step's learning. For testing, we retain the labels of all recognized entity types while setting others to Other-class in the test set.</p><p>Metrics. Considering the class imbalance problem in NER, we adopt Micro F1 and Macro F1 for measuring the model performance. We report the average result on all CL steps (including the first step) as the final result.</p><p>Baselines. We consider four baselines: Extend-NER <ref type="bibr">(Monaikul et al., 2021)</ref>, Self-Training (ST) <ref type="bibr" target="#b27">(Rosenberg et al., 2005;</ref><ref type="bibr" target="#b4">De Lange et al., 2019)</ref>, LUCIR <ref type="bibr" target="#b11">(Hou et al., 2019)</ref> and PODNet <ref type="bibr" target="#b6">(Douillard et al., 2020)</ref>. ExtendNER is the previous state-ofthe-art method in CL-NER. LUCIR and PODNet are state-of-the-art CL methods in computer vision. Detailed descriptions of the baselines and their training settings are demonstrated in Appendix C.</p><p>Hyper-Parameters. We set the number of matched tokens K = 3, the weights W i = 1/2 and W ik = 1 2K . For parameters in the curriculum learning strategy, we set ? 1 = 1, ? m = 0 and m = 10. We set the initial value of balancing weight ? base = 2. More training details are shown in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head><p>Comparisons with State-Of-The-Art. We consider two scenarios for each dataset: (1) training the first model the same as the following CL steps; (2) training the first model with half of all entity types. The former scenario is more challenging, whereas the latter is closer to the real-world scenario since it allows models to learn enough knowledge before incremental learning. Apart from that, we consider fine-tuning without any anti-forgetting techniques (Finetune Only) as a lower bound for comparison.</p><p>The results on I2B2 and OntoNotes5 are sum-marized in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_2">Figure 6</ref>. Due to the space limitation, we provide the results on CoNLL2003 in <ref type="table" target="#tab_8">Table 6</ref> and <ref type="figure">Figure 9</ref> in Appendix F. In most cases, our method achieves the best performance. Especially, our method outperforms the previous state-of-the-art method in CL-NER (i,e., Extend-NER) by a large margin. Besides, we visualize the features of our method and ExtendNER for comparison in Appendix E. The performances of POD-Net and LUCIR are much worse than our methods when more CL steps are performed. The reason could be that neither of them differentiates Otherclass from entity types, and the old knowledge in Other-class is not preserved. Our method encourages the model to review old knowledge from both new entity types and Other-class in the form of distilling causal effects. Hyper-Parameter Analysis. We provide hyperparameter analysis on I2B2 with the setting FG-8-PG-2. We consider three hyper-parameters: the number of matched tokens K, the initial value of balancing weight ? base and the initial value of confidence threshold ? 1 . The results in <ref type="table" target="#tab_5">Table 3</ref> shows that a larger K is beneficial. However, as K becomes larger, the run time increases correspondingly. The reason is that more forward passes are required during training. Therefore, We select K = 3 by default to balance effectiveness and efficiency. Results also show that ? 1 = 1 reaches the best result, which indicates that it is more effective to learn Effect E first and then gradually introduce Effect O during training. Otherwise, the old model's wrong predictions will significantly affect the model's performance. Additionally, we find that the performance drops substantially when ? base is too large. Note that we did not carefully search for the best hyper-parameters, and the default ones are used throughout the experiments. Therefore, elaborately adjusting the hyperparameters may lead to superior performances on specific datasets and scenarios. Combining with Other Baselines. Furthermore, the proposed causal framework can be implemented as a plug-and-play module (denoted as CF). As shown in <ref type="figure" target="#fig_1">Figure 5</ref>, Effect E is based on the cross-entropy loss for classifying new entities, while Effect O is based on the KL divergence loss for the prediction-level distillation. We use LUCIR and ST as the baselines for a demonstration. To combine LUCIR with our method, we substitute the classification loss in LUCIR with Effect E and substitute the feature-level distillation with Effect O . When combining with ST, we only replace the soft label with the hard label when calculating Effect O . The results in <ref type="table" target="#tab_6">Table 4</ref> indicate that our method im-proves LUCIR and ST substantially. It is worth noticing that LUCIR+CF outperforms our method consistently, indicating our method has the potential to combine with other CL methods in computer vision and reach a superior performance in CL-NER. In addition, CFNER outperforms ST+CF due to the fact that soft labels convey more knowledge than hard labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>How can we learn from Other-class for antiforgetting in CL-NER? This question is answered by the proposed causal framework in this study. Although Other-class is "useless" in traditional NER, we point out that in CL-NER, Other-class samples are naturally reserved samples from old classes, and we can distillate the causal effects from them to preserve old knowledge. Moreover, we apply curriculum learning to alleviate the noisy label problem to enhance the distillation of Effect O . We further introduce a self-adaptive weight for dynamically balancing causal effects between new entity types and Other-class. Experimental results show that the proposed causal framework not only outperforms state-of-the-art methods but also can be combined with other methods to further improve performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>For the consideration of ethical concerns, we would make description as follows: <ref type="formula">(1)</ref> We conduct all experiments on existing datasets, which are derived from public scientific researches.</p><p>(2) We describe the statistics of the datasets and the hyperparameter settings of our method. Our analysis is consistent with the experimental results.</p><p>(3) Our work does not involve sensitive data or sensitive tasks. <ref type="formula">(4)</ref> We will open source our code for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although the proposed method alleviates the catastrophic forgetting problem to some extent, its performances are still unsatisfactory when more CL steps are performed. Additionally, calculating causal effects from Other-class depends on the old model predictions, resulting in errors propagating to the following CL steps. Moreover, the proposed method requires more computation and a longer training time since the predictions of matched samples are calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Forgetting in CL-NER</head><p>To identify the cause of forgetting, we consider the differences in prediction Y when the old data S exists or not. For each CL step, the effect of old data S can be calculate as:</p><formula xml:id="formula_7">Effect S = P (? =?|do(S = s)) (8) ? P (? =?|do(S = 0)) = P (? =?|S = s) ? P (? =?|S = 0) (9) = P (? =?) ? P (? =?) (10) = 0,<label>(11)</label></formula><p>where do(?) is the causal intervention <ref type="bibr" target="#b24">(Pearl, 2014</ref><ref type="bibr" target="#b23">(Pearl, , 2009</ref> representing that assigning a certain value to a variable without considering all parent nodes (causes). In the first equation, do(S = 0) represents null intervention, i.e., setting old data to null. In the second equation, P (? =?|do(S) = P (? =?|S) due to the fact that S has no parent nodes. In the third equation, P (? =?|S) = P (? =?) since all causal paths from S to Y are blocked by the collider X 0 . From Eq.(11), we find that the missing old data effect causes the forgetting. We neglect the effect of initial parameters adopted from the old model since it will be exponentially decayed towards 0 during learning <ref type="bibr" target="#b14">(Kirkpatrick et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Greedy Sampling Algorithm for CL-NER</head><p>In real-world scenarios, the new data should focus on the new entity types, i.e., most sentences contain the tokens from new entity types. Suppose we randomly partition a dataset for CL-NER as in <ref type="bibr">Monaikul et al. (2021)</ref>. In that case, each slice contains a large number of sentences whose tokens all belong to Other-class, resulting in that models tend to bias to Other-class when inference. A straightforward solution is to filter out all sentences with only Other-class tokens. However, it brings a new problem: the slices' sizes are imbalanced.</p><p>To address this problem, we propose a sampling algorithm for partitioning a dataset in CL-NER (Algorithm 1). Simply put, we allocate the sentence containing low-frequency entity types to the corresponding slice in priority until the slice contains the required number of sentences. If a sentence contains no entity types or the corresponding slices are full, we randomly allocate the sentence to an incomplete slice. In this way, we partition the dataset into slices with balanced sizes, and each slice mainly contains the entity types to learn.</p><p>For comparing Algorithm 1 and the random sampling as in <ref type="bibr">Monaikul et al. (2021)</ref>, we provide the label distributions in each slices of training data in <ref type="figure" target="#fig_3">Figure 7</ref>. <ref type="figure" target="#fig_3">Figure 7</ref> shows that the greedy sampling generates more realistic datasets for CL-NER. When we use the randomly partitioned dataset for training in the setting FG-1-PG-1, the micro-f1 score of our method is 16.12, 17.43, and 12.75 (%) on OntoNotes5, I2B2, and CoNLL2003, respectively, indicating that the number of entities in each slice is inadequate for learning a NER model. Therefore, the greedy sampling alleviates the need for data in CL-NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baselines Introductions and Settings</head><p>The introductions about the baselines in experiments and their experimental settings are as follows. Note that we do not apply any reserved samples from old classes in LUCIR and PODNet for a fair comparison since our method requires no old data.</p><p>? Self-Training (ST) <ref type="bibr" target="#b27">(Rosenberg et al., 2005;</ref><ref type="bibr"></ref> De Lange et al., 2019): ST first utilizes the old model to annotate the Other-class tokens with old entity types. Then, the new model is trained on new data with annotations of all entity types. Finally, the cross-entropy loss on all entity types is minimized.</p><p>? ExtendNER <ref type="bibr">(Monaikul et al., 2021)</ref>: Extend-NER has a similar idea to ST, except that the old model provides the soft label (i.e., probability distribution) of Other-class tokens. Specifically, the cross-entropy loss is computed for entity types' tokens, and KL divergence loss is computed for Other-class tokens. During training, the sum of cross-entropy loss and KL divergence loss is minimized. Following <ref type="bibr">Monaikul et al. (2021)</ref>, we set the temperature of the teacher (old model) and student model (new model) to 1 and 2, respectively.</p><p>? LUCIR <ref type="bibr" target="#b11">(Hou et al., 2019)</ref>: LUCIR develops a framework for incrementally learning a unified classifier for the continual image classification tasks. The total loss consists of three terms: (1) the cross-entropy loss on the new classes samples;</p><p>(2) the distillation loss on the features extracted by the old model and those by the new one; (3) the margin-ranking loss on the reserved samples for old classes. In our experiments, we compute the cross-entropy loss for new entity types, the distillation loss Algorithm 1: Greedy Sampling Algorithm for CL-NER Input: D = {(s i , t i )} n i=1 : a training set contains n sentences s and their label sequences t; Require : G: the number of slices; E: the entity set; E j : the entity set in the j-th slice; Output: G slices of datasets: D (1) , D (2) , ? ? ? , D (G) 1 Initialize D (1) , ? ? ? , D (G) = {}, ? ? ? , {}; 2 Initialize cnt 1 , ? ? ? , cnt G = 0, ? ? ? , 0; // Calculate the number of sentences to allocate in each slice 3 for j in range(1,G+1) do 4 n j = n * |E j |/|E| 5 end 6 Sort E in ascending order based on the frequency in D; 7 for (s i , t i ) in D do for all entity types, and the margin-ranking loss for Other-class samples instead of the reserved samples. Following <ref type="bibr" target="#b11">(Hou et al., 2019)</ref>, ? base (i.e., loss weight for the distillation loss) is set to 50, K (i.e., the top-K new class embeddings are chosen for the margin-ranking loss) is set to 1 and m (i.e., the threshold of margin ranking loss) is set to 0.5 for all the tasks.</p><p>? PODNet <ref type="bibr" target="#b6">(Douillard et al., 2020)</ref>: PODNet has a similar idea to LUCIR to combat the catastrophic forgetting in continual learning for image classification. The total loss consists of the classification loss and distillation loss. To compute the distillation loss, PodNet constrains the output of each intermediate convolutional layer while LUCIR only considers the final feature embeddings. For classification, PODNet used NCA loss instead of the cross-entropy loss. In our experiments, we constrain the output of each intermediate stage of BERT as PODNet constrains each stage of a ResNet. Following <ref type="bibr" target="#b6">(Douillard et al., 2020)</ref>, we set the ? c (i.e., loss weight for the PODspatial loss) to 3 and ? f (i.e., loss weight for the POD-flat loss) to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training Details</head><p>The models were implemented in Pytorch <ref type="bibr" target="#b22">(Paszke et al., 2019)</ref> on top of the BERT Huggingface implementation <ref type="bibr" target="#b32">(Wolf et al., 2019)</ref>. We use the default hyper-parameters in BERT: hidden dimensions are 768, and the max sequence length is 512. Following <ref type="bibr" target="#b13">Hu et al. (2021)</ref>, we normalize the feature vector output by BERT and compute the cosine similarities between the feature vector and class representations for predictions. We use the BIO tagging schema for all three datasets. For CoNLL2003, we train the model for ten epochs in each CL step. For OntoNotes5 and I2B2, we train the model for 20 epochs when PG=2, and 10 epochs when PG=1. The batch size is set as 8, and the learning rate is set as 4e-4. The experiments are run on GeForce RTX 2080 Ti GPU. Each experiment is repeated 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E T-SNE Visualizations</head><p>To deepen the understanding of the forgetting in CL-NER, we visualize the feature representations from BERT in <ref type="figure" target="#fig_5">Figure 8</ref>. Results show that our method preserves more old knowledge and learns better feature representations than ExtendNER. (a) CoNLL2003 (FG-1-PG-1) (b) CoNLL2003 (FG-2-PG-1) <ref type="figure">Figure 9</ref>: Comparison of the step-wise micro-f1 score on CoNLL2003 (4 entity types).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experimental Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The causal graph for CL-NER: (a) forgetting happens when there are no causal paths from old data to new predictions; (b) anti-forgetting is to build causal paths from old data to new predictions through new entities (D E ) and Other-class samples (D O ). We call the causal effects in these two links Effect E and Effect O , respectively. A demonstration of the colliding effect. The anchor token's feature (anchor features) collides with matched tokens' features (matched features) on the colliding feature in the old feature space. (a) initial: the class boundary is retained since the new model is initialized by the old model condition. (b) w/o Colliding Effect: the class boundary is forgot in the new feature space since there are no causal effects from old data to new predictions. (c) w/ Colliding Effect: the class boundary is preserved after an CL step since the anchor and matched tokens collide in the old feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>A demonstration of the proposed causal framework for CL-NER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of the step-wise micro-f1 score on I2B2 (16 entity types), OntoNotes5 (18 entity types).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of the greedy sampling and random sampling on OntoNotes5. Each slice contains one entity types to lean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>j</head><label></label><figDesc>is index of E j s.t. e belongs to E j ; 14 if cnt j &lt; n j then 15 D (j) = D (j) ? (s i , t i ); j s.t. cnt j &lt; n j ; 23 D (j) = D (j) ? (s i , t i ); D (1) , D (2) , ? ? ? , D (G) ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>The T-SNE visualization of the feature representations from BERT. The model is trained on OntoNotes5 in the setting FG-1-PG-1. We randomly select six classes for a demonstration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Old Model Linear Encoder [O] [PER] [DATE] [GPE] Joint Predictions Chris was in China yesterday Linear Encoder [O] [PER] [DATE] Chris was in China yesterday Predictionsin China yesterday Inputs: New Model KL Divergence Loss Cross-Entropy Loss 1. Chris traveling with the ? 2. Bush said ? 3. Mr. Pilson recalls that ? 1. From yesterday, Tom ? 2. Today, Sir Edmund ? 3. It's Friday, so if that ? 1. If China's trade gap ? 2. China might stave off ? 3. ? their new loans to China. [PER],[DATE]: Old Entity Types Matched Feature</head><label></label><figDesc></figDesc><table><row><cell>Labels:</cell><cell cols="3">[O] [O] [O] [GPE]</cell><cell></cell><cell cols="2">[O]</cell><cell></cell></row><row><cell cols="2">New Feature Space 2 3 1 Chris was Anchor Feature 3 2</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>Forward Path</cell><cell>No Gradient</cell><cell>[GPE]: New Entity Types;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Ablation Study. We ablate our method, and the results are summarized inTable 2. To validate the effectiveness of the proposed causal framework, we only remove the colliding effects in Other-class and new entity types for the settings w/o Effect O and w/o Effect E , respectively. Specifically, in the w/o Effect O setting, we apply knowledge distillation for all Other-class samples, while in w/o Effect E setting, we calculate the cross-entropy loss for classification. Note that our model is the same as ExtendNER when no causal effects are used (i.e., w/o Effect O &amp; Effect E ). The results show that both Effect O and Effect E play essential roles in our framework. Furthermore, the adaptive weight and the curriculum-learning strategy help model better learn causal effects in new data.</figDesc><table><row><cell cols="7">: The ablation study of our method on three</cell></row><row><cell cols="7">datasets in the setting FG-1-PG-1. AW: adaptive</cell></row><row><cell cols="7">weight; CuL: curriculum learning strategy; Mi-F1:</cell></row><row><cell cols="3">micro-F1; Ma-F1: macro-F1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>I2B2</cell><cell cols="2">OntoNotes5</cell><cell cols="2">CoNLL2003</cell></row><row><cell>Methods</cell><cell>Mi-F1</cell><cell>Ma-F1</cell><cell>Mi-F1</cell><cell>Ma-F1</cell><cell>Mi-F1</cell><cell>Ma-F1</cell></row><row><cell>CFNER(Ours)</cell><cell>62.73</cell><cell>36.26</cell><cell>58.94</cell><cell>42.22</cell><cell>80.91</cell><cell>79.11</cell></row><row><cell>w/o AW</cell><cell>61.65</cell><cell>35.86</cell><cell>57.63</cell><cell>40.28</cell><cell>80.75</cell><cell>78.43</cell></row><row><cell>w/o CuL</cell><cell>61.21</cell><cell>34.79</cell><cell>57.95</cell><cell>39.54</cell><cell>80.32</cell><cell>78.71</cell></row><row><cell>w/o AW &amp; CuL</cell><cell>60.78</cell><cell>33.15</cell><cell>56.83</cell><cell>38.95</cell><cell>79.89</cell><cell>77.54</cell></row><row><cell>w/o Effect O</cell><cell>59.68</cell><cell>30.56</cell><cell>53.09</cell><cell>34.99</cell><cell>78.68</cell><cell>76.15</cell></row><row><cell>w/o Effect E</cell><cell>53.62</cell><cell>28.75</cell><cell>54.88</cell><cell>37.29</cell><cell>79.83</cell><cell>77.45</cell></row><row><cell>w/o Effect O &amp; Effect E</cell><cell>42.85</cell><cell>24.05</cell><cell>50.53</cell><cell>32.84</cell><cell>76.36</cell><cell>73.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameter analysis on I2B2 (FG-8-PG-2). Mi-F1: micro-F1; Ma-F1: macro-F1.</figDesc><table><row><cell cols="2">K Mi-F1 Ma-F1</cell><cell>? 1</cell><cell cols="5">Mi-F1 Ma-F1 ? base Mi-F1 Ma-F1</cell></row><row><cell>1</cell><cell>65.48 46.82</cell><cell>0</cell><cell cols="2">65.25 46.33</cell><cell>0.5</cell><cell cols="2">69.27 48.71</cell></row><row><cell>2</cell><cell>67.12 49.37</cell><cell>0.5</cell><cell cols="2">66.09 48.51</cell><cell>1</cell><cell cols="2">69.46 52.45</cell></row><row><cell>3</cell><cell>69.07 51.09</cell><cell>0.9</cell><cell>68.23</cell><cell>49.1</cell><cell>2</cell><cell cols="2">69.07 51.09</cell></row><row><cell>5</cell><cell>70.25 52.51</cell><cell cols="3">0.95 68.64 49.66</cell><cell>5</cell><cell>64.4</cell><cell>46.65</cell></row><row><cell cols="2">10 70.69 52.26</cell><cell>1</cell><cell cols="2">69.07 51.09</cell><cell>10</cell><cell cols="2">54.12 40.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Combining our methods with other baselines</cell></row><row><cell cols="4">on three datasets in the setting FG-1-PG-1. Mi-F1:</cell></row><row><cell cols="4">micro-F1; Ma-F1: macro-F1. CF represents applying</cell></row><row><cell>causal effects.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>I2B2</cell><cell>OntoNotes5</cell><cell>CoNLL2003</cell></row><row><cell>Methods</cell><cell cols="3">Mi-F1 Ma-F1 Mi-F1 Ma-F1 Mi-F1 Ma-F1</cell></row><row><cell cols="4">CFNER(Ours) 62.73 36.26 58.94 42.22 80.91 79.11</cell></row><row><cell>LUCIR</cell><cell cols="3">43.86 31.31 28.18 21.11 74.15 70.48</cell></row><row><cell>LUCIR+CF</cell><cell cols="3">66.27 38.52 62.03 44.34 81.28 79.56</cell></row><row><cell>ST</cell><cell cols="3">31.98 14.76 50.71 33.24 76.17 72.88</cell></row><row><cell>ST+CF</cell><cell cols="3">61.41 33.43 57.06 41.28 80.59 79.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The statistics and the entity sequence for each dataset</figDesc><table><row><cell></cell><cell cols="3"># Class # Sent # Entity</cell><cell>Entity sequence (Alphabetical Order)</cell></row><row><cell>CoNLL2003</cell><cell>4</cell><cell>21k</cell><cell>35k</cell><cell>LOCATION, MISC, ORGANISATION, PERSON</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AGE, CITY, COUNTRY, DATE, DOCTOR, HOSPITAL,</cell></row><row><cell>I2B2</cell><cell>16</cell><cell>141k</cell><cell>29k</cell><cell>IDNUM, MEDICALRECORD, ORGANIZATION, PATIENT, PHONE, PROFESSION, STATE, STREET,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>USERNAME, ZIP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE,</cell></row><row><cell>OntoNotes5</cell><cell>18</cell><cell>77k</cell><cell>104k</cell><cell>LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>WORK_OF_ART</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparisons with state-of-the-art methods on CoNLL2003. The average results as well as standard derivations are provided. Mi-F1: micro-F1; Ma-F1: macro-F1. The best F1 results are bold. 40.64 57.45 43.58 Finetune Only ?0.10 ?0.16 ?0.05 ?0.18 36.74 29.43 59.12 58.39 PODNet ?0.52 ?0.28 ?0.54 ?0.99 74.15 70.48 80.53 77.33 LUCIR ?0.43 ?0.66 ?0.31 ?0.31 76.17 72.88 76.65 66.72 ST ?0.91 ?1.12 ?0.24 ?0.11 76.36 73.04 76.66 66.36 ExtendNER ?0.98 ?1.8 ?0.66 ?0.64 80.91 79.11 80.83 75.20 CoNLL2003 CFNER(Ours) ?0.29 ?0.50 ?0.36 ?0.32</figDesc><table><row><cell></cell><cell></cell><cell>FG-1-PG-1</cell><cell>FG-2-PG-1</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Mi-F1 Ma-F1 Mi-F1 Ma-F1</cell></row><row><cell></cell><cell></cell><cell>50.84</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The curse (s) of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="399" to="400" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CONTaiNER: Few-shot named entity recognition via contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarkar</forename><surname>Snigdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarathi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.439</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6338" to="6353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08383</idno>
		<title level="m">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Podnet: Pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020-16th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="86" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference of the NAACL</title>
		<meeting>the human language technology conference of the NAACL</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling causal effect of data in class-incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3957" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dice loss for dataimbalanced nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simone Filice, and Oleg Rokhlenko. 2021. Continual learning for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natawut</forename><surname>Monaikul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="13570" to="13577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shawn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Griffin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivian</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gainer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Chueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Churchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="130" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncovering main causalities for longtailed information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9683" to="9695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2001" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting, rehearsal and pseudorehearsal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="146" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION&apos;05</title>
		<meeting>the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION&apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Tjong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causality for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and Causal Inference: The Works of Judea Pearl</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="765" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1513" to="1524" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xian-Sheng Hua, and Qianru Sun. 2020. Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="655" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">De-biasing distantly supervised named entity recognition via causal intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4803" to="4813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Counterfactual offpolicy training for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3438" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
