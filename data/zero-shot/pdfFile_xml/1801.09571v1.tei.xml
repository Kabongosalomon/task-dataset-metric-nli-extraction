<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Fine-Grained Action Segmentation and Recognition Using Conditional Random Field Models and Discriminative Sparse Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effrosyni</forename><surname>Mavroudi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divya</forename><surname>Bhaskara</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Comcast AI Research</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahin</forename><surname>Sefati</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haider</forename><surname>Ali</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Johns</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Fine-Grained Action Segmentation and Recognition Using Conditional Random Field Models and Discriminative Sparse Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained action segmentation and recognition is an important yet challenging task. Given a long, untrimmed sequence of kinematic data, the task is to classify the action at each time frame and segment the time series into the correct sequence of actions. In this paper, we propose a novel framework that combines a temporal Conditional Random Field (CRF) model with a powerful frame-level representation based on discriminative sparse coding. We introduce an end-to-end algorithm for jointly learning the weights of the CRF model, which include action classification and action transition costs, as well as an overcomplete dictionary of mid-level action primitives. This results in a CRF model that is driven by sparse coding features obtained using a discriminative dictionary that is shared among different actions and adapted to the task of structured output learning. We evaluate our method on three surgical tasks using kinematic data from the JIGSAWS dataset, as well as on a food preparation task using accelerometer data from the 50 Salads dataset. Our results show that the proposed method performs on par or better than state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal segmentation and recognition of complex activities in long continuous recordings is a useful, yet challenging task. Examples of complex activities comprised of fine-grained goal-driven actions that follow a grammar are surgical procedures <ref type="bibr" target="#b8">[9]</ref>, food preparation <ref type="bibr" target="#b31">[31]</ref> and assembly tasks <ref type="bibr" target="#b35">[35]</ref>. For instance, in the medical field there is a need to better train surgeons in performing surgical procedures using new technologies such as the daVinci robot. One possible approach is to use machine learning and computer vision techniques to automatically determine the skill level of the surgeon from kinematic data of the surgeon's performance recorded by the robot <ref type="bibr" target="#b8">[9]</ref>. Such an approach typically requires an accurate classification of the surgical gesture at each time frame <ref type="bibr" target="#b2">[3]</ref> and a segmentation of the surgical task into the correct sequence of gestures <ref type="bibr" target="#b34">[34]</ref>. Another example of a complex activity with goal-driven fine-grained actions following a grammar is cooking. Although the actions performed while preparing a recipe and their relative ordering can vary, there are still temporal relations among them. For instance, the action stir milk usually happens after pour milk, or the action fry egg usually follows the action crack egg. Robots equipped with the ability to automatically recognize actions during food preparation could assist individuals with cognitive impairments in their daily activities by providing prompts and instructions. However, the task of fine-grained action segmentation and recognition is challenging due to the subtle differences between actions, the variability in the duration and style of execution among users and the variability in the relative ordering of actions.</p><p>Existing approaches to fine-grained action segmentation and recognition use a temporal model to capture the temporal evolution and ordering of actions, such as Hidden Markov Models (HMMs) <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b32">32]</ref>, Conditional Random Fields (CRF) <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, Markov semi-Markov Conditional Random Fields (MsM-CRF) <ref type="bibr" target="#b34">[34]</ref>, Recurrent Neural Networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">28]</ref> and Temporal Convolutional Networks (TCNs) <ref type="bibr" target="#b15">[15]</ref>. However, such models cannot capture subtle differences between actions without a powerful, discriminative and robust representation of frames or short temporal segments. Sparse coding has emerged as a powerful signal representation in which the raw data in a certain time frame is represented as a linear combination of a small number of basis elements from an overcomplete dictionary. The coefficients of this linear combination are called sparse codes and are used as a new representation for temporal modeling. However, since the dictionary is typically learned in an unsupervised manner by minimizing a regularized reconstruction error <ref type="bibr" target="#b0">[1]</ref>, the resulting representation may not be discriminative for a given learning task. Task-driven discriminative dictionary learning addresses this issue by coupling dictionary and classifier learning <ref type="bibr" target="#b24">[24]</ref>. For example, Sefati et al. <ref type="bibr" target="#b30">[30]</ref> propose an approach to fine-grained action recognition called Shared Discriminative Sparse Dictionary Learning (SDSDL), where sparse codes are extracted at each time frame and a frame feature is computed by average pooling the sparse codes over a short temporal window surrounding the frame. The dictionary is jointly learned with the per-frame classifier parameters, resulting in a discriminative mid-level representation that is shared across all actions/gestures. However, their approach lacks a temporal model, which is crucial for modeling temporal dependencies. Although prior work <ref type="bibr" target="#b38">[38]</ref> has combined discriminative dictionary learning with CRFs for the purpose of saliency detection, such work is not directly applicable to fine-grained action recognition.</p><p>In this work we propose a joint model for fine-grained action recognition and segmentation that integrates a CRF for temporal modeling and discriminative sparse coding for frame-wise action representation. The proposed CRF models the temporal structure of long untrimmed activities via unary potentials that represent the cost of assigning an action label to a frame-wise representation of an action obtained via discriminative sparse coding, and pairwise potentials that capture the transitions between actions and encourage smoothness of the predicted label sequence. The parameters of the combined model are trained jointly in an end-to-end manner using a max-margin approach. Our experiments show competitive performance in the task of fine-grained action recognition, especially in the regime of limited training data. In summary, the contributions of this paper are three-fold:</p><p>1. We propose a novel framework for fine-grained action segmentation and recognition which uses a CRF model whose target variables (action labels per time step) are conditioned on sparse codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We introduce an algorithm for training our model in an end-to-end fashion. In particular, we jointly learn a task-specific discriminative dictionary and the CRF unary and pairwise weights by using Stochastic Gradient Descent (SGD).</p><p>3. We evaluate our model on two public datasets focused on goal-driven complex activities comprised of fine-grained actions. In particular, we use robot kinematic data from the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) <ref type="bibr" target="#b8">[9]</ref> dataset and evaluate our method on three surgical tasks. We also experiment with accelerometer data from the 50 Salads <ref type="bibr" target="#b31">[31]</ref> dataset for recognizing actions that are labeled at two levels of granularity. Results show that our method performs on par with most state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of fine-grained action segmentation and recognition has recently received increased attention due to the release of datasets such as MPII Cooking <ref type="bibr" target="#b29">[29]</ref>, JIG-SAWS <ref type="bibr" target="#b8">[9]</ref> and 50 Salads <ref type="bibr" target="#b31">[31]</ref>. In this section, we briefly review some of the main existing approaches for tackling this problem. Besides, we briefly discuss existing work on discriminative dictionary learning. Note that since the focus of this paper is fine-grained action recognition from kinematic data, we do not discuss approaches for feature extraction or object parsing from video data.</p><p>Fine-grained action recognition from kinematic data. A straightforward approach to action segmentation and classification is the use of overlapping temporal windows in conjunction with temporal segment classifiers and nonmaximum suppression (e.g., <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b25">25]</ref>). However this approach does not exploit long-range temporal dependencies.</p><p>Recently, deep learning approaches have started to emerge in the field. For instance, in <ref type="bibr" target="#b7">[8]</ref> a recurrent neural network (Long Short Term Memory network -LSTM) is applied to kinematic data, while in <ref type="bibr" target="#b15">[15]</ref> a Temporal Convolutional Network composed of 1D convolutions, nonlinearities and pooling/upsampling layers is introduced. Although these models yield promising results, they do not explicitly model correlations and dependencies among action labels.</p><p>Another line of work, including our proposed method, takes into account the fact that the action segmentation and classification problem is a structured output prediction problem due to the temporal structure of the sequence of action labels and thus employs structured temporal models such as HMMs and their extensions <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. Among them, the work that is most related to this work is Sparse-HMMs <ref type="bibr" target="#b32">[32]</ref>, which combines dictionary learning with HMMs. However, a Sparse-HMM is a generative model in which a separate dictionary is learned for each action class. In this work we use a CRF, which is a discriminative model, and we learn a dictionary that is shared among all action classes. Discriminative models like CRFs <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, semi-Markov CRFs <ref type="bibr" target="#b34">[34]</ref> have gained popularity since they allow for flexible energy functions. Other types of temporal models include a duration model and language model recently proposed in <ref type="bibr" target="#b27">[27]</ref> for modeling action durations and context. The input to these temporal models are either the kinematic data themselves or features extracted from them. For instance, in the Latent Convolutional Skip Chain CRF (LC-SC-CRF) <ref type="bibr" target="#b17">[17]</ref> the responses to convolutional filters, which capture latent action primitives, are used as features.</p><p>Discriminative Dictionary Learning. Task-driven discriminative dictionary learning was introduced in the seminal work of Mairal et al. <ref type="bibr" target="#b24">[24]</ref> and couples the process of dictionary learning and classifier training, thus incorporating supervised learning to sparse coding. Since then discriminative dictionary learning has enjoyed many successes in diverse areas such as handwritten digit classification <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b39">39]</ref>, face recognition <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b26">26]</ref>, object category recognition <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b4">5]</ref>, scene classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b26">26]</ref>, and action classification <ref type="bibr" target="#b26">[26]</ref>.</p><p>The closest work to ours is the Shared Discriminative Sparse Dictionary Learning (SDSDL) proposed by Sefati et al. <ref type="bibr" target="#b30">[30]</ref>, where sparse codes are used as frame features and a discriminative dictionary is jointly learned with per frame action classifiers for the task of surgical task segmentation. Our work builds on top of this model by replacing the per-frame classifiers, which compute independent predictions per frame, with a structured output temporal model (CRF), which takes into account the temporal dependencies between actions. While prior work has considered joint dictionary and CRF learning <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref> for the tasks of semantic segmentation and saliency estimation, our work differs from these previous approaches in three key aspects. First, to the best of our knowledge, we are the first to apply joint dictionary and CRF learning to the task of action segmentation and classification. Second, we are learning unary CRF classifiers and pairwise transition scores, while in <ref type="bibr" target="#b33">[33]</ref> only two scalar variables encoding the relative weight between the unary and pairwise potentials are learned. Third, we use local temporal average-pooling of sparse codes as a feature extraction process for capturing local temporal context instead of the raw sparse codes used in <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head><p>In this section, we introduce our temporal CRF model and frame-wise representation based on sparse coding and describe our algorithm for training our model. <ref type="figure">Figure 1</ref> illustrates the key components of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>Frame-wise representation. Let X = {x t } T t=1 be a sequence of length T , with x t ? R p being the input at time t (e.g., the robot's joint positions and velocities). Our goal is to compactly represent each x t as a linear combination of a small number of atomic motions using an overcomplete dictionary of representative atomic motions ? ? R p?m , i.e., x t ? ?u t , where u t ? R m is the vector of sparse coefficients obtained for frame t. Such sparse codes can be obtained by considering the following optimization problem:</p><formula xml:id="formula_0">min {ut} T t=1 1 T T t=1 ||x t ? ?u t || 2 2 + ? u ||u t || 1 ,<label>(1)</label></formula><p>where ? u is a regularization parameter controlling the tradeoff between reconstruction error and sparsity of the coefficients. Problem (1) is a standard Lasso regression and can be efficiently solved using existing sparse coding algorithms <ref type="bibr" target="#b23">[23]</ref>. After computing sparse codes u t for each time step of the input sequence, we follow the approach proposed in <ref type="bibr" target="#b30">[30]</ref> to compute feature vectors {z t } T t=1 . Namely, we initially split the positive and negative components of the sparse codes and stack them on top of each other. This <ref type="figure">Figure 1</ref>: Overview of our framework. Given an input time series X, we first extract sparse codes U for each timestep using a dictionary ?. Sparse codes are then average pooled in short temporal windows yielding feature vectors Z per timestep. These feature vectors are then given as inputs to a Linear Chain CRF with weights W. Trainable parameters ? and W are shown in light pink boxes.</p><formula xml:id="formula_1">x t ? u t z t y t</formula><p>step yields a vector a t ? R D , D = 2m, which is given by:</p><formula xml:id="formula_2">a t = max(0, u t ) min(0, u t ) .<label>(2)</label></formula><p>This is a common practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref>, which allows the classification layer to assign different weights to positive and negative responses. Second, we compute a feature vector z t ? R D for each frame by average-pooling vectors a t in a temporal window T t surrounding frame t, i.e.:</p><formula xml:id="formula_3">z t = 1 L j?Tt a j , T t . = t ? L 2 , t + L 2 ,<label>(3)</label></formula><p>where L is the length of the temporal window centered at frame t. This feature vector captures local temporal context. Temporal model. Let Z = {z t } T t=1 be a sequence of length T with z t being the feature vector representing the input at time t, and Y = {y t } T t=1 be the corresponding sequence of action labels per frame, y t ? {1, . . . , N c }, with N c being the number of action classes. Let G = {V, E} be the graph whose nodes correspond to different frames (|V| = T ) and whose edges connect every d frames (with d = 1 corresponding to consecutive frames). Our CRF models the conditional distribution of labels given the input features with a Gibbs distribution of the form P</p><formula xml:id="formula_4">(Y | Z) ? exp E(Z, Y),</formula><p>where the energy E(Z, Y) is factorized into a sum of potential functions defined on cliques of order less than or equal to two. Formally, the energy function can be written as:</p><formula xml:id="formula_5">E(Z, Y) = T t=1 U yt z t + T ?d t=1 P yt,y t+d ,<label>(4)</label></formula><p>where the first term is the unary potential which models the score of assigning label y t to frame t described by feature z t , while the second term is called pairwise potential and models the score of assigning labels y t and y t+d to frames t and t + d respectively (d is a parameter called the skip length and a CRF with d &gt; 1 is called Skip-Chain CRF (SC-CRF) <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>). U yt ? R D is a linear unary classifier corresponding to action class y t and P ? R Nc?Nc is the pairwise transition matrix. Note that there exist different variants to this model. For instance, one can use precomputed unary and pairwise potentials and learn two scalar coefficients that encode the relative weights of the two terms <ref type="bibr" target="#b33">[33]</ref>. We now show how this energy can be written as a linear function with respect to a parameter vector W ? R NcD+N 2 c . The unary term can be rewritten as follows:</p><formula xml:id="formula_6">T t=1 U T yt z t = U 1 , . . . , U Nc ? ? ? ? ? ? ? ? ? T t=1 z t ?(y t = 1) . . . T t=1 z t ?(y t = N c ) ? ? ? ? ? ? ? ? ? = W U ? U (Z, Y),<label>(5)</label></formula><p>where W U and ? U (Z, Y) ? R NcD are, respectively, the unary CRF weights and the unary joint feature. Similarly the pairwise term can be written as:</p><formula xml:id="formula_7">[P 11 , . . . , P NcNc ] ? ? ? ? ? ? ? ? ? T ?d t=1 ?(y t = 1)?(y t+d = 1)</formula><p>. . .</p><formula xml:id="formula_8">T ?d t=1 ?(y t = N c )?(y t+d = N c ) ? ? ? ? ? ? ? ? ? = W P ? P (Y),<label>(6)</label></formula><p>where W P , ? P (Y) ? R N 2 c are the pairwise CRF weights and pairwise joint feature. Therefore, the overall energy function can be written as:</p><formula xml:id="formula_9">E(Z, Y) = W U W P ? U (Z, Y) ? P (Y) = W ?(Z, Y),<label>(7)</label></formula><p>where W is the vector of CRF weights and ?(Z, Y) the joint feature <ref type="bibr" target="#b11">[11]</ref>. At this point, we should emphasize that feature vectors Z = [z 1 , . . . , z T ] are constructed by local average pooling of the sparse codes and are therefore implicitly dependent of the input data X and the dictionary ?.</p><p>For the rest of this manuscript, we will denote this dependency by substituting Z with the notation Z(X, ?). So our energy can be rewritten as:</p><formula xml:id="formula_10">E(Z(X, ?), Y) = W ?(Z(X, ?), Y).<label>(8)</label></formula><p>It should be now clear that if ? is fixed, then the energy is linear with respect to the parameter vector W, like in a standard CRF model. However, if ? is a parameter that needs to be learned, then the energy function is nonlinear with respect to (W, ?) and thus training is not straightforward. The training problem is addressed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Let {X n } Ns n=1 be N s training sequences with associated label sequences {Y n } Ns n=1 . We formulate the training problem as one of minimizing the following regularized loss:</p><formula xml:id="formula_11">J(W, ?) = 1 2 ||W|| 2 F + + C N s Ns n=1 max Y [?(Y n , Y) + W, ?(Z n (X n , ?), Y) ] ? W, ?(Z n (X n , ?), Y n ) ,<label>(9)</label></formula><p>where C is a regularization parameter controlling the regularization of the CRF weights, ?(?, Y) = T t=1 ?(? t = y t ) is the Hamming loss between two sequences of label? Y and Y, and Z n is the matrix of feature vectors extracted from the frames of input sequence X n , i.e., Z n = [z n 1 , . . . , z n T ]. This max-margin formulation performs regularized empirical risk minimization and bounds the hamming loss from above. We use a Stochastic Gradient Descent algorithm for minimizing the objective function in Eq. (9). Our algorithm is based on the task-driven dictionary learning approach developed by Mairal et al. <ref type="bibr" target="#b24">[24]</ref>. Notice that, although the sparse coefficients are computed by minimizing a non-differentiable objective function (Eq. 1), J(W, ?) is differentiable and its gradient can be computed <ref type="bibr" target="#b22">[22]</ref>. In particular, the function relating the sparse codes u t and the dictionary is differentiable almost everywhere, except at the points where the set of non-zero elements of u t (called the support set and denoted by S t ) changes. Assuming that the perturbations of the dictionary atoms are small so that the support set stays the same, we can compute the gradient of the non-zero coefficients with respect to the columns of ? indexed by S t , denoted as ? St , as follows <ref type="bibr" target="#b33">[33]</ref>:</p><formula xml:id="formula_12">?u t (k) ?? St = (x t ?? St (u t ) St )(A ?1 t ) [k] ?(? St A ? t ) k (u t ) St (10) where k ? S t , (u t )</formula><p>St denotes the sub-vector of u t with entries in S t , A t = ? St ? St , and the subscripts [k] and k denote, respectively, the k-th row and column of the corresponding matrix.</p><p>Given the dictionary and CRF weights computed at the (i ? 1)-th iteration, the main s-eps-converted-to.pdf of our iterative algorithm at the i-th iteration are:</p><p>1. Randomly select a training sequence (X i , Y i ).</p><p>2. Compute sparse codes u t with Eq. 1 and feature vectors z t with Eq. 3 using dictionary ? (i?1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Find the sequence? that yields the most violated constraint by solving the loss augmented inference prob-</head><formula xml:id="formula_13">lem:? = argmax Y ?(Y i , Y)+ W (i?1) , ?(Z i (X i , ? (i?1) ), Y)<label>(11)</label></formula><p>using the Viterbi algorithm (see <ref type="bibr" target="#b17">[17]</ref> for details regarding inference when using a SC-CRF (d &gt; 1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Compute gradient with respect to the CRF parameters W :</p><formula xml:id="formula_14">?J ?W = W (i?1) + C(?(Z i (X i , ? (i?1) ),?)? ? ?(Z i (X i , ? (i?1) ), Y i )).<label>(12)</label></formula><p>5. Compute gradients with respect to the dictionary ? using the chain rule:</p><formula xml:id="formula_15">?J ?? = T t=1 ?J ?z t ?z t ?? = T t=1 ?J ?z t 1 L j?Tt ?a j ?? , = 1 L T t=1 j?Tt (x j ? ? (i?1) Sj (u j ) Sj ) A ?1 j ?J ?z t S j ? ? (i?1) Sj A ? j (u j ) Sj ?J ?z t S j ,<label>(13)</label></formula><p>where ?J ?zt = U? t ? U yt ? R D , S j is the set of indices corresponding to the non-zero entries of the vector u j , S j is the set of indices corresponding to the non-zero entries of the vector a j , A j = ? Sj ? Sj , ? Sj denotes the active columns of the dictionary indexed by S j , (u j ) Sj denotes the non-zero entries of vector u j and ( ?J ?zt )S j denotes the entries of the partial derivative corresponding to non-zero entries of vector a j . 6. Update W, ? using stochastic gradient descent. 7. Normalize the dictionary atoms to have unit l 2 norm.</p><p>This step prevents the columns of ? from becoming arbitrarily large, which would result in arbitrarily small sparse coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on two public datasets for finegrained action segmentation and recognition: JIGSAWS <ref type="bibr" target="#b8">[9]</ref> and 50 Salads <ref type="bibr" target="#b31">[31]</ref>. First, we report our results on each dataset and compare them with the state of the art. Next, we examine the effect of different model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>JHU-ISI Gesture and Skill Assessment (JIGSAWS) <ref type="bibr" target="#b8">[9]</ref>. This dataset provides kinematic data of the right and left manipulators of the master and slave da Vinci surgical robot recorded at 30 Hz during the execution of three surgical tasks (Suturing (SU), Knot-tying (KT) and Needle-passing (NP)) by surgeons with varying skill levels. In particular, kinematic data include positions, orientations, velocities etc. (76 variables in total), and there are 8 surgeons performing a total of 39, 36 and 26 trials for the Suturing, Knot-tying and Needle-passing surgical tasks, respectively. This dataset is challenging due to the significant variability in the execution of tasks by surgeons of different skill levels and the subtle differences between fine-grained actions. There are 10, 6 and 8 different action classes for the Suturing, Knot-tying and Needle-passing tasks, respectively. Examples of action classes are orienting needle, reaching for needle with right hand, pulling suture with left hand, and making C loop. We evaluate our method using the standard Leave-One-User-Out (LOUO) and Leave-One-Supertrial-Out (LOSO) cross-validation setups <ref type="bibr" target="#b1">[2]</ref>. 50 Salads <ref type="bibr" target="#b31">[31]</ref>. This dataset provides data recorded by 10 accelerometers attached to kitchen tools, such as knife, peeler, oil bottle etc., during the preparation of a salad by 25 users. This dataset features annotations at four levels of granularity, out of which we use the eval and mid granularities. The former consists of 10 actions that can be reasonably recognized based on the utilization of accelerometerequipped objects, such as add oil, cut, peel etc., while the latter consists of 18 mid-level actions, such as cut tomato, peel cucumber. Both granularities include a background class. We evaluate our method using the ground truth labels and the 5-fold cross-validation setup proposed by the authors of <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b15">15]</ref>.</p><p>In summary, these two datasets provide kinematic/sensor data recorded during the execution of long goal-driven complex activities, which are comprised of multiple finegrained action instances following a grammar. Hence, they are suitable for evaluating our method, which was designed  <ref type="table">Table 1</ref>: Average per-frame action recognition accuracy for surgical task segmentation and recognition on the JIGSAWS dataset <ref type="bibr" target="#b8">[9]</ref>. The results are averaged over three random runs, with the standard deviation reported in parentheses. Best results are shown in bold, while second best results are denoted in italics.* Our results are not directly comparable with those of <ref type="bibr" target="#b7">[8]</ref>, since they were using data downsampled in time (5Hz). For a fair comparison, results for LSTM, BiLSTM on non-downsampled data (30Hz) were obtained using the code and default parameters publicly available from the authors <ref type="bibr" target="#b7">[8]</ref>. ** Our results are not directly comparable with those of LC-SC-CRF <ref type="bibr" target="#b17">[17]</ref>, where authors were using both kinematic data as well as the distance from the tools to the closest object in the scene from the video.</p><p>for kinematic data and features a temporal model that is able to capture action transitions. Other datasets collected for action segmentation with available skeleton data, such as CAD-120 <ref type="bibr" target="#b12">[12]</ref>, Composable Activities <ref type="bibr" target="#b20">[20]</ref>, Watch-n-Patch <ref type="bibr" target="#b36">[36]</ref> and OAD <ref type="bibr" target="#b6">[7]</ref>, have a mean number of 3 to 12 action instances per sequence <ref type="bibr" target="#b21">[21]</ref>, while for example the Suturing task in the JIGSAWS dataset features an average of 20 action instances per sequence, ranging from 17 to 37. It is therefore more challenging for comparing temporal models. Recently, the PKU-MMD dataset <ref type="bibr" target="#b21">[21]</ref> was proposed, which is of larger scale and also contains around 20 action instances per sequence. However, the actions in this dataset are not fine-grained (e.g., hand waving, hugging etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Input data are normalized to have zero mean and unit standard deviation. We apply PCA on the robot kinematic data of the JIGSAWS dataset to reduce their dimension from 76 to 35 following the setup of <ref type="bibr" target="#b30">[30]</ref>. The dictionary is initialized using the SPAMS dictionary learning toolbox <ref type="bibr" target="#b23">[23]</ref> and the CRF parameters are initialized to 0. We use Stochastic Gradient Descent with a batch size of 1 and momentum of 0.9. We also reduce the learning rate by one half every 20 epochs and train our models for 100 epochs. Parameters such as the regularization cost C, initial learning rate ?, temporal window size for average-pooling L, Lasso regularizer parameter ? u , skip chain length d and dictionary size m vary with each dataset, surgical task or granularity. The window size was fixed to 71 for JIGSAWS and 51 for 50 Salads, the dictionary size M was chosen via cross-validation from the values {50, 100, 150, 200}, ? u from values {0.1, 0.5}, C from {0.001, 0.01, 0.1, 1}, ? from {0.0001, 0.001, 0.01} and d from {21, 51, 81}. To perform cross-validation we generate five random splits of the available sequences of each dataset task/granularity. Note that since both datasets have a fixed test setup, with all users appearing in the test set exactly once, it is not clear how to use them for hyperparameter selection without inadvertently training on the test set. Here we randomly crop a temporal segment from each of the videos instead of using the whole sequences for cross-validation, in order to avoid using the exact same video sequences which will be used for evaluating our method. The length of these segments is 80% of the original sequence length. Furthermore, we select m, ? u and d by using the initialized dictionary ? 0 and learning the weights of a SC-CRF, while we choose C and ? by jointly learning the dictionary and the SC-CRF weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Overall performance. We first compare our method with state-of-the-art methods on the JIGSAWS and 50 Salads datasets. The per-frame action recognition accuracies of all the compared methods on JIGSAWS are summarized in <ref type="table">Table 1</ref>. It can be seen that our method yields the best or second best performance for all tasks on both the LOSO and LOUO setups, except for Suturing LOUO, where LC-SC-CRF achieves per-frame action recognition accuracies up to 83%. However, their result is not directly comparable to ours, since they employ additional video-based features. Also note that in <ref type="bibr" target="#b16">[16]</ref> they use a SC-CRF with an addi-Method 50 Salads eval mid LC-SC-CRF <ref type="bibr" target="#b17">[17]</ref> 77.8 55.05* LSTM <ref type="bibr" target="#b18">[18]</ref> 73.3 -TCN <ref type="bibr" target="#b18">[18]</ref> 82.0 -Ours 80.04 (0.11) 56.72 (0.72) <ref type="table">Table 2</ref>: Results for action segmentation and recognition on the 50 Salads dataset using granularities eval and mid. Results are averaged over three random runs, with the standard deviation reported in parentheses. Best results are shown in bold, while second best results are denoted in italics.* LC-SC-CRF <ref type="bibr" target="#b17">[17]</ref> was evaluated on the mid granularity with smoothed out short interstitial background segments <ref type="bibr" target="#b18">[18]</ref>.</p><p>tional pairwise term (skip-length data potentials), which is not incorporated in our model and could potentially improve our results. However, it is worth noting that our method achieves comparable performance to deep recurrent models such as LSTMs <ref type="bibr" target="#b7">[8]</ref> and the newly proposed TCN <ref type="bibr" target="#b18">[18]</ref>, which possibly captures complex temporal patterns, such as action compositions, action durations, and long-range temporal dependencies. Furthermore, our method consistently improves over SDSDL <ref type="bibr" target="#b30">[30]</ref>, which was based on joint sparse dictionary and linear SVM learning, as well as a temporal smoothing of results using the Viterbi algorithm with precomputed action transition probabilities. <ref type="table">Table 2</ref> summarizes our results on the 50 Salads dataset under two granularities. Although the modality used in this dataset is different (accelerometer data), it can be seen that our method is very competitive among all the compared methods, even with respect to methods relying on powerful deep temporal models such as LSTMs. Ablative analysis. In <ref type="table">Tables 4, 3</ref> we analyze the contribution of the key components of our method, namely the contribution of a) using sparse features (Eq. 3) obtained from an unsupervised dictionary in conjunction with a Linear Chain CRF, b) substituting the Linear Chain CRF with a Skip Chain CRF (SC-CRF) and c) jointly learning the dictionary used in sparse coding and the CRF unary and pairwise weights. As expected, using sparse features instead of the raw kinematic features consistently boosts performance across all tasks on JIGSAWS. Similarly, sparse coding of accelerometer data improves performance on 50 Salads and notably this improvement is larger in the case of fine-grained activities (mid granularity). Furthermore, using a SC-CRF further boosts performance as expected, since it is more suitable for capturing action-to-action transition probabilities in contrast to the Linear Chain CRF which captures frame-to-frame action transition probabilities.</p><p>It is however surprising that learning a discriminative dictionary jointly with the CRF weights does not significantly improve performance, yielding an improvement of at most ? 1%. Further investigating this result, we computed  <ref type="table">Table 3</ref>: Analysis of contribution to recognition performance from each model component in the 50 Salads dataset. Results are averaged over three random runs, with the standard deviation reported in parentheses. raw+CRF: use kinematic data as input to a CRF, SF + CRF: use sparse features z as input to a CRF, SF + SC-CRF: use sparse features z as input to a SC-CRF, SDL + SC-CRF: joint dictionary and SC-CRF learning.</p><p>additional metrics for evaluating the segmentation quality on the JIGSAWS dataset. In particular, we report the edit score <ref type="bibr" target="#b17">[17]</ref>, a metric measuring how well the model predictions the ordering of action segments, and segmental-f1@10 score as defined in <ref type="bibr" target="#b15">[15]</ref>. As it can be seen in <ref type="table">Table 5</ref>, performance is similar across all metrics for both unsupervised and discriminative dictionary, except for a consistent improvement in Needle Passing. One possible explanation could be that the computation of features based on average pooling of sparse codes in a temporal window might reduce the impact of the discriminatively trained dictionary. However, repeating the experiment on JIGSAWS (Suturing LOSO) without average temporal pooling leads to the same behavior, i.e. using a dictionary learned via unsupervised training with a SC-CRF yields a per-frame accuracy of 86.64%, while using a dictionary jointly trained with the SC-CRF yields 86.11%. Our findings could be attributed to the limited training data. They also seem to corroborate the conclusions drawn by Coates et al. <ref type="bibr" target="#b5">[6]</ref>, who have experimentally observed that the superior performance of sparse coding, especially when training samples are limited, arises from its non-linear encoding scheme and not from the basis functions that it uses. Qualitative results. In <ref type="figure">Fig. 2</ref> we show examples of ground truth segmentations and predictions for selected testing sequences from JIGSAWS Suturing. As it can be seen, the LOUO setup is more challenging since the model is asked to recognize actions performed by a user it has not seen before and in addition to that there is great variability in experience and styles between surgeons. In all cases our model outputs smooth predictions, without significant over-segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel end-to-end learning framework for fine-grained action segmentation and recognition, which combines features based on sparse coding with a Linear Chain CRF model. We also proposed a max-margin  <ref type="table">Table 4</ref>: Analysis of contribution to recognition performance from each model component in the JIGSAWS dataset. Results are averaged over three random runs, with the standard deviation reported in parentheses. raw+CRF: use kinematic data as input to a Linear Chain CRF, SF + CRF: use sparse features z as input to a CRF, SF + SC-CRF: use sparse features z as input to a SC-CRF, SDL + SC-CRF: joint dictionary and SC-CRF learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SC-CRF 80.24 (0.20) 56.73 (0.08) SDL + SC-CRF 80.54 (0.11) 56.72 (0.72)</figDesc><table><row><cell>Method</cell><cell cols="2">50 Salads</cell></row><row><cell></cell><cell>eval</cell><cell>mid</cell></row><row><cell>raw + CRF</cell><cell cols="2">71.81 (0.55) 44.83 (0.73)</cell></row><row><cell>SF + CRF</cell><cell cols="2">76.65 (0.19) 52.63 (0.23)</cell></row><row><cell>SF +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>CRF 79.57 (0.04) 76.39 (0.09) 66.24 (0.10) 71.77 (0.05) 69.63 (0.06) 59.47 (0.18) SF + CRF 85.70 (0.01) 82.06 (0.03) 71.72 (0.07) 76.64 (0.05) 73.58 (0.07) 60.59 (0.19) SF + SC-CRF 87.60 (0.03) 83.71 (0.03) 74.63 (0.02) 79.95 (0.05) 76.88 (0.14) 65.75 (0.12) SDL + SC-CRF 86.21 (0.34) 83.89 (0.07) 75.19 (0.12) 78.16 (0.42) 76.68 (1.20) 66.25 (0.06)</figDesc><table><row><cell>Method</cell><cell>LOSO</cell><cell></cell><cell></cell><cell>LOUO</cell><cell></cell></row><row><cell>SU</cell><cell>KT</cell><cell>NP</cell><cell>SU</cell><cell>KT</cell><cell>NP</cell></row><row><cell>raw +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Colin Lea and Lingling Tao for their insightful comments and for their help with the JIGSAWS dataset, and Vicente Ord??ez for his useful feedback during this research collaboration. This work was supported by NIH grant R01HD87133.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 5</ref><p>: Comparison of unsupervised and supervised dictionary used for sparse coding on JIGSAWS dataset. Metrics reported are: accuracy/edit score/segmental f1 score. Results are from a single random run. SF + SC-CRF: use sparse features z obtained from unsupervised dictionary as input to a SC-CRF, SDL + SC-CRF: use sparse features z from discriminative dictionary learned jointly with a SC-CRF.  approach for jointly learning the sparse dictionary and the CRF weights, resulting in a dictionary adapted to the task of action segmentation and recognition. Experimental evaluation of our method on two datasets showed that our method performs on par or outperforms most of the state-of-the-art methods. Given the recent success of deep convolutional networks (CNNs), future work will explore using deep fea-tures as inputs to the temporal model and jointly learning the CNN and CRF parameters in a unified framework.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A dataset and benchmarks for segmentation and recognition of gestures in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sefati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>B?jar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surgical gesture classification from video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>B?jar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="660" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="921" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing surgical activities with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="551" to="558" />
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>B?jar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Yuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">JHU-ISI gesture and skill assessment working set (JIGSAWS): a surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Workshop on Modeling and Monitoring of Computer Assisted Interventions M2CAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a discriminative dictionary for sparse coding via label consistent K-SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1697" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="27" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from RGB-D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Applications of Computer Vision Conference</title>
		<meeting><address><addrLine>Lake Placid</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An improved model for segmentation and recognition of fine-grained activities with application to surgical training tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1123" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning convolutional action primitives for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Brave New Ideas on Motion Representation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Max-margin dictionary learning for multiclass image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical modeling of spatio-temporally composable human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Task-driven dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action and event recognition with Fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse coding for classification via discrimination ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5839" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Weakly supervised action learning with RNN based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno>abs/1703.08132</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning shared, discriminative dictionaries for surgical gesture segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sefati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 6th Workshop on Modeling and Monitoring of Computer Assisted Interventions (M2CAI)</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse hidden Markov models for surgical gesture classification and skill evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Computed Assisted Interventions</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse dictionaries for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmentation and recognition of surgical gestures from kinematic and video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Watch-npatch: Unsupervised understanding of actions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4362" to="4370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint CRF and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint CRF and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised translationinvariant sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3517" to="3524" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
