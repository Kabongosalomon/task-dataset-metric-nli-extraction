<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Oslo Metropolitan University</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">NIHR Oxford Biomedical Research Centre</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharib</forename><surname>Ali</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Oslo Metropolitan University</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">NIHR Oxford Biomedical Research Centre</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kumar Tomar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Johansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Halvorsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Norway</roleName><surname>Simulamet</surname></persName>
						</author>
						<title level="a" type="main">Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>segmentation</term>
					<term>minimally invasive surgery</term>
					<term>surgical instruments</term>
					<term>laparoscopy</term>
					<term>real-time</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Minimally Invasive Surgery (MIS) is a surgical intervention used to examine the organs inside the abdomen and has been widely used due to its effectiveness over open surgery. Due to the hardware improvements such as high definition cameras, this procedure has significantly improved and new software methods have demonstrated potential for computer-assisted procedures. However, there exists challenges and requirement to improve detection and tracking of the position of the instruments during these surgical procedures. To this end, we evaluate and compare some popular deep learning methods that can be explored for the automated segmentation of surgical instruments in laparoscopy, an important step towards tool tracking. Our experimental results exhibit that the Dual decoder attention network (DDANet) produces a superior result compared to other recent deep learning methods. DDANet yields a Dice coefficient of 0.8739 and mean intersection-over-union of 0.8183 for the Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of 101.36 framesper-second that is critical for such procedures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Minimally Invasive Surgery (MIS) is one of the most successful surgical intervention methods performed by inserting the surgical instruments inside the body of the patient through one or more small incisions <ref type="bibr" target="#b0">[1]</ref>. Laparoscopy is one of the types of MIS. It is a procedure with less pain, minor cuts, reduced blood loss, shorter hospital stay, and fewer complications than traditional open surgery, i.e., overcoming limitations of the traditional laparoscopic surgery <ref type="bibr" target="#b1">[2]</ref>. However, moving of the surgical instrument is limited due to restricted operating space, which causes problems such as limited field of view of the endoscope, reduction in surgeon's dexterity, and lacks perception of force feedback <ref type="bibr" target="#b2">[3]</ref>. One of the solutions to overcome these difficulties could be specialized surgical training of the surgeon for better visualization <ref type="bibr" target="#b3">[4]</ref>. Another solution could be the initiative to build computer vision based applications to assist surgeons. A computer and robotic-assisted surgical system can enhance the capability of the surgeons <ref type="bibr" target="#b4">[5]</ref> and be helpful for decision making during the surgery <ref type="bibr" target="#b5">[6]</ref>. However, to make this possible, the challenge of understanding the spatial relationships between surgical instruments, cameras, and anatomy for the patient needs to be solved <ref type="bibr" target="#b6">[7]</ref>.</p><p>Segmentation of surgical instruments is a difficult task because it has challenging conditions such as blood, smoke, reflection, and motion artifacts. During surgery, a strong light-ning condition is required for proper visualization. The intense lights can lead to non-negligible specularity on the tissue and the surgical instrument. Due to such specular reflections, the surface color can be confused with other similar colored surfaces having the same hue irrespective of having lower saturation <ref type="bibr" target="#b7">[8]</ref>. In a surgical procedure, specular reflections cause the surgical instrument to appear white. Additionally, shadows can occur in the video frames due to the change in the illumination angle and movement of the surgical tool during surgery <ref type="bibr" target="#b8">[9]</ref>.</p><p>Previous work has targeted instrument segmentation, detection, and tracking in endoscopic video images. However, they fail on challenging images such as images with the presence of specularity, blood, smoke, and motion artifacts <ref type="bibr" target="#b9">[10]</ref>. This study aims to find efficient computer vision methods for surgical instrument segmentation from laparoscopic images. We evaluate several recent state-of-the-art deep learning approaches with different backbones using the ROBUST-MIS challenge dataset acquired from real world clinical surgical procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Surgical computer vision is evolving as promising techniques to segment and track the instruments using endoscopic images are emerging <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Bodenstedt et al. <ref type="bibr" target="#b5">[6]</ref> organized "EndoVis 2015 Instrument sub-challenge 1 ", a part of Endoscopic vision (EndoVis) challenge 2 . In 2017, a similar challenge, "Robotic Instrument Segmentation Sub-Challenge 3 " was organized at the same platform. At this time, the organizers increased the number of tasks to three. The participants were asked to participate in a binary segmentation task, a parts based segmentation task, and an instrument type segmentation task. Both of these challenges provided sufficient insights on the instrument segmentation. However, the insights gained on the robustness and generalization capability of the deep learning methods were limited <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>A similar Challenge "Robust Medical Instrument Segmentation Challenge 2019 4 " was organized by Ro? et al. <ref type="bibr" target="#b9">[10]</ref>.</p><p>The challenge aimed to develop a potential method for minimally invasive surgery to track the surgical instruments in the abdomen. Most of these methods included Mask-RCNN <ref type="bibr" target="#b10">[11]</ref> based instance segmentation for multi-class segmentation and for binary segmentation. Participants also explored methods such as OR-UNet <ref type="bibr" target="#b11">[12]</ref>, DeepLabV3+ <ref type="bibr" target="#b12">[13]</ref>, U-Net <ref type="bibr" target="#b13">[14]</ref> and RASNet <ref type="bibr" target="#b14">[15]</ref>. The best performing methods for the binary instrument segmentation were OR-UNet and the DeepLabv3+ with pre-trained ImageNet encoders. Ceron et al. <ref type="bibr" target="#b15">[16]</ref> recently proposed an attention-based segmentation for MIS instruments on the same datasets that achieved state-of-the-art performance at nearly 45 frames-per-second. In addition to these challenges, there are works on segmentation <ref type="bibr" target="#b16">[17]</ref> and identification of surgical instruments in laparoscopy <ref type="bibr" target="#b17">[18]</ref>. We aim to segment surgical instruments from laparoscopy in real-time with wellestablished deep learning methods in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>ROBUST-MIS is part of the Heidelberg Colorectal (HeiCo) dataset <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which includes a total of 5983 images with their respective manually annotated segmentation masks. These training images are obtained from 16 different surgeries with two different types of procedures (prokto and rectum, 8 surgeries each). The prokto sub-folder consists of 2943 images, and the rectum sub-folder contains 3040 images. Our experiments were performed on the training dataset provided by the ROBUST-MIS Challenge organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Pre-processing</head><p>We have only considered the image dataset for training to reduce the computational cost. The original size of the images is 960?540. We split the dataset into (80-10-10) for training, validation, and testing, respectively. The training dataset was augmented and resized. The total number of images of the training dataset is 4787, and the validation and test dataset contains 598 samples each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental setup and configuration</head><p>All of the experiments were implemented using the PyTorch framework except ResUNet <ref type="bibr" target="#b19">[20]</ref> and ResUNet++ <ref type="bibr" target="#b20">[21]</ref> that were implemented with TensorFlow <ref type="bibr" target="#b21">[22]</ref>. The experiments such as ResUNet <ref type="bibr" target="#b19">[20]</ref>, ResUNet++ <ref type="bibr" target="#b20">[21]</ref>, ColonSegNet <ref type="bibr" target="#b22">[23]</ref>, DDANet <ref type="bibr" target="#b23">[24]</ref> were run on the Experimental Infrastructure for Exploration of Exascale Computing (eX3), NVIDIA DGX-2 machine. These models were trained using offline augmentation. The models such as UNet <ref type="bibr" target="#b13">[14]</ref>, FCN8 <ref type="bibr" target="#b24">[25]</ref>, PSPNet <ref type="bibr" target="#b25">[26]</ref>, DeepLabv3+ <ref type="bibr" target="#b12">[13]</ref> with MobileNetv2 <ref type="bibr" target="#b26">[27]</ref> and ResNet50 <ref type="bibr" target="#b27">[28]</ref> backbones were trained on on NVIDIA Quadro RTX 6000 and an online augmentation strategy was adopted. This is because the experiments were run on two different GPUs. Adam optimizer was adopted for all experiments. The other hyperparameters were tuned based on the empirical evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>The goal of a segmentation method is to label each pixel of the image into either a surgical instrument or background, i.e., the output of the model is a pixel-by-pixel array that determines the class of each pixel. We have compared nine different state-of-the-art recent deep learning methods such as UNet <ref type="bibr" target="#b13">[14]</ref>, FCN8 <ref type="bibr" target="#b24">[25]</ref>, ResUNet <ref type="bibr" target="#b19">[20]</ref>, ResUNet++ <ref type="bibr" target="#b20">[21]</ref>, DDANet <ref type="bibr" target="#b23">[24]</ref>, ColonSegNet <ref type="bibr" target="#b22">[23]</ref>, PSP-Net <ref type="bibr" target="#b25">[26]</ref>, and DeepLabv3+ <ref type="bibr" target="#b12">[13]</ref> with ResNet50 <ref type="bibr" target="#b27">[28]</ref> and MobileNetv2 <ref type="bibr" target="#b26">[27]</ref> backbones. The main motivation behind choosing these networks is that they have shown state-ofthe-art results for various medical image segmentation tasks. A detailed explanation about the overall architectures, idea behind using each component in the architecture, advantage of the blocks used, and performance on different imaging modalities can be found in their respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section, we present the experimental results. We report the result in <ref type="table" target="#tab_0">Table I</ref> using standard metrics used for computer vision tasks. From <ref type="table" target="#tab_0">Table I</ref>, we observe that DDANet produces the highest dice coefficient (DSC) of 0.8739, mean Intersection over Union (mIoU) of 0.8183, recall of 0.8703, precision of 0.9348, F2 of 0.8613, accuracy of 0.9897 and realtime speed of 101.36. The other most competitive method was ColonsegNet, as it has the competitive DSC, mIoU, precision, and F2. ColonsegNet also has the highest recall of 0.8899 and highest FPS of 185.54. The precision for DeepLabv3+ with ResNet50 backbone is the best. From the quantitative result analysis, we can see that there is a significant improvement in deep learning methods as we compare methods from 2015 to 2021. The current methods are efficient in terms of both accuracy and speed.</p><p>To show the effectiveness of the different models, we have shown the qualitative analysis results of nine different deep learning methods on easy, medium, and hard images as shown in <ref type="figure">Figure 1</ref>. From the qualitative analysis, we can see that all the models perform satisfactorily on the easy cases, although it has one instrument or multiple instruments <ref type="figure">(Figure 1</ref>). For the medium cases, the deep learning methods such as Deeplabv3, ColonsegNet, and DDANet are performing satisfactorily. However, for ColonsegNet and DDANet, we can also observe over-segmentation. For the hard cases, Deeplabv3 with both the backbones shows better segmentation results for some scenarios, whereas ColonSegNet and DDANet show over-segmentation. For some challenging scenarios, DDANet shows promising results (please refer to the last image of <ref type="figure">Figure 1</ref>). Overall, the preferred choice is DDANet, although it shows over-segmentation for some scenarios. The strength is also because of its capability to segment surgical instruments in real-time with a high FPS that was not achieved by other methods such as DeepLabv3+ with different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>MIS is a progressive field where computer-assisted techniques are being iterated and developed for clinical use. While, segmentation of surgical instruments can play a vital role in tracking and a step forward towards automated systems, the real-time requirement hinders the clinical applicability of many recent deep learning methods. We have presented different recent segmentation methods and identified efficient architectures for the pixel-wise segmentation of surgical instruments on the laparoscopy dataset. As observed from <ref type="table" target="#tab_0">Table I</ref>, DDANet architecture provides the highest metric performances over other methods and has real-time capability (101.36 FPS). As evident from <ref type="figure">Figure 1</ref>, DDANet is a robust encoder-decoder network that can capture sharper surgical instrument boundaries by continuously retrieving the spatial information. The encoder branch in the DDANet helps boost performance by generating the attention map (spatial attention), which is used in the segmentation branch to improve the feature representation. Since the ground truth is not available for the test set, we can not directly compare the results with the challenge participants. However, our results on the validation set show promise and reliance of the DDANet. Furthermore, we also explored the ability of models on a low-end NVIDIA 1060 Ti GPU <ref type="bibr">(6 GB)</ref>. For this, we achieved an FPS of 120.05 on ColonSegNet and 95.05 on DDANet which suggests the effectiveness of these models.</p><p>We explicitly choose to use a pre-trained encoder with the DeepLabv3+ because we hypothesize that the ImageNet pre-trained network will improve performance. Although it provided a promising DSC of 0.8582, the methods such as ColonSegNet and DDANet showed competitive or better results without learning from any pre-trained encoders. The main reason for higher FPS for these two models is the lightweight network architecture that making the model efficient for realtime segmentation facilitating clinical deployment. The detailed analysis of the qualitative results <ref type="figure">(Figure 1)</ref> show that the best methods are robust to samples with specularity, blood, and smoke, although it shows some areas with over segmentation. This is also evident from the dice score of the best performing method. However, the challenge still exists with the images having moving instruments and the over/under segmentation problem with the hard cases (see bottom rows, <ref type="figure">Figure 1</ref>) that need to be addressed in further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper explores several deep learning methods with different backbones for the segmentation of surgical instruments in laparoscopy. A comparison of different state-ofthe-art methods shows that DDANet is the most efficient method providing a strong baseline on the ROBUST-MIS challenge 2019 dataset. DDANet provides the most accurate segmentation of the surgical instruments with a real-time speed of 101.36 FPS. The provided comparison shows the strength of simple and small networks such as ColonSegNet and DDANet to achieve real-time performance with competitive scores compared to that of complex and hybrid networks. In the domains where real-time inference is vital, exploring such networks are important to accelerate both research and clinical applicability of models. The achieved results also highlight the potential for the extension of the work to achieve higher accuracy by incorporating attention mechanisms or use of skip-connections during the feature flow but maintaining the speed at the required real-time performances, e.g., 60 FPS for most devices. We plan to investigate multi-instance segmentation and a composite instrument tracking technique for future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ACKNOWLEDGEMENTD.</head><label></label><figDesc>Jha is funded by Research Council of Norway project number 263248 (Privaton). The computations in this paper were performed on equipment provided by the Experimental Infrastructure for Exploration of Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract 270053.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>OF SURGICAL INSTRUMENTS SEGMENTATION TASK ON THE ROBUST-MIS CHALLENGE 2019 DATASET Qualitative results comparison of nine deep learning methods on easy cases, medium cases, and hard cases</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell><cell>F2</cell><cell>Accuracy</cell><cell>FPS</cell></row><row><cell>UNet (MICCAI 2015) [14]</cell><cell>ResNet34 [28]</cell><cell>0.4214</cell><cell>0.3318</cell><cell>0.5071</cell><cell>0.4224</cell><cell>0.4502</cell><cell>0.9294</cell><cell>35.00</cell></row><row><cell>FCN8 (CVPR 2015) [25]</cell><cell>-</cell><cell>0.7774</cell><cell>0.6825</cell><cell>0.7737</cell><cell>0.8481</cell><cell>0.7600</cell><cell>0.9817</cell><cell>24.91</cell></row><row><cell>ResUNet (GRSL 2017) [20]</cell><cell>-</cell><cell>0.7097</cell><cell>0.6193</cell><cell>0.8056</cell><cell>0.7627</cell><cell>0.7111</cell><cell>0.9755</cell><cell>10.40</cell></row><row><cell>PSPNet (CVPR 2017) [26]</cell><cell>-</cell><cell>0.8110</cell><cell>0.7235</cell><cell>0.7677</cell><cell>0.9278</cell><cell>0.7715</cell><cell>0.9855</cell><cell>16.80</cell></row><row><cell>Deeplabv3+ (ECCV 2018) [13]</cell><cell>MobileNetv2 [27]</cell><cell>0.8497</cell><cell>0.7780</cell><cell>0.8338</cell><cell>0.9246</cell><cell>0.8281</cell><cell>0.9882</cell><cell>32.50</cell></row><row><cell>Deeplabv3+ (ECCV 2018) [13]</cell><cell>ResNet50 [28]</cell><cell>0.8582</cell><cell>0.7895</cell><cell>0.8397</cell><cell>0.9301</cell><cell>0.8362</cell><cell>0.9889</cell><cell>27.90</cell></row><row><cell>ResUNet++ (ISM 2019) [21]</cell><cell>-</cell><cell>0.8252</cell><cell>0.7576</cell><cell>0.8460</cell><cell>0.8903</cell><cell>0.8319</cell><cell>0.9811</cell><cell>8.13</cell></row><row><cell>ColonSegNet (IEEE Access 2021) [23]</cell><cell>-</cell><cell>0.8495</cell><cell>0.7943</cell><cell>0.8899</cell><cell>0.8871</cell><cell>0.8443</cell><cell>0.9898</cell><cell>185.54</cell></row><row><cell>DDANet (ICPRW 2020) [24]</cell><cell>-</cell><cell>0.8739</cell><cell>0.8183</cell><cell>0.8703</cell><cell>0.9348</cell><cell>0.8613</cell><cell>0.9897</cell><cell>101.36</cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://endovissub-instrument.grand-challenge.org/ EndoVisSub-Instrument/ 2 https://endovis.grand-challenge.org/ 3 https://endovissub2017-roboticinstrumentsegmentation.grand-challenge. org/ 4 https://robustmis2019.grand-challenge.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computer assisted minimally invasive surgery: is medical computer vision the answer to improving laparosurgery?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourdel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Canis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Hypotheses</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="858" to="863" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segmentation of surgical instruments for minimally-invasive robot-assisted procedures using generative deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Azqueta-Gavaldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fr?hlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03486</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time surgical instrument tracking in robot-assisted surgery using multi-domain convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare Technology Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="159" to="164" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Application of computer vision in surgical training and surgical robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zahiri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>The University of Nebraska-Lincoln</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-guided interventions: technology review and clinical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="119" to="142" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02475</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for instrument segmentation in robotic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>of International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Saturation-preserving specular reflection separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3725" to="3733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-guided lightweight network for real-time segmentation of robotic surgical instruments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9939" to="9945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparative validation of multi-instance instrument segmentation in endoscopy: Results of the robust-mis 2019 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ro?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101920</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision</title>
		<meeting>of the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Or-unet: an optimized robust residual u-net for instrument segmentation in endoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European conference on computer vision (ECCV)</title>
		<meeting>of the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<meeting>of Medical Image Computing and Computer Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rasnet: segmentation for tracking surgical instruments in surgical videos using refined attention segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5735" to="5738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Assessing YOLACT++ for real time and robust instance segmentation of medical instruments in endoscopic procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C A</forename><surname>C?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ochoa-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15997</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmentation of surgical instruments in laparoscopic videos: training dataset generation and deep-learning-based framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Plishker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Medical Imaging 2019: Image-Guided Procedures, Robotic Interventions, and Modeling</title>
		<meeting>of Medical Imaging 2019: Image-Guided edures, Robotic Interventions, and Modeling</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10951</biblScope>
			<biblScope unit="page">109511</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying surgical instruments in laparoscopy using deep learning instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kletz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Husslein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Content-Based Multimedia Indexing (CBMI)</title>
		<meeting>of International Conference on Content-Based Multimedia Indexing (CBMI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heidelberg colorectal data set for surgical data science in the sensor operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Symposium on Multimedia (ISM)</title>
		<meeting>of International Symposium on Multimedia (ISM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Proceeding of USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>of eeding of USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ieee Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="40" to="496" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DDANet: Dual decoder attention network for automatic polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPRW</title>
		<meeting>of ICPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
