<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ResLT: Residual Learning for Long-tailed Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">ResLT: Residual Learning for Long-tailed Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Residual Learning</term>
					<term>Imbalanced Learning</term>
					<term>Long-tailed Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning algorithms face great challenges with long-tailed data distribution which, however, is quite a common case in real-world scenarios. Previous methods tackle the problem from either the aspect of input space (re-sampling classes with different frequencies) or loss space (re-weighting classes with different weights), suffering from heavy over-fitting to tail classes or hard optimization during training. To alleviate these issues, we propose a more fundamental perspective for long-tailed recognition, i.e., from the aspect of parameter space, and aims to preserve specific capacity for classes with low frequencies. From this perspective, the trivial solution utilizes different branches for the head, medium, tail classes respectively, and then sums their outputs as the final results is not feasible. Instead, we design the effective residual fusion mechanism -with one main branch optimized to recognize images from all classes, another two residual branches are gradually fused and optimized to enhance images from medium+tail classes and tail classes respectively. Then the branches are aggregated into final results by additive shortcuts. We test our method on several benchmarks, i.e., long-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and iNaturalist 2018. Experimental results manifest the effectiveness of our method. Our code is available at https://github.com/jiequancui/ResLT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C ONVOLUTIONAL neural networks (CNNs) have achieved impressive success on various tasks, including large-scale image classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, object detection <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref> and semantic segmentation <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>. Especially, with the rise of neural network search <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, performance of CNNs have further taken a big step. However, the incredible progress stems in part from highquality and large-scale datasets, such as ImageNet <ref type="bibr" target="#b24">[24]</ref>, MS COCO <ref type="bibr" target="#b25">[25]</ref> and Places <ref type="bibr" target="#b26">[26]</ref>. These datasets are carefully designed with balanced distributions over different classes. In real-world applications, data could follow an unexpected long-tailed distribution where only a few head classes and a large number of tail classes exist. The long-tailed phenomenon may lead to severe degradation of performance for all models that do not take it into consideration. We in this paper address long-tailed recognition, i.e., recognition on data with long-tailed distributions.</p><p>One of the greatest challenges in the long-tailed setting is data imbalance. It is also the principal reason for head classes dominating the training procedure, making head classes enjoy much higher accuracy than tail classes. In the literature, two kinds of methods tackle the long-tailed problem via re-sampling and re-weighting <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>. It is found that over-sampling tail-class images <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> may still suffer from heavy overfitting to tail classes while under-sampling by dropping a large number of head-class images <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b36">[36]</ref> inevitably impairs the generalization ability of deep models. Also, for re-weighting methods, previous works <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref> have demonstrated that re-weighting strategies may cause optimization difficulties during training on large-scale and realworld datasets.</p><p>Regarding procedures, re-sampling rebalances head and tail classes in input space by constructing balanced minibatches during training as shown in <ref type="figure">Fig. 1(a)</ref>. Differently, re-weighting deals with the loss space by assigning different weights for classes according to the respective numbers of samples as shown in <ref type="figure">Fig. 1(b)</ref>. Albeit the procedural difference, we note that these two lines, by nature, both eventually make effects on model parameters to adjust tail classes response. This finding leads to our key idea of re-balancing in parameter space directly.</p><p>Our direct operations on the head and tail classes regarding parameter space can avoid heavy over-fitting to tail classes and the difficult optimization problem mentioned above. To illustrate that designing these effective operations is nontrivial, we show in <ref type="figure">Fig. 1(c)</ref>(II) a naive solution. It preserves specialized parameters for the head, medium, and tail classes respectively via three branches of N h , N m and N t . The branches are optimized for respective recognition and the final result is obtained by summing or averaging outputs of them. Unfortunately, this naive solution does not work well in experiments.</p><p>Contrary to these naive operations, we model the longtailed recognition problem as one residual learning process and propose a novel residual fusion mechanism, as shown in <ref type="figure">Fig. 1(c)</ref> <ref type="bibr">(III)</ref>. It has one main branch (N h+m+t ) optimized to classify images of all classes while the other two residual branches (N m+t and N t ) are optimized to recognize images in medium+tail and tail classes respectively. Outputs of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Parameters</head><p>A class-balanced mini-batch: same number of images for each of the selected classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Parameters Shared Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>Classifier Classifier  <ref type="figure">Fig. 1</ref>. Comparison between our method and previous ones. Previous re-sampling and re-weighting balance input space and loss space respectively, with effect on the model parameters. Our method is built regarding the most fundamental parameter space, individually preserves specialized parameters for the head, medium, and tail classes with three sub-branches. These sub-branches are combined finally to enhance classification results of the tail and medium classes by the proposed residual fusion mechanism. In (b), (X, Y ) is a batch of images and their corresponding labels. weight represents a vector of class-wise weights which usually is in reverse proportion to the number of samples of classes. The classifier is a fully connected (FC) layer that is shared among the three branches in (II) and (III) of (c).</p><p>these three branches are aggregated into the final result by additive shortcuts.</p><p>An intuitive explanation of our strategy is to gradually enhance classification results on tail classes with other learned residual branches. It is noteworthy that samples belonging to the head, medium, and tail classes still dominate N h+m+t , N m+t and N t individually in the training procedure, and thus guarantee parameter specialization of the head, medium, and tail classes, which is coherent with the experimental phenomenon in 4.2.3.</p><p>We extensively validate our method on several longtailed benchmark datasets using long-tailed versions of CIFAR-10, CIFAR-100, ImageNet, Places, and iNaturalist 2018 data. Experimental results manifest the effectiveness of our method for long-tailed recognition. Our key contributions are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We study the problem from a new perspective of parameter space that leads to an effective re-balance between head and tail classes in the long-tailed setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose the residual fusion mechanism, making re-balance in the aspect of parameter space feasible and alleviating limitations in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We validate our method on representative benchmarks. Note that this is the first time that a one-stage method consistently surpasses two-stage methods on long-tailed CIFAR-10, CIFAR-100, ImageNet, iNaturalist 2018 all these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-sampling Strategy</head><p>There are two groups of re-sampling strategies: over-sampling the tail class images <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> and under-sampling the head class images <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b36">[36]</ref>. Over-sampling is regularly useful on large datasets and often suffers from heavy over-fitting to tail classes especially on small datasets. For under-sampling, it discards a large portion of data, which inevitably causes degradation of the generalization ability of deep models. Moreover, the under-sampling strategy is not suitable when data is largely imbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-weighting Strategy</head><p>Re-weighting <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref> is another prominent strategy. It assigns different weights for classes and even samples. The vanilla reweighting method gives class weights in reverse proportion to the number of samples of classes. However, with largescale data, re-weighting makes the deep models difficult to optimize during training <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>. Cui et al. <ref type="bibr" target="#b29">[29]</ref> relieved the problem using "effective numbers" to calculate class weights. Tan et al. <ref type="bibr" target="#b43">[43]</ref> conducted re-weighting from the perspective of gradients by ignoring the suppressed gradients of tail classes.</p><p>Another line of work is to adaptively re-weight each sample. Focal loss <ref type="bibr" target="#b44">[44]</ref> assigned smaller weights for wellclassified samples. Li et al. <ref type="bibr" target="#b45">[45]</ref> down-weighted samples with very small or large gradients in response to the principle that samples with small gradients are usually wellclassified and samples with large gradients are usually out of the distribution. <ref type="bibr">Cao et al. [46]</ref> first observed that reweighting and re-sampling are inferior to the vanilla empirical risk minimization (ERM) algorithm before annealing the learning rate. A two-stage deferred re-balancing optimization schedule was proposed, which trains using vanilla ERM with the LDAM <ref type="bibr" target="#b46">[46]</ref> loss before annealing learning rate, and then deploys a re-weighted LDAM loss with a much smaller learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stage Methods</head><p>Recently, Kang et al. <ref type="bibr" target="#b47">[47]</ref> and Zhou et al. <ref type="bibr" target="#b48">[48]</ref> concluded that although class re-balance strategies matter when jointly training representation and classifier, uniform sampling gives more general representations. Based on this observation, Kang et al. <ref type="bibr" target="#b47">[47]</ref> achieved state-of-the-art results on long-tailed ImageNet (ImageNet-LT) by decomposing representation and classifier learning, i.e., first train the deep models with uniform sampling, then fine-tune the classifier with class-balanced sampling while keeping parameters of representation learning fixed. Similarly, Zhou et al. <ref type="bibr" target="#b48">[48]</ref> integrated mixup into the proposed cumulative learning strategy with which they bridged the representation learning and classifier re-balancing and achieved state-of-the-art results on long-tailed CIFAR (CIFAR-LT) and iNaturalist 2018.</p><p>The two-stage design defies the end-to-end merit that we used to believe since the deep learning era. But why does the two-stage training outperform the end-to-end one largely in long-tailed classification? The work <ref type="bibr" target="#b49">[49]</ref> by Tang et al. analyzed the reason from the perspective of causal graph and concluded that the bad momentum causal effects played a vital role. Differently, we address the problem in terms of residual learning. <ref type="bibr" target="#b39">[39]</ref>, by Wang et al., learned a meta regression network from head classes and used it to construct the classifier for tail classes. OLTR <ref type="bibr" target="#b50">[50]</ref> used memory banks to store mid-and high-level features from head to tail classes. Then they transferred knowledge from head classes to tail classes with a dynamic-meta embedding mechanism. Abdullah Jamal et al. analyzed imbalanced learning from a domain adaptation perspective in <ref type="bibr" target="#b42">[42]</ref> and proposed a meta learning framework for long-tailed recognition. Saurabh et al. <ref type="bibr" target="#b51">[51]</ref> trained individual experts and transferred knowledge from these experts to the student model. However, the performance with meta learning or transfer learning methods still far from two-stage methods and the training procedure is usually much more complicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta Learning and Transfer Learning MetaModelNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Motivation As illustrated in <ref type="figure">Fig. 1(a)</ref>, re-sampling strategies build class-balanced mini-batch data for training. However, over-sampling tail class images is easy to get into trouble of heavy over-fitting to tail classes while under-sampling head class images hurts the generalization ability of deep models due to discarding a large number of head-class images that are conducive in learning good representations. For reweighting methods, previous works <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref> demonstrate that it makes deep models difficult to optimize. We, unlike these solutions, explore another way to re-balance regarding parameter space directly, avoiding above disadvantages and making further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Re-balancing in Parameter Space</head><p>We are the first to propose re-balancing regarding parameter space. To understand the difficulty, we show a naive way by preserving several specialized parameters for the head, medium, tail classes separately by three branches (i.e., N h , N m , and N t ). They are shown in <ref type="figure">Fig. 1</ref>(c)(II). After sorting regarding the number of images, all the classes are divided into 3 groups for the head, medium, tail classes, and we guarantee that the three groups have the same imbalance factor ?. ?= Nmin Nmax where N max and N min are the numbers of training samples for the most frequent class and the least frequent class in each group. Under this case, the core issue is the way to fuse the outputs of these branches into the final prediction because it is impossible to know whether one image belongs to head or medium or tail classes at inference time. Therefore, fusing the three branches is not trivial. Here we compare three straightforward fusion methods with the basic vanilla baseline using cross-entropy. They are set as described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Pseudocode of baselines implementation in a</head><p>PyTorch-like style. <ref type="bibr">1:</ref> Input: x ? R c?h?w is an image feature map from the network backbone. Specifically, c is the number of channels, h and w are spatial dimensions. N h (?), N m (?), and N t (?) are three branches for the head, medium, and tail classes respectively. cls(?) is the shared linear classifier among the branches with the weight w ? R k?c . k is the number of classes. ?(?) is the softmax activation function. 2: out h , out m , out t = cls(N h (x)), cls(N m (x)), cls(N t (x)); 3: Baseline (1): <ref type="bibr">4:</ref> out a = torch.stack([out h , out m , out t ], dim=1); 5: pred = torch.argmax(out a .max(1), dim=1). <ref type="bibr" target="#b6">6</ref>: Baseline (2): 7: out a = out h + out m + out t ; 8: pred = torch.argmax(out a , dim=1). 9: Baseline (3): 10: out a = ?(out h ) + ?(out m ) + ?(out t ); 11: pred = torch.argmax(out a , dim=1).</p><p>The experimental results are summarized in the <ref type="table" target="#tab_0">Table 1</ref>. It shows that these straightforward fusion methods do not work satisfyingly and even yield inferior results compared to the vanilla baseline. We instead propose an effective residual fusion mechanism to handle this challenging issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Fusion Mechanism</head><p>Under the long-tailed setting, models usually achieve high accuracy on head classes while the performance on tail classes is unsatisfactory. Based on this phenomenon, we model the long-tailed recognition problem as one residual learning process. It uses extra learned residual branches to enhance classification results on tail classes. Here we stress that the "residual" denotes the nested class assignments for different branches in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>Classifier Classifier Classifier Shared Parameters As shown in <ref type="figure">Fig. 1</ref>(c)(III), the model parameters are divided into two parts: shared and specialized parameters. Shared parameters are for common features across all the classes while specialized parameters are to preserve specific capacity for the head, medium, tail classes with three branches N h+m+t , N m+t and N t correspondingly. In fact, the three branch networks are implemented by only one extra 1x1 grouped convolution with group number 3. Though only a few parameters are introduced, this parameter specialization mechanism plays an important role as demonstrated in Sec. 3.3.1 with experimental verification.</p><formula xml:id="formula_0">Classifier (a) (b) (c) (d) (e)</formula><p>To effectively aggregate the three branches, we propose the residual fusion module shown in <ref type="figure">Fig. 1</ref>(c)(III). We model the long-tailed recognition problem as such residual learning process where one main branch N h+m+t learns to recognize images of all the classes and the other two residual branches N m+t , N t are used to classify medium+tail classes and tail classes respectively. The outputs of the three branches are added together as the final result. It is worth noting that head, medium, and tail classes still dominate branches N h+m+t , N m+t , N t respectively, and thus the parameter specialization of these classes are preserved.</p><p>In our residual fusion mechanism, loss functions are deployed in training process as</p><formula xml:id="formula_1">L f usion = J (N h+m+t (X)+Nm+t(X)+Nt(X), Y ),<label>(1)</label></formula><formula xml:id="formula_2">L branch = i?{h+m+t,m+t,t} J (Ni(SXi), SYi),<label>(2)</label></formula><formula xml:id="formula_3">L all = (1 ? ?)L f usion + ?L branch ,<label>(3)</label></formula><p>where (X, Y ) denotes the uniformly sampled images and labels of a mini-batch. (SX h+m+t , SY h+m+t ) is the same as (X, Y ) consisting of all class images. (SX m+t , SY m+t ) is a subset of (X, Y ) only containing images of medium and tail classes. (SX t , SY t ) is a subset of (X, Y ) only containing images belonging to tail classes. J is cross-entropy loss and ? is a hyper-parameter. The fusion loss item L f usion in Eq. (1) optimizes for all the classes. The outputs of branches N m+t and N t are added to the outputs of main branch N m+n+t to obtain the fused outputs. Therefore, during inference, we just sum outputs of the three branches as the final result. The branchindependent loss item L branch in Eq. (2) is for N h+m+t , N m+t and N t respectively, further encouraging parameter specialization for head, medium, and tail classes respec-tively. L all is our final loss which is a weighted sum of L f usion and L branch with a trade-off hyper-parameter ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of Our Method</head><p>Our method depends on two key components: parameter specialization mechanism and residual learning mechanism.</p><p>Here we analyze each of them with extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Importance of Parameter Specialization</head><p>Though the three sub-branches are implemented by only a 1x1 grouped convolution with group number 3, we show that it is important to reserve specialized parameters for head, medium and tail classes. We compare strategies with and without the parameter specialization mechanism on CIFAR-10-LT and CIFAR-100-LT with the imbalance factor 0.02. For the setting without parameter specialization, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(e), we let outputs of the three branches be the same while keeping the same loss functions in Eqs.</p><p>(1), (2) and (3) in implementation. As shown in <ref type="figure">Fig. 3</ref>(a) and <ref type="figure">Fig. 3(b)</ref>, the model with the parameter specialization mechanism yields much higher accuracy than the model without it, which manifests the vast importance of our parameter specialization mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Analysis of Residual Fusion Module</head><p>To further study the proposed residual fusion mechanism, we conduct ablation study to understand the role of each component in our residual fusion module by comparing its variants on CIFAR-10-LT and CIFAR-100-LT with the imbalance factor 0.02:  <ref type="figure" target="#fig_1">Fig. 2(d)</ref>, we remove the additive shortcuts from ResLT. During inference, we take the output of main branch N h+m+t as the final results. The branches N m+t and N t only play the role of regularization during training.</p><p>As shown in <ref type="figure">Fig. 3</ref>(c) and <ref type="figure">Fig. 3(d)</ref>, it is obvious that both the variants (b) and (c) yield lower performance than our     <ref type="figure">Fig. 3</ref>. Ablation studies for the importance of parameter specialization mechanism with (a), (b) and the effectiveness of residual fusion mechanism with (c), (d).</p><p>full residual fusion module. Variant (a) can enjoy better performance than (b) and (c) but is inferior to ResLT. This phenomenon demonstrates that the residual relationship (nested class assignments) among branch networks and the additive shortcuts are two core factors that make it feasible to solve long-tailed recognition in parameter space. Moreover, it shows the fundamental difference between our proposed residual learning mechanism and previous work <ref type="bibr" target="#b3">[4]</ref>.</p><p>Comparison with Re-weighting Our method may reminiscent attentive readers of re-weighting methods considering that the loss with respect to tail class images is calculated by three branches while the loss for head class images is calculated by only N h+m+t branch. In fact, the comparison between ResLT and the structure of <ref type="figure" target="#fig_1">Fig. 2(e)</ref> in Sec. 3.3.2 shows the importance of the parameter specialization mechanism. At the same time, it also implies that our high performance doesn't benefit from the naive re-weighting scheme. Specifically, for the structure of <ref type="figure" target="#fig_1">Fig. 2(e)</ref>, the loss for tail class images is also calculated by three branches. However, there is a huge performance gap when compared with the structure of our residual fusion module, i.e., ResLT, as indicated by <ref type="figure">Fig. 3(a)</ref> and <ref type="figure">Fig. 3(b)</ref>.</p><p>Here we further clarify some fundamental differences between our method and re-weighting. We explicitly preserve specific capacity for the head, medium, tail classes. Besides, re-weighting independently pre-defines scalar factors as the class weights simply according to the number of images in the class while our method adaptively enhances classification results of tail classes with the learned residual branches of N m+t and N t . Further, our method captures the important relationship between different classes within each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on ResLT and RIDE [52]</head><p>Training with longtailed data, deep models can easily overfit to tail classes and thus lead to high variance of model predictions. The motivation for RIDE proposed by Wang et al. is just to reduce the predictions variance by an improved model ensemble strategy. It constructs multiple experts and a distributionaware diversity loss is deployed to promote high diversity among these experts. In contrast, different from reweighting rebalancing from loss space and re-sampling rebalancing from input space, ResLT conducts rebalance in the aspect of parameter space by allocating individual parameters for tail classes and train the model with the designed residual mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2</head><p>Top-1 accuracy (%) on iNaturalist 2018. Rows with ? denote results directly copied from <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>. denotes model is trained with 360 epochs data. " ?" denotes the model is trained with mixup and initialized with ImageNet pre-trained model by torchvision. Knowledge distillation is not applied to all methods for fair comparison. Reducing the variance of model predictions is a more general idea. A simple model ensemble strategy can even reduce the predictions variance of a model trained with balanced datasets, e.g., full ImageNet, and thus further improve the performance. Differently, ResLT is just designed for long-tailed data and it should be compatible with techniques that aim to reduce the variance of model predictions. This analysis is coherent with our experimental results shown in <ref type="table" target="#tab_8">Table 7</ref>. Further, the multiple experts structure in RIDE makes it hard to reuse public pre-trained models, e.g., ImageNet pre-trained models that are usually adopted for Places-LT in the literature. Instead, ResLT is more compatible with previous work in this aspect. CIFAR-100-LT IMB 0.02 <ref type="figure">Fig. 5</ref>. We conduct ablation studies for choices of ? on CIFAR-LT and ImageNet-LT. For CIFAR-LT, ResNet-32 is used, while we apply ResNet-10 on ImageNet-LT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Top-1 accuracy (%) on Places-LT for ResNet-152. ? denotes results directly copied from <ref type="bibr" target="#b47">[47]</ref>. " ?" denotes pre-trained model on ImageNet is trained with strong data augmentation (cutmix) provided by <ref type="bibr" target="#b58">[58]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10-LT and CIFAR-100-LT datasets CIFAR-10 and</head><p>CIFAR-100 both have 60,000 images -50,000 for training and 10,000 for validation with 10 categories and 100 categories respectively. For a fair comparison, we use the long-tailed versions of CIFAR datasets with the same setting as those used in <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b48">[48]</ref>. Following <ref type="bibr" target="#b48">[48]</ref>, we conduct experiments with imbalance factors 0.01, 0.02, and 0.1. iNaturalist 2018 The iNaturalist 2018 <ref type="bibr" target="#b61">[61]</ref> is one species classification dataset, which is on a large scale and suffers from extremely imbalanced label distributions. It is composed of 437.5K images from 8,142 categories. In addition to the extreme imbalance, iNaturalist 2018 dataset also confronts the fine-grained problem <ref type="bibr" target="#b62">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT and Places-LT ImageNet-LT and Places-LT</head><p>Evaluation Protocol Following Liu et al. <ref type="bibr" target="#b50">[50]</ref> and Kang et al. <ref type="bibr" target="#b47">[47]</ref>, on ImageNet-LT, Places-LT, and iNaturalist 2018, we report results on three splits of the set of classes: Many-shot (more than 100 images), Medium-shot (20 ? 100 images) and Few-shot (less than 20 images). For CIFAR-10-LT and CIFAR-100-LT, we sort all the classes by the number of images in decreasing order. Specifically, we divide CIFAR-10-LT into 3 splits as Many-shot (class 0th ? class 2th), Medium-shot (class 3th ? class 5th) and Few-shot (class 6th ? class 9th). For CIFAR-100-LT, we divided it into 3 splits as Many-shot (class 0th ? class 34th), Medium-shot (class 35th ? class 69th) and Few-shot (class 70th ? class 99th).</p><p>Training Details For long-tailed CIFAR datasets, we follow <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b48">[48]</ref> to pre-process images. We randomly crop a 32x32 patch from the original image or its horizontal flip with 4 pixels padded on each side and normalize the pixel values into [0,1]. To be consistent with previous setting <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b46">[46]</ref>, Top-1 accuracy (%) of ResNet-32 on long-tailed CIFAR-10 and CIFAR-100 (best results are marked in bold font). ? denotes results directly copied from <ref type="bibr" target="#b48">[48]</ref>. " ?" denotes the model is trained with strong data augmentation (mixup and autoaugment). Knowledge distillation is not applied to all methods for fair comparison. we adopt ResNet-32 <ref type="bibr" target="#b3">[4]</ref> as our backbone network for all experiments. SGD optimizer with momentum 0.9 is adopted. We train all models for 200 epochs for our ResLT method. The initial learning rate is set to 0.1 and the first five epochs are trained with the linear warm-up. The learning rate is decayed at the 160 and 180 epochs by 0.1 respectively. The batch size 128 is used through all the experiments. For a fair comparison, we adopt the same experimental setting as <ref type="bibr" target="#b47">[47]</ref> on ImageNet-LT, Places-LT and iNaturalist 2018. For Places-LT, following previous setting <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b50">[50]</ref>, we choose ResNet-152 as the backbone network, pre-train it on the full ImageNet-2012 dataset (provided by torchvision), and finely tune it for 30 epochs on Places-LT. On ImageNet-LT, we report results with ResNet-10, ResNeXt-50 and ResNeXt-101. For consistency with previous setting, ResNet-50 and ResNet-152 are used for iNaturalist 2018 and we train 200 epochs following <ref type="bibr" target="#b47">[47]</ref>. For all experiments, we use SGD optimizer with momentum 0.9 on 4 NVIDIA 2080Ti GPUs. For ResNet-10, ResNet-50, and ResNeXt-50, we adopt cosine learning rate schedule gradually decaying from 0.1 to 0. We use image resolution 224 ? 224 and batch size 256. For ResNet-152 and ResNeXt-101, we adopt cosine learning rate schedule gradually decaying from 0.05 to 0, image resolution 224?224 and batch size 128 for the limited GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Grouped convolution or 3 separate convolution</head><p>We conduct ablation studies with an extra grouped convolution or 3 separate convolutions inserted in the top of the ResNet-32 on CIFAR-10-LT and CIFAR-100-LT. As shown in <ref type="figure" target="#fig_11">Fig. 7</ref>, our method significantly surpasses these two baselines, which indicates our high performance does not come from the small number of extra parameters.</p><p>Moreover, it is worthy to note that BBN <ref type="bibr" target="#b41">[41]</ref> also has extra parameters. Our method significantly outperforms BBN as shown in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">How does the model size affect our method?</head><p>To go deeper in understanding the effects of model size on long-tailed recognition, we explore various models with dif-ferent parameters on CIFAR100-LT, CIFAR10-LT with imbalance factor 0.02, and ImageNet-LT datasets. For CIFAR100-LT, CIFAR10-LT, we evaluate ResNet-32(1x), ResNet-32(2x) and ResNet-32(3x). Specifically 1x, 2x, 3x mean the multiplier of channels in each layer. For ImageNet-LT, ResNet-10, ResNeXt-50, ResNeXt-101 are used. As shown in <ref type="figure">Fig. 4</ref>, with the increase of model parameters, our method can consistently surpass BBN on CIFAR-LT and LWS on ImageNet-LT, which indicates the effectiveness of the proposed ResLT method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Individual branch performance</head><p>Our parameter specialization mechanism reserves individual capacity for the head, medium, and tail classes. Along with the proposed residual fusion module, we make it possible to rebalance directly in the aspect of parameter space. To deeply understand the role of residual branches, we collect performance on Many-shot, Medium-shot, and Few-shot of each branch with experiments on CIFAR-100-LT, CIFAR-10-LT with an imbalance ratio of 0.01, and ImageNet-LT.</p><p>As shown in Figs. 6, head classes, medium classes and tail classes respectively dominate branch N h+m+t , N m+t , and N t , realizing the parameter specialization mechanism. Meanwhile, with the two residual branches N m+t and N t , the final performance on medium classes and tail classes is significantly enhanced compared with the single main branch N h+m+t , testifying the advantages of residual learning mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Accuracy on Many-shot, Medium-shot, and Few-shot</head><p>For ImageNet-LT, the detailed numbers of performance on Many-shot, Medium-shot and Few-shot are summarized in <ref type="table" target="#tab_7">Table 6</ref> while for iNaturalist 2018, the detailed numbers are put in <ref type="table" target="#tab_9">Table 8</ref>. To highlight the advantages of ResLT, we plot the comparisons on ImageNet-LT with previous methods in <ref type="figure" target="#fig_16">Fig 8.</ref> We observe that ResLT usually can achieve higher performance on Medium-shot and Few-shot classes than previous methods, e.g, LWS, cRT. And the overall improvements just come from small performance degradation on Many-shot classes and such strong performance on Medium-shot and Few-shot classes, which demonstrates      ResLT does a better trade-off among Many-shot, Mediumshot, and Few-shot classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">How does the hyper-parameter ? affect our method?</head><p>We conduct ablation study on choosing a proper ? on CIFAR-10-LT, CIFAR-100-LT with imbalance factor 0.02, and ImageNet-LT datasets. For CIFAR-10-LT and CIFAR-100-LT, ? = 0.995 is adopted, while we use 0.99 and 0.90 for ImageNet-LT and iNaturalist 2018 respectively in our experiments. As shown in <ref type="figure">Fig. 5</ref>, even when ? = 1 that only with L branch loss in training, the performance is even much better than baselines in <ref type="table" target="#tab_0">Table 1</ref> mentioned in Sec. 3.1, demonstrating the importance of the residual mechanismnested class assignments for different branches.      Here we have further explored the effects of the different number of groups on CIFAR-10-LT, CIFAR-100-LT with an imbalance factor 0.01. As shown in <ref type="table" target="#tab_10">Table 9</ref>, when the number of groups is larger than 3, there are no obvious improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.7">Feature visualizations</head><p>To go deeper into understanding the effects of ResLT method, we visualize representation learned by vanilla cross-entropy and ResLT training method on CIFAR-10-LT and CIFAR-100-LT with an imbalance ratio of 0.01. Specifically, for CIFAR-100-LT, we visualize the randomly selected 10 classes or 20 classes. As shown in <ref type="figure" target="#fig_17">Fig 9,</ref> The models trained with ResLT usually can produce more compact features, leading to more obvious separation and better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.8">More discussion on ResLT</head><p>Zhang et al. <ref type="bibr" target="#b63">[63]</ref> identified the trade-off between model robustness and accuracy. For long-tailed classification, Cao et al. <ref type="bibr" target="#b54">[54]</ref> derived a margin-based generalization bound for the optimal trade-off between a head class and a tail class   We leave more theoretical analysis as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with Previous Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison methods</head><p>In experiments, we compared with two kinds of methods: one-stage methods (with a consistent training strategy throughout the training) and state-of-theart two-stage methods (input sampling strategy or loss function may differ in different stages).</p><p>? One-stage methods. For one-stage methods, we mainly compare with mixup <ref type="bibr" target="#b55">[55]</ref>, LDAM <ref type="bibr" target="#b46">[46]</ref>, BBN <ref type="bibr" target="#b48">[48]</ref> and Causal Norm <ref type="bibr" target="#b49">[49]</ref> methods.</p><p>? Two-stage methods. For two-stage methods, we mainly compare with the recently proposed decoupling representation and classifier learning <ref type="bibr" target="#b47">[47]</ref> and RIDE <ref type="bibr" target="#b52">[52]</ref>. For fair comparison with RIDE, we report the experimental results without knowledge distillation trick. <ref type="table" target="#tab_2">Table 3</ref> shows experimental results on ImageNet-LT. Here, we mainly compare with the recent SOTA method <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b52">[52]</ref>. We observe that 90 epochs training is not enough to converge for deep models on ImageNet-LT. Under longer training with 180 epochs, Decouple method <ref type="bibr" target="#b47">[47]</ref> can be significantly further improved while it seems to be helpless for Causal Norm <ref type="bibr" target="#b49">[49]</ref>. We use this strong setting for comparisons. As shown in <ref type="table" target="#tab_2">Table 3</ref>, ResNext-50 trained with our ResLT method in an end-to-end fashion enjoys 1.4% higher than [47] and 1.0% higher than <ref type="bibr" target="#b49">[49]</ref>. Moreover, as shown in <ref type="figure">Fig. 4</ref>, we also test our method on ResNet-10 and ResNext-101. Our method consistently surpasses LWS <ref type="bibr" target="#b47">[47]</ref> by 0.8% ? 1.4% across small models to large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on ImageNet-LT</head><p>For fairly comparing with RIDE, we replace ResNeXt-50 with RIDEResNeXt-50 adopted by RIDE. As shown in <ref type="table" target="#tab_8">Table 7</ref>, ResLT model achieves 57.6% top-1 accuracy with 3 experts in 180 epochs training, largely outperforming RIDE by 1.6%. With RIDEResNeXt-50 and 3 experts, our model only needs an extra 0.04G FLOPs computational cost at inference time. We also calculate the inference time with an image size 224 ? 224 in <ref type="table" target="#tab_8">Table 7</ref>. Our model costs 17.2ms when fed with a batch of 64 images while RIDE uses a comparable time of 17.0 ms. <ref type="table">Table 4</ref> lists experimental results on Places-LT. Following previous setting <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b50">[50]</ref>, we use ResNet-152 pre-trained on full ImageNet dataset as our backbone (provided by torchvision). As shown in <ref type="table">Table 4</ref>, the model trained with our ResLT in an end-to-end manner achieves 39.8% Top-1 accuracy, which is 1.9% higher than previous Decoupling methods <ref type="bibr" target="#b47">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on Places-LT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on iNaturalist 2018</head><p>Experimental results on iNaturalist 2018 are summarized in <ref type="table" target="#tab_8">Table 7</ref>. We mainly compare our method with BBN and Kang et al. <ref type="bibr" target="#b47">[47]</ref> with 200 training epochs. Note that, though Zhou et al. <ref type="bibr" target="#b48">[48]</ref> claimed BBN(2x) trained with 180 epochs, they actually used 360 epochs data due to their dual sampler strategy. Under the same amount of data for training, our method outperforms <ref type="bibr" target="#b48">[48]</ref> by 0.9%. Compared with <ref type="bibr" target="#b47">[47]</ref>, our ResLT model trained in an end-to-end manner achieves 70.2% top-1 accuracy, surpassing it by 0.7%. Further, with larger network ResNet-152, our method achieves 73.2% top-1 accuracy, surpassing <ref type="bibr" target="#b47">[47]</ref> by 0.7% under same training setting. When compared with RIDE, ResLT model achieves 72.9% top-1 accuracy with 3 experts in 200 epochs training schedule, significantly surpassing RIDE by 1.2% as shown in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>Comparison on CIFAR-10-LT and CIFAR-100-LT We conduct extensive experiments on CIFAR-10 and CIFAR-100 with imbalance factors of 0.01, 0.02 and 0.1, same as previous setting <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b48">[48]</ref>. The experimental results are summarized in <ref type="table" target="#tab_5">Table 5</ref>. Our method outperforms BBN by 2.78%, 2.96% and 1.76% on CIFAR-100-LT with imbalance factors 0.01, 0.02, and 0.1 respectively. Compared with Causal Norm by Tang et al., ResLT surpasses it by 1.24% and 1.19% separately with imbalance ratio 0.01 and 0.1 on CIFAR-100. With RIDEResNet, ResLT model boosts performance to 49.73% on CIFAR-100 under a imbalance ratio 0.01, surpassing RIDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we provide a novel perspective to understand and address the long-tailed recognition problem. The proposed residual fusion mechanism effectively re-balances head and tail classes under this new point of view in parameter space. Extensive experimental results on various representative and challenging benchmarks manifest the effectiveness of our method.</p><p>To understand our method thoroughly, we conduct sound ablation studies to show that parameter specialization and residual learning mechanism are two key components to make it work. And we leave more theoretical analysis for ResLT as our future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Ablation study for the residual fusion and parameter specialization mechanisms. (a), (b), (c), (d), and (e) are variants of ResLT. (a) is a weak version of ResLT. For (b) and (c), the three branches have no residual relationship -nested class assignments, unlike ResLT. For (d), there is no additive shortcut, different from ResLT. (e) is without parameter specialization mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Possible variant (a), (b), (c): as shown in Fig. 2(a), (b), (c), we explore the necessary of the residual mechanism -the nested classes assignment for different branches in ResLT. N h , N m , N t and N h+m+t are optimized to classify images of head, medium, tail and all classes respectively. ? Possible variant (d): as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-LT IMB 0.02 Without With (a) Importance of parameter specialization on CIFAR-10-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-LT IMB 0.02 Without With (b) Importance of parameter specialization on CIFAR-100-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Residual fusion module analysis on CIFAR-10-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Residual fusion module analysis on CIFAR-100-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Individual branch performance on ImageNet-LT with ResNet-10. Individual branch performance on ImageNet-LT with ResNeXt-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Individual branch performance analysis. "All" denotes results are obtained with the aggregated final outputs. Main branch is the N h+m+t . "Residual branch I" represents the residual branch N m+t while "Residual branch II" denotes the residual branch Nt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>convolution (a) Ablation study for 1-Grouped convolution and 3-Separate convolutions on CIFAR-10-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>convolution (b) Ablation study for 1-Grouped convolution and 3-Separate convolutions on CIFAR-100-LT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Ablation study for 1-Grouped convolution and 3-Separate convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Many-shot performance on ImageNet-LT with various ResNet backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Medium-shot performance on ImageNet-LT with various ResNet backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Few-shot performance on ImageNet-LT with various ResNet backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Over-all performance on ImageNet-LT with various ResNet backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 8 .</head><label>8</label><figDesc>Many-shot, Medium-shot, and Few-shot performance analysis on ImageNet-LT with various ResNet backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization comparison between ResLT trained model and cross-entropy trained model. Numbers in all the figures are class indexes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Zhisheng</head><label></label><figDesc>Zhong received the B.Eng. Degree in Communication Engineering from Beijing University of Posts and Telecommunications (BUPT) in 2016. He received the Master Degree in Computer Science from Peking University (PKU) in 2019. Now he is a Ph.D. student at the Department of Computer Science Engineering (CSE), the Chinese University of Hong Kong (CUHK). He serves as a reviewer for NeurIPS, CVPR, ICCV, ICLR and etc. His research interests lie in deep learning and computer vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Top</head><label>1</label><figDesc></figDesc><table><row><cell cols="5">-1 accuracy (%) of baselines on CIFAR-10-LT with the imbalance</cell></row><row><cell></cell><cell cols="3">factor 0.02. ResNet-32 is adopted.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Many-shot Medium-shot Few-shot</cell><cell>All</cell></row><row><cell>Baseline (CE)</cell><cell>93.60</cell><cell>76.33</cell><cell>68.55</cell><cell>78.50</cell></row><row><cell>Baseline (1)</cell><cell>90.70</cell><cell>54.73</cell><cell>41.48</cell><cell>60.22</cell></row><row><cell>Baseline (2)</cell><cell>90.83</cell><cell>55.63</cell><cell>40.75</cell><cell>60.24</cell></row><row><cell>Baseline (3)</cell><cell>86.73</cell><cell>45.50</cell><cell>55.33</cell><cell>61.80</cell></row><row><cell>ResLT (Ours)</cell><cell>85.83</cell><cell>81.47</cell><cell>81.43</cell><cell>83.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Top-1 accuracy (%) on ImageNet-LT for ResNeXt-50. ? denotes results directly copied from<ref type="bibr" target="#b50">[50]</ref>. " ?" denotes the model is trained with strong data augmentation (mixup and autoaugment). Knowledge distillation is not applied to all methods for fair comparison.</figDesc><table><row><cell>Method</cell><cell>One-stage</cell><cell>Many</cell><cell>Medium</cell><cell>Few</cell><cell>All</cell></row><row><cell>Baseline(CE)</cell><cell></cell><cell>67.6</cell><cell>42.8</cell><cell>13.4</cell><cell>47.6</cell></row><row><cell>Mixup [55]</cell><cell></cell><cell>67.3</cell><cell>41.0</cell><cell>11.1</cell><cell>46.3</cell></row><row><cell>Lifted Loss [56]  ?</cell><cell></cell><cell>41.1</cell><cell>35.4</cell><cell>24.0</cell><cell>35.2</cell></row><row><cell>Focal Loss [44]  ?</cell><cell></cell><cell>41.1</cell><cell>34.8</cell><cell>22.4</cell><cell>34.6</cell></row><row><cell>Range Loss [57]  ?</cell><cell></cell><cell>41.1</cell><cell>35.4</cell><cell>23.2</cell><cell>35.1</cell></row><row><cell>? -normalized [47]</cell><cell></cell><cell>60.9</cell><cell>49.2</cell><cell>36.8</cell><cell>51.5</cell></row><row><cell>Causal Norm [49]</cell><cell></cell><cell>64.9</cell><cell>48.0</cell><cell>28.9</cell><cell>51.9</cell></row><row><cell>cRT [47]</cell><cell></cell><cell>63.7</cell><cell>47.6</cell><cell>28.3</cell><cell>50.7</cell></row><row><cell>LWS [47]</cell><cell></cell><cell>63.3</cell><cell>48.4</cell><cell>32.0</cell><cell>51.5</cell></row><row><cell>ResLT</cell><cell></cell><cell>63.0</cell><cell>50.5</cell><cell>35.5</cell><cell>52.9</cell></row><row><cell>ResLT  ?</cell><cell></cell><cell>63.6</cell><cell>55.7</cell><cell>38.9</cell><cell>56.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Top-1 accuracy (%) of Many-shot, Medium-shot and Few-shot on ImageNet-LT with various ResNet backbones. ? denotes results directly copied from their original paper.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell cols="2">Backbone Model</cell><cell>One-stage</cell><cell>Many-shot</cell><cell>Medium-shot</cell><cell>Few-shot</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell>CE (baseline)</cell><cell></cell><cell cols="2">ResNet-10</cell><cell></cell><cell>59.7</cell><cell>29.4</cell><cell>5.7</cell><cell>37.3</cell></row><row><cell></cell><cell></cell><cell>SEQL ?</cell><cell></cell><cell cols="2">ResNet-10</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.4</cell></row><row><cell></cell><cell></cell><cell>cRT</cell><cell></cell><cell cols="2">ResNet-10</cell><cell></cell><cell>53.8</cell><cell>41.3</cell><cell>25.4</cell><cell>43.2</cell></row><row><cell></cell><cell></cell><cell>? -normalize</cell><cell></cell><cell cols="2">ResNet-10</cell><cell></cell><cell>50.4</cell><cell>42.1</cell><cell>26.7</cell><cell>42.7</cell></row><row><cell></cell><cell></cell><cell>LWS</cell><cell></cell><cell cols="2">ResNet-10</cell><cell></cell><cell>51.8</cell><cell>41.6</cell><cell>27.6</cell><cell>43.0</cell></row><row><cell></cell><cell></cell><cell>ResLT</cell><cell></cell><cell cols="2">ResNet-10</cell><cell></cell><cell>52.3</cell><cell>41.6</cell><cell>29.5</cell><cell>43.8</cell></row><row><cell></cell><cell></cell><cell>CE (baseline)</cell><cell></cell><cell cols="2">ResNeXt-50</cell><cell></cell><cell>66.8</cell><cell>42.0</cell><cell>14.0</cell><cell>47.0</cell></row><row><cell></cell><cell></cell><cell>cRT</cell><cell></cell><cell cols="2">ResNeXt-50</cell><cell></cell><cell>63.7</cell><cell>47.6</cell><cell>28.3</cell><cell>50.6</cell></row><row><cell></cell><cell></cell><cell>? -normalize</cell><cell></cell><cell cols="2">ResNeXt-50</cell><cell></cell><cell>62.3</cell><cell>48.9</cell><cell>33.7</cell><cell>51.5</cell></row><row><cell></cell><cell></cell><cell>LWS</cell><cell></cell><cell cols="2">ResNeXt-50</cell><cell></cell><cell>63.3</cell><cell>48.4</cell><cell>32.0</cell><cell>51.5</cell></row><row><cell></cell><cell></cell><cell>ResLT</cell><cell></cell><cell cols="2">ResNeXt-50</cell><cell></cell><cell>63.0</cell><cell>50.5</cell><cell>35.5</cell><cell>53.0</cell></row><row><cell></cell><cell></cell><cell>CE (baseline)</cell><cell></cell><cell cols="3">ResNeXt-101-32x4d</cell><cell>69.6</cell><cell>44.6</cell><cell>15.6</cell><cell>49.6</cell></row><row><cell></cell><cell></cell><cell>cRT</cell><cell></cell><cell cols="3">ResNeXt-101-32x4d</cell><cell>66.2</cell><cell>50.4</cell><cell>30.8</cell><cell>53.3</cell></row><row><cell></cell><cell></cell><cell>? -normalize</cell><cell></cell><cell cols="3">ResNeXt-101-32x4d</cell><cell>65.3</cell><cell>51.5</cell><cell>35.2</cell><cell>53.9</cell></row><row><cell></cell><cell></cell><cell>LWS</cell><cell></cell><cell cols="3">ResNeXt-101-32x4d</cell><cell>65.7</cell><cell>51.4</cell><cell>34.7</cell><cell>54.0</cell></row><row><cell></cell><cell></cell><cell>ResLT</cell><cell></cell><cell cols="3">ResNeXt-101-32x4d</cell><cell>63.3</cell><cell>53.3</cell><cell>40.3</cell><cell>55.1</cell></row><row><cell></cell><cell></cell><cell cols="3">CIFAR100-LT IMB 0.01</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Accuracy (%)</cell><cell>20% 40% 60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>All</cell><cell cols="2">Main branch</cell><cell cols="2">Residual branch I</cell><cell>Residual branch II</cell></row><row><cell></cell><cell></cell><cell>Many-shot</cell><cell cols="2">Medium-shot</cell><cell>Few-shot</cell><cell></cell></row><row><cell cols="7">(a) Individual branch performance on CIFAR100-LT with imbalance</cell></row><row><cell cols="2">ratio 0.01.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Top-1 accuracy with 180/200 training epochs and inference speed on ImageNet-LT and iNaturalist 2018. Inference time is calculated with a batch of 64 images on Nvidia GeForce 2080Ti GPU, CUDA11.6, Pytorch1.5, Python3.6.</figDesc><table><row><cell>Method</cell><cell>Epochs</cell><cell>Speed (ms)</cell><cell>Many-shot</cell><cell>Medium-shot</cell><cell>Few-shot</cell><cell>All</cell></row><row><cell></cell><cell cols="4">With RIDEResNeXt-50 on ImageNet-LT</cell><cell></cell><cell></cell></row><row><cell>RIDE (2 experts) [52]</cell><cell>100</cell><cell>13.8</cell><cell>67.3</cell><cell>50.7</cell><cell>27.2</cell><cell>53.9</cell></row><row><cell>RIDE (3 experts) [52]</cell><cell>100</cell><cell>17.0</cell><cell>68.6</cell><cell>52.2</cell><cell>27.8</cell><cell>55.1</cell></row><row><cell>RIDE (2 experts) [52]</cell><cell>180</cell><cell>13.8</cell><cell>66.9</cell><cell>51.2</cell><cell>31.2</cell><cell>54.5</cell></row><row><cell>RIDE (3 experts) [52]</cell><cell>180</cell><cell>17.0</cell><cell>68.4</cell><cell>52.9</cell><cell>32.2</cell><cell>56.0</cell></row><row><cell>ResLT (2 experts)</cell><cell>180</cell><cell>13.9</cell><cell>63.3</cell><cell>55.4</cell><cell>44.3</cell><cell>56.4</cell></row><row><cell>ResLT (3 experts)</cell><cell>180</cell><cell>17.2</cell><cell>64.0</cell><cell>56.6</cell><cell>44.8</cell><cell>57.6</cell></row><row><cell></cell><cell cols="4">With RIDEResNet-50 on iNaturalist 2018</cell><cell></cell><cell></cell></row><row><cell>RIDE (2 experts) [52]</cell><cell>100</cell><cell>12.4</cell><cell>56.5</cell><cell>69.9</cell><cell>70.9</cell><cell>69.0</cell></row><row><cell>RIDE (3 experts) [52]</cell><cell>100</cell><cell>15.9</cell><cell>62.0</cell><cell>71.6</cell><cell>72.0</cell><cell>70.8</cell></row><row><cell>RIDE (2 experts) [52]</cell><cell>200</cell><cell>12.4</cell><cell>62.5</cell><cell>71.0</cell><cell>70.9</cell><cell>70.1</cell></row><row><cell>RIDE (3 experts) [52]</cell><cell>200</cell><cell>15.9</cell><cell>68.3</cell><cell>72.6</cell><cell>71.8</cell><cell>71.7</cell></row><row><cell>ResLT (2 experts)</cell><cell>200</cell><cell>12.6</cell><cell>71.5</cell><cell>72.0</cell><cell>72.4</cell><cell>72.1</cell></row><row><cell>ResLT (3 experts)</cell><cell>200</cell><cell>16.2</cell><cell>73.0</cell><cell>72.6</cell><cell>73.1</cell><cell>72.9</cell></row><row><cell cols="3">4.2.6 How does the number of groups affect our method?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Top-1 accuracy (%) of Many-shot, Medium-shot and Few-shot on iNaturalist 2018 with ResNet-50 backbone.</figDesc><table><row><cell>Method</cell><cell>Backbone Model</cell><cell>One-stage</cell><cell>Many-shot</cell><cell>Medium-shot</cell><cell>Few-shot</cell><cell>All</cell></row><row><cell>CE (baseline)</cell><cell>ResNet-50</cell><cell></cell><cell>75.7</cell><cell>66.9</cell><cell>61.7</cell><cell>65.8</cell></row><row><cell>cRT</cell><cell>ResNet-50</cell><cell></cell><cell>73.2</cell><cell>68.8</cell><cell>66.1</cell><cell>68.2</cell></row><row><cell>? -normalize</cell><cell>ResNet-50</cell><cell></cell><cell>71.1</cell><cell>68.9</cell><cell>69.3</cell><cell>69.3</cell></row><row><cell>LWS</cell><cell>ResNet-50</cell><cell></cell><cell>71.0</cell><cell>69.8</cell><cell>68.8</cell><cell>69.5</cell></row><row><cell>ResLT</cell><cell>ResNet-50</cell><cell></cell><cell>68.5</cell><cell>69.9</cell><cell>70.4</cell><cell>70.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Ablation study for different number of groups. "#" represents the number of groups. Top-1 accuracy (%) are reported. We use ResNet-32 as backbone in experiments.</figDesc><table><row><cell>Dataset</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell></row><row><cell>CIFAR-10-LT</cell><cell>78.83</cell><cell>80.44</cell><cell>80.80</cell><cell>80.44</cell></row><row><cell>CIFAR-100-LT</cell><cell>45.11</cell><cell>45.34</cell><cell>45.34</cell><cell>45.14</cell></row><row><cell cols="5">margins in the binary classification problem, which also</cell></row><row><cell cols="5">implied that there was a trade-off between head classes</cell></row><row><cell cols="5">accuracy and tail classes accuracy. As discussed in Sec. 4.2.4,</cell></row><row><cell cols="5">we observe ResLT can learn a better trade-off between head</cell></row><row><cell cols="5">classes accuracy and tail classes accuracy with the proposed</cell></row><row><cell cols="2">residual learning mechanism.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(e) CIFAR-100-LT random selected 10 classes with cross-entropy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and practical neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge,&quot; IJCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for ccene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards good practices for recognition &amp; detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Metaweight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for longtailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bbn: Bilateralbranch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02413</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Largescale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Long-tailed recognition using class-balanced experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">ELF: an early-exiting framework for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhamnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11979</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Piecewise classifier mappings: Learning fine-grained learners for novel categories with few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">He is currently a Ph.D. Candidate at the Chinese University of Hong Kong (CUHK), under the supervision of Prof. Jiaya Jia</title>
	</analytic>
	<monogr>
		<title level="m">He serves as a reviewer for IJCV, CVPR, ICCV, ECCV, ICLR, NeurIPS. His research interests include model generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Jiequan Cui received the B.Eng. degree (Honors) in Computer Science from the School of Computer Science and Technology, ShanDong University</orgName>
		</respStmt>
	</monogr>
	<note>neural architecture search, adversarial robustness, imbalanced learning, and image segmentation</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">He received the BS degree from Huazhong University of Science and Technology and the PhD degree from the Chinese University of Hong Kong. He was the winner of 2017 COCO Instance Segmentation Competition and received the Outstanding Reviewer of ICCV in 2019. He continuously served as a reviewer for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cvpr</forename><surname>Tpami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neurips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shu Liu now serves as Co-Founder and Technical Head in SmartMore</title>
		<imprint/>
	</monogr>
	<note>His research interests lie in deep learning and computer vision</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">He is currently a 3rd year Ph.D. student at the Chinese University of Hong Kong (CUHK), under the supervision of Prof. Jiaya Jia. He serves as a reviewer for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cvpr</forename><surname>Ijcv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eccv</forename><surname>Iccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaai</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Zhuotao Tian received the B.Eng. degree (Honors) in Computer Science from the School of Computer Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include few-shot learning, semi-supervised learning. semantic segmentation and scene text detection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
