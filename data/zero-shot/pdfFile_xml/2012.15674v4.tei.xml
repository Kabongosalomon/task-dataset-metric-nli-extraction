<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
							<email>ouyangxuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
							<email>wangshuohuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Pang</surname></persName>
							<email>pangchao04@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<email>tianhao@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wu_hua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have demonstrated that pretrained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for lowresource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent studies have demonstrated that the pretraining of cross-lingual language models can significantly improve their performance in crosslingual natural language processing tasks <ref type="bibr" target="#b4">(Devlin et al., 2018;</ref><ref type="bibr" target="#b14">Lample and Conneau, 2019;</ref>. Existing pretraining methods include multilingual masked language modeling (MMLM; <ref type="bibr" target="#b4">Devlin et al. 2018</ref>) and translation language modeling (TLM; <ref type="bibr" target="#b14">Lample and Conneau 2019)</ref>, of which the key point is to learn a shared language-invariant feature space among multiple languages. MMLM implicitly models the semantic representation of each language in a unified feature space by learning them separately. TLM is an extension of MMLM that is trained with a parallel corpus and captures semantic alignment by learning a pair of parallel sentences simultaneously. This study shows that the use of parallel corpora can significantly improve the performance in downstream cross-lingual understanding and generation tasks. However, the sizes of parallel corpora are limited <ref type="bibr" target="#b26">(Tran et al., 2020)</ref>, restricting the performance of the cross-lingual language model.</p><p>To overcome the constraint of the parallel corpus size on the model performance, we propose ERNIE-M, a novel cross-lingual pre-training method to learn semantic alignment among multiple languages on monolingual corpora. Specifically, we propose cross-attention masked language modeling (CAMLM) to improve the cross-lingual transferability of the model on parallel corpora, and it trains the model to predict the tokens of one language by using another language. Then, we utilize the transferability learned from parallel corpora to enhance multilingual representation. We propose back-translation masked language modeling (BTMLM) to train the model, and this helps the model to learn sentence alignment from monolingual corpora. In BTMLM, a part of the tokens in the input monolingual sentences is predicted into the tokens of another language. We then concatenate the predicted tokens and the input sentences as pseudo-parallel sentences to train the model. In this way, the model can learn sentence alignment with only monolingual corpora and overcome the constraint of the parallel corpus size while improving the model performance.</p><p>ERNIE-M is implemented on the basis of XLM-R , and we evaluate its performance on five widely used cross-lingual benchmarks: XNLI <ref type="bibr" target="#b3">(Conneau et al., 2018)</ref> for crosslingual natural language inference, MLQA <ref type="bibr" target="#b16">(Lewis et al., 2019)</ref> for cross-lingual question answering, <ref type="bibr">CoNLL (Sang and De Meulder, 2003)</ref> for named entity recognition, cross-lingual paraphrase adversaries from word scrambling (PAWS-X)  for cross-lingual paraphrase identification, and Tatoeba  for cross-lingual retrieval. The experimental results demonstrate that ERNIE-M outperforms existing cross-lingual models and achieves new state-of-the-art (SoTA) results.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multilingual Language Models</head><p>Existing multilingual language models can be classified into two main categories: (1) discriminative models; (2) generative models.</p><p>In the first category, a multilingual bidirectional encoder representation from transformers (mBERT; <ref type="bibr" target="#b4">Devlin et al. 2018</ref>) is pre-trained using MMLM on a monolingual corpus, which learns a shared language-invariant feature space among multiple languages. The evaluation results show that the mBERT achieves significant performance in downstream tasks <ref type="bibr" target="#b32">(Wu and Dredze, 2019)</ref>. XLM (Lample and  is extended on the basis of mBERT using TLM, which enables the model to learn cross-lingual token alignment from parallel corpora. XLM-R  demonstrates the effects of models when trained on a large-scale corpus. It used 2.5T data extracted from Common Crawl  that involves 100 languages for MMLM training. The results show that a large-scale training corpus can significantly improve the performance of the cross-lingual model. Unicoder <ref type="bibr" target="#b10">(Huang et al., 2019)</ref> achieves gains on downstream tasks by employing a multitask learning framework to learn cross-lingual semantic representations with monolingual and parallel corpora. ALM <ref type="bibr" target="#b34">(Yang et al., 2020)</ref> improves the model's transferability by enabling the model to learn cross-lingual code-switch sentences. IN-FOXLM <ref type="bibr" target="#b1">(Chi et al., 2020b</ref>) adds a contrastive learning task for cross-lingual model training. HICTL  learns cross-lingual semantic representation from multiple facets (at word-levels and sentence-levels) to improve the performance of cross-lingual models. VECO  presents a variable encoder-decoder framework to unify the understanding and generation tasks and achieves significant improvement in both downstream tasks.</p><p>The second category includes MASS <ref type="bibr" target="#b24">(Song et al., 2019)</ref>, mBART , XNLG <ref type="bibr" target="#b0">(Chi et al., 2020a)</ref> and mT5 <ref type="bibr">(Xue et al., 2020)</ref>. MASS <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> proposed a training objective for restore the input sentences in which successive token fragments are masked which improved the model's performance on machine translation. Similar to MASS, mBART pre-trains a denoised sequence-to-sequence model and uses an autoregressive task to train the model. XNLG focuses on multilingual question generation and abstractive summarization and updates the parameters of the encoder and decoder through auto-encoding and autoregressive tasks. mT5 uses the same model structure and pre-training method as T5 <ref type="bibr" target="#b19">(Raffel et al., 2019)</ref>, and extends the parameters of the cross-lingual model to 13B, significantly improving the performance of the cross-language downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Back Translation and Non-Autoregressive Neural Machine Translation</head><p>Back translation (BT) is an effective neuralnetwork-based machine translation method proposed by <ref type="bibr" target="#b23">Sennrich et al. (2015)</ref>. It can significantly improve the performance of both supervised and unsupervised machine translation via augment the parallel training corpus <ref type="bibr" target="#b15">(Lample et al., 2017;</ref><ref type="bibr" target="#b5">Edunov et al., 2018)</ref>. BT has been found to particularly useful when the parallel corpus is sparse <ref type="bibr" target="#b11">(Karakanta et al., 2018)</ref>. Predicting the token of the target language in one batch can also improve the speed of non-auto regressive machine translation (NAT; <ref type="bibr" target="#b7">Gu et al. 2017;</ref><ref type="bibr" target="#b28">Wang et al. 2019a</ref>). Our work is inspired by NAT and BT. We generate the tokens of another language in batches and then use these in pre-training to help sentence alignment learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first introduce the general workflow of ERNIE-M and then present the details of the model training.</p><p>Cross-lingual Semantic Alignment. The key idea of ERNIE-M is to utilize the transferability learned from parallel corpora to enhance the model's learning of large-scale monolingual corpora, and thus enhance the multilingual semantic representation. Based on this idea, we propose two pre-training objectives, cross-attention masked language modeling (CAMLM) and back-translation  masked language modeling (BTMLM). CAMLM is to align the cross-lingual semantic representation on parallel corpora. Then, the transferability learned from parallel corpora is utilized to enhance the multilingual representation. Specifically, we train the ERNIE-M by using BTMLM, enabling the model to align the semantics of multiple languages from monolingual corpora and improve the multilingual representation of the model. The MMLM and TLM are used by default because of the strong performance shown in Lample and Conneau 2019. We combine MMLM, TLM with CAMLM, BTMLM to train ERNIE-M. In the following sections, we will introduce the details of each objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-attention Masked Language Modeling.</head><p>To learn the alignment of cross-lingual semantic representations in parallel corpora, we propose a new pre-training objective, CAMLM. We denote a parallel sentence pair as &lt;source sentence, target sentence&gt;. In CAMLM, we learn the multilingual semantic representation by restoring the MASK token in the input sentences. When the model restores the MASK token in the source sentence, the model can only rely on the semantics of the target sentence, which means that the model has to learn how to represent the source language with the semantics of the target sentence and thus align the semantics of multiple languages. <ref type="figure" target="#fig_0">Figure 1</ref> (b) and (c) show the differences between TLM <ref type="bibr" target="#b14">(Lample and Conneau, 2019)</ref> and CAMLM. TLM learns the semantic alignment between languages with both the source and target sentences while CAMLM only relies on one side of the sentence to restore the MASK token. The advantage of CAMLM is that it avoids the information leakage that the model can attend to a pair of input sentences at the same time, which makes learning of BTMLM possible. The selfattention matrix of the example in <ref type="figure" target="#fig_0">Figure 1</ref> is shown in <ref type="figure">Figure 2</ref>. For TLM, the prediction of the MASK token relies on the input sentence pair. When the model learns CAMLM, the model can only predict the MASK token based on the sentence of its corresponding parallel sentence and the MASK symbol of this sentence, which provides the position and language information. Thus, the probability of the MASK token M 2 is p(x 2 |M 2 , y 4 , y 5 , y 6 , y 7 ), p(y 5 |x 1 , x 2 , x 3 , M 5 ) for M 5 , and p(y 6 |x 1 ,</p><formula xml:id="formula_0">x 2 , x 3 , M 6 ) for M 6 in CAMLM. x1 M2 x 3 y 4 y7 x1 M 2 x3 y4 M 5 M 6 y7 M5 M6 x 2 y5 y6 x2 y5 y6 x1 M 2 x3 y4 M 5 M 6 y7 x2 y5 y6 x1 M 2 x3 y4 M 5 M 6 y7 x2 y5 y6 (a) MMLM x1 M2 x 3 y 4 y7 x1 M 2 x3 y4 M 5 M 6 y7 M5 M6 x 2 y5 y6 x2 y5 y6 x1 M 2 x3 y4 M 5 M 6 y7 x2 y5 y6 x1 M 2 x3 y4 M 5 M 6 y7 x2 y5 y6 (b) TLM x1 M2 x 3 y 4 y7 x1 M 2 x3 y4 M 5 M 6 y7 M5 M6 x 2 y5 y6 x2 y5 y6 x1 M 2 x3 y4 M 5 M 6 y7 x2 y5 y6 x1 M 2 x3 y4 M 5 M 6 y7 x2 y5 y6</formula><p>(c) CAMLM <ref type="figure">Figure 2</ref>: Self-attention mask matrix in MMLM, TLM and CAMLM. We use different self-attention masks for different pre-training objectives.</p><p>Given the input in a bilingual corpus X src = {x 1 , x 2 , ? ? ? , x s }, and its corresponding MASK position, M src = {m 1 , m 2 , ? ? ? , m ms }, the target sentence is X tgt = {x s+1 , x s+2 , ? ? ? , x s+t }, and its corresponding MASK position is M tgt = {m ms+1 , m ms+2 , ? ? ? , m ms+mt }. In TLM, the model can attend to the tokens in the source and target sentences, so the probability of masked tokens</p><formula xml:id="formula_1">is m?M p(x m |X/ M ), where M = M src ? M tgt . X/ M denotes all input tokens x in X except x in M , where X = X src ? X tgt .</formula><p>x m denotes the token with position m. In CAMLM, the probability of the MASK token in the source sentence is m?Msrc p(x m |X/ M ?Xsrc ), which means that when predicting the MASK tokens in the source sentence, we only focus on the target sentence. As for the target sentence, the probability of the MASK token is m?Mtgt p(x m |X/ M ?Xtgt ), which means that the MASK tokens in the target sentence will be predicted based only on the source sentence. Therefore, the model must learn to use the corresponding sentence to predict and learn the alignment across multiple languages. The pre-training loss of CAMLM in the source/target sentence is</p><formula xml:id="formula_2">L CAM LM (src) = ? x?D B log m?Msrc p(x m |X/ M ?Xsrc ) L CAM LM (tgt) = ? x?D B log m?Mtgt p(x m |X/ M ?Xtgt )</formula><p>where D B is the bilingual training corpus. The CAMLM loss is</p><formula xml:id="formula_3">L CAM LM = L CAM LM (src) + L CAM LM (tgt)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Back-translation Masked Language Modeling.</head><p>To overcome the constraint that the parallel corpus size places on the model performance, we propose a novel pre-training objective inspired by NAT <ref type="bibr" target="#b7">(Gu et al., 2017;</ref><ref type="bibr" target="#b28">Wang et al., 2019a)</ref> and BT methods called BTMLM to align cross-lingual semantics with the monolingual corpus. We use BTMLM to train our model, which builds on the transferability learned through CAMLM, generating pseudoparallel sentences from the monolingual sentences and the generated pseudo-parallel sentences are then used as the input of the model to align the cross-lingual semantics, thus enhancing the multilingual representation. The training process for BTMLM is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The learning process for the BTMLM is divided into two stages. Stage 1 involves the generation of pseudo-parallel tokens from monolingual corpora. Specifically, we fill in several placeholder MASK at the end of the monolingual sentence to indicate the location and the language we want to generate, and let the model generate its corresponding parallel language token based on the original monolingual sentence and the corresponding position of  the pseudo-token. In this way, we generate the tokens of another language from the monolingual sentence, which will be used in learning cross-lingual semantic alignment for multiple languages. The self-attention matrix for generating pseudotokens in <ref type="figure" target="#fig_1">Figure 3</ref> is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. In the pseudo-token generating process, the model can only attend to the source sentence and the placeholder MASK tokens, which indicate the language and position we want to predict by using language embedding and position embedding. The probability of mask token M 5 is</p><formula xml:id="formula_4">p(y 5 |x 1 , x 2 , x 3 , x 4 , M 5 ), p(y 6 |x 1 , x 2 , x 3 , x 4 , M 6 ) for M 6 and p(y 7 |x 1 , x 2 , x 3 , x 4 , M 7 ) for M 7 .</formula><p>Stage 2 uses the pseudo-tokens generated in Stage 1 to learn the cross-lingual semantics alignment. The process in Stage 2 is shown in the righthand diagram of <ref type="figure" target="#fig_1">Figure 3</ref>. In the training process of Stage 2, the input of the model is the concatenation of the monolingual sentences and the generated pseudo-parallel tokens, and the learning objective is to restore the MASK tokens based on the original sentences and the generated pseudo-parallel tokens. Because the model can rely not only on the input monolingual sentence but also the generated pseudo-tokens in the process of inference MASK to-kens, the model can explicitly learn the alignment of the cross-lingual semantic representation from the monolingual sentences.</p><p>The learning process of the BTMLM can be interpreted as follows: given the input in monolingual corpora X = {x 1 , x 2 , ? ? ? , x s }, the positions of masked tokens M = {m 1 , m 2 , ? ? ? , m m } and the position of the pseudo-token to be predicted, M pseudo = {m s+1 , m s+2 , ? ? ? , m s+p }, we first generate pseudo-tokens P = {h s+1 , h s+2 , ? ? ? , h s+p }, as described earlier; we then concatenate the generated pseudo-token with input monolingual sentence as a new parallel sentence pair and use it to train our model. Thus, the probability of the masked tokens in BTMLM</p><formula xml:id="formula_5">is m?M p(x m |X/ M , P ), where X/ M denotes all input tokens x in X except x in M . The pre-training loss of BTMLM is L BT M LM = ? x?D M log m?M p(x m |X/ M , P )</formula><p>where D M is the monolingual training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We consider five cross-lingual evaluation benchmarks: XNLI for cross-lingual natural language inference, MLQA for cross-lingual question answering, CoNLL for cross-lingual named entity recognition, PAWS-X for cross-lingual paraphrase identification, and Tatoeba for cross-lingual retrieval. Next, we first describe the data and pre-training details and then compare the ERNIE-M with the existing state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Model</head><p>ERNIE-M is trained with monolingual and parallel corpora that involved 96 languages. For the monolingual corpus, we extract it from CC-100 according to ; . For the bilingual corpus, we use the same corpus as INFOXLM <ref type="bibr" target="#b1">(Chi et al., 2020b)</ref>, including MultiUN <ref type="bibr" target="#b35">(Ziemski et al., 2016)</ref>, IIT Bombay <ref type="bibr" target="#b13">(Kunchukuttan et al., 2017)</ref>, OPUS <ref type="bibr" target="#b25">(Tiedemann, 2012)</ref>, and WikiMatrix  We use a transformer-encoder <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> as the backbone of the model. For the ERNIE-M BASE model, we adopt a structure with 12 layers, 768 hidden units, 12 heads. For ERNIE-M LARGE model , we adopt a structure with 24 layers, 1024 hidden units, 16 heads. The activation function used is GeLU <ref type="bibr" target="#b8">(Hendrycks and Gimpel, 2016)</ref>. Following <ref type="bibr" target="#b1">Chi et al. 2020b and</ref>, we initialize the parameters of ERNIE-M with XLM-R. We use the Adam optimizer (Kingma and Ba, 2014) to train ERNIE-M; the learning rate is scheduled with a linear decay with 10K warm-up steps, and the peak learning rate is 2e ? 4 for the base model and 1e ? 4 for the large model. We conduct the pre-training experiments using 64 Nvidia V100-32GB GPUs with 2048 batch size and 512 max length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Evaluation</head><p>Cross-lingual Natural Language Inference. The cross-lingual natural language inference (XNLI; <ref type="bibr" target="#b3">Conneau et al. 2018</ref>) task is a multilingual language inference task. The goal of XNLI is to determine the relationship between the two input sentences. We evaluate ERNIE-M in (1) cross-lingual transfer <ref type="bibr" target="#b3">(Conneau et al., 2018)</ref> setting: fine-tune the model with an English training set and evaluate the foreign language XNLI test and (2) translatetrain-all <ref type="bibr" target="#b10">(Huang et al., 2019)</ref> setting: fine-tune the model on the concatenation of all other languages and evaluate on each language test set. <ref type="table" target="#tab_3">Table 1</ref> shows the results of ERNIE-M in XNLI task. The result shows that ERNIE-M outperforms all baseline models including XLM <ref type="bibr" target="#b14">(Lample and Conneau, 2019)</ref> Named Entity Recognition. For the namedentity-recognition task, we evaluate ERNIE-M on the <ref type="bibr">CoNLL-2002</ref><ref type="bibr">and CoNLL-2003</ref><ref type="bibr">datasets (Sang and De Meulder, 2003</ref>, which is a cross-lingual named-entity-recognition task including English, Dutch, Spanish and German. We consider ERNIE-M in the following setting: (1) fine-tune on the   <ref type="table">Table 2</ref>: Evaluation results on CoNLL named entity recognition. The results of ERNIE-M are averaged over five runs. Results with " ?" and " * " are from , and (Wu and Dredze, 2019), respectively.</p><p>English dataset and evaluate on each cross-lingual dataset to evaluate cross-lingual transfer and <ref type="formula">(2)</ref> fine-tune on all training datasets to evaluate crosslingual learning. For each setting, we reported the F1 score for each language. <ref type="table">Table 2</ref> shows the results of ERNIE-M, XLM-R, and mBERT on <ref type="bibr">CoNLL-2002 and</ref><ref type="bibr">CoNLL-2003</ref>. The results of XLM-R and mBERT are reported from . ERNIE-M model yields SoTA performance on both settings and outperforms XLM-R by 0.45 F1 when trained on English and 0.70 F1 when trained on all languages in the base model. Similar to the performance in the XNLI task, ERNIE-M shows better performance on low-resource languages. For large models and finetune in all languages setting, ERNIE-M is 2.21 F1 higher than SoTA in Dutch (nl) and 1.6 F1 higher than SoTA in German (de).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual Question</head><p>Answering. For the question answering task, we use a multilingual question answering (MLQA) dataset to evaluate ERNIE-M. MLQA has the same format as SQuAD v1.1 <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> and is a multilingual language question answering task composed of seven languages. We fine-tune ERNIE-M by training on English data and evaluating on seven crosslingual datasets. The fine-tune method is the same as in <ref type="bibr" target="#b16">Lewis et al. (2019)</ref>, which concatenates the question-passage pair as the input. <ref type="table" target="#tab_5">Table 3</ref> presents a comparison of ERNIE-M and several baseline models on MLQA. We report the F1 and extract match (EM) scores based on the average over five runs. The performance of ERNIE-M in MLQA is significantly better than the previous models, and it achieves a SoTA score. We outperform INFOXLM 0.8 in F1 and 0.5 in EM.</p><p>Cross-lingual Paraphrase Identification. For cross-lingual paraphrase identification task, we use the PAWS-X  dataset to evaluate our model. The goal of PAWS-X was to determine whether two sentences were paraphrases. We evaluate ERNIE-M on both the cross-lingual transfer setting and translate-train-all setting. <ref type="table">Table 4</ref> shows a comparison of ERNIE-M and various baseline models on PAWS-X. We report the accuracy score on each language test set based on the average over five runs. The results show that Model en es de ar hi vi zh Avg mBERT <ref type="bibr" target="#b16">(Lewis et al., 2019</ref><ref type="bibr">) 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.3 57.7 / 41.6 XLM (Lewis et al., 2019</ref><ref type="bibr">) 74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.6 61.6 / 43.5 XLM-R (Conneau et al., 2019</ref><ref type="bibr">) 77.1 / 64.6 67.4 / 49.6 60.9 / 46.7 54.9 / 36.6 59.4 / 42.9 64.5 / 44.7 61.8 / 39.3 63.7 / 46.3 INFOXLM (Chi et al., 2020b</ref> 81.   <ref type="table">Table 4</ref>: Evaluation results on PAWS-X. The results of ERNIE-M are averaged over five runs. Results with " ?" and " * " are from  and , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ERNIE-M outperforms all baseline models on most languages and achieves a new SoTA.</head><p>Cross-lingual Sentence Retrieval. The goal of the cross-lingual sentence retrieval task was to extract parallel sentences from bilingual corpora. We used a subset of the Tatoeba  dataset, which contains 36 language pairs to evaluate ERNIE-M. Following , we used the averaged representation in the middle layer of the best XNLI model to evaluate the retrieval task. <ref type="table" target="#tab_7">Table 5</ref> shows the results of ERNIE-M in the retrieval task; XLM-R results are reported from . ERNIE-M achieves a score of 87.9 in the Tatoeba dataset, outperforming VECO 1.0 and obtaining new SoTA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Avg</head><p>XLM-R LARGE    To further evaluate the performance of ERNIE-M in retrieval task, we use hardest negative binary cross-entropy loss <ref type="bibr" target="#b29">(Wang et al., 2019b;</ref><ref type="bibr" target="#b6">Faghri et al., 2017)</ref> to fine-tune ERNIE-M with the same bilingual corpus in pre-training. <ref type="figure">Figure 5</ref> shows the details of accuracy on each language in Tatoeba.</p><p>After fine-tuning, ERNIE-M shows a significant improvement in all languages, with the average accuracy in all languages increasing from 87.9 to 93.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Tatoeba results for each language</p><p>After fine-tuning XNLI model <ref type="figure">Figure 5</ref>: Tatoeba results for each language. The languages are sorted according to their size in the pretrained corpus from smallest to largest. Fine-tuning can significantly improve the accuracy of different language families in the cross-lingual retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To understand the effect of aligning semantic representations of multiple languages in the training process of ERNIE-M, we conducted an ablation study as reported in <ref type="table" target="#tab_9">Table 6</ref>. exp 0 was directly finetuning XLM-R model on the XNLI and the CoNLL. We trained (1) only MMLM on the monolingual corpus, and the purpose of exp 1 was to measure how much performance gain could be achieved by continuing training based on the XLM-R model, (2) MMLM on the monolingual corpus, and TLM on the bilingual corpus, (3) MMLM on the monolingual corpus and CAMLM on the bilingual corpus, (4) MMLM and BTMLM on the monolingual corpus and CAMLM on the bilingual corpus and (5) full strategy of ERNIE-M. We use the base model structure for our experiments, and to speed up the experiments, we use the XLM-R BASE model to initialize the parameters of ERNIE-M, all of which run 50,000 steps with the same hyperparameters with a batch size of 2048, and the score reported in the downstream task is the average score of five runs.</p><p>Comparing exp 0 and exp 1 , we can observer that there is no gain in the performance of the crosslingual model by continuing pre-training XLM-   R model. Comparing exp 2 exp 3 exp 4 with exp 1 , we find that the learning of cross-lingual semantic alignment on parallel corpora is helpful for the performance of the model. Experiments that use the bilingual corpus for training show a significant improvement in XNLI. However, there are a surprised result that the using of TLM objective hurt the performance of NER task as exp 1 and exp 2 shows. Comparing exp 2 with exp 4 , we find that our proposed BTMLM and CAMLM training objective are better for capturing the alignment of cross-lingual semantics. The training model with CAMLM and BTMLM objective results in a 0.3 improvement on XNLI and a 1.3 improvement on CoNLL compared to the training model with TLM.</p><p>Comparing exp 3 to exp 4 , we find that there is a 0.5 improvement on XNLI and 0.1 improvement on CoNLL after the model learns BTMLM. This demonstrates that our proposed BTMLM can learn cross-lingual semantic alignment and improve the performance of our model. To further analyze the effect of our strategy, we trained the small-sized ERNIE-M model from scratch. <ref type="table" target="#tab_12">Table 8</ref> shows the results of XNLI and CoNLL. Both XNLI and CoNLL results are the average of each languages. We observe that, ERNIE-M SMALL can outperform XLM-R SMALL by 4.4 in XNLI and 6.6 in CoNLL. It suggests that our models can benefit from align cross-lingual semantic representation. <ref type="table" target="#tab_10">Table 7</ref> shows the gap scores for English and other languages in the downstream task. This gap score is the difference between the English testset and the average performance on the testset in other languages. So, a smaller gap score represents a better transferability of the model. We can no-   To measure the computation cost of ERNIE-M, we trained ERNIE-M and XLM-R (MMLM + TLM) from scratch. The result shows that the training speed of ERNIE-M is 1.075x compared with XLM-R, so the overall computational of ERNIE-M is 1.075x compared with XLM-R. With the same computational overhead, the performance of ERNIE-M is 69.9 in XNLI and 69.7 in CoNLL, while XLM-R's performance is 67.3 in XNLI and 65.6 in CoNLL. The results demonstrate that ERNIE-M performs better than XLM-R even with the same computational overhead.</p><p>In addition, we explored the effect of the number of generated pseudo-parallel tokens on the convergence of the model. In particular, we compare the impact on the convergence speed of the model when generating a 5%, 10%, 15%, and 20% proportion of pseudo-tokens. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, we can find that the perplexity (PPL) of the model decreases as the proportion of generated tokens increases, which indicates that the generated pseudo-parallel tokens are helpful for model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To overcome the constraint that the parallel corpus size places on the cross-lingual models performance, we propose a new cross-lingual model, ERNIE-M, which is trained using both monolingual and parallel corpora. The contribution of ERNIE-M is to propose two training objectives. The first objective is to enhance the multilingual representation on parallel corpora by applying CAMLM, and the second objective is to help the model to align cross-lingual semantic representations from a monolingual corpus by using BTMLM. Experiments show that ERNIE-M achieves SoTA results in various downstream tasks on the XNLI, MLQA, CoNLL, PAWS-X, and Tatoeba datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pre-training Data</head><p>We follow  to reconstruct CC-100 data for ERNIE-M training. The monolingual training corpus contains 96 languages, as shown in <ref type="table" target="#tab_14">Table 9</ref>. Note that several languages have the same ISO code, e.g., zh represents both Simplified Chinese and Traditional Chinese; ur represents both Urdu and Urdu Romanized. <ref type="table" target="#tab_3">Table 10</ref> shows the statistics of the parallel data in each language.</p><p>Code Size (GB) Code Size (GB) Code Size (GB)    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameters for Fine-tuning</head><p>Tables 12 and 13 list the fine-tuning parameters on XNLI, MLQA, CoNLL and PAWS-X. For each task, we select the model with the best performance on the validation set, and the test set score is the average of five runs with different random seeds. Tables 14 list the fine-tuning parameters on Tatoeba.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Results for 15 languages model</head><p>To better evaluate the performance of ERNIE-M, we train the ERNIE-M-15 model for 15 languages. The languages of training corpora is the same as that of HICTL . We evaluate ERNIE-M-15 on the XNLI dataset. <ref type="table" target="#tab_3">Table 15</ref> shows the results of 15 languages models. The ERNIE-M-15 model outperforms the current best 15-language cross-lingual model on the XNLI task, achieving a score of 77.5 in the cross-lingual transfer setting, outperforming HICTL 0.2 and a score of 80.7 in the translate-train-all setting, outperforming HICTL 0.7. <ref type="table" target="#tab_3">Table 16</ref> shows the details of accuracy on each language in the cross-lingual retrieval task. For a fair comparison with VECO, we use the averaged representation in the middle layer of best XNLI model for cross-lingual retrieval task. ERNIE-M outperforms VECO in most languages and achieves state-of-the-art results. We also proposed a new method for cross-lingual retrieval. We use hardest negative binary cross-entropy loss <ref type="bibr" target="#b29">(Wang et al., 2019b;</ref><ref type="bibr" target="#b6">Faghri et al., 2017)</ref> to fine-tune ERNIE-M with the same bilingual corpora in pre-training.      <ref type="table" target="#tab_3">Table 16</ref>: Tatoeba results for each language. " ?" indicates the results after fine-tuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Results for Cross-lingual Retrieval</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of MMLM, TLM and CAMLM training. The input sentences in sub-figure (a) are monolingual sentences; x and y represent monolingual input sentences in different languages. The input sentences in subfigures (b) and (c) are parallel sentences; x and y denote the source and target sentences of the parallel sentences, respectively. h indicates the token predicted by the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of BTMLM training; the left figure represents the first stage of BTMLM, predicting the pseudo-tokens. The right figure represents the second stage of the BTMLM, making predictions based on the predicted pseudo-tokens and original sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Self-attention matrix of BTMLM Stage 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, Unicoder<ref type="bibr" target="#b10">(Huang et al., 2019)</ref>, XLM-R, INFOXLM<ref type="bibr" target="#b1">(Chi et al., 2020b)</ref> and VECO on both the evaluation settings on XNLI. The final scores on the test set are averaged over five runs with different random seeds. On cross-lingual transfer setting, ERNIE-M achieves 77.3 average accuracy, outperforming INFOXLM by 1.1, ERNIE-M LARGE achieves 82.0 accuracy, outperforming IN-FOXLM LARGE by 0.6. ERNIE-M also yields outstanding performance in low-resource languages, including 69.5 in Swahili (sw) and 68.8 in Urdu (ur). In the case of translate-train-all, ERNIE-M improves the performance and reaches an accuracy of 80.6, outperforming INFOXLM by 0.9, ERNIE-M LARGE achieves 84.2 accuracy, a new SoTA for XNLI, outperforming XLM-R LARGE by 0.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>PPL in BTMLM training with different mask prob, prob means the proportion of pseudo-tokens generated in BTMLM Stage 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fine-tune cross-lingual model on English training set (Cross-lingual Transfer)XLM<ref type="bibr" target="#b14">(Lample and Conneau, 2019)</ref> 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3 75.1 Unicoder (Huang et al., 2019) 85.1 79.0 79.4 77.8 77.2 77.2 76.3 72.8 73.5 76.4 73.6 76.2 69.4 69.7 66.7 75.4 XLM-R (Conneau et al., 2019) 85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.3 76.2 INFOXLM (Chi et al., 2020b) 86.4 80.6 80.8 78.9 77.8 78.9 77.6 75.6 74.0 77.0 73.7 76.7 72.0 66.4 67.1 76.2 ERNIE-M 85.5 80.1 81.2 79.2 79.1 80.4 78.1 76.8 76.3 78.3 75.8 77.4 72.9 69.5 68.8 77.3</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>fr</cell><cell>es</cell><cell>de</cell><cell>el</cell><cell>bg</cell><cell>ru</cell><cell>tr</cell><cell>ar</cell><cell>vi</cell><cell>th</cell><cell>zh</cell><cell>hi</cell><cell>sw</cell><cell>ur</cell><cell>Avg</cell></row><row><cell cols="17">XLM-R LARGE (Conneau et al., 2019) 89.1 84.1 85.1 83.9 82.9 84.0 81.2 79.6 79.8 80.8 78.1 80.2 76.9 73.9 73.8 80.9</cell></row><row><cell cols="17">INFOXLM LARGE (Chi et al., 2020b) 89.7 84.5 85.5 84.1 83.4 84.2 81.3 80.9 80.4 80.8 78.9 80.9 77.9 74.8 73.7 81.4</cell></row><row><cell>VECO LARGE (Luo et al., 2020)</cell><cell cols="16">88.2 79.2 83.1 82.9 81.2 84.2 82.8 76.2 80.3 74.3 77.0 78.4 71.3 80.4 79.1 79.9</cell></row><row><cell>ERNIE-M LARGE</cell><cell cols="16">89.3 85.1 85.7 84.4 83.7 84.5 82.0 81.2 81.2 81.9 79.2 81.0 78.6 76.2 75.4 82.0</cell></row><row><cell cols="6">Fine-tune cross-lingual model on all training sets (Translate-Train-All)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="17">XLM (Lample and Conneau, 2019) 85.0 80.8 81.3 80.3 79.1 80.9 78.3 75.6 77.6 78.5 76.0 79.5 72.9 72.8 68.5 77.8</cell></row><row><cell>Unicoder (Huang et al., 2019)</cell><cell cols="16">85.6 81.1 82.3 80.9 79.5 81.4 79.7 76.8 78.2 77.9 77.1 80.5 73.4 73.8 69.6 78.5</cell></row><row><cell>XLM-R (Conneau et al., 2019)</cell><cell cols="16">85.4 81.4 82.2 80.3 80.4 81.3 79.7 78.6 77.3 79.7 77.9 80.2 76.1 73.1 73.0 79.1</cell></row><row><cell>INFOXLM (Chi et al., 2020b)</cell><cell cols="16">86.1 82.0 82.8 81.8 80.9 82.0 80.2 79.0 78.8 80.5 78.3 80.5 77.4 73.0 71.6 79.7</cell></row><row><cell>ERNIE-M</cell><cell cols="16">86.2 82.5 83.8 82.6 82.4 83.4 80.2 80.6 80.5 81.1 79.2 80.5 77.7 75.0 73.3 80.6</cell></row><row><cell cols="17">XLM-R LARGE (Conneau et al., 2019) 89.1 85.1 86.6 85.7 85.3 85.9 83.5 83.2 83.1 83.7 81.5 83.7 81.6 78.0 78.1 83.6</cell></row><row><cell>VECO LARGE (Luo et al., 2020)</cell><cell cols="16">88.9 82.4 86.0 84.7 85.3 86.2 85.8 80.1 83.0 77.2 80.9 82.8 75.3 83.1 83.0 83.0</cell></row><row><cell>ERNIE-M LARGE</cell><cell cols="16">89.5 86.5 86.9 86.1 86.0 86.8 84.1 83.8 84.1 84.5 82.1 83.5 81.1 79.4 77.9 84.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>LARGE 94.01 93.81 89.23 86.20 90.81</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>nl</cell><cell>es</cell><cell>de</cell><cell>Avg</cell></row><row><cell cols="3">Fine-tune on English dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT  *</cell><cell cols="5">91.97 77.57 74.96 69.56 78.52</cell></row><row><cell>XLM-R  ?</cell><cell cols="5">92.25 78.08 76.53 69.60 79.11</cell></row><row><cell>ERNIE-M</cell><cell cols="5">92.78 78.01 79.37 68.08 79.56</cell></row><row><cell>XLM-R  ? LARGE</cell><cell cols="5">92.92 80.80 78.64 71.40 80.94</cell></row><row><cell cols="6">ERNIE-M LARGE 93.28 81.45 78.83 72.99 81.64</cell></row><row><cell cols="2">Fine-tune on all dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R  ?</cell><cell cols="5">91.08 89.09 87.28 83.17 87.66</cell></row><row><cell>ERNIE-M</cell><cell cols="5">93.04 91.73 88.33 84.20 89.32</cell></row><row><cell>XLM-R  ? LARGE</cell><cell cols="5">92.00 91.60 89.52 84.60 89.43</cell></row><row><cell>ERNIE-M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Evaluation results on XNLI cross-lingual natural language inference. We report the accuracy on each of the 15 XNLI languages and the average accuracy. Our ERNIE-M results are based on five runs with different random seeds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>3 / 68.2 69.9 / 51.9 64.2 / 49.6 60.1 / 40.9 65.0 / 47.5 70.0 / 48.6 64.7 / 41.2 67.9 / 49.7 ERNIE-M 81.6 / 68.5 70.9 / 52.6 65.8 / 50.7 61.8 / 41.9 65.4 / 47.5 70.0 / 49.2 65.6 / 41.0 68.7 / 50.2 XLM-R LARGE (Conneau et al., 2019) 80.6 / 67.8 74.1 / 56.0 68.5 / 53.6 63.1 / 43.5 62.9 / 51.6 71.3 / 50.9 68.0 / 45.4 70.7 / 52.7 INFOXLM LARGE (Chi et al., 2020b) 84.5 / 71.6 75.1 / 57.3 71.2 / 56.2 67.6 / 47.6 72.5 / 54.2 75.2 / 54.1 69.2 / 45.4 73.6 / 55.2 ERNIE-M LARGE 84.4 / 71.5 74.8 / 56.6 70.8 / 55.9 67.4 / 47.2 72.6 / 54.7 75.0 / 53.7 71.1 / 47.5 73.7 / 55.3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on MLQA cross-lingual question answering. We report the F1 and exact match (EM) scores. The results of ERNIE-M are averaged over five runs.</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>Avg</cell></row><row><cell cols="2">Cross-lingual Transfer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT  ?</cell><cell cols="8">94.0 85.7 87.4 87.0 73.0 69.6 77.0 81.9</cell></row><row><cell>XLM  ?</cell><cell cols="8">94.0 85.9 88.3 87.4 69.3 64.8 76.5 80.9</cell></row><row><cell>MMTE  ?</cell><cell cols="8">93.1 85.1 87.2 86.9 72.0 69.2 75.9 81.3</cell></row><row><cell>XLM-R  ? LARGE VECO  *  LARGE</cell><cell cols="8">94.7 89.7 90.1 90.4 78.7 79.0 82.3 86.4 96.2 91.3 91.4 92.0 81.8 82.9 85.1 88.7</cell></row><row><cell cols="9">ERNIE-M LARGE 96.0 91.9 91.4 92.2 83.9 84.5 86.9 89.5</cell></row><row><cell cols="2">Translate-Train-All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VECO  *  LARGE</cell><cell cols="8">96.4 93.0 93.0 93.5 87.2 86.8 87.9 91.1</cell></row><row><cell cols="9">ERNIE-M LARGE 96.5 93.5 93.3 93.8 87.9 88.4 89.2 91.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Evaluation results on Tatoeba. " ?" indicates the results after fine-tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on each task in ERNIE-M.</figDesc><table><row><cell>Model</cell><cell cols="2">MLQA XNLI Avg</cell></row><row><cell>mBERT XLM-R INFOXLM ERNIE-M</cell><cell>23.3 17.6 15.7 15.0</cell><cell>16.9 20.1 10.4 14.0 10.9 13.3 8.8 11.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Cross-lingual transfer gap score, smaller gap indicates better transferability.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">: XNLI and CoNLL accuracy under the cross-</cell></row><row><cell cols="4">lingual transfer setting. All the models are small-sized</cell></row><row><cell cols="4">trained from scratch. The small-sized model has the</cell></row><row><cell cols="4">same hyperparameter as base model except that the</cell></row><row><cell cols="4">number of layers is 6. ERNIE-M  *  is the result in down-</cell></row><row><cell cols="4">stream tasks with the same computational overhead as</cell></row><row><cell cols="4">XLM-R. All the models have the same training steps</cell></row><row><cell cols="3">except ERNIE-M  *  .</cell></row><row><cell cols="4">tice that the gap scores of ERNIE-M are smaller</cell></row><row><cell cols="4">compared to XLM-R and INFOXLM in both the</cell></row><row><cell cols="4">XNLI and MLQA tasks, which indicates a better</cell></row><row><cell cols="4">transferability of ERNIE-M.</cell></row><row><cell></cell><cell>5.4 5.8</cell><cell></cell><cell>PPL in each mask prob prob 5% prob 10% prob 15% prob 20%</cell></row><row><cell>PPL</cell><cell>5.0</cell><cell></cell></row><row><cell></cell><cell>4.6</cell><cell></cell></row><row><cell></cell><cell>4.2</cell><cell>0</cell><cell>Steps 2000 4000 6000 8000 10000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Statistics of CC-100 used for ERNIE-M pretraining.</figDesc><table><row><cell cols="4">ISO Code Size (GB) ISO Code Size (GB)</cell></row><row><cell>ar</cell><cell>9.8</cell><cell>ru</cell><cell>8.3</cell></row><row><cell>bg</cell><cell>2.2</cell><cell>sw</cell><cell>0.1</cell></row><row><cell>de</cell><cell>10.7</cell><cell>th</cell><cell>3.3</cell></row><row><cell>el</cell><cell>4.0</cell><cell>tr</cell><cell>1.1</cell></row><row><cell>es</cell><cell>8.8</cell><cell>ur</cell><cell>0.7</cell></row><row><cell>fr</cell><cell>13.7</cell><cell>vi</cell><cell>0.8</cell></row><row><cell>hi</cell><cell>0.3</cell><cell>zh</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Statistics of parallel data used for ERNIE-M pre-training.A.2 Hyperparameters for Pre-trainingTable 11lists the hyperparameters for pre-training. We use the XLM-R model to initialize the parameters of base and large model, for the small model, we train it from scratch. The vocab of ERNIE-M is the same as that of XLM-R.</figDesc><table><row><cell>Hyperparameters</cell><cell cols="2">SMALL BASE</cell><cell>LARGE</cell></row><row><cell>Layers</cell><cell>6</cell><cell>12</cell><cell>24</cell></row><row><cell>Hidden size</cell><cell>768</cell><cell>768</cell><cell>1024</cell></row><row><cell cols="2">FFN inner hidden size 3,072</cell><cell>3,072</cell><cell>4,096</cell></row><row><cell>FFN dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Attention heads</cell><cell>12</cell><cell>12</cell><cell>16</cell></row><row><cell>Attention dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Embedding size</cell><cell>768</cell><cell>768</cell><cell>1024</cell></row><row><cell>Training steps</cell><cell>240K</cell><cell>150K</cell><cell>200K</cell></row><row><cell>Batch size</cell><cell>1,024</cell><cell>2,048</cell><cell>2,048</cell></row><row><cell>Learning rate</cell><cell>3e-4</cell><cell>2e-4</cell><cell>1e-4</cell></row><row><cell cols="2">Learning rate schedule Linear</cell><cell cols="2">Linear Linear</cell></row><row><cell>Adam ?</cell><cell>1e-6</cell><cell>1e-6</cell><cell>1e-6</cell></row><row><cell>Adam ? 1</cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Adam ? 2</cell><cell>0.999</cell><cell>0.999</cell><cell>0.999</cell></row><row><cell>Weight decay</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Warmup steps</cell><cell>10,000</cell><cell cols="2">10,000 10,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters used for pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16</head><label>16</label><figDesc>report the results after fine-tuning, the average accuracy of Tatoeba improve from 87.9 to 93.3.</figDesc><table><row><cell>Batch size</cell><cell>32</cell><cell>128</cell><cell>32</cell><cell>8</cell><cell>8</cell></row><row><cell>Learning rate</cell><cell>5e-5</cell><cell>5e-5</cell><cell>3e-4</cell><cell>4e-4</cell><cell>3e-4</cell></row><row><cell>Layerwise LR decay</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>LR schedule</cell><cell cols="2">Linear Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Warmup faction</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell></row><row><cell>Weight decay</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Epoch</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>10</cell><cell>10</cell></row></table><note>Hyperparameters XNLI XNLI* MLQA CoNLL CoNLL*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters used for ERNIE-M SMALL and ERNIE-M BASE fine-tuning; parameters with "*" are in the translate-train-all setting, and those without "*" are in the cross-lingual setting.</figDesc><table><row><cell cols="3">Hyperparameters XNLI XNLI  Batch size 32 128</cell><cell>32</cell><cell>8</cell><cell>8</cell><cell>64</cell><cell>64</cell></row><row><cell>Learning rate</cell><cell>5e-5</cell><cell>5e-5</cell><cell>8e-5</cell><cell>4e-4</cell><cell>3e-4</cell><cell>5e-5</cell><cell>7e-5</cell></row><row><cell>Layerwise LR decay</cell><cell>0.8</cell><cell>0.8</cell><cell>0.9</cell><cell>0.8</cell><cell>0.8</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>LR schedule</cell><cell cols="2">Linear Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Warmup faction</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell><cell>10%</cell></row><row><cell>Weight decay</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Epoch</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>2</cell></row></table><note>* MLQA CoNLL CoNLL* PAWS-X PAWS-X*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters used for ERNIE-M LARGE fine-tuning; parameters with "*" are in the translate-train-all setting, and those without "*" are in the cross-lingual setting.</figDesc><table><row><cell>Hyperparameters</cell><cell>LARGE</cell></row><row><cell>Training steps</cell><cell>200K</cell></row><row><cell>Batch size</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>5e-5</cell></row><row><cell cols="2">Learning rate schedule Linear</cell></row><row><cell>Weight decay</cell><cell>0.0</cell></row><row><cell>Warmup faction</cell><cell>10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Hyperparameters used for ERNIE-M LARGE fine-tuneing in Tatoeba. Fine-tune cross-lingual model on English training set (Cross-lingual Transfer) XLM (Lample and Conneau, 2019) 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3 75.1 HICTL (Wei et al., 2020) 86.3 80.5 81.3 79.5 78.9 80.6 79.0 75.4 74.8 77.4 75.7 77.6 73.1 69.9 69.7 77.3 ERNIE-M-15 85.9 80.5 81.3 79.8 79.3 80.7 78.7 76.8 76.8 78.0 76.1 77.4 72.9 68.9 68.9 77.5 Fine-tune cross-lingual model on all training sets (Translate-Train-All) XLM (Lample and Conneau, 2019) 85.0 80.8 81.3 80.3 79.1 80.9 78.3 75.6 77.6 78.5 76.0 79.5 72.9 72.8 68.5 77.8 HICTL (Wei et al., 2020) 86.5 82.3 83.2 80.8 81.6 82.2 81.3 80.5 78.1 80.4 78.6 80.7 76.7 73.8 73.9 80.0 ERNIE-M-15 86.4 82.4 83.5 82.7 83.1 83.2 81.0 80.6 80.5 80.9 79.2 80.6 77.7 75.8 72.8 80.7</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>fr</cell><cell>es</cell><cell>de</cell><cell>el</cell><cell>bg</cell><cell>ru</cell><cell>tr</cell><cell>ar</cell><cell>vi</cell><cell>th</cell><cell>zh</cell><cell>hi</cell><cell>sw</cell><cell>ur</cell><cell>Avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Evaluation results on XNLI cross-lingual natural language inference for 15 languages model. 80.9 85.1 91.3 78.1 98.5 89.5 97.4 94.8 79.8 93.1 95.4 93.7 85.8 94.2 93.8 93.0 92.2 92.8 ERNIE-M LARGE 88.6 88.9 92.2 84.8 98.8 92.4 96.3 82.4 78.8 92.5 92.2 94.0 86.2 95.4 88.7 91.5 90.3 86.7 ERNIE-M ? LARGE 92.6 94.3 96.6 89.2 99.7 96.8 98.8 92.5 87.4 96.0 97.1 96.5 90.1 97.9 95.5 95.7 95.2 96.9 Luo et al., 2020) 35.1 83.0 74.1 88.7 94.8 82.5 95.9 94.6 92.2 69.7 82.4 91.0 94.7 73.0 95.2 63.8 95.1 93.9 ERNIE-M LARGE 48.0 84.2 78.2 85.3 95.2 87.6 96.1 92.6 93.1 59.4 86.9 94.0 95.6 75.4 96.3 90.8 94.5 91.7 ERNIE-M ? LARGE 65.2 94.9 88.0 94.1 98.5 90.8 98.1 94.5 95.7 68.4 91.8 97.9 98.4 86.0 98.3 94.9 98.1 96.7</figDesc><table><row><cell>Model</cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell><cell>ja</cell></row><row><cell>VECO LARGE (Model</cell><cell>jv</cell><cell>ka</cell><cell>kk</cell><cell>ko</cell><cell>ml</cell><cell>mr</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>sw</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>zh</cell></row><row><cell>VECO LARGE (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and models are available at https://github. com/PaddlePaddle/ERNIE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual natural language generation via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7570" to="7577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Infoxlm: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07834</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05053</idno>
		<title level="m">Xnli: Evaluating crosslingual sentence representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">Vse++: Improving visualsemantic embeddings with hard negatives</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Nonautoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11080</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00964</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural machine translation for low-resource languages without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Karakanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Dehdari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="189" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02855</idno>
		<title level="m">The iit bombay english-hindi parallel corpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Crosslingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07475</idno>
		<title level="m">Mlqa: Evaluating cross-lingual extractive question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Veco: Variable encoder-decoder pre-training for cross-lingual understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16046</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Meulder</surname></persName>
		</author>
		<idno>cs/0306050</idno>
		<title level="m">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05791</idno>
		<title level="m">Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-lingual retrieval for iterative self-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with auxiliary regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5377" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Camp: Cross-modal adaptive message passing for text-image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15960</idno>
		<title level="m">On learning universal representations across languages</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09077</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Alternating language modeling for cross-lingual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9386" to="9393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The united nations parallel corpus v1. 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Ziemski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3530" to="3534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
