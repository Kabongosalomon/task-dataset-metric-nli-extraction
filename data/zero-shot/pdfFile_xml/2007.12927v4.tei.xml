<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 NEURAL NETWORKS WITH LATE-PHASE WEIGHTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Von Oswald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">-equal contribution Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Z?rich</orgName>
								<orgName type="institution" key="instit2">ETH Z?rich Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seijin</forename><surname>Kobayashi</surname></persName>
							<email>seijink@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">-equal contribution Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Z?rich</orgName>
								<orgName type="institution" key="instit2">ETH Z?rich Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Meulemans</surname></persName>
							<email>ameulema@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">-equal contribution Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Z?rich</orgName>
								<orgName type="institution" key="instit2">ETH Z?rich Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
							<email>henningc@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">-equal contribution Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Z?rich</orgName>
								<orgName type="institution" key="instit2">ETH Z?rich Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
							<email>bgrewe@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">-equal contribution Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Z?rich</orgName>
								<orgName type="institution" key="instit2">ETH Z?rich Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Sacramento</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">-equal contribution Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Z?rich</orgName>
								<orgName type="institution" key="instit2">ETH Z?rich Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 NEURAL NETWORKS WITH LATE-PHASE WEIGHTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The largely successful method of training neural networks is to learn their weights using some variant of stochastic gradient descent (SGD). Here, we show that the solutions found by SGD can be further improved by ensembling a subset of the weights in late stages of learning. At the end of learning, we obtain back a single model by taking a spatial average in weight space. To avoid incurring increased computational costs, we investigate a family of low-dimensional late-phase weight models which interact multiplicatively with the remaining parameters. Our results show that augmenting standard models with late-phase weights improves generalization in established benchmarks such as CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a theoretical analysis of a noisy quadratic problem which provides a simplified picture of the late phases of neural network learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural networks trained with SGD generalize remarkably well on a wide range of problems. A classic technique to further improve generalization is to ensemble many such models <ref type="bibr" target="#b41">(Lakshminarayanan et al., 2017)</ref>. At test time, the predictions made by each model are combined, usually through a simple average. Although largely successful, this technique is costly both during learning and inference. This has prompted the development of ensembling methods with reduced complexity, for example by collecting models along an optimization path generated by SGD <ref type="bibr" target="#b27">(Huang et al., 2017)</ref>, by performing interpolations in weight space , or by tying a subset of the weights over the ensemble <ref type="bibr" target="#b44">(Lee et al., 2015;</ref><ref type="bibr">Wen et al., 2020)</ref>.</p><p>An alternative line of work explores the use of ensembles to guide the optimization of a single model <ref type="bibr" target="#b21">(Zhang et al., 2015;</ref><ref type="bibr" target="#b3">Pittorino et al., 2020)</ref>. We join these efforts and develop a method that fine-tunes the behavior of SGD using late-phase weights: late in training, we replicate a subset of the weights of a neural network and randomly initialize them in a small neighborhood. Together with the stochasticity inherent to SGD, this initialization encourages the late-phase weights to explore the loss landscape. As the late-phase weights explore, the shared weights accumulate gradients. After training we collapse this implicit ensemble into a single model by averaging in weight space.</p><p>Building upon recent work on ensembles with shared parameters <ref type="bibr">(Wen et al., 2020)</ref> we explore a family of late-phase weight models involving multiplicative interactions <ref type="bibr" target="#b32">(Jayakumar et al., 2020)</ref>. We focus on low-dimensional late-phase models that can be ensembled with negligible overhead. Our experiments reveal that replicating the ubiquitous batch normalization layers <ref type="bibr" target="#b29">(Ioffe &amp; Szegedy, 2015)</ref> is a surprisingly simple and effective strategy for improving generalization 1 . Furthermore, we find that late-phase weights can be combined with stochastic weight averaging , a complementary method that has been shown to greatly improve generalization.</p><formula xml:id="formula_0">? k .</formula><p>(1)</p><p>Hence, the complexity of inference is independent of K, and equivalent to that of the original model.</p><p>Late-phase weight initialization. We initialize our late-phase weights from a reference base weight. We first learn a base parameter ? 0 from time step t = 0 until T 0 , treating ? 0 as any other base parameter in ?. Then, at time t = T 0 , each configuration ? k is initialized in the vicinity of ? 0 . We explore perturbing ? 0 using a symmetric Gaussian noise model,</p><formula xml:id="formula_1">? k = ? 0 + ? 0 Z(? 0 ) k ,<label>(2)</label></formula><p>where k is a standard normal variate of appropriate dimension and ? 0 is a hyperparameter controlling the noise amplitude. We allow for a ? 0 -dependent normalization factor, which we set so as to ensure layerwise scale-invariance, which helps finding a single ? 0 that governs the initialization of the entire network. More concretely, for a given neural network layer l with weights ? (l) 0 of dimension D (l) , we choose Z(? (l) 0 ) = ? D (l) / ? (l) 0 . Our perturbative initialization (Eq. 2) is motivated by ongoing studies of the nonconvex, highdimensional loss functions that arise in deep learning. Empirical results and theoretical analyses of simplified models point to the existence of dense clusters of connected solutions with a locallyflat geometry <ref type="bibr" target="#b25">(Hochreiter &amp; Schmidhuber, 1997a)</ref> that are accessible by SGD <ref type="bibr" target="#b27">(Huang et al., 2017;</ref><ref type="bibr" target="#b3">Baldassi et al., 2020)</ref>. Indeed, the eigenspectrum of the loss Hessian evaluated at weight configurations found by SGD reveals a large number of directions of low curvature <ref type="bibr" target="#b37">(Keskar et al., 2017;</ref><ref type="bibr" target="#b7">Chaudhari et al., 2019;</ref><ref type="bibr">Sagun et al., 2018)</ref>. For not yet completely understood reasons, this appears to be a recurring phenomenon in overparameterized nonlinear problems <ref type="bibr" target="#b5">(Brown &amp; Sethna, 2003;</ref><ref type="bibr">Waterfall et al., 2006)</ref>.</p><p>Based on these observations, we assume that the initial parameter configuration ? 0 can be perturbed in a late phase of learning without leading to mode hopping across the different models w k . While mode coverage is usually a sought after property when learning neural network ensembles <ref type="bibr" target="#b13">(Fort et al., 2020)</ref>, here it would preclude us from taking the averaged model at the end of learning (Eq. 1).</p><p>Stochastic learning algorithm. Having decomposed our weights into base and late-phase components, we now present a stochastic algorithm which learns both ? and ?. Our algorithm works on the standard stochastic (minibatch) neural network optimization setting <ref type="bibr" target="#b4">(Bottou, 2010)</ref>. Given a loss function L(D, w) = 1 We proceed by iteration over W. At each step k, we sample a minibatch M k and immediately update the late-phase weights ? k , while accumulating gradients over the shared base weights ?. Such gradient accumulation has been previously used when learning ensembles <ref type="bibr" target="#b44">(Lee et al., 2015;</ref><ref type="bibr">Wen et al., 2020)</ref> and multi-task models <ref type="bibr">(Rebuffi et al., 2017)</ref> with shared base parameters. A single iteration is finally concluded by changing the base weights in the direction opposite of the accumulated gradient. We scale the accumulated gradient by ? ? ; setting ? ? = 1/K recovers the original step size in ?, but other choices are possible. In particular, we find that a large ? ? of unit size is in practice often tolerated, resulting in accelerated learning.</p><p>Algorithm 1: Late-phase learning Require: Base weights ?, late-phase weight set ?, dataset D, gradient scale factor ? ? , loss L Require:</p><formula xml:id="formula_2">Training iteration t &gt; T 0 for 1 ? k ? K do M k ? Sample minibatch from D ?? k ? ? ? L(M k , ?, ? k ) ? k ? U ? (? k , ? ? k L(M k , ?, ? k )) ? ? U ? (?, ? ? K k=1 ?? k )</formula><p>We summarize an iteration of our method in Algorithm 1, where the loss L(M, ?, ?) is now seen as a function of ? and ?. We opt for a general presentation using unspecified gradient-based update operators U ? and U ? . These operators can be set to optimizers of choice. For instance, our method might benefit from additional noise injection onto parameter updates <ref type="bibr">(Welling &amp; Teh, 2011)</ref>. Furthermore, late-phase optimizers need not coincide with the optimizer used in the early phase. In our work we typically set U ? and U ? to a single step of SGD with Nesterov momentum <ref type="bibr">(Nesterov, 2004)</ref>, and explore Adam (Kingma &amp; <ref type="bibr" target="#b38">Ba, 2015)</ref> and plain SGD in a smaller set of experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LATE-PHASE WEIGHT MODELS</head><p>As detailed next, we consider a number of distinct late-phase weight models in our experiments.</p><p>In particular, we explore weight interaction functions h in which late-phase weights have low dimensionality, to avoid a large increase in complexity with the ensemble size K. To counteract this reduced dimensionality, we make extensive use of multiplicative base-late weight interactions. This design choice is motivated by the large expressive power of multiplicative interactions despite low dimensionality, which has been demonstrated in a wide range of settings <ref type="bibr" target="#b32">(Jayakumar et al., 2020)</ref>.</p><p>Late-phase batch normalization layers. Batch normalization layers (BatchNorm; <ref type="bibr" target="#b29">Ioffe &amp; Szegedy, 2015)</ref> are a staple of current deep neural network models. Besides standardizing the activity of the layer they are applied to, BatchNorm units introduce a learnable multiplicative (scale) parameter ? and an additive (shift) parameter ?. While being low-dimensional, these additional parameters have large expressive power: it has been shown that learning only ? and ? keeping the remaining weights frozen can lead to significantly lower loss than when learning random subsets of other weights of matching dimensionality <ref type="bibr" target="#b14">(Frankle et al., 2020;</ref><ref type="bibr">Mudrakarta et al., 2019)</ref>.</p><p>We take the scale and shift parameters of BatchNorm layers as our first choice of late-phase weights; the base weights are the remaining parameters of the model. Batch statistics are also individually estimated for each model in W. This late-phase weight parameterization is motivated by (i) the expressive power of ? and ? discussed above, and by (ii) practical considerations, as BatchNorm layers are generally already present in feedforward neural network models, and are otherwise easy to implement efficiently.</p><p>More concretely, let us consider an affine transformation layer l which maps an input vector r (l?1) to ? (l) w r (l?1) + ?  b are already standardized using the respective batch statistics. For this standard layer, our model introduces a multiplicative interaction between base and late-phase weights, diag(? (l) ) ? (l) w , and an additive interaction between base and late-phase bias parameters, ?</p><formula xml:id="formula_3">(l) b + ? (l) .</formula><p>Late-phase rank-1 matrix weights. We also study a closely related late-phase weight model, where existing weight matrices -the base components, as before -are multiplied elementwise by rank-1 matrices <ref type="bibr">(Wen et al., 2020)</ref>. For a given affine layer l, we define a late-phase weight matrix with resort to a pair of learnable vectors, ? (l) = u (l) v (l) T . Taking the Hadamard product with the base weight matrix yields the effective weights W (l) = ? (l) ? ? <ref type="bibr">(l)</ref> .</p><p>With this parameterization, we recover the ensemble proposed by <ref type="bibr">Wen et al. (2020)</ref>, except that here it is generated late in training using our perturbative initialization (Eq. 2). Unlike BatchNorm layers, which include the shift parameter, rank-1 late-phase weights interact in a purely multiplicative manner with base weights. We study this model since it is easy to implement on neural networks which do not feature BatchNorm layers, such as standard long short-term memories (LSTMs; <ref type="bibr" target="#b26">Hochreiter &amp; Schmidhuber, 1997b)</ref>.</p><p>Hypernetworks with late-phase weight embeddings. Additionally, we generalize the late-phase weight models described above using hypernetworks <ref type="bibr" target="#b19">(Ha et al., 2017)</ref>. A hypernetwork generates the parameters w of a given target neural network f w based on a weight embedding. In our framework, we can use a hypernetwork to implement the interaction function w = h(?, ?) directly, with parameters ? corresponding to base weights and embeddings ? to late-phase weights.</p><p>We experiment with linear hypernetworks and use the same hypernetwork to produce the weights of multiple layers, following Savarese &amp; Maire (2019); <ref type="bibr" target="#b19">Ha et al. (2017);</ref><ref type="bibr">von Oswald et al. (2020)</ref>. In this scheme, the weight embedding input specifies the target layer whose parameters are being generated. More specifically, the weight matrix for some layer l belonging to a group of layers g which share a hypernetwork is given by W (g,l) = ? (g) ? <ref type="bibr">(g,l)</ref> , where ? (g) and ? <ref type="bibr">(g,l)</ref> are appropriatelysized tensors. Sharing ? (g) over a layer group g allows countering an increase in the overall number of parameters. We parameterize our hypernetworks such that the weight embedding vectors ? <ref type="bibr">(g,l)</ref> are small, and therefore cheap to ensemble.</p><p>Late-phase classification layers. Finally, inspired by <ref type="bibr" target="#b44">Lee et al. (2015)</ref>, in classification experiments we take the weights of the last linear layer as late-phase weights by default. In modern neural network architectures these layers do not usually comprise large numbers of parameters, and our architecture explorations indicated that it is typically beneficial to ensemble them. We therefore include W (L) in our late-phase weights ?, where W (L) denotes the weights of the final layer L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NOISY QUADRATIC PROBLEM ANALYSIS</head><p>Before turning to real-world learning problems, we first focus on a simplified stochastic optimization setup which can be analytically studied. We consider the noisy quadratic problem (NQP; <ref type="bibr">Schaul et al., 2013;</ref><ref type="bibr">Martens, 2016;</ref><ref type="bibr">Wu et al., 2018;</ref><ref type="bibr">Zhang et al., 2019a;</ref><ref type="bibr">b)</ref>, where the goal is to minimize the scalar loss</p><formula xml:id="formula_4">L = 1 2 (w ? w * + ) T H (w ? w * + )<label>(3)</label></formula><p>with respect to w ? R n . In the equation above, w * denotes the target weight vector, which is randomly shifted by a noise variable assumed to follow a Gaussian distribution N (0, ?). The (constant) Hessian matrix H controls the curvature of the problem. Despite the simplicity of Eq. 3, the NQP captures a surprising number of empirically-observed aspects of neural network learning <ref type="bibr">(Zhang et al., 2019a)</ref>. Here, we motivate its study as a model of late stages of learning, by Taylor expanding the loss around a minimum w * . Thus, for a sufficiently late initialization time T 0 (and small ? 0 ) the NQP is particularly well suited to study our algorithm.</p><p>There are three main strategies to improve the expected NQP loss after convergence: (i) increase the minibatch size B, (ii) use more members K in an ensemble, and (iii) decrease the learning rate ? (Zhang et al., 2019a). Our Algorithm 1 combines the first two strategies in a non-trivial manner. First, the gradients for base weights ? are averaged during the inner loop over all ensemble members, corresponding to a minibatch-size rescaling by K. Second, we introduce K ensemble members, to be averaged in weight space, that only differ in their late-phase weights ?.</p><p>In Appendix C, we show analytically that this combination of an increased effective minibatch size for ? and introducing K ensemble members for ? is successful, resulting in a scaling of the expected loss after convergence by 1 K . This analysis holds for general ? and H, and for both scalar and hypernetwork multiplicative late-phase weights. Hence, our approach combines the benefits of an increased effective minibatch size and of ensembling, while yielding a single model after training.</p><p>We present a numerical validation of this theoretical result in <ref type="figure" target="#fig_2">Fig. 1</ref>. Our model includes a multiplicative late-phase weight, w k = ? ? k with ? k ? R and ? ? R n . We simulate a standard instance of the NQP, with diagonal Hessian H ii = 1/i and ? = H ?1 (cf. <ref type="bibr">Zhang et al., 2019a)</ref>, and report the average loss after convergence. Hyperparameters are given in Appendix C. As predicted by the theory, the loss falls as ? 1/K with increasing ensemble size K, and our algorithm performs on par with a full ensemble of K models trained independently with gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CIFAR-10/100 EXPERIMENTS</head><p>To test the applicability of our method to more realistic problems, we next augment standard neural network models with late-phase weights and examine their performance on the CIFAR-10 and CIFAR-100 image classification benchmarks <ref type="bibr" target="#b40">(Krizhevsky, 2009)</ref>. We use standard data preprocessing methods (cf. Appendix A) and train our models for 200 epochs from random initializations, except when noted otherwise. All evaluated methods are trained using the same amount of data.</p><p>Besides SGD (with Nesterov momentum), we also investigate stochastic weight averaging (SWA; , a recent reincarnation of Polyak averaging <ref type="bibr">(Polyak &amp; Juditsky, 1992</ref>) that can strongly improve neural network generalization. For completeness, we present pseudocode for SWA in Algorithm 2 and SGD with Nesterov momentum in Algorithm 3 (cf. Appendix A). When learning neural networks with late-phase weights we set U ? and U ? to one step of SGD (or SGD wrapped inside SWA).</p><p>We compare our method to dropout <ref type="bibr">(Srivastava et al., 2014)</ref>, a popular regularization method that can improve generalization in neural networks. Like our approach, dropout produces a single model at the end of training. We also consider its Monte Carlo variant (MC-dropout; <ref type="bibr" target="#b15">Gal &amp; Ghahramani, 2016)</ref>, and the recently proposed BatchEnsemble <ref type="bibr">(Wen et al., 2020)</ref>. This method generates an ensemble using rank-1 matrices as described in Section 2.2. Predictions still need to be averaged over multiple models, but this averaging step can be parallelized in modern hardware.</p><p>Additionally, we report single-seed results obtained with an ensemble of K independently-trained models (a deep ensemble, <ref type="bibr" target="#b41">Lakshminarayanan et al., 2017)</ref>. Deep ensembles provide a strong baseline, at the expense of large computational and memory costs. Therefore, they are not directly comparable to the other methods considered here, and serve the purpose of an upper baseline. By contrast, augmenting the architectures considered here with late-phase weights results in negligible additional costs during learning (with the exception of hypernetworks, which require additional tensor products) and none during testing. In principle, a set of independently-trained models yielded by our algorithm can therefore even be used as the basis of a deep ensemble, when the memory and compute budget allows for one. We present proof-of-concept experiments exploring this option.</p><p>Throughout our CIFAR-10/100 experiments we set K = 10, use a fast base gradient scale factor of ? ? = 1, and set our late-phase initialization hyperparameters to T 0 = 120 (measured henceforth in epochs; T 0 = 100 for SWA) and do not use initialization noise, ? 0 = 0. These hyperparameters were tuned manually once on CIFAR-100 and then kept fixed unless otherwise noted. We use standard learning rate scheduling, optimized for SGD and SWA on the base model (cf. Appendices A and B). Last-layer weights are included by default in our late-phase weight set ?.</p><p>CIFAR-10. For CIFAR-10 we focus on the WRN architecture, a high-performance residual network (WRN; Zagoruyko &amp; Komodakis, 2016) which features BatchNorm layers. Taking advantage of this we implement a late-phase weight model consisting of BatchNorm shift and scale parameters.</p><p>All algorithms achieve a training error close to zero (cf. Appendix B). The resulting predictive accuracies are shown in <ref type="table" target="#tab_0">Table 1</ref>. We find that augmenting the WRN 28-10 (a standard WRN configuration) with BatchNorm late-phase weights leads to a systematic improvement in generalization, reducing the gap with a deep ensemble of K = 10 models. Initializing our ensemble from the onset (T 0 = 0) fails to meet the performance of the base model, reaching only 95.68 ? 0.23% (cf. Appendix 12).</p><p>We also investigate initializing a late-phase (full) deep ensemble at T 0 = 120. This results in a test set accuracy of 96.32?0.09%, in between late-phase BatchNorm weights and no late-phase weights at all. This speaks to the data-efficiency of our low-dimensional late-phase ensembles which can be trained with as little data as a single model, besides being memory efficient.</p><p>In addition, we consider a larger instance of the WRN model (the WRN 28-14), trained for 300 epochs using cutout data augmentation <ref type="bibr" target="#b9">(DeVries &amp; Taylor, 2017)</ref>, as well as a small convolution neural network without skip connections, cf. <ref type="table" target="#tab_3">Table 3</ref>. When late-phase weights are employed in combination with SWA, we observe significant accuracy gains on the WRN 28-14. Thus, our latephase weights impose an implicit regularization that is effective on models with many weights. Similarly, we observe larger gains when training on a random subset of CIFAR-10 with only 10 4 examples (cf. Appendix B). 82.01 ?0.17 83.62 -CIFAR-100. We next turn to the CIFAR-100 dataset, which has 10-fold less examples per class and more room for improvements. We study the WRN 28-10, as well as the larger WRN 28-14 variant (using cutout data augmentation as before) and a PyramidNet <ref type="bibr" target="#b20">(Han et al., 2017)</ref> with ShakeDrop regularization <ref type="bibr">(Yamada et al., 2019)</ref>. The latter are trained for 300 epochs.</p><p>Predictive accuracy is again highest for our neural networks with late-phase weights, trained with SGD or SWA, cf.  <ref type="bibr" target="#b13">(Fort et al., 2020)</ref> can provide benefits that cannot be captured by ensembling models in a small neighborhood. Notably, SWA can achieve high predictive accuracy with a large constant learning rate . We reproduce these previous results and show that they improve when learning with late-phase weights, cf. <ref type="figure">Fig. 2</ref>. Substantial progress is made both when entering the latephase learning period and when activating SWA.</p><p>Out-of-distribution (OOD) generalization. Deep ensembles are an effective technique for improving the behavior of neural networks in OOD data <ref type="bibr" target="#b41">(Lakshminarayanan et al., 2017)</ref>. We ask whether our implicit ensembles modeled during late-phase learning could confer a similar advantage to our final averaged model.   <ref type="bibr">(2015)</ref>; Tiny ImageNet; CIFAR-10) and present them to a WRN 28-10 trained on CIFAR-100. We use Shannon's entropy <ref type="bibr" target="#b8">(Cover &amp; Thomas, 2006)</ref> to measure the uncertainty in the output predictive distribution, which should be high for OOD and low for CIFAR-100 data. Overall performance is summarized using the area under the receiver operating characteristics curve (AUROC), averaged over all datasets. We report per-dataset results in Appendix B <ref type="table" target="#tab_0">(Table 16</ref>) alongside experiments measuring robustness to corruptions in the input data <ref type="bibr" target="#b23">(Hendrycks &amp; Dietterich, 2019)</ref>.</p><p>We compare our results to alternative methods with strong uncertainty representation: MC-dropout <ref type="bibr" target="#b15">(Gal &amp; Ghahramani, 2016)</ref>, SWA-Gaussian (SWAG; <ref type="bibr">Maddox et al., 2019)</ref> and <ref type="bibr">BatchEnsemble (Wen et al., 2020)</ref>. All three methods require integrating predictions over an ensemble at test time.</p><p>We find that learning with late-phase weights increases prediction uncertainty in OOD data, allowing for a significantly better separation between in and out-of-distribution examples, cf.  <ref type="figure">Figure 3</ref>: Flatness score. Mean score ? std. over 5 seeds, WRN 28-10, CIFAR-100, SGD, with and without Batch-Norm late-phase weights. Slower increase with ? z is better.</p><p>Despite our improved performance on both predictive accuracy (with late-phase BatchNorm) and OOD discrimination (with late-phase BatchNorm and hypernetwork embeddings), the test set negative log-likelihood (NLL; often used to assess predictive uncertainty, <ref type="bibr" target="#b18">Guo et al., 2017)</ref> is surprisingly slightly worse for our solutions. This is aligned with the finding that SWA does not always significantly reduce NLL, even though predictive accuracy increases <ref type="bibr">(Maddox et al., 2019)</ref>.</p><p>Flatness. Why do our networks generalize better? Approximate Bayesian inference suggests that flat minima generalize better than sharp minima <ref type="bibr" target="#b25">(Hochreiter &amp; Schmidhuber, 1997a;</ref><ref type="bibr" target="#b48">MacKay, 1992)</ref>. Due to symmetries that are present in neural networks there is some debate surrounding this argument <ref type="bibr" target="#b10">(Dinh et al., 2017)</ref>, but current evidence seems favorable <ref type="bibr" target="#b33">(Jiang et al., 2020)</ref>.</p><p>We hypothesize that sharing base weights over K late-phase weight configurations can implicitly lead to flatter solutions. To investigate whether our algorithm finds flatter minima, we examine a simple flatness score that correlates well with generalization <ref type="bibr" target="#b3">(Pittorino et al., 2020;</ref><ref type="bibr" target="#b33">Jiang et al., 2020)</ref>. Concretely, we add multiplicative Gaussian noise z i ? N (0, w 2 i ? 2 z ) to each weight w i and then measure the change in the loss ?L = E z [L(w + z) ? L(w)]. Our final weight configurations are indeed in flatter regions of weight space according to this measure: ?L increases more slowly with ? z for the WRN 28-10 models that are learned with BatchNorm late-phase weights, <ref type="figure">Fig. 3</ref>. To investigate whether our gains translate to large-scale learning problems, we train deep residual networks <ref type="bibr" target="#b22">(He et al., 2016)</ref> and a densely-connected convolutional network (DenseNet; <ref type="bibr" target="#b28">Huang et al., 2018)</ref> on the ImageNet dataset <ref type="bibr">(Russakovsky et al., 2015)</ref>. We start from pretrained models and contrast BatchNorm late-phase weight learning to fine-tuning with SGD for 20 epochs, with ? ? = 1/K and ? 0 = 0 (cf. Appendix A). For simplicity we do not include last-layer weights in ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IMAGENET EXPERIMENTS</head><p>Fine-tuning with late-phase weights improves the final top-1 validation accuracy of this pretrained model significantly with only minor training, as seen in <ref type="table" target="#tab_7">Table 5</ref>. These results serve as a proof-ofconcept that existing models can be further improved, taking our late-phase initialization T 0 as the time the previous experimenter stopped training. In Appendix B, we present additional CIFAR-100 experiments where we apply late-phase learning starting at the suboptimal end-of-training T 0 = 200, to mimic the pretrained condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LSTM LANGUAGE MODELING EXPERIMENTS</head><p>Finally, we conduct experiments on the language modeling benchmark enwik8. To show that the benefits of late-phase weights extend to recurrent neural networks, we augment a standard LSTM with multiplicative late-phase weights consisting of rank-1 matrices (Wen et al., 2020, cf. Section 2.2). Overfitting is a major issue when training LSTMs. Recent studies have shown that by leveraging vast amounts of computation and smart black-box optimizers <ref type="bibr" target="#b17">(Golovin et al., 2017)</ref>, properly regularized LSTMs can outperform previously published state-of-the-art models <ref type="bibr">(Melis et al., 2017)</ref>. To avoid this issue, we train models where the number of parameters (?1.56M) is drastically smaller than the number of training data points (90M), such that we do not observe any overfitting. Thus, we do not apply any regularization. This helps minimize the effects of hyperparameter tuning. Our only hyperparameter is the learning rate (0.001 here), which we tune via grid search to maximize base model performance.</p><p>We train our LSTM with 500 units for 50 epochs, optimizing every weight with Adam (Kingma &amp; <ref type="bibr" target="#b38">Ba, 2015)</ref>. We apply a multiplicative rank-1 matrix elementwise to the recurrent weight matrix. Interestingly, merely adding the multiplicative parameters to the LSTM (Base) accelerates training and leads to better training and test set performance (measured in bits per character, BPC) with no additional changes to the optimizer (Base + Rank1, <ref type="table" target="#tab_8">Table 6</ref>). Further improvements can be achieved with our late-phase weights. We generate K = 10 late-phase weight components at epoch 30 with ? 0 = 0.35 and set ? ? = 1. Additionally, we find that SWA (starting at epoch 40) substantially improves all scores, with smaller gains on the models with multiplicative weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Our late-phase weights define an ensemble with the special property that every model shares the same base weights. Such parameter sharing is an established method for ensembling neural networks while controlling for the memory and time complexity of learning <ref type="bibr" target="#b44">(Lee et al., 2015)</ref>. In designing our late-phase weight models, we draw directly from recent work which proposes sharing a set of base parameters over K rank-1 matrices <ref type="bibr">(Wen et al., 2020)</ref> or K heads <ref type="bibr" target="#b44">(Lee et al., 2015)</ref>.</p><p>The elastic averaging SGD algorithm learns K neural networks in parallel, coupled through an additional central model <ref type="bibr">(EASGD;</ref><ref type="bibr" target="#b21">Zhang et al., 2015)</ref>. Like our algorithm, EASGD often yields solutions which generalize better than those found by standard SGD <ref type="bibr" target="#b3">(Pittorino et al., 2020)</ref>. Our latephase weight learning is intimately related to EASGD, as we optimize the performance of a central model through an ensemble. However, thanks to parameter sharing and late-phase ensembling, we do not find the need to introduce a coupling term to our loss function. Additionally, as we replicate a small number of parameters only, the complexity of our algorithm is greatly reduced in comparison to EASGD, which requires learning a full ensemble of models.</p><p>Splitting the weights of a neural network into a set of fast and slow components which vary on different timescales is a classic technique <ref type="bibr" target="#b24">(Hinton &amp; Plaut, 1987;</ref><ref type="bibr">Schmidhuber, 1992</ref>) that has proven useful in a wide range of problems. This list includes applications to few-shot learning <ref type="bibr">(Munkhdalai &amp; Yu, 2017;</ref><ref type="bibr">Nichol et al., 2018;</ref><ref type="bibr">Perez et al., 2018;</ref><ref type="bibr">Zintgraf et al., 2019;</ref><ref type="bibr" target="#b12">Flennerhag et al., 2020</ref><ref type="bibr">), optimization (Zhang et al., 2019b</ref><ref type="bibr" target="#b7">Chaudhari et al., 2019)</ref>, improving recurrent neural networks <ref type="bibr" target="#b2">(Ba et al., 2016;</ref><ref type="bibr" target="#b19">Ha et al., 2017)</ref>, and continual learning with biologically-realistic synapses <ref type="bibr" target="#b36">(Kaplanis et al., 2018;</ref><ref type="bibr" target="#b45">Leimer et al., 2019)</ref>, to name a few. Although there is no explicit separation of timescales in our weight components, the update accumulation in ? as ? k varies (cf. Algorithm 1) suggests interpreting the base ? as slow weights and the late-phase ? as fast weights.</p><p>This accumulation is reminiscent of a recent meta-learning algorithm <ref type="bibr">(Zintgraf et al., 2019)</ref>, which first separates parameters into task-shared and task-specific, and then differentiates through a sequence of accumulated updates performed over the task-specific parameters <ref type="bibr" target="#b11">(Finn et al., 2017)</ref>. Continuing with the fast-slow weight analogy, our averaging over fast weights at the end of learning (Eq. 1) could be thought of as a synaptic consolidation step which integrates the fast weight components onto a slow, persistent form of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed to replicate and learn in parallel a subset of weights in a late phase of neural network learning. These late-phase weights define an ensemble of models which share every other weight. We studied convolutional neural networks, a common recurrent neural network, and a simple quadratic problem. Surprisingly, across these cases, we found that a small number of appropriately chosen such weights can quickly guide SGD towards solutions that generalize well. Most of our experiments relied on BatchNorm late-phase weights, making our method easy to implement in a wide range of existing models, including pretrained ones. We expect future work to uncover new effective late-phase weight models.  <ref type="formula" target="#formula_1">(2019)</ref>, who studied high-performing linear hypernetwork architectures for WRNs. We do not use dropout or biases in the convolutional layers. The parameters of every convolutional layer are hypernetwork-generated, with one hypernetwork per layer group <ref type="table" target="#tab_10">(Table 7)</ref>. The remaining parameters, namely those of BatchNorm units and final linear layer weights, are non-hypernetwork-generated.</p><p>Following Savarese &amp; Maire (2019) we turn off weight decay for the model embeddings and initialize these parameters with a random pseudo-orthogonal initialization over layers. The hypernetwork parameters are initialized using a standard Kaiming initialization <ref type="bibr" target="#b21">(He et al., 2015)</ref>. Small ConvNet model. We train a slight modification of the classic LeNet-5 <ref type="bibr" target="#b42">(Lecun et al., 1998)</ref> for 200 epochs on CIFAR-10. Both convolutional and fully-connected layers are left unchanged, but we use rectified linear units on the hidden layers. Furthermore, after each such activation, Batch-Norm units are inserted. We optimize the model with SGD and use late-phase BatchNorm weights, with T 0 = 50 and ? 0 = 0.5. For simplicity of implementation, we do not include the last linear layer in the late-phase weight set ?.</p><p>Optimization. We optimize the cross-entropy loss, using either SGD with Nesterov momentum (0.9) or SGD with Nesterov momentum (0.9) wrapped inside SWA  <ref type="figure">Figure 4</ref>: Pseudocode for a single parameter update for SWA and SGD with Nesterov momentum, the two main optimizers used in our experiments. These are either used standalone, or as U ? and U ? in Algorithm 1 (main text). U in Algorithm 2 (SWA) serves as a placeholder for a parameter update rule such as SGD (with Nesterov momentum) or Adam. Training iteration t is counted from the activation of SWA in Algorithm 1.</p><formula xml:id="formula_5">Training iteration t M ? Sample minibatch from D ?? ? ? ? L(M, ?) ? ? U (?, ?, ??) ? SWA ? (t ? SWA + ?)/(t + 1) t ? t + 1 Algorithm 3: SGD with Nesterov mo- mentum Require: Base weights ?, dataset D, learning rate ?, momentum ?, loss L M ? Sample minibatch from D ?? ? ? ? L(M, ? + ? ?) ? ? ?? ? ? ?? ? ? ? + ?</formula><p>Batch normalization units. Whenever we use SWA, we follow  and perform a full pass over the training set to re-estimate BatchNorm unit statistics before testing. This correction is required since the online BatchNorm mean and variance estimates track the activations produced with the raw (non-averaged) weights during training, while the averaged solution is the one used when predicting at test time.</p><p>Data augmentation and preprocessing. On both CIFAR and ImageNet datasets, all images are normalized channelwise by subtracting the mean and dividing by the standard deviation; both statistics are computed on the training dataset. The same transformation is then applied when testing, including to OOD data. Following a standard procedure (e.g., <ref type="bibr">Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b22">He et al., 2016)</ref> we augment our training datasets using random crops (with a 4-pixel padding for CIFAR) and random horizontal flips. The ImageNet training dataset is augmented with random horizontal flips, as well as random cropping of size 224, while a centered cropping of size 224 was used on the test set. Our OOD datasets are resized to fit whenever necessary; we used the resized images made available by <ref type="bibr" target="#b43">Lee et al. (2018)</ref>.</p><p>ImageNet experiments. The pretrained model for the ImageNet experiment is obtained from torchvision's models subpackage. We fine-tune the model for 20 additional epochs on ImageNet.</p><p>Algorithm 4: Late-phase learning Require: Base weights ?, late-phase weight set ?, dataset D, gradient scale factor ? ? , learning rate ?, ensemble size K, initialization noise ? 0 , initialization time T 0 , number of training iterations T , loss L Initialization: <ref type="figure">Figure 5</ref>: Complete pseudocode for an entire training session using late-phase weights. To avoid notational clutter T , T 0 and t are measured in numbers of minibatches consumed. In the paper, we measure T 0 and T in epochs. For simplicity, we present the case where U ? and U ? are set to plain SGD (without momentum) and ? k of dimension 1. Other optimization algorithms (e.g., Algorithm 2 or Algorithm 3) can be used to replace U ? and U ? , as described in Algorithm 1. Note that we increase t inside the inner loop. This highlights (i) that every specialist parameter is trained only on 1/K data samples after t &gt; T 0 compared to ?, and (ii) that we count every minibatch drawn from the data to compare fairly to algorithms without an inner loop.</p><formula xml:id="formula_6">K ? 0, s ? 0, t ? 1 while t ? T do if t = T 0 then // generate late-phase weights for 1 ? k ? K do sample ? N (0, 1) ? k ? ? 0 + ?0 ?0 // set range for specialists trainin? K ? K s ? 1 for s ? k ?K do M k ? Sample minibatch from D ?? k ? ? ? L(M k , ?, ? k ) ? k ? ? k ? ? ? ? k L(M k , ?, ? k ) t ? t + 1 ? ? ? ? ? ? ? K k=1 ?? k</formula><p>We use a multistep learning rate scheduler, starting at 0.001 then decreasing at the 10th epoch to 0.0001. We use SGD with momentum (set to 0.9) and weight decay (set to 0.0001) as our optimizer, with a batch size of 256. We use ? 0 = 0 and K = 10 for our late-phase model. Code forks. Our hypernetwork implementation was inspired by the code made publicly available by <ref type="bibr">Savarese &amp; Maire (2019)</ref>. Our implementation of SWA was adapted from the code accompanying the work of , now available on the torchcontrib Python package. The SWAG method was evaluated directly using the code provided by the authors <ref type="figure" target="#fig_2">(Maddox et al., 2019)</ref>. We used the same base WRN model as <ref type="bibr">Maddox et al. (2019)</ref>, which can be retrieved from https://github.com/ meliketoy/wide-resnet.pytorch.</p><p>LSTM All experiments are conducted using the Tensorflow Python framework <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. All base weights are initialized uniform in [?0.01, 0.01] whereas the initial rank-1 matrix weights are centered around 1 i.e. [1 ? 0.01, 1 + 0.01] to mitigate strong difference in initialization compared to the base model. We use the Tensorflow default values (? 1 = 0.9, ? 2 = 0., = 10 ?8 ) for the Adam optimiser. We perform a grid search over ? 0 ? [0, 0.5] (in steps of size 0.05) for our LSTM experiments (fixing K = 10 and varying T 0 ? {0, 30}) and obtain the values reported in the main text, T 0 = 30 and ? 0 = 0.35.  <ref type="figure">Figure 6</ref>: Sensitivity analysis of T 0 . Mean AUROC score (OOD) and test set accuracy for different values of T 0 for WRN 28-10, CIFAR-100, SGD, with BatchNorm late-phase weights. Pretrained CIFAR-100. We apply our method to a standard WRN 28-10 pretrained on CIFAR-100 (i.e., we set T 0 = 200) and train for an additional 20 epochs. At the beginning of the finetuning, the learning rate is reset to 0.01, then annealed linearly to 0.001 for 10 epochs. It is then held constant for the remainder of the fine-tuning process. We observe that augmenting with BatchNorm late-phase weights yields an improved predictive accuracy compared to additional fine-tuning with SGD (Base), cf. <ref type="table" target="#tab_13">Table 9</ref>. Both methods improve over the initial baseline (Initial), including the base model. This can be explained by the optimization restart and the accompanying spike in the learning rate introduced by our scheduler <ref type="bibr" target="#b47">(Loshchilov &amp; Hutter, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EXPERIMENTS</head><p>Importantly, we find that fine-tuning only BatchNorm late-phase weights while keeping all other weights fixed does not even match the Base control. Together with the finding that the optimal latephase weight initialization time is at T * 0 = 120 (when learning for 200 epochs), this result speaks to the importance of jointly optimizing both base and late-phase weights through our Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient accumulation control.</head><p>Here we show that the improved generalization we report in the main text is not merely due to gradient accumulation over larger batches. We take our base WRN 28-10 model (without late-phase weights) and start accumulating gradients over K = 10 minibatches at T 0 = 120, experimenting both with ? ? = 1/K and ? ? = 1. The models are trained with SGD using otherwise standard optimization settings. Both controls fail to improve (even match) the performance of the base model trained without any gradient accumulation.</p><p>Sensitivity to T 0 , K and ? 0 . We present a hyperparameter exploration on the CIFAR-100 dataset using BatchNorm late-phase weights in <ref type="table" target="#tab_0">Tables 8, 11</ref> and 12. We find that our algorithm is largely  robust to ? 0 when T 0 can be set to its optimal value, which is at 60% of training. See also <ref type="figure">Figure  6</ref> for a visualisation of the same data, specifically the change in mean AUROC score and test set accuracy when changing T 0 . This result holds also on CIFAR-10, cf. <ref type="table" target="#tab_0">Table 12</ref>. When starting from a pretrained condition (T 0 = 200), finite ? 0 leads to a significant improvement in performance, cf. <ref type="table" target="#tab_0">Table 11</ref>. We therefore report results obtained with ? 0 = 0 for every CIFAR and ImageNet experiment in the main text. The exception to this is the non-averaged (ensemble) late-phase BatchNorm model presented in <ref type="table" target="#tab_5">Table 4</ref>, which was optimized for best OOD performance (corresponding to ? 0 = 0.5).  Related work. Here we provide details for the training setups of alternative methods we compare against in the main text. For the results reported for dropout <ref type="bibr">(Srivastava et al., 2014)</ref> and MC-dropout <ref type="bibr" target="#b15">(Gal &amp; Ghahramani, 2016)</ref>, we simply train a WRN 28-10 on CIFAR-100 with the exact same configuration as for our base model, see above, but include dropout layers as usually done <ref type="bibr">(Zagoruyko &amp; Komodakis, 2016)</ref> after the first convolution in each residual block. For a scan over the dropout probability p in this setup, see <ref type="table" target="#tab_0">Table 13</ref>. p = 0.2 is reported in the main text -for CIFAR-100 and CIFAR-10. Note that p was only tuned for CIFAR-100.</p><p>For the reported results of BatchEnsemble <ref type="bibr">(Wen et al., 2020)</ref>, we simply execute the code provided by the authors at https://github.com/ google/uncertainty-baselines with their fine-tuned configuration for CIFAR-10/100. Notably, the authors use a different setup than followed in this manuscript. First, the WRN 28-10 is trained for 250 epochs (we allow for this increased budget exceptionally for BatchEnsemble), with a multi-step learning rate annealing at <ref type="bibr">[80,</ref><ref type="bibr">160,</ref><ref type="bibr">180]</ref> with a learning rate decay factor of 0.2. Second, a weight decay of 3 ? 10 ?4 is used.  For the results reported for SWAG <ref type="bibr">(Maddox et al., 2019)</ref>, we use the code provided by the authors at https://github.com/wjmaddox/swa_gaussian, and the proposed fine-tuned configuration which coincides with the configuration used to obtain all CIFAR-100 results reported in this manuscript, except for BatchEnsembles (see above). We report results for SWAG after training on 200 epochs for fair comparison.</p><p>Training losses. We provide the final achieved training losses for the base model and when augmenting it with BatchNorm late-phase weights on <ref type="table" target="#tab_0">Table 14</ref>, for both CIFAR-10 and CIFAR-100. Using a fast gradient accumulation scale factor of ? ? = 1 leads to a higher training loss on CIFAR-100 than that of the standard model, but we found this setting crucial to achieve the largest improvement on test set generalization.</p><p>CIFAR-10 with a reduced training set. Here we evaluate the performance of our method on a reduced training set of CIFAR-10. We randomly pick 10000 training data out of the 50000 available, and use this new set to train different models. After training, the models are evaluated on the standard CIFAR-10 test set. Results are shown in <ref type="table" target="#tab_0">Table 15</ref>.</p><p>Detailed OOD results and mean corruption error (mCE) experiments. In order to test the robustness of late-phase weights against input data corruption, we used the corruptions and dataset proposed by <ref type="bibr" target="#b23">Hendrycks &amp; Dietterich (2019)</ref>, freely available at https://github.com/ hendrycks/robustness. The authors propose 15 noise sources such as random Gaussian noise, spatter or contrast changes to deform the input data and report the model test set accuracy on the corrupted dataset under 5 severity levels (noise strengths). For each source noise, its corruption error is computed by averaging the prediction error over the severity levels. The average of the corruption error of all 15 noises gives us the Mean Corruption Error (mCE). See <ref type="table" target="#tab_0">Table 16</ref> for the mCE computed on the corrupted CIFAR-100 dataset.</p><p>Training run time. Here we compare the training run time of our method with the baseline. The result was computed in Python 3.7, using the automatic differentiation and GPU acceleration package PyTorch (version 1.4.0). We used the standard datasets (including training and test splits) as provided by the torchvision package unless stated otherwise. We used a single NVIDIA GeForce 2080 Ti GPU for the experiment. Results are presented in <ref type="table" target="#tab_0">Table 17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C THEORETICAL ANALYSIS OF THE NOISY QUADRATIC PROBLEM</head><p>In this section, we consider a noisy quadratic problem (NQP) that can be theoretically analyzed and that captures important characteristics of the stochasticity of a minibatch-based optimizer <ref type="bibr">(Schaul et al., 2013;</ref><ref type="bibr">Martens, 2016;</ref><ref type="bibr">Wu et al., 2018;</ref><ref type="bibr">Zhang et al., 2019a;</ref><ref type="bibr">b)</ref>. The NQP does a second-order Taylor expansion of the loss function around the optimum w * and models the minibatch noise as  </p><formula xml:id="formula_7">w ? w * + 1 ? B ) T H(w ? w * + 1 ? B )<label>(4)</label></formula><p>with ? N (0, ?) and B the minibatch size. Note that we use boldface notation for vectors in this analysis for notational clarity. The NQP can be seen as an approximation of the loss function in the final phase of learning, where we initialize the late-phase ensemble. Despite its apparent simplicity, it remains a challenging optimization problem that has important similarities with stochastic minibatch training in deep neural networks <ref type="bibr">(Schaul et al., 2013;</ref><ref type="bibr">Martens, 2016;</ref><ref type="bibr">Wu et al., 2018;</ref><ref type="bibr">Zhang et al., 2019a;</ref><ref type="bibr">b)</ref>. For the simple loss landscape of the NQP, there are three main strategies to improve the expected loss after convergence: (i) increase the mini-batch size B (Zhang et al., 2019a), (ii) use more members K in an ensemble (c.f. Section C.3 and (iii) decrease the learning rate ? <ref type="bibr">(Schaul et al., 2013;</ref><ref type="bibr">Martens, 2016;</ref><ref type="bibr">Wu et al., 2018;</ref><ref type="bibr">Zhang et al., 2019a;</ref><ref type="bibr">b)</ref>. The late-phase weights training combines the two first strategies in a non-trivial manner by (i) averaging over the base-weights gradients for all ensemble members and (ii) averaging the late-phase weights in parameter space to obtain a mean-model. The goal of this theoretical analysis is to show that the expected loss after convergence scales inversely with the number of late-phase ensemble members K, which indicates that the non-trivial combination of the two strategies is successful.</p><p>To model the multiplicative weight interaction between late-phase weights and base weights, we use linear hypernetworks of arbitrary dimension. The linear hypernetworks parameterize the weights as w = ?e, with ? ? R n?d the hypernetwork parameters and e ? R d the embedding vector. The embedding vectors e are used as late-phase weights (? in the main manuscript) to create a latephase ensemble with K members, while using a shared hypernetwork ? as base-weights: w k = ?e k . Ultimately, we are interested in the expected risk of the the mean model at steady state:</p><formula xml:id="formula_8">E[L (ss) ] = E ?ss [ 1 2 (w ? w * ) T H(w ? w * )]<label>(5)</label></formula><p>withw 1 K k ?e k = ? 1 K k e k ?? and ? ss the steady-state distribution of the parameters. Note that we cannot put w * = 0 without loss of generality, because the overparameterization of the hypernetworks makes the optimization problem nonlinear.</p><p>We start with investigating the discrete time dynamics induced by late-phase learning, after which we derive the corresponding continuous time dynamics to be able to use the rich stochastic dynamical systems literature for analyzing the resulting nonlinear stochastic dynamical system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 DISCRETE TIME DYNAMICS</head><p>As we want to investigate the multiplicative interaction between the shared and late-phase parameters, we substitute w = ?e into equation 4, instead of computing a new Taylor approximation in the hypernetwork parameter space. Let us take t as the index for the outer loop (updating ?) and k the index for the ensemble member. Then we have the following stochastic minibatch loss:</p><formula xml:id="formula_9">L (t,k) = 1 2 (? (t) e (t) k ? w * + 1 ? B (t,k) ) T H(? (t) e (t) k ? w * + 1 ? B (t,k) ),<label>(6)</label></formula><p>which gives rise to the following parameter updates using late-phase learning with learning rate ? and minibatch size B: </p><formula xml:id="formula_10">? (t+1) = ? (t) ? ? 1 K k H(? (t) e (t) k ? w * )e (t)T k + ? ? B 1 K k H (t,k) e (t)T k (7) e (t+1) k = e (t) k ? ?? (t)T H(? (t) e (t) k ? w * ) + ? ? B ? (t)T H (t,k)<label>(8)</label></formula><formula xml:id="formula_11">? t vec(? t ) (11) x t [? T t , e T t ] T (12) t [ (t,1)T . . . (t,K)T ] T ,<label>(13)</label></formula><p>where vec(?) concatenates the columns of ? in a vector. Then the discrete time dynamics (equation 7 and equation 8) can be rewritten as:</p><formula xml:id="formula_13">x t+1 = x t ? ?F (x t ) + ? ? B G(x t ) t (15) with F (x t ) 1 K k e (t) k ? H ? t e (t) k ? w * I ? (? T t H? t ) e t ? 1 ? (? T t Hw * ) (16) G(x t 1 K E t ? H I ? (? T t H)<label>(17)</label></formula><p>with ? the Kronecker product, I an identity matrix of the appropriate size and 1 a vector full of ones of the appropriate size. As a linear transformation of Gaussian variables remains a Gaussian variable, we can rewrite eq. equation 15 as follows:</p><formula xml:id="formula_15">x t+1 = x t ? ?F (x t ) + ? ? B D(x t )? t<label>(19)</label></formula><p>with D(x t ) G(x t )(I ? ?)G(x t ) T 0.5 and ? ? N (0, I). Following Liu &amp; Theodorou (2019) and <ref type="bibr" target="#b6">Chaudhari &amp; Soatto (2018)</ref>, the corresponding continuous-time dynamics are:</p><formula xml:id="formula_16">dx t = ?F (x t )dt + 2? ?1 D(x t )dW t<label>(20)</label></formula><p>with W t Brownian motion and ? 2B ? the inverse temperature. Note that ? ? is incorporated in the noise covariance, such that the correct limit to stochastic continuous time dynamics can be made <ref type="bibr" target="#b46">(Liu &amp; Theodorou, 2019;</ref><ref type="bibr" target="#b6">Chaudhari &amp; Soatto, 2018;</ref><ref type="bibr">but see Yaida, 2018)</ref>. For computing the expected loss E[L t ] of the mean model, we need to have the stochastic dynamics of this loss. Using the It? lemma <ref type="bibr" target="#b30">(It?, 1951;</ref><ref type="bibr" target="#b46">Liu &amp; Theodorou, 2019)</ref>, which is an extension of the chain rule in the ordinary calculus to the stochastic setting, we get</p><formula xml:id="formula_17">dL(x t ) = ? ?L(x t ) T F (x t ) + 1 2 Tr D H LD dt + ?L(x t ) TD dW t<label>(21)</label></formula><p>withD 2? ?1 D(x t ) for notational simplicity and H L the Hessian of L w.r.t. x t . As we are interested in the expected risk (equation 5), we can take the expectation of equation 21 over the parameter distribution ? t (x) to get the dynamics of the first moment of the loss (also known as the backward Kolmogorov equation <ref type="bibr" target="#b39">(Kolmogorov, 1931)</ref>):</p><formula xml:id="formula_18">dE ?t L(x t ) = E ?t ? ?L(x t ) T F (x t ) + 1 2 Tr D 2 H L dt<label>(22)</label></formula><p>In order to obtain the dynamics of the parameter distribution, the Fokker-Planck equation can be used <ref type="bibr">(Jordan et al., 1998)</ref>. However, due to the nonlinear nature of the stochastic dynamical system, the distribution is non-Gaussian and it is not possible (to our best knowledge) to obtain an analytical solution for equation 22. Nevertheless, we can still gain important insights by investigating the steady-state of equation 22. After convergence, the left-hand side (LHS) is expected to be zero. Hence, we have that</p><formula xml:id="formula_19">E ? ss ?L(x ss ) T F (x ss ) = 1 2 E ? ss Tr[D 2 H L ]<label>(23)</label></formula><p>The remainder of our arguments is structured as follows. First, we will show that the left-hand-side (LHS) of equation 23 is the expectation of an approximation of a weighted norm of the gradient ?L, after which we will connect this norm to the loss L of the mean model. Second, we will investigate the RHS to show that the late-phase learning with ensembles lowers the expected risk of the NQP at steady-state. For clarity and ease of notation, we will drop the ss subscripts. The gradient of the mean-model loss is given by:</p><formula xml:id="formula_20">?L(x) = ? ? H ?? ? w * 1 K 1 ? ? T H?? ? w *<label>(24)</label></formula><p>By introducing ?e k e k ?? and using that k ?e k = 0, we can rewrite F (x) as:</p><formula xml:id="formula_21">F (x) = I 0 0 KI ?L(x) + (? ? H)? I ? (? T H?) ?e<label>(25)</label></formula><p>with ? 1 K k ?e k ?e T k and ?e T [e T 1 ...e T K ]. We see that F is an approximation of the gradient ?L where the lower block of ?L is scaled by K. Importantly, the lower block of the second element of the RHS of equation 25 (the approximation error) will disappear when taking the inner product with ?L and the upper block is not influenced by the number of ensemble members K, which we will need later. The LHS of equation 23 can now be rewritten as: </p><p>with M the diagonal matrix of equation 25 (first element of the RHS). The first term of the RHS of equation 26 is the expectation of a weighted squared norm of ?L, while the second term is an approximation error due to the covariance of ?e k . Hence, we see that the LHS of equation 23 can be seen as an approximation of a weighted norm of the gradient ?L. By investigating the term ?L(x) T M ?L(x) further, we show that it is closely connected to the loss L.</p><p>?L <ref type="formula">(</ref> with ? the covariance matrix of . For an appropriate ?, the above equations converge to the following fixed points at steady-state:</p><formula xml:id="formula_23">E ? ss w = 0 (36) vec C ? ss w = ? 2 KB I ? (I ? ?H) ? (I ? ?H) ?1 vec H?H)<label>(37)</label></formula><p>We see that the steady-state covariance ofw and hence of the risk L scales with 1 K (E ? ss [L] = E ? ss [w T Hw] = Tr HC ? ss [w] ). The expected risk E ? ss [L] obtained with computationally expensive full ensembles can be seen as a lower limit that we try to reach with the economical ensembles of shared weights ? and late-phase weights ? k . Note that for the NQP, increasing the batchsize B has a similar influence as increasing the number of ensemble members K, as can be seen in equation 37.</p><p>Continuous time stochastic dynamics. We can also do a similar continuous time analysis as Section C.2 for the case of full ensembles, to better relate it to the results of the late-phase learning with shared parameters. Following the same approach, we get the following expression for the trace term:</p><p>1 2 E ? ss Tr[D 2 H L ] = Tr 1 ? I ? (H?H) 1</p><formula xml:id="formula_24">K 2 1 ? H (38) = 1 K? Tr H?H 2 ]<label>(39)</label></formula><p>When comparing to equation 31, we see that the economical ensembles with shared parameters reach the same scaling with 1 K as a result of ensembling, however, some extra terms that vanish asymptotically for big K appear as a result of the interplay between shared and late-phase parameters.</p><p>Experimental details for <ref type="figure" target="#fig_2">Fig. 1</ref>. We take the model w = ? ? (i.e., K = 1) as our baseline, since this overparameterization could already result in accelerated learning <ref type="bibr" target="#b1">(Arora et al., 2018)</ref>. Our parameters are randomly initialized and scaled such thatw has a fixed distance to w * of 1. Since the NQP mimics a late phase of learning we set T 0 = 0. We study a problem of dimension n = 100 and train the model with gradient descent (without momentum).</p><p>To validate the theoretical results, we show in <ref type="figure" target="#fig_2">Fig. 1</ref> that the steady-state reached by our method scales inversely with K, similarly to an ensemble of independently-trained models. We run experiments with K ? <ref type="bibr">[2,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">25]</ref> and train every configuration for 2 ? 10 7 iterations until convergence. We average over the last 10 4 weight updates and over 5 different random seeds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Steady-state loss for varying K, of multiplicative late-phase weights (Ours) compared to an ensemble of models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>E</head><label></label><figDesc>? ss ?L(x) T F (x) = E ? ss ?L(x) T M ?L(x) + E ? ss Tr[H??H(?? ? w * )? T ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>x) T M ?L(x) = (w ? w * ) T (? T? H 2 + H?? T H)(w ? w * )(27)and covariance of the parameters. Taking the expectation and variance of equation 33 results in:E w (t+1) = (I ? ?H)E w (t) (34) C w (t+1) = (I ? ?H)C w (t) (I ? ?H)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CIFAR-10, WRN 28-10. Mean ? std. over 5 seeds. Late-phase BatchNorm (LPBN).</figDesc><table><row><cell>Model</cell><cell>Test acc. (%)</cell></row><row><cell>Base (SGD)</cell><cell>96.16 ?0.12</cell></row><row><cell>Dropout (SGD)</cell><cell>96.02 ?0.06</cell></row><row><cell>MC-Dropout (SGD)</cell><cell>96.03 ?0.09</cell></row><row><cell>BatchEnsemble (SGD)</cell><cell>96.19 ?0.18</cell></row><row><cell>Late-phase (SGD)</cell><cell>96.46 ?0.15</cell></row><row><cell>Base (SWA)</cell><cell>96.48 ?0.04</cell></row><row><cell>Late-phase (SWA)</cell><cell>96.81 ?0.07</cell></row><row><cell>Deep ensemble (SGD)</cell><cell>96.91</cell></row><row><cell cols="2">Deep ensemble (LPBN, SGD) 96.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean CIFAR-100 test set accuracy (%) ? std. over 5 seeds, WRN 28-10. Different latephase weight augmentations are compared to the base architecture and to an upper bound consisting of an ensemble of models. Deep ens. stands for deep ensemble, LPBN for late-phase BatchNorm. Base BatchNorm Hypernetwork Deep ens. Deep ens. (LPBN) SGD 81.35 ?0.16 82.87 ?0.22 81.55 ?0.31 84.09 84.69 SWA 82.46 ?0.09 83.06 ?0.08</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>. We observe that the simplest BatchNorm late-phase weight model reaches the highest accuracy, with late-phase hypernetwork weight embeddings yielding essentially no im- provements. Once again, the setting of T 0 = 0 (onset ensemble learning) fails to match base model performance, finishing at 80.26 ? 0.42% test accuracy. As for CIFAR-10, a late-phase full deep en- semble only reached intermediate improvements, at 82.17?0.15% test accuracy. Furthermore, a gap towards deep ensembles persists. This suggests that covering different modes of the loss</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Additional architectures, CIFAR-10 (C10) and CIFAR-100 (C100). Mean test set acc. ? std. over 3 seeds (%). Late-phase BatchNorm weights. ?0.23 77.94 ?0.37 C10 WRN 28-14 (SWA) 96.75 ?0.05 97.45 ?0.10 C100 WRN 28-14 (SWA) 84.01 ?0.29 85.00 ?0.25 C100 PyramidNet (SGD) 84.04 ?0.28 84.35 ?0.14 The final averaged solutions found with late-phase weights are strong base models to build a deep ensemble of independently-trained networks. The fact that our algorithm yields a single model allows further pushing the upper bound of what can be achieved when unrestricted full ensemble training is possible. This improvement comes at no cost compared to a standard deep ensemble.We train additional neural network architectures restricting our experiments to the BatchNorm latephase weight model, which can be readily implemented without architectural modifications. Again, learning with late-phase weights yields a consistent improvement over the baseline, cf.Table 3.</figDesc><table><row><cell></cell><cell>Base</cell><cell>Late-phase</cell></row><row><cell>C10 ConvNet (SGD)</cell><cell>77.41</cell></row></table><note>(%) ? std. over 5 seeds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Additionally, we evaluate the performance of a late-phase weight ensemble obtained with large initialization noise ? 0 = 0.5 (at T 0 = 100), skipping the final weight averaging step. This requires integrating predictions over K late-phase ensemble members at test time, y(x) = 1 Unlike standard deep ensembles, training this ensemble is still as cheap as training a single model.</figDesc><table><row><cell>K</cell><cell>K k=1 y(x, w</cell></row></table><note>k ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>CIFAR-100, WRN-28-10, uncertainty representation results. Mean ? std. over 5 seeds (except deep ensembles). This first group of methods yield a single model; the second group requires test-time averaging over models while training as efficiently as K=1; the last group are full deep ensembles which require training K=10 models from scratch (Deep ens.). We report in-distribution test set acc. (%) and negative log-likelihood (NLL), and in-distribution vs. out-of-distribution (OOD) discrimination performance (average AUROC over four OOD datasets, see main text). ?0.0048 0.8285?0.0189   Late-phase BatchNorm (SGD, non-averaged) 82.71 ?0.10 0.7512 ?0.0069 0.8624 ?0.0094We draw novel images from a collection of datasets(SVHN, Netzer et al. (2011); LSUN, Yu et al.</figDesc><table><row><cell>Test acc. (%) Test NLL</cell><cell>OOD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The OOD performance of late-phase BatchNorm weights compares favorably to the alternative methods including deep ensembles, even when using a single weight-averaged model, while maintaining high predictive accuracy. Remarkably, keeping the late-phase BatchNorm ensemble at test time allows reaching the highest OOD performance throughout. Paired with non-zero initialization noise ? 0 &gt; 0 (cf. Appendix B), this method results in the best OOD performance.</figDesc><table><row><cell></cell><cell>15</cell><cell>K=1</cell></row><row><cell>?L/L</cell><cell>5 10</cell><cell>K=10</cell></row><row><cell></cell><cell>0.00</cell><cell>0.05</cell><cell>0.10</cell></row><row><cell></cell><cell></cell><cell>? z</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Validation set acc. (%) on ImageNet. Mean ? std. over 5 seeds. BatchNorm late-phase and baseline trained for 20 epochs with SGD. ?0.06 76.87 ?0.03 ResNet-152 78.31 78.37 ?0.01 78.77 ?0.01 DenseNet-161 77.65 78.17 ?0.01 78.31 ?0.01</figDesc><table><row><cell></cell><cell>Initial Base</cell><cell>Late-phase</cell></row><row><cell>ResNet-50</cell><cell>76.15 76.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>enwik8 results measured in bits per character (BPC), LSTM with 500 units. Mean over 5 seeds, with std. ? &lt; 0.01 for all results.</figDesc><table><row><cell>Model</cell><cell>Train Test Test (SWA)</cell></row><row><cell>Base</cell><cell>1.570 1.695 1.626</cell></row><row><cell>Base + Rank1</cell><cell>1.524 1.663 1.616</cell></row><row><cell cols="2">Late-phase Rank1 1.522 1.633 1.615</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Joshua J. Waterfall, Fergal P. Casey, Ryan N. Gutenkunst, Kevin S. Brown, Christopher R. Myers, Piet W. Brouwer, Veit Elser, and James P. Sethna. Sloppy-model universality class and the Vandermonde matrix. Physical Review Letters, 97(15):150601, October 2006. Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In International Conference on Machine Learning, 2011. Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: an alternative approach to efficient ensemble and lifelong learning. In International Conference on Learning Representations, 2020.</figDesc><table><row><cell>Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in</cell></row><row><cell>stochastic meta-optimization. In International Conference on Learning Representations, March</cell></row><row><cell>2018.</cell></row><row><cell>Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint</cell></row><row><cell>arXiv:1810.00004, 2018.</cell></row><row><cell>Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, and Koichi Kise. Shakedrop regularization</cell></row><row><cell>for deep residual learning. IEEE Access, 7:186126-186136, 2019.</cell></row><row><cell>7693-7702,</cell></row><row><cell>May 2019.</cell></row><row><cell>A ADDITIONAL IMPLEMENTATION DETAILS</cell></row><row><cell>Hypernetwork model. The base neural network architecture we use when parameterizing our</cell></row><row><cell>weights using a hypernetwork is identical to the WRN 28-10 described by Zagoruyko &amp; Komodakis</cell></row><row><cell>(2016). Our hypernetwork implementation closely follows Savarese &amp; Maire</cell></row></table><note>Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016. Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? Insights from a noisy quadratic model. In Advances in Neural Information Processing Systems 32, pp. 8196-8207. 2019a. Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead Optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems 32, pp. 9597-9608. 2019b. Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging SGD. In Advances in Neural information Processing Systems, pp. 685-693, 2015. Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: its behavior of escaping from sharp minima and regularization effects. arXiv preprint arXiv:1803.00195, 2018.Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, pp.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Specification of the hypernetwork used for each convolutional layer of the WRN, indexed by its depth in the network. A depth marked by * refers to the residual connection spanning across the specified layers. The characteristics of each layer is described in the format input-channels ? [kernel-size] ? output-channels under Conv-layer. Layers within the same group are generated by the same hypernetwork. Each hypernetwork has a unique parameter tensor of shape Hnet-PS, which, when multiplied by a layer and weight embedding of shape Emb-PS and reshaped appropriately, generates the primary network parameter of shape Base-PS.</figDesc><table><row><cell>Depth</cell><cell>Conv-layer</cell><cell>Base-PS</cell><cell cols="2">Layer group Hnet-PS</cell><cell>Emb-PS</cell></row><row><cell>1</cell><cell>3?[3?3]?16</cell><cell>[16, 3, 3, 3]</cell><cell>0</cell><cell>[16, 3, 3, 3, 10]</cell><cell>[10, 1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>. LSTM: Our LSTM experiments use Adam with constant learning rate 0.001, batch size 128, and no regularizers such as weight decay or dropout. WRN-28-10: For our WRN experiments on the CIFAR datasets we use the learning rate annealing schedule of, according to which an initial learning rate of 0.1 is linearly decreased at every epoch from the end of the 100th epoch (80th for SWA) to the end of the 180th epoch (144th for SWA; SWA is activated at epoch 160), when a final value of 0.001 (0.05 for SWA) is reached. Our optimizers use Nesterov momentum (set to 0.9), a batch size of 128 and weight decay (set to 0.0005). On CIFAR-100 (SGD) we set the weight decay of late-phase weights proportional to the ensemble size, 0.0005K. WRN-28-14: The WRN 28-14 models are trained for 300 epochs on CIFAR-100. The learning rate is initialized at 0.1, then annealed to 0.05 from the 80th epoch to the 240th epoch. SWA is activated at epoch 160. All other hyperparameters are identical to those of WRN 28-10. ConvNet: Same as for the WRN 28-10 model, except that we anneal the learning rate until the 160th epoch.</figDesc><table><row><cell>Algorithm 2: Stochastic weight averaging</cell></row><row><cell>(SWA)</cell></row><row><cell>Require: Base weights ?, dataset D,</cell></row><row><cell>hyperparameter ?, loss L</cell></row><row><cell>Require:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>CIFAR-100 test set accuracy (%) depending on different values of K for WRN 28-10, SGD. Mean ? std. over 5 seeds.</figDesc><table><row><cell cols="2">K Test acc. (%)</cell></row><row><cell>1</cell><cell>81.35 ?0.16</cell></row><row><cell>5</cell><cell>82.44 ?0.22</cell></row><row><cell cols="2">10 82.87 ?0.22</cell></row><row><cell cols="2">15 83.01 ?0.27</cell></row><row><cell cols="2">20 82.86 ?0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Applying late-phase weights to a pretrained WRN 28-10, CIFAR-100, SGD. Mean ? std. over 5 seeds. Late-phase BatchNorm, frozen base weight 81.50?0.20    </figDesc><table><row><cell>Model</cell><cell>Test acc. (%)</cell></row><row><cell>Initial</cell><cell>81.35 ?0.16</cell></row><row><cell>Base</cell><cell>81.47 ?0.14</cell></row><row><cell>Late-phase BatchNorm</cell><cell>82.02 ?0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Gradient accumulation control, CIFAR-100, WRN 28-10, SGD. Mean ? std. over 5 seeds.</figDesc><table><row><cell>Model</cell><cell>Test acc. (%)</cell></row><row><cell>Base (SGD)</cell><cell>81.35 ?0.16</cell></row><row><cell>Base + gradient accumulation (? ? = 1)</cell><cell>80.76 ?0.26</cell></row><row><cell cols="2">Base + gradient accumulation (? ? = 1/K) 80.34 ?0.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>CIFAR-100 test set accuracy (%) depending on different values of ? 0 for WRN 28-10 SGD with late-phase BatchNorm weights (LPBN). Mean ? std. over 5 seeds.</figDesc><table><row><cell></cell><cell>CIFAR-100</cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell>CIFAR-100</cell></row><row><cell></cell><cell>(LPBN)</cell><cell></cell><cell cols="2">(LPBN, non-averaged)</cell><cell>(LPBN, pretrained)</cell></row><row><cell>? 0</cell><cell cols="2">Test acc. (%) OOD</cell><cell cols="2">Test acc. (%) OOD</cell><cell>Test acc. (%) OOD</cell></row><row><cell>0</cell><cell>82.87 ?0.22</cell><cell cols="2">0.833 ?0.005 83.20 ?0.20</cell><cell cols="2">0.854 ?0.017 81.70 ?0.19</cell><cell>0.803 ?0.017</cell></row><row><cell cols="2">0.25 82.77 ?0.19</cell><cell cols="2">0.836 ?0.012 82.68 ?0.32</cell><cell cols="2">0.861 ?0.013 82.02 ?0.12</cell><cell>0.808 ?0.017</cell></row><row><cell>0.5</cell><cell>82.78 ?0.18</cell><cell cols="2">0.837 ?0.011 82.71 ?0.10</cell><cell cols="2">0.862 ?0.009 81.15 ?0.29</cell><cell>0.797 ?0.007</cell></row><row><cell cols="2">0.75 82.41 ?0.20</cell><cell cols="2">0.839 ?0.012 82.43 ?0.15</cell><cell cols="2">0.855 ?0.013 -</cell><cell>-</cell></row><row><cell>1.0</cell><cell>81.52 ?1.09</cell><cell cols="2">0.840 ?0.017 82.38 ?0.15</cell><cell cols="2">0.848 ?0.014 -</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Performance of a WRN 28-10 on CIFAR-100 with different dropout probability p. For MC-dropout we average over 10 different samples. Mean ? std. over 5 seeds.</figDesc><table><row><cell></cell><cell>p</cell><cell cols="2">Test acc. (%) Test NLL</cell><cell>OOD</cell></row><row><cell>Dropout</cell><cell cols="2">0.1 81.46 ?0.13</cell><cell>0.7476 ?0.0059 0.8031 ?0.0064</cell></row><row><cell>Dropout</cell><cell cols="2">0.2 81.31 ?0.20</cell><cell>0.7736 ?0.0025 0.8022 ?0.0299</cell></row><row><cell>Dropout</cell><cell cols="2">0.3 80.93 ?0.19</cell><cell>0.8342 ?0.0098 0.7833 ?0.0239</cell></row><row><cell cols="3">MC-Dropout 0.1 81.51 ?0.14</cell><cell>0.7197 ?0.0054 0.8149 ?0.0087</cell></row><row><cell cols="3">MC-Dropout 0.2 81.55 ?0.11</cell><cell>0.7105 ?0.0026 0.8225 ?0.0488</cell></row><row><cell cols="3">MC-Dropout 0.3 81.36 ?0.31</cell><cell>0.7150 ?0.0069 0.8040 ?0.0135</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12</head><label>12</label><figDesc>?0.23 74.38 ?0.71 40 96.34 ?0.08 79.69 ?0.11 60 96.42 ?0.10 80.53 ?0.21 80 96.50 ?0.11 81.72 ?0.18 100 96.45 ?0.08 82.48 ?0.21 120 96.48 ?0.20 82.87 ?0.22 140 96.26 ?0.17 82.53 ?0.21 160 96.23 ?0.11 81.41 ?0.31 180 96.25 ?0.23 81.43 ?0.27 200 96.16 ?0.12 81.35 ?0.16</figDesc><table><row><cell></cell><cell>: CIFAR-10 and CIFAR-100 test</cell></row><row><cell cols="2">set accuracy (%) depending on different late</cell></row><row><cell cols="2">phase timing T 0 for WRN 28-10, SGD. Mean</cell></row><row><cell cols="2">? std. over 5 seeds.</cell></row><row><cell>T 0</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell>0</cell><cell>95.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Final training set loss on CIFAR datasets, WRN 28-10, SGD. Mean ? std. over 5 seeds.</figDesc><table><row><cell>Training loss</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 :</head><label>15</label><figDesc>Performance of models trained on a reduced CIFAR-10 training set and evaluated on the full CIFAR-10 test set. Mean ? std. over 5 seeds.</figDesc><table><row><cell>Model</cell><cell>Test acc. (%)</cell></row><row><cell>Base (SGD)</cell><cell>88.98 ?0.18</cell></row><row><cell cols="2">Late-phase BN (SGD) 89.58 ?0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 16 :</head><label>16</label><figDesc>OOD performance measured by the AUROC, and robustness measured by the Mean Corruption Error (mCE). We train the models on CIFAR-100 and attempt to discriminate test set images from novel ones drawn from the SVHN, LSUN, Tiny ImageNet (TIN) and CIFAR-10 dataset. The mCE value is the average across 75 different corruptions from the CIFAR-100-C dataset. LPBN and LP HNET stand respectively for late-phase BatchNorm and late-phase hypernetwork.?0.024 0.798 ?0.036 0.776 ?0.038 0.818 ?0.003 47.84 ?0.41 LPBN 0.831 ?0.021 0.862 ?0.017 0.838 ?0.023 0.814 ?0.002 45.59 ?0.25 LPBN (non-avg.) 0.877 ?0.008 0.883 ?0.015 0.863 ?0.023 0.827 ?0.002 46.21 ?0.29 LP HNET 0.815 ?0.022 0.842 ?0.023 0.816 ?0.027 0.811 ?0.002 47.84 ?0.42 ?0.093 0.807 ?0.040 0.788 ?0.044 0.822 ?0.003 48.97 ?0.33 MC-Dropout 0.806 ?0.082 0.842 ?0.046 0.817 ?0.041 0.824 ?0.003 48.09 ?0.36 SWAG 0.824 ?0.012 0.839 ?0.054 0.835 ?0.041 0.816 ?0.004 -BatchEnsemble 0.848 ?0.020 0.828 ?0.018 0.820 ?0.030 0.829 ?0.019 -</figDesc><table><row><cell>SVHN</cell><cell>LSUN</cell><cell>TIN</cell><cell>CIFAR-10</cell><cell>mCE</cell></row><row><cell>Base 0.814 Dropout (Mean) 0.792 Deep ens. 0.839</cell><cell>0.836</cell><cell>0.812</cell><cell>0.839</cell><cell>44.21</cell></row><row><cell>Deep ens. (LPBN) 0.855</cell><cell>0.884</cell><cell>0.856</cell><cell>0.834</cell><cell>43.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 17</head><label>17</label><figDesc></figDesc><table><row><cell cols="4">: Training time in seconds and hours on CIFAR-10 for 200 epochs on a single NVIDIA</cell></row><row><cell>GeForce 2080 Ti GPU.</cell><cell></cell><cell></cell></row><row><cell cols="3">Model</cell><cell>seconds hours</cell></row><row><cell cols="3">Base (SGD)</cell><cell>17714</cell><cell>? 4.92</cell></row><row><cell cols="4">Late-phase BN (SGD) 17772</cell><cell>? 4.94</cell></row><row><cell cols="4">a random translation of the optimum, while keeping the curvature H the same. This gives us the</cell></row><row><cell>following minibatch loss:L</cell><cell></cell><cell></cell></row><row><cell>=</cell><cell>1 2</cell><cell>(</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>The above discrete time dynamics are nonlinear, giving rise to a non-Gaussian parameter distribution ?. Hence, it is not possible to characterize these dynamics by the moment-propagating equations of the first and second moment as done inZhang et al. (2019a;b);Schaul et al. (2013)  andWu et al.  (2018), without having full access of the parameter distribution ?. Furthermore, because of the hypernetwork parameterization, we cannot decouple the system of equations, even if H and ? are diagonal, which is a common approach in the literature. Therefore, we investigate the corresponding continuous time dynamics, such that we can use the rich literature on stochastic dynamical systems.C.2 CONTINUOUS TIME DYNAMICSFirst, let us define some compact notations for the various parameters.</figDesc><table><row><cell>e t [e (t)T 1</cell><cell>. . . e (t)T K ] T</cell><cell>(9)</cell></row><row><cell cols="2">E t [e (t) 1 . . . e (t) K ]</cell><cell>(10)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the Swiss National Science Foundation (B.F.G. CRSII5-173721 and 315230 189251), ETH project funding (B.F.G. ETH-20 19-01), the Human Frontiers Science Program (RGY0072/2019) and funding from the Swiss Data Science Center (B.F.G, C17-18, J.v.O. P18-03). Jo?o Sacramento was supported by an Ambizione grant (PZ00P3 186027) from the Swiss National Science Foundation. We would like to thank Nicolas Zucchet, Simon Schug, Xu He, Angelo Cardoso and Angelika Steger for feedback, Mark van Rossum for discussions on flat minima, Simone Surace for his detailed feedback on Appendix C, and Asier Mujika for providing very useful starter code for our LSTM experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When comparing to the mean-model loss L = (w ? w * ) T H(w ? w * ) we see that the two are tightly connected, both using a weighted distance measure betweenw and w * , with only a different weighting. Taken everything together, we see that we can take the LHS of equation 23 (and hence also the RHS) as a rough proxy for the expected risk under the steady-state distribution (equation 5), which will be important to investigate the influence of the amount of ensemble members on the expected risk. <ref type="bibr">Zhu et al. (2018)</ref> highlighted this trace quantitiy in equation 23 as a measurement of the escaping efficiency out of poor minima. However, we assume that we are in the final valley of convergence (emphasized by this convex NQP), so now this interpretation does not hold and the quantity should be considered as a proxy measurement of the width of the steady-state parameter distribution around the minimum. The trace quantity has H L and D(x ss ) 2 as main elements, which we structure in block matrices below (for clarity and ease of notation, we drop the subscripts ss).</p><p>with 1 a matrix or vector of the appropriate size full of ones,? 1/K k e k and the rows of Q ? R d?nd given by:</p><p>with ? i the i-th column of an appropriately sized identity matrix. After some intermediate calculations and rearranging of terms, we reach the following expression for the RHS of equation 23:</p><p>with? 2 1 K k e k e T k = 1 K EE T Note that everything between the big brackets in the RHS is independent of K in expectation. Hence, we see that the RHS of equation 23 scales inversely by K, exactly as the case for full ensembles (see Section C.3). Importantly, the approximation errors in equation 25 are independent of K, hence, the found scaling of 1 K in equation 31 translates to a scaling of 1 K of the expected risk of the NQP, following the above argumentation. Hence, we see that the non-trivial combination of (i) averaging over the base-weights gradients for all ensemble members and (ii) averaging the late-phase weights e k in parameter space to obtain a mean-model, succeeds in scaling the expected loss after convergence inversely by K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 NQP WITH FULL ENSEMBLES</head><p>As a comparison for the above theoretical results, we also analyze the NQP that uses an ensemble of K full weight configurations w k to get a mean modelw, instead of shared weights ? and ensemblemember-specific weights ? k . For the case of linear models, the averaging in weight space to obtain a mean model is equivalent to the averaging of the predictions over the ensemble, which is conventionally done using ensembles. Without loss of generality, we can take w * = 0 (corresponding with a simple reparameterization of w). Using equation 4, this results in the following parameter updates for the ensemble members:</p><p>The mean modelw 1 K k w k has the following corresponding discrete dynamics:</p><p>Exact moment propagating equations. As this is a discrete linear system with Gaussian noise, the resulting parameter distributions will also be linear and can be fully characterized by the mean</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the optimization of deep networks: implicit acceleration by overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shaping the learning landscape in neural networks around wide flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Pittorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="170" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical mechanical approaches to models with many poorly known parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Sethna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21904</biblScope>
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory and Applications Workshop (ITA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entropy-SGD: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124018</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hujun Yin, and Raia Hadsell. Meta-learning with warped gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02757</idno>
		<title level="m">Deep ensembles: a loss landscape perspective</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00152</idno>
		<title level="m">Training batchnorm and only batchnorm: on the expressive power of random features in CNNs</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Google vizier: a service for black-box optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02915</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the Cognitive Science Society</title>
		<meeting>the ninth annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note>Snapshot ensembles: train 1, get M for free</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On stochastic differential equations. Number 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyosi</forename><surname>It?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiplicative interactions and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fantastic generalization measures and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The variational formulation of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Otto</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fokker-Planck equation</title>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1998" />
			<publisher>Publisher: SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continual reinforcement learning with complex synapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kaplanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Clopath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mudigere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On analytical methods in probability theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Nikolaevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kolmogorov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Ann</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="415" to="458" />
			<date type="published" when="1931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Why M heads are better than one: training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Synaptic weight decay with selective consolidation enables fast learning without catastrophic forgetting. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Leimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Senn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="page">613265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep learning theory review: An optimal control and dynamical systems perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Horng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos A</forename><surname>Theodorou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10920</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>2 16?[3?3]?160 [160, 3, 3, 16] 1 [160, 3, 3, 16, 7. 7, 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
