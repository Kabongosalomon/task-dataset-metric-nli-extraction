<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion Guided 3D Pose Estimation from Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
							<email>jbwang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">AWS/Amazon AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Motion Guided 3D Pose Estimation from Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Pose Estimation</term>
					<term>Motion Loss</term>
					<term>Graph Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new loss function, called motion loss, for the problem of monocular 3D Human pose estimation from 2D pose. In computing motion loss, a simple yet effective representation for keypoint motion, called pairwise motion encoding, is introduced. We design a new graph convolutional network architecture, U-shaped GCN (UGCN). It captures both short-term and long-term motion information to fully leverage the additional supervision from the motion loss. We experiment training UGCN with the motion loss on two large scale benchmarks: Human3.6M and MPI-INF-3DHP. Our model surpasses other state-ofthe-art models by a large margin. It also demonstrates strong capacity in producing smooth 3D sequences and recovering keypoint motion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose estimation aims at reconstructing 3D body keypoints from thier 2D projections, such as images <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b26">26]</ref>, videos <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b35">35]</ref>, 2D pose <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b15">15]</ref>, or their combination <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b34">34]</ref>. Unlike the 2D pose estimation, this problem is illposed in the sense that the lack of depth information in the 2D projections input leads to ambiguities. To obtain the perception of depth, recent works <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b28">28]</ref> utilized multiple synchronized cameras for observing objects from different angles and has achieved considerable progress. However, compared with monocular methods, multi-view methods are not practical in reality because of their strict prerequisites for devices and environments.</p><p>Recent years, video-based 3D human pose estimation <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b5">5]</ref> receives attention quickly. Taking a video as input, models are able to perceive the 3D structure of an object in motion and better infer the depth information for each moment. It significantly promotes the estimation performance under the monocular camera. Unlike image-based models, video-based models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b2">2]</ref> are supervised by a long sequence of 3D pose, which increase the dimensionality of solution space by hundreds of times. In most existing works, the common loss function for supervising 3D pose estimation models is Minkowski Distance, such as 1 -loss and 2 -loss. It independently computes the overall location error of the predicted keypoints in 3D space with respect to their ground-truth locations.</p><p>There is a critical limitation for the Minkowski Distance. It does not consider the similarity of temporal structure between the estimated pose sequence and the arXiv:2004.13985v1 [cs.CV] <ref type="bibr" target="#b29">29</ref> Apr 2020</p><p>x Time Pendulum</p><p>Motion Loss L1 Loss <ref type="figure">Fig. 1</ref>. A toy sample, the location estimation of pendulum motion. We show the horizontal location as time varies, a sine curve, denoted in gray, and three estimated traces, denoted in blue, orange and cyan. They have the same 1 mean distance to the groundtruth but have different temporal structure. Which estimated trace better describes the pendulum motion? The loss under different matrices is also shown in the right figure. Obviously, motion loss is good at answering the above question.</p><p>groundtruth. We illustrate this issue by a toy sample, the trace estimation of a pendulum motion. It is similar to pose estimation, but only includes one "joint". We compare three estimated trajectories of pendulum motion in <ref type="figure">Figure.</ref>1. The first trace function has a shape similar to the groudtruth. The second one has a different tendency but still keep smoothness. And the last curve just randomly fluctuates around the groudtruth. Both of them have the same 1 mean distance to the groudtruth but have various temporal structures. Because the Minkowski Distance is calculated independently for each moment, it failed to examine the inner dependencies of a trajectory. The keypoints in a pose sequence describe the human movement, which are strongly correlated especially in the time. Under the supervision of Minkowski Distance as loss, same as the above toy sample, it is difficult for models to learn from the motion information in the groundtruth keypoint trajectories and thus hard to obtain natural keypoints movement in the model's prediction due to the high dimensional solution space.</p><p>We address this issue by proposing motion loss, a novel loss function that explicitly involves motion modeling into the learning. Motion loss works by requiring the model to reconstruct the keypoint motion trajectories in addition to the task of reconstructing 3D locations of keypoints. It evaluates the motion reconstruction quality by computing the difference between predicted keypoint locations and the ground-truth locations in the space of a specific representation called motion encoding. The motion encoding is built as a differentiable operator in the following manner. We first roughly decompose a trajectory into a set of pairwise coordinate vectors with various time intervals corresponding to different time scales. A basic differentiable binary vector operator, for instance, subtraction, inner product or cross product, is applied to each pair. Then the obtained results are concatenated to construct the full motion encoding. Though simple, this representation is shown in the <ref type="figure">Figure 1</ref> (taking subtraction operator for example) to be effective in assessing the quality of the temporal structure. The difference in motion loss values clearly distinguishes the motion reconstruction quality of the three trajectories. By applying it to the training of 3D pose estimation models, we also observe that motion loss can significantly improve the accuracy of 3D pose estimation.</p><p>To estimate the pose trajectories with reasonable human movements, the 3D pose estimation model must have the capacity to model motion in both short temporal intervals and long temporal ranges, as human actions usually have varying speeds over time. To achieve this property we propose a novel graph convolutional network based architecture for 3D pose estimation model. We start by repurposing an ST-GCN <ref type="bibr" target="#b38">[38]</ref> model, initially proposed for skeletonbased action recognition, to take as input 2D pose sequences and output 3D pose sequences. Inspired by the success of U-shaped CNNs used in semantic segmentation and object detection, we construct a similar U-shaped structure on the temporal axis of the ST-GCN <ref type="bibr" target="#b38">[38]</ref> model. The result is a new architecture, called U-shaped GCN (UGCN), with strong capacity in capturing both shortterm and long-term temporal dependencies, which is essential in characterizing the keypoint motion.</p><p>We experiment the motion loss and UGCN for video-based 3D pose estimation from 2D pose on two large scale 3D human pose estimation benchmarks: Human3.6M <ref type="bibr" target="#b9">[9]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b18">[18]</ref>. We first observe a significant boost in position accuracy when the motion loss is used in training. This corroborates the importance of motion-based supervision. When the motion loss is combined with UGCN, our model surpasses the current state of the art models in terms of location accuracy by a large margin. Besides improved location accuracy, we also observe that UGCN trained with the motion loss is able to produce smooth 3D sequences without imposing any smoothness constraint during training or inference. Our model also halves the velocity error <ref type="bibr" target="#b27">[27]</ref> compared with other state of the art models, which again validates the importance of having motion information in the supervision. We provide detailed ablation study and visualization 1 to further demonstrate the potential of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>3D pose estimation. Before the era of deep learning, early methods for 3D human pose estimation were based on handcraft features <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b8">8]</ref>. In recent years, most works depend on powerful deep neural networks and achieve promising improvements, which can be divided into two types.</p><p>In the first type, estimators predict 3D poses from 2D images directly <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b33">33]</ref>. For example, <ref type="bibr" target="#b14">[14]</ref> jointly regresses joint locations and detects body parts by sliding window on the image. <ref type="bibr" target="#b35">[35]</ref> directly regresses the 3D pose from an aligned spatial-temporal feature map. <ref type="bibr" target="#b26">[26]</ref> predicts per voxel likelihoods for each joint based on the stacked hourglass architecture. <ref type="bibr" target="#b33">[33]</ref> utilizes an auto-encoder to learn a latent pose representation for modeling the joint dependencies.</p><p>Another typical solution build on a two-stage pipeline <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b15">15]</ref>. Thereon, a 2D pose sequence is firstly predicted by a 2D pose estimator from a video frame by frame and lifted to 3D by another estimator. For instance, <ref type="bibr" target="#b17">[17]</ref> proposes a simple baseline composed of several fully-connected layers, which takes as input a single 2D pose. <ref type="bibr" target="#b27">[27]</ref> generate 3D poses from 2D keypoint sequences by a temporal-convolution method. <ref type="bibr" target="#b2">[2]</ref> introduces a local-to-global network based on graph convolution. <ref type="bibr" target="#b15">[15]</ref> factorize a 3D pose sequence into trajectory bases and train a deep network to regress the trajectory coefficient matrix.</p><p>Although the appearance information is dropped in the first stage, the data dimension is dramatically decreased as well, which makes long-term video-based 3D pose estimation possible. Our method also builds on the two-stage pipeline.</p><p>Graph convolution. Modeling skeleton sequence via spatial-temporal graphs (st-graph) <ref type="bibr" target="#b38">[38]</ref> and performing graph convolution thereon have significantly boosted the performance in many human understanding tasks including action recognition <ref type="bibr" target="#b38">[38]</ref>, pose tracking <ref type="bibr" target="#b23">[23]</ref> and motion synthesis <ref type="bibr" target="#b37">[37]</ref>. The designs for graph convolution mainly fall into two stream: spectral based <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12]</ref> and spatial based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">22]</ref>. They extended standard convolution to irregular graph domain by Fourier transformation and neighborhood partitioning respectively. Following <ref type="bibr" target="#b38">[38]</ref>, we perform spatial graph convolution on skeleton sequences represented by st-graphs. <ref type="figure">Figure.</ref> 2 illustrates our pipeline for estimating 3D pose sequences. Given the 2D projections of a pose sequence estimated from a video P = {p t,j |t = 1, ..., T ; j = 1, ..., M }, we aim to reconstruct their 3D coordinates S = {s t,j |t = 1, ..., T ; j = 1, ..., M }, where T is the number of video frames, N is the number of human joints, p t,j and s t,j are vectors respectively representing the 2D and 3D locations of joint j in the frame t. We structure these 2D keypoints by a spatial-temporal graph and predict their 3D locations via our U-shaped Graph Convolution Net-  works (UGCN). The model is supervised by a multiscale motion loss and trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motion Loss</head><p>In this work, motion loss is defined as the distance in the space of motion. Therefore, a motion encoder is required for projecting skeleton sequences to this space. Though there are myriad possible designs, we are empirically sums up a few guiding principles: differentiability, nonindependence, and multiscale. Differentiability is the prerequisite for the end-to-end training. And the calculation should be across time for modeling the temporal dependencies, i.e., nonindependence. Since the speed of motion is different, multiscale modeling is also significant. In this section, we introduce how we design a simple but effective encoding, named pairwise motion encoding.</p><p>Pairwise motion encoding. We first consider the simplest case: the length of the pose sequences is 2. The motion encoding on the joint j can be denoted as:</p><formula xml:id="formula_0">m j = s 0,j s 1,j ,<label>(1)</label></formula><p>where can be any differentiable binary vector operator, such as subsection, inner-product and cross-prodcut. In the common case, the pose sequence is longer. We can expand an extra dimension in the motion encoding:</p><formula xml:id="formula_1">m t,j = s t,j s t+1,j .<label>(2)</label></formula><p>Note that, this representation only models the relationship between two adjacent moments. Since the speed of human motion has a large variation range, it inspires us to encode human motion on multiple temporal scales:</p><formula xml:id="formula_2">m t,j,? = s t,j s (t+? ),j .<label>(3)</label></formula><p>where ? is the time interval. As shwon in the <ref type="figure">Figure.</ref> 3, to cauculate the motion loss of the full pose sequence, we compute the ell 1 Distance on the encoded space for all joints, moments and several time intervals. Mathematically, we have:</p><formula xml:id="formula_3">L m = ? ?T T ?? t=1 M j=1 m t,j,? ? m gt t,j,? ,<label>(4)</label></formula><p>where the interval set T includes different ? for multiple time scales. Pairwise motion encoding decomposes a trajectory into coordinate pairs and extracts features for each pair by a differentiable operation . As the first work to explore the supervision of motion for 3D pose estimation, intuitively, we choose the three most basic operations in the experiments: subsection, inner-product, and cross-product. And we conducted extensive experiments to evaluate the effectiveness of these encoding methods in Section 4.3.</p><p>Loss Function. The motion loss only considers the second-order correlations in the formulation of pairwise motion encoding, while the absolute location information is absent. Therefore, we add a traditional reconstruction loss term to the overall training objectives:</p><formula xml:id="formula_4">L p = T t=1 M j=1 s t,j ? s gt t,j 2 2 .<label>(5)</label></formula><p>The model is supervised in an end-to-end manner with the combined loss:</p><formula xml:id="formula_5">L = L p + ?L m ,<label>(6)</label></formula><p>where ? is a hyper parameter for balancing two objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">U-shaped Graph Convolutional Networks</head><p>Intuitively, the 3D pose estimator needs stronger long-term perception for exploring the motion priors. Besides that, keeping the spatial resolution is also required by estimating 3D pose accurately. Therefore, we represent the skeleton sequence as a spatial temporal graph <ref type="bibr" target="#b38">[38]</ref> to maintain their topologies, and aggregating information by an U-shaped graph convolution network (UGCN).</p><p>Graph Modeling It is an ill-posed problem to recover the 3D location of a keypoint from its 2D coordinates independently. In general, the information from other keypoints, especially the neighboring ones, play essential roles in 3D pose reconstruction. To model the relationship with these relative keypoints, it is natural to organize a skeleton sequence via a spatial temporal graph (stgraph) <ref type="bibr" target="#b38">[38]</ref>. In particular, a st-graph G is determined by a node set and an edge set. The node set V = {v t,j |t = 1, . . . , T, j = 1, . . . , M } includes all the keypoints in a sequence of pose. And the edge set E is composed of two parts: one for connecting adjacent frames on each joint, one for the connecting endpoint of each bone in every single frame. These edges construct the temporal dependencies and spatial configuration together. Then, a series of graph convolution operations are conducted on this graph.</p><p>Graph Convolution. In this work, we adopt spatial temporal graph convolution (st-gcn) <ref type="bibr" target="#b38">[38]</ref> as the basic unit to aggregate features of nodes on a st-graph. It can be regarded as a combination of two basic operations: a spatial graph convolution and a temporal convolution. The temporal convolution Conv t is a standard convolution operation applied on the temporal dimension for each joint, while the spatial graph convolution Conv g is performed on the skeleton for each time position independently. Given an input feature map f in , the output of two operations can be written as:</p><formula xml:id="formula_6">f s = Conv g (f in ) (7) f out = Conv t (f s )<label>(8)</label></formula><p>, where f s is the output of the spatial graph convolution. Formally, we have:</p><formula xml:id="formula_7">f s (v t,j ) = vt,i?B(vt,j ) 1 Z t,j (v t,i ) f in (v t,i ) ? w(l t,j (v t,i )),<label>(9)</label></formula><p>where B(v t,j ) is the neighbor set of node v t,j , l t,j maps a node in the neighborhood to its subset label, w samples weights according to a subset label, and Z t,j (v t,i ) is a normalization term equivalent the cardinality of the corresponding subset. Since the human torso and limbs act in very different ways, it inspires us to give the model spatial perception for distinguishing the central joints and limbic joints. To make spatial configuration explicit in the 3D pose estimation, we divide the neighborhood into three subsets:</p><formula xml:id="formula_8">l t,j (v t,i ) = ? ? ? ? ? 0 if h t,j = h t,i 1 if h t,j &lt; h t,i 2 if h t,j &gt; h t,i<label>(10)</label></formula><p>, where h t,i is the number of hops from v t,i to the root node (i.e. central hip in this work).  <ref type="figure">Fig. 4</ref>. Network structure. We proposed a U-shaped graph convolution network (UGCN) as the backbone of our pose estimation model to incorporate both local and global information with a high resolution. This network consists of three stages: downsampling, upsampling and merging. The network first aggregates long-range information by temporal pooling operations in the downsampling stage. And then recovers the resolution by upsampling layers. To keep the low-level information, the features in the downsampling stage are also added to the upsample branch by some shortcuts. Finally, the multi-scale feature maps are merged to predicted 3D skeletal joints. In this way, UGCN incorporates both short-term and long-term information, making it an ideal fit for the supervision of the motion loss.</p><p>Network structure. As shown in <ref type="figure">Figure 4</ref>, the basic units for building networks are st-gcn blocks, which include five basic operations: a spatial graph convolution, a temporal convolution, a batch normalization, a dropout and an activation function ReLu. Our networks are composed of three stages: downsampling, upsampling, and merging.</p><p>In the downsampling stage, we utilize 9 st-gcn blocks for aggregating temporal features. In addition, we set stride = 2 for the second, fourth, sixth, and eighth st-gcn blocks to increase the receptive field in the time dimension. This stage embeds the global information of the full skeleton sequence.</p><p>The upsampling stage contains four st-gcn blocks. Each block is followed by an upsampling layer. Thanks to the regular temporal structure in st-graph, the upsampling in the time dimension can be simply implemented with the following formula:</p><formula xml:id="formula_9">f up (v t,j ) = f in (v t ,i ),<label>(11)</label></formula><p>where t = t 2 . With successive upsampling operations, the temporal resolution gradually recovers and the global information spread to the full graph. Since the 2D inputs are projections of 3D outputs, the low-level information may provide strong geometric constraints for estimating 3D pose. It motivated us to keep low-level information in the networks. Thus, we add features in the first stage to the upsampling stage with the same temporal resolution.</p><p>In the merging stage, the feature maps with various time scales in the second stage are transformed to the same shape and fused to obtain the final embedding.</p><p>Obviously, this embedding contains abundant information on multiple temporal scales.</p><p>In the end, the 3D coordinate for each keypoint is estimated by a st-gcn regressor. This model is supervised by the motion loss in an end-to-end manner. Other details have been depicted in the <ref type="figure">Figure 4</ref>.</p><p>Training &amp; inference. We use st-gcn blocks with the temporal kernel size of 5 and the dropout rate of 0.5 as our basic cells to construct a UGCN. The networks take as input a 2D pose sequence with 96 frames. We perform horizontal flip augmentation at the time of training and testing. It is supervised by a motion loss with ? = 1. We optimize the model using Adam for 110 epochs with the batch size of 256 and the initial learning rate of 10 ?2 . We decay the learning rate by 0.1 after 80, 90 and 100 epochs. To avoid the overfitting, we set the weight decay factor to 10 ?5 for parameters of convolution layers.</p><p>In the inference stage, we apply the sliding window algorithm with the step length of 5 to estimate a variable-length pose sequence with fixed input length, and average all results on different time positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate models on two large-scale datasets for 3D pose estimation: Hu-man3.6M and MPI-INF-3DHP. In particular, we first perform detailed ablation study on the Human3.6M dataset to examine the effectiveness of the proposed components. To exclude the interference of 2D pose estimator, all experiments in this ablation study take 2D ground truth as input. Then, we compare the estimated results of UGCN with other state-of-the-art methods on two datasets. All experiments are conducted on PyTorch tools with one single TITANX GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Human3.6M: Human3.6M <ref type="bibr" target="#b10">[10]</ref> is a large-scale indoor dataset for 3D human pose estimation. This widely used dataset consists of 3.6 million images which are captured from 4 different cameras. There are 11 different subjects and 15 different actions in this dataset, such as "Sitting", "Walking", and"Phoning". The 3D ground truth and all parameters of the calibrated camera systems are provided in this dataset. However, we do not exploit the camera parameters in the proposed approach. Following the recent works, we utilize (S1, S5, S6, S7, S8) for training and (S9, S11) for testing. The video from all views and all actions are trained by a single model. For this dataset, we conduct ablation studies based on the ground truth of 2D skeleton. Besides that, we also report the results of our approach taking as input predicted 2D poses. from widely used pose estimators. MPI-INF-3DHP: MPI-INF-3DHP <ref type="bibr" target="#b19">[19]</ref> is a recently released 3D human pose estimation dataset. And this dataset is captured in both indoor environment and in-the-wild outdoor environment. Similar to Human3.6M, this dataset also provides videos from different cameras, subjects, and actions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>For both Human3.6M and MPI-INF-3DHP dataset, we report the mean per joint position error(MPJPE) <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b2">2]</ref> as the evaluation metric. In general, there are two protocals, Protocal-1 and Protocal-2, used in the previous works to evaluate 3D pose estimation. Metric Protocol-1 first aligns the root joint(central hip) and then calculates the average Euclidean distance of the estimated joints. While in the Protocol-2, the estimated results are further aligned to the ground truth via a rigid transformation before computing distance.</p><p>In MPI-INF-3DHP, we evaluate models under two additional metrics. The first one is the area under the curve (AUC) <ref type="bibr" target="#b41">[41]</ref> on the percentage of correct keypoints(PCK) score for different error thresholds. Besides, PCK with the threshold of 150mm is also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we demonstrate the effectiveness of the proposed UGCN and our motion loss on the Human3.6M dataset. Experiments in this section directly take 2D ground-truth as input to eliminate the interference of 2D pose estimator.</p><p>Effect of motion loss. We start our ablation study from observing the impact of the temporal interval ? in the single scale motion loss. In other words, the interval set for motion loss has only one element. The value of this element controls the temporal scale of motion loss. We conduct experiments on three binary operators proposed in Section 3.1, i.e. subtraction, inner production and cross production.</p><p>As shown in <ref type="table">Table 1</ref>, the cross production achieves the lowest MPJPE error with almost all temporal intervals. Besides, the MPJPE error decrease first and then increase, and reduce the error by 4.9mm (from 32.0 to 27.1) with the time interval of 12 and the cross production encoding. There are two observations. First, compared to the result without motion term (denoted as ?), even the temporal interval is large (24 frames), the performance gain is still positives. It implies that motion prior is not momentary. And the model might need longterm perception for better capturing the motion information. Second, motion loss boosts the performance with temporal interval ? in a large variation range (2?36 frames), which means the time scale of motion priors is also various.</p><p>Thus, it is reasonable to adopt motion loss with multiple time intervals. We select four best ? as candidates and adopt the most effective binary operator in <ref type="table">Table.</ref> 1, cross production. The experimental results have been depicted in <ref type="table">Table.</ref> 2. Under the supervision of multiscale motion loss, our model decrease the MPJPE by 1.5mm <ref type="bibr">(27.1 ? 25.6)</ref>. <ref type="table">Table 3</ref>. We remove all downsampling and upsampling operations from the standard UGCN, and add them back pair by pair. The MPJPE performance of our system increases remarkably in this process. With motion loss, the achieved gain is even large.  Design choices in UGCN. We first examine the impact of the U-shaped architecture. We remove all downsampling and upsampling operations from the standard UGCN, and add them back pair by pair. The experimental results have been depicted in <ref type="table">Table.</ref> 3. It can be seen that U-shaped structure brings significant improvement (6.6mm) to UGCN. This structure even leads to a larger performance gain (9.8mm) with the supervision of motion loss. And the gap caused by motion loss is growing with the increasing number of downsampling and upsampling. These results validate our assumption: the motion loss requires long-term perception.</p><p>We also explore other design choices in the UGCN. As shown in <ref type="table">Table.</ref> 4, the spatial configuration bring 7.2mm improvement. Removing the merging stage only slightly enlarge the error. However, when the model is supervised by motion loss, the performance drop is more remarkable (0.5mm vs. 2.8mm). That is to say, multiscale temporal information is important to the learning of motion prior. Design choices in motion loss. The formula of offset encoding is similar to the Derivative Loss <ref type="bibr" target="#b30">[30]</ref> which regularizes the joint offset between adjacent frames. This loss is under the the hypothesis that the motion is smooth between the neighborhood frames. We extend it to our motion loss formulation. Since only short-term relation is considered, the improvement achieved by Derivative Loss is minor. Then we compare the results of our method supervised by the motion loss with different combination of the proposed binary operators. The results have been shown in <ref type="table">Table.</ref> 5. The combination of these three representations is not able to bring any improvement. Therefore, we adopt cross production as the pairwise motion encoder in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state-of-the-art</head><p>Results on Human3.6M In this section, we compare the proposed approach to several state-of-the-art algorithms in monocular 3D pose estimation from an agnostic camera on Human3.6M dataset. We trained our model on 2D poses predicted by cascaded pyramid network (CPN) <ref type="bibr" target="#b3">[3]</ref>. It is the most typical 2D estimator used in previous works. The results on two protocols are shown in the <ref type="table">Table 6</ref>. As shown in the table, our method achieves promising results on Human3.6 under two metrics(45.6 MPJPE on Protocal 1 and 35.5 P-MPJPE on Protocal 2 ) which surpass all other baselines. We also examine the result on a more powerful 2D pose estimator HR-Net <ref type="bibr" target="#b31">[31]</ref>. It further brings roughly 3mm MPJPE improvement. Several state-of-the-arts report their results on 2D ground-truth to explore their upper bound in 3D pose estimation. The results are illustrated in the <ref type="table">Table 7</ref>. It can be seen that our method achieves the best performance (25.6 MPJPE) outperforming all other methods with the ground-truth input. <ref type="table">Table 6</ref>. Results showing the errors action-wise on Human3.6M under Protocol-1 and Protocol-2. (CPN) and (HRNET) respectively indicates the model trained on 2D poses estimated by CPN <ref type="bibr" target="#b3">[3]</ref>, and HR-Net <ref type="bibr" target="#b31">[31]</ref>. ? means the methods adopt the same refine module as <ref type="bibr" target="#b2">[2]</ref>. <ref type="bibr">Protocol</ref>  Following <ref type="bibr" target="#b27">[27]</ref>, we evaluate the dynamic quality of predicted 3D pose sequences by Mean per Joint Velocity Error(MPJVE). This metric measures the smoothness of predicted pose sequences. As shown in <ref type="table">Table 8</ref>, with motion loss, our method significantly reduces the MPJVE by 32% (from 3.4mm to 2.3mm) and outperforms other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MPI-INF-3DHP</head><p>We compare the results of PCK, AUC, and MPJPE against the other state-of-the-art methods on MPI-INF-3DHP dataset with the input of groud truth 2d skeleton sequences. As shown in <ref type="table" target="#tab_7">Table 9</ref>, our approach achieves a significant improvement against other methods. Our method finally achieves 86.9 PCK, 62.1 AUC and 68.1 MPJPE on this dataset. The proposed motion loss significantly improves the accuracy and reduces the error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization results</head><p>The qualitative results on Human3.6M and MPI-INF-3DHP are shown in <ref type="figure">Figure 5</ref>. We choose samples with huge movements and hard actions to show the effectiveness of our system. More visualization results comparing with other previous works can be find in the appendix section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel objective function, motion loss. It explicitly involves motion modeling into learning. To better optimize model under the supervision of motion loss, the 3D pose estimation should have a long-term perception of pose sequences. It motivated us to design an U-shaped model to capture both short-term and long-term temporal dependencies. On two large datasets, the proposed UGCN with motion loss achieves state-of-the-art performance. The motion loss may inspire other skeleton-based tasks such as action forecasting, action generation and pose tracking. Mehta <ref type="bibr" target="#b20">[20]</ref> 75.7 39.3 -Mehta(ResNet=50) <ref type="bibr" target="#b21">[21]</ref> 77.8 41.0 -Mehta(ResNet=101) <ref type="bibr" target="#b21">[21]</ref> 79.4 41.6 -Lin(F=25) <ref type="bibr" target="#b15">[15]</ref> 83.6 51.4 79.8 Lin(F=50) <ref type="bibr" target="#b15">[15]</ref> 82 Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of 2D Pose Estimators</head><p>As shwon in <ref type="table">Table 6</ref> of the manuscript, we achieved a lower MPJPE when using HR-Net <ref type="bibr" target="#b31">[31]</ref> as the 2D pose estimator than using CPN <ref type="bibr" target="#b3">[3]</ref>. To explore the impacts of the 2D pose estimator on the final performance, we combined the predicted 2D pose and the groudtruth by weighted addition for simulating a series of new 2D pose estimators. UGCN was trained taking as input these synchronized 2D pose.</p><p>The results are shwon in the <ref type="figure" target="#fig_3">Figure 6</ref>. We can observer a near linear relationship between MPJPE of 3D poses and two-norm errors of 2D poses. Curves from two estimators have very similar tendency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE (mm)</head><p>Mean Errors of 2D Estimator </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Results</head><p>Visual results of estimated 3D pose by our UGCN are shwon in the <ref type="figure" target="#fig_4">Figure 7</ref>. More visualized results can be find in the supplementary video, including the following aspects: impacts of motion loss, the comparison with previous works, and the estimation results on noisy 2D poses. <ref type="table" target="#tab_3">Ours   GT   T1  T2  T3  T4   Image   DCT   Ours   GT   T1  T2  T3  T4   Image</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image DCT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our proposed pipeline for estimating 3D poses from consecutive 2D poses. We structure 2D skeletons by a spatial-temporal graph and predict 3D locations via our U-shaped Graph Convolution Networks (UGCN). The model is supervised in the space of motion encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Motion loss. By concatenating pairwise cross product vectors between the coordinate vectors of the same joints across time with various intervals, we construct multiscale motion encoding on pose sequences. The motion loss requires the model to reconstruct this encoding. It explicitly involves motion modeling into learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Relationship between the performace of 3D pose estimation and the accuracy of input 2D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>3D pose sequences estimated by UGCN on two datasets: MPI-INF-3DHP (top) and Human3.6M (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>16,96) Motion Encoding Motion Encoding Groud Truth Estimation Loss Motion Loss Encoding Process Upsample</head><label></label><figDesc></figDesc><table><row><cell>BN Input (2,17,96)</cell><cell>STGCN(2,16)</cell><cell>STGCN(16,32) F:(16,17,96)</cell><cell>STGCN(32, 32)</cell><cell>STGCN(32, 64, F:(32,17,48) stride=2)</cell><cell>STGCN(64,64)</cell><cell>STGCN(64, 128, F:(64,17,24) stride=2) STGCN(128,</cell><cell>128)</cell><cell>STGCN(128, F:(128,17,12) 256,stride=2) STGCN(256,256)</cell><cell>STGCN(256,128)</cell><cell>STGCN(128,64) Upsample F:(128,17,6)</cell><cell>STGCN(64,32) Upsample F:(64,17,12)</cell><cell>Upsample F:(32,17,24)</cell><cell>STGCN(32, 16)</cell><cell>Upsample STGCN(128,16) F:(16,17,48)</cell><cell>STGCN(64,16)</cell><cell>F:(16,17,96) STGCN(32,16)</cell><cell>STGCN(2,16)</cell><cell>Conv F:(3,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F:(256,17.6)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Downsampling Stage</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Upsampling Stage</cell><cell></cell><cell cols="3">Merging Stage</cell><cell></cell><cell>Loss</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Performance of our UGCN model supervised by motion loss with different basic operators and time intervals. The empty set ? denotes that the motion loss is not utilized. The best MPJPE is achieved by the cross product operator with interval of 12. We select the 4 best time intervals according to theTable.1, and add them to the interval set one by one. More keypoint pairs with different intervals involve the calculation of mothion encoding. The MPJPE is improved in this process.</figDesc><table><row><cell>Interval set T</cell><cell>?</cell><cell>{2}</cell><cell>{4}</cell><cell>{8}</cell><cell cols="2">{12} {16} {24} {36} {48}</cell></row><row><cell>Subtraction</cell><cell cols="5">32.0 31.4 30.8 29.7 28.9</cell><cell>29.3</cell><cell>30.6</cell><cell>31.8</cell><cell>32.8</cell></row><row><cell cols="5">Inner Product 32.0 31.8 31.7 31.0</cell><cell>30.2</cell><cell>29.8</cell><cell>31.2</cell><cell>32.6</cell><cell>33.7</cell></row><row><cell cols="6">Cross Product 32.0 31.2 30.4 28.2 27.1</cell><cell>28.3</cell><cell>30.2</cell><cell>31.6</cell><cell>32.7</cell></row><row><cell>Operator</cell><cell cols="6">? = 8 ? = 12 ? = 16 ? = 24 # Time Scales MPJPE(mm)</cell></row><row><cell>Cross Product</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>27.1</cell></row><row><cell>Cross Product</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>26.3</cell></row><row><cell>Cross Product</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>25.7</cell></row><row><cell>Cross Product</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>25.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>We explore the importance of each individual component by removing them from standard setting. The increased MPJPE error for each module is listed below.</figDesc><table><row><cell>Backbone</cell><cell cols="2">MPJPE(mm) ?</cell></row><row><cell>UGCN</cell><cell>32.0</cell><cell>-</cell></row><row><cell>UGCN w/o Spatial Graph</cell><cell>39.2</cell><cell>7.2</cell></row><row><cell>UGCN w/o Merging Stage</cell><cell>32.5</cell><cell>0.5</cell></row><row><cell>UGCN + Motion Loss</cell><cell>25.6</cell><cell>-</cell></row><row><cell>UGCN + Motion Loss w/o Merging Stage</cell><cell>28.4</cell><cell>2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The MPJPE performance of our system with different supervision. Combining motion loss functions with different basic operators does not bring obvious improvement.</figDesc><table><row><cell>Loss Function</cell><cell>Interval set T</cell><cell cols="2">MPJPE(mm) ?</cell></row><row><cell>-</cell><cell>?</cell><cell>32.0</cell><cell>-</cell></row><row><cell>Derivative loss [30]</cell><cell>{1}</cell><cell>31.6</cell><cell>0.4</cell></row><row><cell>Cross product</cell><cell>{12}</cell><cell>27.1</cell><cell>4.9</cell></row><row><cell>Subtraction+ Cross product</cell><cell>{12}</cell><cell>27.1</cell><cell>4.9</cell></row><row><cell>Subtraction + Inner + Cross product</cell><cell>{12}</cell><cell>27.1</cell><cell>4.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>1</head><label></label><figDesc>Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Ave. Net) 28.4 32.5 34.4 32.3 32.5 40.9 30.4 29.3 42.6 45.2 33.0 32.0 33.2 24.2 22.9 32.7</figDesc><table><row><cell>Mehta [20]</cell><cell>57.5 68.6 59.6 67.3 78.1</cell><cell>82.4 56.9 69.1 100.0 117.5 69.4 68.0</cell><cell>55.2</cell><cell>76.5</cell><cell>61.4</cell><cell>72.9</cell></row><row><cell>Pavlakos [26]</cell><cell>67.4 71.9 66.7 69.1 72.0</cell><cell>77.0 65.0 68.3 83.7 96.5 71.7 65.8</cell><cell>74.9</cell><cell>59.1</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell>Zhou [40]</cell><cell>54.8 60.7 58.2 71.4 62.0</cell><cell>65.5 53.8 55.6 75.2 111.6 64.1 66.0</cell><cell>51.4</cell><cell>63.2</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell>Martinez [17]</cell><cell>51.8 56.2 58.1 59.0 69.5</cell><cell>78.4 55.2 58.1 74.0 94.6 62.3 59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Sun [32]</cell><cell>52.8 54.8 54.2 54.3 61.8</cell><cell>67.2 53.1 53.6 71.7 86.7 61.5 53.4</cell><cell>61.6</cell><cell>47.1</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Fang [7]</cell><cell>50.1 54.3 57.0 57.1 66.6</cell><cell>73.3 53.4 55.7 72.8 88.6 60.3 57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Pavlakos [25]</cell><cell>48.5 54.4 54.4 52.0 59.4</cell><cell>65.3 49.9 52.9 65.8 71.1 56.6 52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell>Lee [13]</cell><cell>43.8 51.7 48.8 53.1 52.2</cell><cell>74.9 52.7 44.6 56.9 74.3 56.7 66.4</cell><cell>68.4</cell><cell>47.5</cell><cell>45.6</cell><cell>55.8</cell></row><row><cell>Zhou [39]</cell><cell cols="4">87.4 109.3 87.1 103.2 116.2 143.3 106.9 99.8 124.5 199.2 107.4 118.1 114.2 79.4</cell><cell cols="2">97.7 113.0</cell></row><row><cell>Lin [16]</cell><cell>58.0 68.2 63.3 65.8 75.3</cell><cell>93.1 61.2 65.7 98.7 127.7 70.4 68.2</cell><cell>72.9</cell><cell>50.6</cell><cell>57.7</cell><cell>73.1</cell></row><row><cell>Hossain [30]</cell><cell>48.4 50.7 57.2 55.2 63.1</cell><cell>72.6 53.0 51.7 66.1 80.9 59.0 57.3</cell><cell>62.4</cell><cell>46.6</cell><cell>49.6</cell><cell>58.3</cell></row><row><cell>Lee [13](F=3)</cell><cell>40.2 49.2 47.8 52.6 50.1</cell><cell>75.0 50.2 43.0 55.8 73.9 54.1 55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>Dabral [5]</cell><cell>44.8 50.4 44.7 49.0 52.9</cell><cell>61.4 43.5 45.5 63.1 87.3 51.7 48.5</cell><cell>52.2</cell><cell>37.6</cell><cell>41.9</cell><cell>52.1</cell></row><row><cell>Pavllo [27]</cell><cell>45.2 46.7 43.3 45.6 48.1</cell><cell>55.1 44.6 44.3 57.3 65.8 47.1 44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Cai [2] ?</cell><cell>44.6 47.4 45.6 48.8 50.8</cell><cell>59.0 47.2 43.9 57.9 61.9 49.7 46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell>Lin [15]</cell><cell>42.5 44.8 42.6 44.2 48.5</cell><cell>57.1 42.6 41.4 56.5 64.5 47.4 43.0</cell><cell>48.1</cell><cell>33.0</cell><cell>35.1</cell><cell>46.6</cell></row><row><cell>UGCN(CPN)</cell><cell>41.3 43.9 44.0 42.2 48.0</cell><cell>57.1 42.2 43.2 57.3 61.3 47.0 43.5</cell><cell>47.0</cell><cell>32.6</cell><cell>31.8</cell><cell>45.6</cell></row><row><cell>UGCN(CPN) ?</cell><cell>40.2 42.5 42.6 41.1 46.7</cell><cell>56.7 41.4 42.3 56.2 60.4 46.3 42.2</cell><cell>46.2</cell><cell>31.7</cell><cell>31.0</cell><cell>44.5</cell></row><row><cell cols="7">UGCN(HR-Net) 38.2 41.0 45.9 39.7 41.4 51.4 41.6 41.4 52.0 57.4 41.8 44.4 41.6 33.1 30.0 42.6</cell></row><row><cell>Protocol 2</cell><cell cols="6">Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Ave.</cell></row><row><cell>Martinez [17]</cell><cell>39.5 43.2 46.4 47.0 51.0</cell><cell>56.0 41.4 40.6 56.5 69.4 49.2 45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Sun [32]</cell><cell>42.1 44.3 45.0 45.4 51.5</cell><cell>53.0 43.2 41.3 59.3 73.3 51.0 44.0</cell><cell>48.0</cell><cell>38.3</cell><cell>44.8</cell><cell>48.3</cell></row><row><cell>Fang [7]</cell><cell>38.2 41.7 43.7 44.9 48.5</cell><cell>55.3 40.2 38.2 54.5 64.4 47.2 44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Lee [13]</cell><cell>38.0 39.3 46.3 44.4 49.0</cell><cell>55.1 40.2 41.1 53.2 68.9 51.0 39.1</cell><cell>56.4</cell><cell>33.9</cell><cell>38.5</cell><cell>46.2</cell></row><row><cell>Pavlakos [25]</cell><cell>34.7 39.8 41.8 38.6 42.5</cell><cell>47.5 38.0 36.6 50.7 56.8 42.6 39.6</cell><cell>43.9</cell><cell>32.1</cell><cell>36.5</cell><cell>41.8</cell></row><row><cell>Hossain [30]</cell><cell>35.7 39.3 44.6 43.0 47.2</cell><cell>54.0 38.3 37.5 51.6 61.3 46.5 41.4</cell><cell>47.3</cell><cell>34.2</cell><cell>39.4</cell><cell>44.1</cell></row><row><cell>Pavllo [27]</cell><cell>34.1 36.1 34.4 37.2 36.4</cell><cell>42.2 34.4 33.6 45.0 52.5 37.4 33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Dabral [5]</cell><cell>28.0 30.7 39.1 34.4 37.1</cell><cell>44.8 28.9 31.2 39.3 60.6 39.3 31.1</cell><cell>37.8</cell><cell>25.3</cell><cell>28.4</cell><cell>36.3</cell></row><row><cell>Cai [2] ?</cell><cell>35.7 37.8 36.9 40.7 39.6</cell><cell>45.2 37.4 34.5 46.9 50.1 40.5 36.1</cell><cell>41.0</cell><cell>29.6</cell><cell>33.2</cell><cell>39.0</cell></row><row><cell>Lin [15]</cell><cell>32.5 35.3 34.3 36.2 37.8</cell><cell>43.0 33.0 32.2 45.7 51.8 38.4 32.8</cell><cell>37.5</cell><cell>25.8</cell><cell>28.9</cell><cell>36.8</cell></row><row><cell>UGCN(CPN)</cell><cell>32.9 35.2 35.6 34.4 36.4</cell><cell>42.7 31.2 32.5 45.6 50.2 37.3 32.8</cell><cell>36.3</cell><cell>26.0</cell><cell>23.9</cell><cell>35.5</cell></row><row><cell>UGCN(CPN) ?</cell><cell>31.8 34.3 35.4 33.5 35.4</cell><cell>41.7 31.1 31.6 44.4 49.0 36.4 32.2</cell><cell>35.0</cell><cell>24.9</cell><cell>23.0</cell><cell>34.5</cell></row><row><cell>UGCN(HR-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>To exclude the interference of 2D pose estimator, we compare our models and state-of-the-arts trained on ground truth 2D pose. Results showing the action-wise errors on Human3.6M under Protocol-1. 1 (GT) Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Ave. Results show the velocity error of our methods and other state-of-the-arts on Human3.6M. Our result without motion loss is denoted as (*). Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Ave.</figDesc><table><row><cell cols="5">Protocol Pavlakos [25] 47.5 50.5 48.3 49.3</cell><cell>50.7</cell><cell cols="2">55.2 46.1</cell><cell>48.0</cell><cell cols="5">61.1 78.1 51.05 48.3</cell><cell>52.9</cell><cell>41.5</cell><cell>46.4</cell><cell>51.9</cell></row><row><cell cols="5">Martinez [17] 37.7 44.4 40.3 42.1</cell><cell>48.2</cell><cell cols="2">54.9 44.4</cell><cell>42.1</cell><cell cols="2">54.6 58.0</cell><cell cols="2">45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>Hossain [30]</cell><cell cols="3">35.7 39.3 44.6</cell><cell>43</cell><cell>47.2</cell><cell cols="2">54.0 38.3</cell><cell>37.5</cell><cell cols="2">51.6 61.3</cell><cell cols="2">46.5</cell><cell>41.4</cell><cell>47.3</cell><cell>34.2</cell><cell>39.4</cell><cell>44.1</cell></row><row><cell>Lee [13]</cell><cell cols="4">34.6 39.7 37.2 40.9</cell><cell>45.6</cell><cell cols="2">50.5 42.0</cell><cell>39.4</cell><cell cols="2">47.3 48.1</cell><cell cols="2">39.5</cell><cell>38.0</cell><cell>31.9</cell><cell>41.5</cell><cell>37.2</cell><cell>40.9</cell></row><row><cell>Pavllo [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.2</cell></row><row><cell>Cai [2]</cell><cell cols="4">32.9 38.7 32.9 37.0</cell><cell>37.3</cell><cell cols="2">44.8 38.7</cell><cell>36.1</cell><cell cols="2">41.0 45.6</cell><cell cols="2">36.8</cell><cell>37.7</cell><cell>37.7</cell><cell>29.5</cell><cell>31.6</cell><cell>37.2</cell></row><row><cell>Lin [15]</cell><cell cols="4">30.1 33.7 28.7 31.0</cell><cell>33.7</cell><cell cols="2">40.1 33.8</cell><cell>28.5</cell><cell cols="2">38.6 40.8</cell><cell cols="2">32.4</cell><cell>31.7</cell><cell>33.8</cell><cell>25.3</cell><cell>24.3</cell><cell>32.8</cell></row><row><cell>UGCN</cell><cell cols="5">23.0 25.7 22.8 22.6 24.1</cell><cell cols="8">30.6 24.9 24.5 31.1 35.0 25.6 24.3</cell><cell>25.1</cell><cell>19.8</cell><cell>18.4</cell><cell>25.6</cell></row><row><cell cols="3">MPJVE Dir. Pavllo [27] 3.0 3.1</cell><cell>2.2</cell><cell>3.4</cell><cell>2.3</cell><cell>2.7</cell><cell>2.7</cell><cell>3.1</cell><cell>2.1 2.9</cell><cell cols="2">2.3</cell><cell>2.4</cell><cell>3.7</cell><cell>3.1</cell><cell>2.8</cell><cell>2.8</cell></row><row><cell>Lin [15]</cell><cell cols="2">2.7 2.8</cell><cell>2.1</cell><cell>3.1</cell><cell>2.0</cell><cell>2.5</cell><cell>2.5</cell><cell>2.9</cell><cell>1.8 2.6</cell><cell cols="2">2.1</cell><cell>2.3</cell><cell>3.7</cell><cell>2.7</cell><cell>3.1</cell><cell>2.7</cell></row><row><cell cols="3">UGCN(CPN)* 3.5 3.6</cell><cell>3.0</cell><cell>3.9</cell><cell>3.0</cell><cell>3.4</cell><cell>3.2</cell><cell>3.6</cell><cell>2.9 3.7</cell><cell cols="2">3.0</cell><cell>3.1</cell><cell>4.2</cell><cell>3.4</cell><cell>3.7</cell><cell>3.4</cell></row><row><cell>UGCN(CPN)</cell><cell cols="2">2.3 2.5</cell><cell>2.0</cell><cell>2.7</cell><cell>2.0</cell><cell>2.3</cell><cell>2.2</cell><cell>2.5</cell><cell>1.8 2.7</cell><cell cols="2">1.9</cell><cell>2.0</cell><cell>3.1</cell><cell>2.2</cell><cell>2.5</cell><cell>2.3</cell></row><row><cell>UGCN(GT)</cell><cell cols="2">1.2 1.3</cell><cell>1.1</cell><cell>1.4</cell><cell>1.1</cell><cell>1.4</cell><cell>1.2</cell><cell>1.4</cell><cell>1.0 1.3</cell><cell cols="2">1.0</cell><cell>1.1</cell><cell>1.7</cell><cell>1.3</cell><cell>1.4</cell><cell>1.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Comparison with previous work on the MPI-INF-3DHP dataset. The boldfaced numbers represent the best, while underlined numbers represent the second best.</figDesc><table><row><cell>Method</cell><cell>PCK[?]</cell><cell>AUC[?]</cell><cell>MPJPE(mm)[?]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The demo video is in https://www.youtube.com/watch?v=VHhsXG6OXnI&amp;t=87s.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
	<note>Text Image Pred GT Image Pred GT</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Visulation results of our full system on Human3.6M and MPI-INF-3DHP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05754</idno>
		<title level="m">Learnable triangulation of human pose</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<title level="m">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="810" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1109/3dv.2017.00064</idno>
		<ptr target="https://doi.org/10.1109/3dv.2017.00064" />
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1109/3dv.2017.00064</idno>
		<ptr target="https://doi.org/10.1109/3dv.2017.00064" />
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02822</idno>
		<title level="m">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3941" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="991" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional sequence generation for skeleton-based action synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4394" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
