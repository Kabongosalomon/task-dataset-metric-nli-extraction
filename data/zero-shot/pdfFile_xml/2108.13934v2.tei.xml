<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Retrieval Augmented Generation for Zero-shot Slot Filling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Thomas J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Thomas J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><forename type="middle">Mahbub</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Thomas J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI Thomas J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Retrieval Augmented Generation for Zero-shot Slot Filling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [ENTITY, SLOT, ?], a system is asked to 'fill' the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Slot filling is a sub-task of Knowledge Base <ref type="bibr">Population (KBP)</ref>, where the goal is to recognize a pre-determined set of relations for a given entity and use them to populate infobox like structures. This can be done by exploring the occurrences of the input entity in the corpus and gathering information about its slot fillers from the context in which it is located. A slot filling system processes and indexes a corpus of documents. Then, when prompted with an entity and a number of relations, <ref type="figure">Figure 1</ref>: Slot Filling task it fills out an infobox for the entity. Some slot filling systems provide evidence text to explain the predictions. <ref type="figure">Figure 1</ref> illustrates the slot filling task.</p><p>Many KBP systems described in the literature commonly involve complex pipelines for named entity recognition, entity co-reference resolution and relation extraction <ref type="bibr" target="#b5">(Ellis et al., 2015)</ref>. In particular, the task of extracting relations between entities from text has been shown to be the weakest component of the chain. The community proposed different solutions to improve relation extraction performance, such as rule-based <ref type="bibr" target="#b1">(Angeli et al., 2015)</ref>, supervised <ref type="bibr" target="#b34">(Zhang et al., 2017)</ref>, or distantly supervised <ref type="bibr" target="#b7">(Glass et al., 2018)</ref>. However, all these approaches require a considerable human effort in creating hand-crafted rules, annotating training data, or building well-curated datasets for bootstrapping relation classifiers.</p><p>Recently, pre-trained language models have been used for slot filling , opening a new research direction that might provide an effective solution to the aforementioned problems. In particular, the KILT benchmark , standardizes two zero-shot slot filling tasks, zsRE <ref type="bibr" target="#b15">(Levy et al., 2017)</ref> and T-REx <ref type="bibr" target="#b6">(Elsahar et al., 2018)</ref>, providing a competitive evaluation framework to drive advancements in slot filling. However, the best performance achieved by the current retrieval-based models on the two slot filling tasks in KILT are still not satisfactory. This is mainly due to the lack of retrieval performance that affects the generation of the filler as well.</p><p>In this work, we propose KGI (Knowledge Graph Induction), a robust system for slot filling based on advanced training strategies for both Dense Passage Retrieval (DPR) and Retrieval Augmented Generation (RAG) that shows large gains on both T-REx (+38.24% KILT-F1) and zsRE (+21.25% KILT-F1) datasets if compared to previously submitted systems. We extend the training strategies of DPR with hard negative mining <ref type="bibr" target="#b28">(Simo-Serra et al., 2015)</ref>, demonstrating its importance in training the context encoder.</p><p>In addition, we explore the idea of adapting KGI to a new domain. The domain adaptation process consists of indexing the new corpus using our pretrained DPR and substituting it in place of the original Wikipedia index. This enables zero-shot slot filling on the new dataset with respect to a new schema, avoiding the additional effort needed to rebuild NLP pipelines. We provide a few additional examples for each new relation, showing that zeroshot performance quickly improves with a few-shot learning setup. We explore this approach on a variant of the TACRED dataset <ref type="bibr" target="#b0">(Alt et al., 2020)</ref> that we specifically introduce to evaluate the zero/fewshot slot filling task for domain adaption.</p><p>The contributions of this work are as follows:</p><p>1. We describe an end-to-end solution for slot filling, called KGI, that improves the state-ofthe-art in the KILT slot filling benchmarks by a large margin.</p><p>2. We demonstrate the effectiveness of hard negative mining for DPR when combined with end-to-end training for slot filling tasks.</p><p>3. We evaluate the domain adaptation of KGI using zero/few-shot slot filling, demonstrating its robustness on zero-shot TACRED, a benchmark released with this paper.</p><p>4. We publicly release the pre-trained models and source code of the KGI system.</p><p>Section 2 present an overview of the state of the art in slot filling. Section 3 describes our KGI system, providing details on the DPR and RAG models and describing our novel approach to hard negatives. Our system is evaluated in Sections 4 and 5 which include a detailed analysis. Section 6 concludes the paper and highlights some interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The use of language models as sources of knowledge <ref type="bibr" target="#b23">(Petroni et al., 2019;</ref><ref type="bibr" target="#b25">Roberts et al., 2020;</ref><ref type="bibr" target="#b31">Wang et al., 2020;</ref>, has opened tasks such as zero-shot slot filling to pre-trained transformers. Furthermore, the introduction of retrieval augmented language models such as RAG <ref type="bibr" target="#b17">(Lewis et al., 2020b)</ref> and REALM <ref type="bibr" target="#b8">(Guu et al., 2020)</ref> also permit providing textual provenance for the generated slot fillers.</p><p>KILT  was introduced with a number of baseline approaches. The best performing of these is RAG <ref type="bibr" target="#b17">(Lewis et al., 2020b)</ref>. The model incorporates DPR <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> to first gather evidence passages for the query, then uses a model initialized from BART <ref type="bibr" target="#b16">(Lewis et al., 2020a)</ref> to do sequence-to-sequence generation from each evidence passage concatenated with the query in order to generate the answer. In the baseline RAG approach only the query encoder and generation component are fine-tuned on the task. The passage encoder, trained on Natural Questions <ref type="bibr" target="#b12">(Kwiatkowski et al., 2019)</ref> is held fixed. Interestingly, while it gives the best performance of the baselines tested on the task of producing slot fillers, its performance on the retrieval metrics is worse than BM25 . This suggests that fine-tuning the entire retrieval component could be beneficial. Another baseline in KILT is BART LARGE fine-tuned on the slot filling tasks but without the usage of the retrieval model.</p><p>In an effort to improve the retrieval performance, Multi-task DPR <ref type="bibr" target="#b22">(Maillard et al., 2021</ref>) used the multi-task training of the KILT suite of benchmarks to train the DPR passage and query encoder. The top-3 passages returned by the resulting passage index were then combined into a single sequence with the query and a BART model was used to produce the answer. This resulted in large gains in retrieval performance.</p><p>DensePhrases <ref type="bibr" target="#b13">(Lee et al., 2021</ref>) is a different approach to knowledge intensive tasks with a short answer. Rather than index passages which are then consumed by a reader or generator component, it indexes the phrases in the corpus that can be potential answers to questions, or fillers for slots. Each phrase is represented by the pair of its start and end token vectors from the final layer of a transformer initialized from SpanBERT <ref type="bibr" target="#b10">(Joshi et al., 2020)</ref>. <ref type="bibr">GENRE (Cao et al., 2021)</ref> addresses the retrieval task in KILT slot filling by using a sequence-to-sequence transformer to generate the title of the Wikipedia page where the answer can be found. This method can produce excellent scores for retrieval but it does not address the problem of producing the slot filler. It is trained on BLINK  and all KILT tasks jointly.</p><p>Open Retrieval Question Answering (ORQA)  introduced neural information retrieval for the related task of factoid question answering. Like DPR, the retrieval is based on a biencoder BERT  model. Unlike DPR, ORQA projects the BERT <ref type="bibr">[CLS]</ref> vector to a lower dimensional (128) space. It also uses the inverse cloze pre-training task for retrieval, while DPR does not use retrieval specific pre-training.</p><p>3 Knowledge Graph Induction <ref type="figure">Figure 2</ref> shows KGI, our approach to zero-shot slot filling, combining a DPR model and RAG model, both trained for slot filling. We initialize our models from the Natural Questions <ref type="bibr" target="#b12">(Kwiatkowski et al., 2019)</ref> trained models for DPR and RAG available from Hugging Face <ref type="bibr" target="#b32">(Wolf et al., 2020)</ref> 2 . We then employ a two phase training procedure: first we train the DPR model, i.e. both the query and context encoder, using the KILT provenance ground truth. Then we train the sequence-to-sequence generation and further train the query encoder using only the target tail entity as the objective. It is important to note that the same query encoder component is trained in both phases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DPR for Slot Filling</head><p>Our approach to DPR training for slot filling is an adaptation of the question answering training in the 2 https://github.com/huggingface/ transformers original DPR work <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref>. We first index the passages using a traditional keyword search engine, Anserini 3 . The head entity and the relation are used as a keyword query to find the topk passages by BM25. Passages with overlapping paragraphs to the ground truth are excluded as well as passages that contain a correct answer. The remaining top ranked result is used as a hard negative for DPR training. This is the hard negative mining strategy used by DPR <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> and Multi-DPR <ref type="bibr" target="#b22">(Maillard et al., 2021)</ref>. After locating a hard negative for each query, the DPR training data is a set of triples: query, positive passage (given by the KILT ground truth provenance) and the hard negative passage. <ref type="figure">Figure  3</ref> shows the training process for DPR. For each batch of training triples, we encode the queries and passages independently. The passage and query encoders are BERT  models. Then we find the inner product of all queries with all passages. The negatives for a given query are therefore the hard negative and the batch negatives, i.e. the positive and hard negative passages for other queries in the batch. After applying a softmax to the score vector for each query, the loss is the negative log-likelihood for the positive passages.</p><p>Using the trained DPR passage encoder we generate vectors for the approximately 32 million passages in our segmentation of the KILT knowledge source. Though this is a computationally expensive step, it is easily parallelized. The passage-vectors are then indexed with an ANN (Approximate Nearest Neighbors) data structure, in this case HNSW (Hierarchical Navigable Small World) <ref type="bibr" target="#b19">(Malkov and Yashunin, 2018)</ref> using the open source FAISS library <ref type="bibr">(Johnson et al., 2017) 4</ref> . We use scalar quantization down to 8 bits to reduce the memory size.</p><p>The query encoder is also trained for slot filling alongside the passage encoder. We inject the trained query encoder into the RAG model for Natural Questions. Due to the loose coupling between the query encoder and the sequence-to-sequence generation of RAG, we can update the pre-trained model's query encoder without disrupting the quality of the generation.</p><p>Unlike previous work on zero-shot slot filling, we are training the DPR model specifically for the slot filling task. In contrast, the RAG baseline  used DPR pre-trained on Natural Questions, and Multi-DPR <ref type="bibr" target="#b22">(Maillard et al., 2021)</ref> trained on all KILT tasks jointly. <ref type="figure">Figure 4</ref> illustrates the architecture of RAG <ref type="bibr" target="#b17">(Lewis et al., 2020b</ref>). The RAG model is trained to predict the ground truth tail entity from the head and relation query. First the query is encoded to a vector and the top-k (we use k = 5) relevant passages are retrieved from the ANN index. The query is concatenated to each passage and the generator predicts a probability distribution over the possible next tokens for each sequence. These predictions are weighted according to the score between the query and passage -the inner product of the query vector and passage vector. Marginalization then combines the weighted probability distributions to give a single probability distribution for the next token. This enables RAG to train the query encoder through its impact in generation, learning to give higher weight to passages that contribute to generating the correct tokens. Formally, the inputs to the BART model are sequences (s j = p j [SEP] q) that comprise a query q plus retrieved passage p j . The probability for each sequence is determined from the softmax over the retrieval scores (z r ) for the passages. The probability for each output token t i given the sequence s j is a softmax over BART's token prediction logits. Therefore the total probability for each token t i is the log-likelihood summed over all sequences, weighted by each sequence's probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RAG for Slot Filling</head><formula xml:id="formula_0">P (s j ) = sof tmax(z r ) j P (t i |s j ) = sof tmax(BART(s j ) i ) t i P (t i ) = j P (t i |s j ) ? P (s j )</formula><p>Beam search is used at inference time to select the overall most likely tail entity. This is the standard beam search for natural language generation in deep neural networks <ref type="bibr" target="#b29">(Sutskever et al., 2014)</ref>, the only difference is in the way the next-token probabilities are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dense Negative Sampling</head><p>As <ref type="figure">Figure 2</ref> shows, the DPR question encoder is trained both by DPR and later by RAG. To examine the influence of this additional training from RAG on the retrieval performance, we compare retrieval metrics before and after RAG fine-tuning. <ref type="table" target="#tab_1">Table  1</ref> shows the large gains from training with RAG after DPR. Note that RAG training is using the weak supervision of the passage's impact in producing the correct answer, rather than the ground truth provenance of DPR training. Since this is likely a disadvantage, we explore the other key difference with DPR and RAG training: RAG uses negatives drawn from the trained index rather than from BM25.  To replicate this feature of RAG in DPR, we introduce hard negatives mined from the learned index. Using the KILT trained DPR models, we index the passages. Then we gather hard negatives for DPR training, with one difference: rather than locating the hard negative passages by BM25, we find the passage by ANN search over the learned dense vector index. We train for an additional two epochs using these hard negatives. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of the different approaches to retrieval. DPR N Q is the DPR model pre-trained  After training with DNS the FAISS indexing with scalar quantization becomes prohibitively slow. We therefore remove all quantization and use four shards (the index is split into four, with the results of each query merged) for our experiments with DNS enabled KGI. <ref type="table" target="#tab_3">Table 2</ref> gives statistics on the two zero-shot slot filling datasets in KILT. While the T-REx dataset is larger by far in the number of instances, the training sets have a similar number of distinct relations. We use only 500k training instances of T-REx in our experiments to increase the speed of experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KILT Experiments</head><p>Since the transformers for passage encoding and generation can accept a limited sequence length, we segment the documents of the KILT knowledge source (2019/08/01 Wikipedia snapshot) into passages. The ground truth provenance for the slot filling tasks is at the granularity of paragraphs, so we align our passage segmentation on paragraph boundaries when possible. If two or more paragraphs are short enough to be combined, we combine them into a single passage and if a single paragraph is too long, we truncate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KGI Hyperparameters</head><p>We have not done hyperparameter tuning, instead using hyperparameters similar to the original works   <ref type="table" target="#tab_5">Table 3</ref> shows the hyperparameters used in our experiments. We train our models on T-REx using only the first 500k instances. For KGI 1 we use the same hyperparameters except that zsRE is trained for two epochs.</p><p>In both KGI systems we use the default of five passages retrieved for each query for use in RAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Details</head><p>Number of parameters KGI is based on RAG and has the same number of parameters: 2 ? 110M for the BERT BASE query and passage encoders and 400M for the BART LARGE sequenceto-sequence generation component: 620M in total.</p><p>Computing infrastructure Using a single NVIDIA V100 GPU DPR training of two epochs takes approximately 24 hours for T-REx and 2 hours for zsRE. Using a single NVIDIA P100 GPU RAG training for 500k T-REx instances takes two days and 147k instances of zsRE takes 15 hours. The FAISS index on the KILT knowledge source requires a machine with large memory, we use 256GB memory -128GB is insufficient for the indexes without scalar quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Slot Filling Evaluation</head><p>As an initial experiment we tried RAG with its default index of Wikipedia, distributed through Hugging Face. We refer to this as RAG-KKS, or RAG without the KILT Knowledge Source, as reported in <ref type="table" target="#tab_7">Table 4</ref>. Since the passages returned are not aligned to the KILT provenance ground truth, we do not report retrieval metrics for this experiment. Motivated by the low retrieval performance reported for the RAG baseline by , we experimented with replacing the DPR retrieval with simple BM25 (RAG+BM25) over the KILT knowledge source. We provide the raw BM25 scores for the passages to the RAG model, to weight their impact in generation. We also experimented with the Natural Questions trained DPR,  We use the approach explained in Section 3 to train both the DPR and RAG models. KGI 0 is a version of our system using DPR with hard negative samples from BM25. The successor system, KGI 1 incorporates DPR training using DNS.</p><p>The metrics we report include accuracy and F1 on the slot filler, where F1 is based on the recall and precision of the tokens in the answer, allowing for partial credit on slot fillers. Our systems, except for RAG-KKS, also provide provenance information for the top answer. R-Precision and Recall@5 measure the quality of this provenance against the KILT ground truth provenance. Finally, KILT-Accuracy and KILT-F1 are combined metrics that measure the accuracy and F1 of the slot filler only when the correct provenance is provided. <ref type="table" target="#tab_7">Table 4</ref> reports an evaluation on the development set, while <ref type="table" target="#tab_9">Table 5</ref> reports the test set performance of the top systems on the KILT leaderboard. KGI 0 and KGI 1 are our systems, while DensePhrases, GENRE, Multi-DPR, RAG for KILT and BART LARGE are explained briefly in Section 2. KGI 1 gains dramatically in slot filling accuracy over the previous best systems, with gains of over 14 percentage points in zsRE and even more in T-REx. The combined metrics of KILT-AC and KILT-F1 show even larger gains, suggesting that the KGI 1 approach is effective at providing justifying evidence when generating the correct answer. We achieve gains of 21 to 41 percentage points in KILT-AC.</p><p>Relative to Multi-DPR, we see the benefit of weighting passage importance by retrieval score and marginalizing over multiple generations, com-pared to the strategy of concatenating the top three passages and running a single sequence-tosequence generation. GENRE is still best in retrieval for T-REx, suggesting that at least for a corpus such as Wikipedia, generating the title of the page can be very effective. A possible explanation for this behaviour is that most relations for a Wikipedia entity are mentioned in its corresponding page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>To explore the effect of retrieval on downstream performance we consider two variants of our systems: one using random passages from the index, forcing the system to depend on implicit knowledge, and the another using passages from the ground truth provenance, to measure the upper bound performance for the ideal retrieval system. Evaluation is reported in <ref type="table">Table 6</ref> for 3 systems. By supplying these systems with the gold standard passages, we can see both the improvement possible through better retrieval, and the value of good retrieval during training. The best system, KGI 1 is the most effective at generating slot fillers from relevant explicit knowledge because it was trained on more cases of justifying explicit knowledge. However, given random passages it is the worst. It has sacrificed some implicit knowledge for better capabilities in using explicit knowledge.</p><p>As shown in <ref type="table" target="#tab_9">Table 5</ref>, BART LARGE , which is the best implicit-knowledge baseline system for KILT slot filling, is approximately 40 points lower in in accuracy on T-REx if compared to KGI 1 . To understand the impact of the explicit knowledge provided by DPR, we examine the improvement of KGI over BART LARGE . We consider two main hypotheses: 1) the value of explicit knowledge depends on the relation, and 2) the value of explicit knowledge depends on the corpus frequency of the entities related.</p><p>To evaluate hypothesis 1, we consider the most frequent 20 relations in the T-REx Dev set, each occurring at least 40 times. The relations with the lowest relative performance gain are taxonomy and partonomy relations: TAXON-RANK, SUBCLASS-OF, INSTANCE-OF, PART-OF and PARENT-TAXON as well as LANGUAGES-SPOKEN,-WRITTEN-OR-SIGNED and SPORT. This suggest that essential properties of entities are well encoded in the language model itself. Inspecting the LANGUAGES-SPOKEN,-WRITTEN-OR-SIGNED we find that sur-   <ref type="table">Table 6</ref>: T-REx Accuracy with Random and Gold Retrieval face level information (i.e. French name vs. Russian name) is often sufficient for the correct prediction.</p><p>In contrast, the relations that gain the most from explicit knowledge are: PERFORMER, MEMBER-OF-SPORTS-TEAM, AUTHOR, PLACE-OF-BIRTH, COUNTRY-OF-ORIGIN, CAST-MEMBER, DIREC-TOR. These relations are not central to the meaning of the head entity, like the taxonomy and partonomy relations, and are not typically predictable from surface-level features.</p><p>Regarding our second hypothesis, we might expect that more frequent entities have better representations in the parameters of a pre-trained language model, and that therefore the gain in performance due to use of explicit knowledge will show a strong dependence on the corpus frequency of the head or tail entity.</p><p>To test it, we group the Dev instances in T-Rex according to the decile of the head or tail entity frequency. We compute a macro-accuracy, weighting all relations equally. <ref type="figure" target="#fig_5">Figure 5</ref> shows the macroaccuracy of BART LARGE and KGI 1 for each decile of head and tail entity frequency. Although there is a general trend of higher accuracy for more fre-quent tail entities and lower accuracy for more frequent head entities, there is no pattern to the gain of explicit knowledge over implicit knowledge from entity frequency. There is a similar picture when considering the decile of the minimum of the head or tail entity frequency. This falsifies our second hypothesis and suggests implicit knowledge is distinct in kind from explicit knowledge, rather than merely under-trained for low frequency entities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Domain Adaptation Experiments</head><p>In this section, we evaluate the domain adaptation capability of KGI. For this purpose, we re-organize a dataset specifically designed to evaluate standard supervised relation extraction models, such as TA-CRED, with the aim to create a zero-shot (and fewshot) slot filling benchmark where the documents are written with a different style than Wikipedia, and the relations in the KG are different from those in Wikidata. In order to perform an in-depth comparison and analysis, we also propose a new set of ranking baselines and use metrics which are suitable to better evaluate the slot filling task in a zero-shot setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Zero-shot TACRED</head><p>The TACRED dataset was originally proposed by <ref type="bibr" target="#b34">Zhang et al. (2017)</ref> with the goal to provide a high-quality training set to supervise a relation extraction model which is shown to be competitive on TAC-KBP 2015 <ref type="bibr" target="#b5">(Ellis et al., 2015)</ref>. The target KG schema consists of two infoboxes modeling the person and organization entity types, with 41 relation types in total. For our experiments, we adopt a revisited version of TACRED <ref type="bibr" target="#b0">(Alt et al., 2020)</ref>, in which a second stage crowdsourcing is performed to further improve the quality of the annotations and resolve conflicts among relations. In a typical supervised relation extraction setup, a model is trained to predict (i.e. classify) the right relation type given a textual passage and two entity mentions as inputs. In this paper we used the TACRED dataset as a slot filling benchmark, using the following procedure: 1) we first create the corpus by merging all the plain textual passages from the instances in the train, dev and test sets; 2) we collect the annotated triples, i.e. subjectrelation-object, from the test data to come up with a ground-truth KG to be used for slot filling evaluation 5 ; 3) we remove all the triples from the original test set where the subjects are pronouns. The resulting KG consists of 2673 slot filling test instances. Similarly, we acquire a KG from the train/dev sets to further fine-tune the KGI system as described in the next section. To enable zero-shot experiments, we also convert each relation label into a relation phrase by removing the namespaces per: and org:, and replacing the '_' character with a space. Finally, for each pre-annotated entity in the corpus, we pre-compute an inverted index consisting of a list of co-occurring entities in the textual passages. We use this inverted index to compare our model with a set of ranking baselines.</p><p>An example of the obtained ground truth is illustrated in <ref type="table" target="#tab_11">Table 7</ref>: given the query [Dominick Dunne, employee of, ?], a slot filling system is supposed to identify the missing slot with Vanity Fair, i.e. the gold standard object in the KG, by retrieving it from the collection of passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Slot Filling Evaluation</head><p>Task Given a slot filling query (e, s, ?) and a list of possible slot values [v 1 , ..., v n ], where e is the entity as subject, s is the slot/relation and v i are the object candidates that co-occur with e in the corpus, we can frame the zero-shot slot filling as a ranking problem: argmax i score M (e, s, v i ). score M is a function that takes as input a triple and provide a score based on the model M . Turning the slot filling into a ranking problem has two advantages: 1) we can compare the generative approach with a new set of baselines, and 2) we can limit the generation of the slot values to a pre-defined set of domain specific entities.</p><p>Models In order to adapt KGI 1 , as pre-trained on T-REx, to the TACRED corpus, we indexed the textual passages using DPR, as described in Section 3. Then we replaced the original Wikipedia index with this new index. During the inference step, we restrict the generation of the slot values using the list of object candidates, i.e. the entities which co-occur with the subject from the inverted index, to facilitate comparability to a set of ranking baselines. To this aim, we adopt the technique described by  to restrict the vocabulary of tokens during the generation.</p><p>We use three baselines to compare with our approach for this zero-shot slot filling task. PMI is implemented using the pointwise mutual information between e and v i based on their co-occurrence in the corpus. Also, we train a Word2Vec <ref type="bibr" target="#b20">(Mikolov et al., 2013</ref>) skip-gram model on the textual corpus, and we use it to implement the scoring function as cosine(e + s, v i ), for each candidate filler v i . It is based on the assumption that a relation s between two (multi)word embeddings e and v can be represented as an offset vector (v ? e) = s ?? (e+s) = v <ref type="bibr" target="#b26">(Rossiello et al., 2019;</ref><ref type="bibr" target="#b30">Vylomova et al., 2016)</ref>. Finally, GPT-2 computes the perplexity of the fragment of text by concatenating the tokens in e, s and each v i <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>.</p><p>Metrics Due to the similarity of slot filling with the knowledge base completion task, we use Mean Reciprocal Rank (MRR) and HIT@k, with k = [1, 5, 10], as evaluation metrics <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. Note that HIT@1 has the same meaning of the accuracy for the downstream task on KILT.    Results <ref type="table" target="#tab_12">Table 8</ref> reports the results of our evaluation. KGI 1 achieves substantially better performance than the aforementioned zero-shot baselines on all evaluation metrics. However, HIT@1 is ? 28% which is significantly lower compared with the numbers reported on the datasets in KILT. This begs the question, how to further improve the transfer learning capabilities of these generative models? Interestingly, HIT@5/10 are high (i.e. ? 64%/76%). This indicates our approach would be useful in a human-in-the-loop scenario by providing valuable candidates for the fillers that can be further validated.</p><p>For this purpose, we also conduct few-shot experiments to understand the robustness of KGI 1 by fine-tuning it with very limited amounts of training examples. We randomly pick n example(s) for each relation type from the TACRED training set, with n = <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>. <ref type="table" target="#tab_13">Table 9</ref> gives our hyperparameters for the TACRED few-shot experiments. We show that our system benefits from additional domain specific training data selected from TACRED. Just using one example and four examples per relation, HIT@1 improves ? 5 and ? 10 percentage points respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented KGI, a novel approach to zero-shot slot filling. KGI improves Dense Passage Retrieval using hard negatives from the dense index, and implements a robust training procedures for Retrieval Augmented Generation. We evaluated KGI on both T-REx and zsRE slot filling datasets, ranking at top-1 position in the KILT leaderboard with a net improvement of +38.24 and +21.25 percentage points in KILT-F1, respectively. Moreover, we proposed and release a new benchmark for zero/few-shot slot filling based on TACRED to evaluate domain adaptation where our system obtained much better zero-shot results compared with the baselines. In addition, we have observed significant improvement in results for KGI when rapidly fine-tuned in a few-shot setting. This work opens promising future research directions for slot filling and other related tasks. We plan to apply DPR with dense negative sampling to other tasks in the KILT benchmark, including dialogue and question answering. Likewise, an in-depth investigation on more effective strategies for domain adaptation, such as the combination of zero-shot and few-shot learning involving human-in-the-loop techniques, would be another interesting direction to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2: KGI Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: DPR Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: RAG Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Performance as a function of entity frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Analysis of retrieval by DPR and RAG on Dev sets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Slot filling datasets in KILT on Natural Questions. DPR BM 25 further trains DPR N Q on the KILT data with BM25 hard negatives. Rows with +RAG further train the question encoder through RAG. The row DPR DN S (Dense Negative Sampling) shows the performance of retrieval immediately after DNS training. Surprisingly, this results in lower performance for T-REx relative to DPR BM 25 . However, further training</figDesc><table /><note>the DNS model with RAG results in our best per- formance for both T-REx and zsRE. Since RAG does not update the context encoder, DNS training is the only training for the context encoder when negatives are drawn from the dense vector index.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>KGI hyperparameters on training DPR and RAG.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Dev sets performance for different retrieval methods</figDesc><table /><note>with only RAG training on KILT (RAG+DPR N Q ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>KILT leaderboard top systems performance on slot filling tasks</figDesc><table><row><cell cols="3">Passages RAGNQ KGI0 KGI1</cell></row><row><cell>Retrieved</cell><cell>70.58</cell><cell>76.58 84.04</cell></row><row><cell>Gold</cell><cell>88.66</cell><cell>89.46 90.20</cell></row><row><cell>Random</cell><cell>38.84</cell><cell>39.26 36.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Dunne, the author, television personality and Vanity Fair reporter who covered the trials of socialite Claus von Bulow. Dominick Dunne per:age 83 Dominick Dunne, 83, a crime story author, in New York City. Dominick Dunne per:siblings John Gregory Dunne Dominick Dunne, author of crime stories dies Born in 1925 in Hartford, Connecticut, was part of a famous family that included his brother, novelist and screenwriter John Gregory Dunne.</figDesc><table><row><cell>Subject</cell><cell>Relation/Slot</cell><cell>Object</cell><cell>Passage</cell></row><row><cell cols="2">Dominick Dunne per:employee_of</cell><cell>Vanity Fair</cell><cell></cell></row><row><cell>ALICO</cell><cell cols="2">org:country_of_headquarters US</cell><cell>AIA says IPO raised 205 billion US dollars AIG said Monday it</cell></row><row><cell></cell><cell></cell><cell></cell><cell>had also raised 162 billion dollars by selling unit American Life</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Insurance Company (ALICO) to MetLife Inc.</cell></row><row><cell>ALICO</cell><cell>org:top_members</cell><cell>Christopher J. Swift</cell><cell>Alico's chief financial officer, Christopher J. Swift, added that</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the bonds were issued by companies in many commercial sectors,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>which diversified the portfolio.</cell></row><row><cell>ALICO</cell><cell>org:parents</cell><cell>AIG</cell><cell>AIG said it had transferred ownership to the Federal Reserve</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Bank of parts of two subsidiaries, ALICO which is active in life</cell></row><row><cell></cell><cell></cell><cell></cell><cell>assurance in the United States and AIA which provides life assur-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ance abroad.</cell></row></table><note>Dominick</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Examples of annotations from TACRED dataset for both person and organization infoboxes.</figDesc><table><row><cell cols="4">MRR HIT@1 HIT@5 HIT@10</cell></row><row><cell>PMI 20.20</cell><cell>10.89</cell><cell>26.49</cell><cell>37.30</cell></row><row><cell>Word2Vec 25.24</cell><cell>13.83</cell><cell>34.92</cell><cell>47.60</cell></row><row><cell>GPT-2 17.62</cell><cell>8.37</cell><cell>23.72</cell><cell>35.34</cell></row><row><cell>KGI1 0-shot 43.98</cell><cell>28.51</cell><cell>64.31</cell><cell>76.06</cell></row><row><cell>KGI1 1-shot 48.86</cell><cell>33.89</cell><cell>66.63</cell><cell>78.75</cell></row><row><cell>KGI1 4-shot 53.28</cell><cell>38.8</cell><cell>70.45</cell><cell>79.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Zero/few-shots results on TACRED</figDesc><table><row><cell cols="2">Hyperparameter RAG</cell></row><row><cell>learn rate</cell><cell>3e-6</cell></row><row><cell>batch size</cell><cell>1</cell></row><row><cell>epochs</cell><cell>3</cell></row><row><cell>warmup instances</cell><cell>0</cell></row><row><cell cols="2">learning schedule linear</cell></row><row><cell>max grad norm</cell><cell>1</cell></row><row><cell>weight decay</cell><cell>0</cell></row><row><cell>Adam epsilon</cell><cell>1e-8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>KGI 1 hyperparameters for TACRED few-shot</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our source code is available at: https://github. com/IBM/kgi-slot-filling</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/castorini/anserini 4 https://github.com/facebookresearch/ faiss</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">79.5% of the overall instances are labeled as no relation. We exclude these instances from the ground truth KG, but we retain them in the textual corpus.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin Jose Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Overview of linguistic resources for the TAC KBP 2015 evaluations: Methodologies and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Getman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Fore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Kuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<editor>TAC. NIST</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inducing implicit relations from text using distantly supervised deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandana</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11136</biblScope>
			<biblScope unit="page" from="38" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning dense representations of phrases at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6634" to="6647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledgeintensive nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00117</idno>
		<title level="m">Veselin Stoyanov, and Gargi Ghosh. 2021. Multi-task retrieval for knowledge-intensive tasks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmitry A Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2523" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Language models as knowledge bases?</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning relational representations by analogy using hierarchical Siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Fauceglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1327</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019</title>
		<meeting>the 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3235" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1158</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1671" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Language models are open knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno>abs/2010.11967</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
