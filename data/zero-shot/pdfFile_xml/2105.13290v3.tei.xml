<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CogView: Mastering Text-to-Image Generation via Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group ? BAAI</orgName>
								<orgName type="institution">Tsinghua University ? DAMO Academy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CogView: Mastering Text-to-Image Generation via Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E. 1 A tiger is playing football.</p><p>A coffee cup printed with a cat. Sky background.</p><p>A beautiful young blond woman talking on a phone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Big Ben clock towering over the city of London.</head><p>A man is flying to the moon on his bicycle A couple wearing leather biker garb rides a motorcycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super-resolution mid-lake pavilion</head><p>Chinese traditional drawing. Statue of Liberty. Oil painting. Lion. Cartoon. A tiger is playing football. Sketch. Houses.</p><p>Figure 1: Samples generated by CogView. The text in the first line is either from MS COCO (outside our training set) or user queries on our demo website. The images in the second line are finetuned results for different styles or super-resolution. The actual input text is in Chinese, which is translated into English here for better understanding. More samples for captions from MS COCO are included in Appendix F. 1 Codes and models are at https://github.com/THUDM/CogView. We also have a demo website of our latest model at https://wudao.aminer.cn/CogView/index.html (without post-selection). 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2105.13290v3 [cs.CV] 5 Nov 2021 A recent work DALL-E [39] independently proposed the same idea, and was released earlier than CogView. Compared with DALL-E, CogView steps forward on the following four aspects: ? CogView outperforms DALL-E and previous GAN-based methods at a large margin according to the Fr?chet Inception Distance (FID) [25] on blurred MS COCO, and is the first open-source large text-to-image transformer.</p><p>? Beyond zero-shot generation, we further investigate the potential of finetuning the pretrained CogView. CogView can be adapted for diverse downstream tasks, such as style learning (domain-specific text-to-image), super-resolution (image-to-image), image captioning (image-to-text), and even text-image reranking.</p><p>? The finetuned CogView enables self-reranking for post-selection, and gets rid of an additional CLIP model <ref type="bibr" target="#b37">[38]</ref> in DALL-E. It also provides a new metric Caption Loss to measure the quality and accuracy for text-image generation at a finer granularity than FID and Inception Score (IS) <ref type="bibr" target="#b42">[43]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"There are two things for a painter, the eye and the mind... eyes, through which we view the nature; brain, in which we organize sensations by logic for meaningful expression." (Paul C?zanne <ref type="bibr" target="#b16">[17]</ref>)</p><p>As contrastive self-supervised pretraining has revolutionized computer vision (CV) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>, visual-language pretraining, which brings high-level semantics to images, is becoming the next frontier of visual understanding <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>. Among various pretext tasks, text-to-image generation expects the model to <ref type="bibr" target="#b0">(1)</ref> disentangle shape, color, gesture and other features from pixels, <ref type="bibr" target="#b1">(2)</ref> understand the input text, (2) align objects and features with corresponding words and their synonyms and (4) learn complex distributions to generate the overlapping and composite of different objects and features, which, like painting, is beyond basic visual functions (related to eyes and the V1-V4 in brain <ref type="bibr" target="#b21">[22]</ref>), requiring a higher-level cognitive ability (more related to the angular gyrus in brain <ref type="bibr" target="#b2">[3]</ref>).</p><p>The attempts to teach machines text-to-image generation can be traced to the early times of deep generative models, when Mansimov et al. <ref type="bibr" target="#b34">[35]</ref> added text information to DRAW <ref type="bibr" target="#b19">[20]</ref>. Then Generative Adversarial Nets <ref type="bibr" target="#b18">[19]</ref> (GANs) began to dominate this task. Reed et al. <ref type="bibr" target="#b41">[42]</ref> fed the text embeddings to both generator and discriminator as extra inputs. StackGAN <ref type="bibr" target="#b53">[54]</ref> decomposed the generation into a sketch-refinement process. AttnGAN <ref type="bibr" target="#b50">[51]</ref> used attention on words to focus on the corresponding subregion. ObjectGAN <ref type="bibr" target="#b28">[29]</ref> generated images following a text?boxes?layouts?image process. DM-GAN <ref type="bibr" target="#b54">[55]</ref> and DF-GAN <ref type="bibr" target="#b44">[45]</ref> introduced new architectures, e.g. dyanmic memory or deep fusion block, for better image refinement. Although these GAN-based models can perform reasonable synthesis in simple and domain-specific dataset, e.g. Caltech-UCSD Birds 200 (CUB), the results on complex and domain-general scenes, e.g. MS COCO <ref type="bibr" target="#b30">[31]</ref>, are far from satisfactory.</p><p>Recent years have seen a rise of the auto-regressive generative models. Generative Pre-Training (GPT) models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref> leveraged Transformers <ref type="bibr" target="#b47">[48]</ref> to learn language models in large-scale corpus, greatly promoting the performance of natural language generation and few-shot language understanding <ref type="bibr" target="#b32">[33]</ref>. Auto-regressive model is not nascent in CV. PixelCNN, PixelRNN <ref type="bibr" target="#b46">[47]</ref> and Image Transformer <ref type="bibr" target="#b35">[36]</ref> factorized the probability density function on an image over its sub-pixels (color channels in a pixel) with different network backbones, showing promising results. However, a real image usually comprises millions of sub-pixels, indicating an unaffordable amount of computation for large models. Even the biggest pixel-level auto-regressive model, ImageGPT <ref type="bibr" target="#b6">[7]</ref>, was pretrained on ImageNet at a max resolution of only 96 ? 96.</p><p>The framework of Vector Quantized Variational AutoEncoders (VQ-VAE) <ref type="bibr" target="#b45">[46]</ref> alleviates this problem. VQ-VAE trains an encoder to compress the image into a low-dimensional discrete latent space, and a decoder to recover the image from the hidden variable in the stage 1. Then in the stage 2, an auto-regressive model (such as PixelCNN <ref type="bibr" target="#b46">[47]</ref>) learns to fit the prior of hidden variables. This discrete compression loses less fidelity than direct downsampling, meanwhile maintains the spatial relevance of pixels. Therefore, VQ-VAE revitalized the auto-regressive models in CV <ref type="bibr" target="#b40">[41]</ref>. Following this framework, Esser et al. <ref type="bibr" target="#b14">[15]</ref> used Transformer to fit the prior and further switches from L 2 loss to GAN loss for the decoder training, greatly improving the performance of domain-specific unconditional generation.</p><p>The idea of CogView comes naturally: large-scale generative joint pretraining for both text and image (from VQ-VAE) tokens. We collect 30 million high-quality (Chinese) text-image pairs and pretrain a Transformer with 4 billion parameters. However, large-scale text-to-image generative pretraining could be very unstable due to the heterogeneity of data. We systematically analyze the reasons and solved this problem by the proposed Precision Bottleneck Relaxation and Sandwich Layernorm. As a result, CogView greatly advances the quality of text-to-image generation.</p><p>? We proposed PB-relaxation and Sandwich-LN to stabilize the training of large Transformers on complex datasets. These techniques are very simple and can eliminate overflow in forwarding (characterized as NaN losses), and make CogView able to be trained with almost FP16 (O2 2 ). They can also be generalized to the training of other transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theory</head><p>In this section, we will derive the theory of CogView from VAE 3 <ref type="bibr" target="#b25">[26]</ref>: CogView optimizes the Evidence Lower BOund (ELBO) of joint likelihood of image and text. The following derivation will turn into a clear re-interpretation of VQ-VAE if without text t.</p><formula xml:id="formula_0">Suppose the dataset (X, T) = {x i , t i } N i=1 consists of N i.i.d.</formula><p>samples of image variable x and its description text variable t. We assume the image x can be generated by a random process involving a latent variable z: (1) t i is first generated from a prior p(t; ?). (2) z i is then generated from the conditional distribution p(z|t = t i ; ?). (3) x i is finally generated from p(x|z = z i ; ?). We will use a shorthand form like p(x i ) to refer to p(x = x i ) in the following part.</p><p>Let q(z|x i ; ?) be the variational distribution, which is the output of the encoder ? of VAE. The log-likelihood and the evidence lower bound (ELBO) can be written as:</p><formula xml:id="formula_1">log p(X, T; ?, ?) = N i=1 log p(t i ; ?) + N i=1 log p(x i |t i ; ?, ?) (1) ? ? N i=1 ? log p(t i ; ?) NLL loss for text + E zi?q(z|xi;?) [? log p(x i |z i ; ?)] reconstruction loss + KL q(z|x i ; ?) p(z|t i ; ?)</formula><p>KL between q and (text conditional) prior</p><formula xml:id="formula_2">. (2)</formula><p>The framework of VQ-VAE differs with traditional VAE mainly in the KL term. Traditional VAE fixes the prior p(z|t i ; ?), usually as N (0, I), and learns the encoder ?. However, it leads to posterior collapse <ref type="bibr" target="#b22">[23]</ref>, meaning that q(z|x i ; ?) sometimes collapses towards the prior. VQ-VAE turns to fix ? and fit the prior p(z|t i ; ?) with another model parameterized by ?. This technique eliminates posterior collapse, because the encoder ? is now only updated for the optimization of the reconstruction loss. In exchange, the approximated posterior q(z|x i ; ?) could be very different for different x i , so we need a very powerful model for p(z|t i ; ?) to minimize the KL term.</p><p>Currently, the most powerful generative model, Transformer (GPT), copes with sequences of tokens over a discrete codebook. To use it, we make z ? {0, ..., |V | ? 1} h?w , where |V | is the size of codebook and h ? w is the number of dimensions of z. The sequences z i can be either sampled from q(z|x i ; ?), or directly z i = argmax z q(z|x i ; ?). We choose the latter for simplicity, so that q(z|x i ; ?) becomes a one-point distribution on z i . The Equation <ref type="formula">(2)</ref> can be rewritten as:</p><formula xml:id="formula_3">? N i=1 E zi?q(z|xi;?) [? log p(x i |z i ; ?)] reconstruction loss ? log p(t i ; ?) NLL loss for text ? log p(z i |t i ; ?) NLL loss for z .<label>(3)</label></formula><p>The learning process is then divided into two stages: (1) The encoder ? and decoder ? learn to minimize the reconstruction loss. (2) A single GPT optimizes the two negative log-likelihood (NLL) losses by concatenating text t i and z i as an input sequence.</p><p>As a result, the first stage degenerates into a pure discrete Auto-Encoder, serving as an image tokenizer to transform an image to a sequence of tokens; the GPT in the second stage undertakes most of the modeling task. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the framework of CogView.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tokenization</head><p>In this section, we will introduce the details about the tokenizers in CogView and a comparison about different training strategies about the image tokenizer (VQVAE stage 1).</p><p>Tokenization for text is already well-studied, e.g. BPE <ref type="bibr" target="#b15">[16]</ref> and SentencePiece <ref type="bibr" target="#b27">[28]</ref>. In CogView, we ran SentencePiece on a large Chinese corpus to extract 50,000 text tokens.</p><p>The image tokenizer is a discrete Auto-Encoder, which is similar to the stage 1 of VQ-VAE <ref type="bibr" target="#b45">[46]</ref> or d-VAE <ref type="bibr" target="#b38">[39]</ref>. More specifically, the Encoder ? maps an image The training of the image tokenizer is non-trivial due to the existence of discrete selection. Here we introduce four methods to train an image tokenizer.</p><formula xml:id="formula_4">x of shape H ? W ? 3 into Enc ? (x) of shape h ? w ? d,</formula><p>? The nearest-neighbor mapping, straight-through estimator <ref type="bibr" target="#b1">[2]</ref>, which is proposed by the original VQVAE. A common concern of this method <ref type="bibr" target="#b38">[39]</ref> is that, when the codebook is large and not initialized carefully, only a few of embeddings will be used due to the curse of dimensionality. We did not observe this phenomenon in the experiments.</p><p>? Gumbel sampling, straight-through estimator. If we follow the original VAE to reparameterize a categorical distribution of latent variable z based on distance between vectors,</p><formula xml:id="formula_5">i.e. p(z i?w+j = v k |x) = e ? v k ?Enc ? (x) ij 2 /? |V |?1 k=0 e ? v k ?Enc ? (x) ij 2 /? , an unbiased sampling strategy is z i?w+j = argmax k g k ? v k ? Enc ? (x) ij 2 /?, g k ? Gumbel(0, 1),</formula><p>where the temperature ? is gradually decreased to 0. We can further use the differentiable softmax to approximate the one-hot distribution from argmax. DALL-E adopts this method with many other tricks to stabilize the training.</p><p>? The nearest-neighbor mapping, moving average, where each embedding in the codebook is updated periodically during training as the mean of the vectors recently mapped to it <ref type="bibr" target="#b45">[46]</ref>.</p><p>? The nearest-neighbor mapping, fixed codebook, where the codebook is fixed after initialized. Comparison. To compare the methods, we train four image tokenizers with the same architecture on the same dataset and random seed, and demonstrate the loss curves in <ref type="figure" target="#fig_1">Figure 2</ref>. We find that all the methods are basically evenly matched, meaning that the learning of the embeddings in the codebook is not very important, if initialized properly. In pretraining, we use the tokenizer of moving average method.</p><p>The introduction of data and more details about tokenization are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Auto-regressive Transformer</head><p>The backbone of CogView is a unidirectional Transformer (GPT). The Transformer has 48 layers, with the hidden size of 2560, 40 attention heads and 4 billion parameters in total. As shown in <ref type="figure" target="#fig_3">Figure 3</ref> The pretext task of pretraining is left-to-right token prediction, a.k.a. language modeling. Both image and text tokens are equally treated. DALL-E <ref type="bibr" target="#b38">[39]</ref> suggests to lower the loss weight of text tokens; on the contrary, during small-scale experiments we surprisingly find the text modeling is the key for the success of text-to-image pretraining. If the loss weight of text tokens is set to zero, the model will fail to find the connections between text and image and generate images totally unrelated to the input text.  We hypothesize that text modeling abstracts knowledge in hidden layers, which can be efficiently exploited during the later image modeling.</p><formula xml:id="formula_6">Transformer (GPT) z } | { &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W k m k O Q q V 4 y / G 2 C w E G j e y + G F e k F c = " &gt; A A A C A n i c b V D L S g M x F M 3 U V 6 2 v U V f i J l g E V 2 V G i 7 o s u H F Z w T 6 g M 5 R M e q c N z W S G J C O U o b j x V 9 y 4 U M S t X + H O v z H T z k J b D 4 Q c z r n 3 J v c E C W d K O 8 6 3 V V p Z X V v f K G 9 W t r Z 3 d v f s / Y O 2 i l N J o U V j H s t u Q B R w J q C l m e b Q T S S Q K O D Q C c Y 3 u d 9 5 A K l Y L O 7 1 J A E / I k P B Q k a J N l L f P v J i Y w e S U M i 8 k U r y + 9 J J 9 H T a t 6 t O z Z k B L x O 3 I F V U o N m 3 v 7 x B T N M I h K a c K N V z z R w / I 1 I z y m F a 8 V I F Z v 6 Y D K F n q C A R K D + b r T D F p 0 Y Z 4 D C W 5 g i N Z + r v j o x E S k 2 i w F R G R I / U o p e L / 3 m 9 V I f X f s Z E k m o Q d P 5 Q m H K s Y 5 z n g Q d M A t V 8 Y g i h k p m / Y j o i J g 9 t U q u Y E N z F l Z d J + 7 z m X t S c u 3 q 1 U S / i K K N j d I L O k I u u U A P d o i Z q I Y o e 0 T N 6 R W / W k / V i v V s f 8 9 K S V f Q c o j + w P n 8 A 7 1 2 X u A = = &lt; / l a t e x i t &gt; z } | { &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W k m k O Q q V 4 y / G 2 C w E G j e y + G F e k F c = " &gt; A A A C A n i c b V D L S g M x F M 3 U V 6 2 v U V f i J l g E V 2 V G i 7 o s u H F Z w T 6 g M 5 R M e q c N z W S G J C O U o b j x V 9 y 4 U M S t X + H O v z H T z k J b D 4 Q c z r n 3 J v c E C W d K O 8 6 3 V V p Z X V v f K G 9 W t r Z 3 d v f s / Y O 2 i l N J o U V j H s t u Q B R w J q C l m e b Q T S S Q K O D Q C c Y 3 u d 9 5 A K l Y L O 7 1 J A E / I k P B Q k a J N l L f P v J i Y w e S U M i 8 k U r y + 9 J J 9 H T a t 6 t O z Z k B L x O 3 I F V U o N m 3 v 7 x B T N M I h K a c K N V z z R w / I 1 I z y m F a 8 V I F Z v 6 Y D K F n q C A R K D + b r T D F p 0 Y Z 4 D C W 5 g i N Z + r v j o x E S k 2 i w F R G R I / U o p e L / 3 m 9 V I f X f s Z E k m o Q d P 5 Q m H K s Y 5 z n g Q d M A t V 8 Y g i h k p m / Y j o i J g 9 t U q u Y E N z F l Z d J + 7 z m X t S c u 3 q 1 U S / i K K N j d I L O k I u u U A P d o i Z q I Y o e 0 T N 6 R W / W k / V i v V s f 8 9 K S V f Q c o j + w P n 8 A 7 1 2 X u A = = &lt; / l a t e x i t &gt;</formula><p>We train the model with batch size of 6,144 sequences (6.7 million tokens per batch) for 144,000 steps on 512 V100 GPUs (32GB). The parameters are updated by Adam with max lr = 3 ? 10 ?4 , ? 1 = 0.9, ? 2 = 0.95, weight decay = 4 ? 10 ?2 . The learning rate warms up during the first 2% steps and decays with cosine annealing <ref type="bibr" target="#b33">[34]</ref>. With hyperparameters in an appropriate range, we find that the training loss mainly depends on the total number of trained tokens (tokens per batch ? steps), which means that doubling the batch size (and learning rate) results in a very similar loss if the same number of tokens are trained. Thus, we use a relatively large batch size to improve the parallelism and reduce the percentage of time for communication. We also design a three-region sparse attention to speed up training and save memory without hurting the performance, which is introduced in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Stabilization of training</head><p>Currently, pretraining large models (&gt;2B parameters) usually relies on 16-bit precision to save GPU memory and speed up the computation. Many frameworks, e.g. DeepSpeed ZeRO <ref type="bibr" target="#b39">[40]</ref>, even only support FP16 parameters. However, text-to-image pretraining is very unstable under 16-bit precision. Training a 4B ordinary pre-LN Transformer will quickly result in NaN loss within 1,000 iterations. To stabilize the training is the most challenging part of CogView, which is well-aligned with DALL-E.</p><p>We summarize the solution of DALL-E as to tolerate the numerical problem of training. Since the values and gradients vary dramatically in scale in different layers, they propose a new mixed-precision framework per-resblock loss scaling and store all gains, biases, embeddings, and unembeddings in 32-bit precision, with 32-bit gradients. This solution is complex, consuming extra time and memory and not supported by most current training frameworks.</p><p>CogView instead regularizes the values. We find that there are two kinds of instability: overflow (characterized by NaN losses) and underflow (characterized by diverging loss). The following techniques are proposed to solve them.</p><p>Precision Bottleneck Relaxation (PB-Relax). After analyzing the dynamics of training, we find that overflow always happens at two bottleneck operations, the final LayerNorm or attention.</p><p>? In the deep layers, the values of the outputs could explode to be as large as 10 4 ? 10 5 , making the variation in LayerNorm overflow. Luckily, as LayerNorm(x) = LayerNorm(x/ max(x)), we can relax this bottleneck by dividing the maximum first 4 .</p><p>? The attention scores Q T K/ ? d could be significantly larger than input elements, and result in overflow. Changing the computational order into Q T (K/ ? d) alleviates the problem. To eliminate the overflow, we notice that softmax(Q T K/ constant), meaning that we can change the computation of attention into</p><formula xml:id="formula_7">? d) = softmax(Q T K/ ? d ?</formula><formula xml:id="formula_8">softmax( Q T K ? d ) = softmax Q T ? ? d K ? max( Q T ? ? d K) ? ? ,<label>(4)</label></formula><p>where ? is a big number, e.g. ? = 32. <ref type="bibr" target="#b4">5</ref> In this way, the maximum (absolute value) of attention scores are also divided by ? to prevent it from overflow. A detailed analysis about the attention in CogView is in Appendix C.</p><p>Sandwich LayerNorm (Sandwich-LN). The LayerNorms <ref type="bibr" target="#b0">[1]</ref> in Transformers are essential for stable training. Pre-LN <ref type="bibr" target="#b49">[50]</ref> is proven to converge faster and more stable than the original Post-LN, and becomes the default structure of Transformer layers in recent works. However, it is not enough</p><formula xml:id="formula_9">for text-to-image pretraining. The output of LayerNorm (x?x) ? d ? i (xi?x) 2 ? + ? is basically proportional to the square root of the hidden size of x, which is ? d = ? 2560 ? 50 in CogView.</formula><p>If input values in some dimensions are obviously larger than the others -which is true for Transformers -output values in these dimensions will also be large (10 1 ? 10 2 ). In the residual branch, these large values are magnified and be added back to the main branch, which aggravates this phenomenon in the next layer, and finally causes the value explosion in the deep layers.</p><p>This reason behind value explosion inspires us to restrict the layer-by-layer aggravation. We propose Sandwich LayerNorm, which also adds a LayerNorm at the end of each residual branch. Sandwich-LN ensures the scale of input values in each layer within a reasonable range, and experiments on training 500M model shows that its influence on convergence is negligible. <ref type="figure" target="#fig_4">Figure 4</ref>(a) illustrates different LayerNorm structures in Transformers.</p><p>Toy Experiments. <ref type="figure" target="#fig_4">Figure 4(b)</ref> shows the effectiveness of PB-relax and Sandwich-LN with a toy experimental setting, since training many large models for verification is not realistic. We find that deep transformers (64 layers, 1024 hidden size), large learning rates (0.1 or 0.01), small batch size (4) can simulate the value explosion in training with reasonable hyperparameters. PB-relax + Sandwich-LN can even stabilize the toy experiments.</p><p>Shrink embedding gradient. Although we did not observe any sign of underflow after using Sandwich-LN, we find that the gradient of token embeddings is much larger than that of the other parameters, so that simply shrinking its scale by ? = 0.1 increases the dynamic loss scale to further prevent underflow, which can be implemented by emb=emb*alpha+emb.detach()*(1-alpha) in Pytorch. It seems to slow down the updating of token embeddings, but actually does not hurt performance in our experiments, which also corresponds to a recent work MoCo v3 <ref type="bibr" target="#b8">[9]</ref>.</p><p>Discussion. The PB-relax and Sandwich-LN successfully stabilize the training of CogView and a 8.3B-parameter CogView-large. They are also general for all Transformer pretraining, and will enable the training of very deep Transformers in the future. As an evidence, we used PB-relax successfully eliminating the overflow in training a 10B-parameter GLM <ref type="bibr" target="#b13">[14]</ref>. However, in general, the precision problems in language pretraining is not so significant as in text-to-image pretraining. We hypothesize that the root is the heterogeneity of data, because we observed that text and image tokens are distinguished by scale in some hidden states. Another possible reason is hard-to-find underflow, guessed by DALL-E. A thorough investigation is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Finetuning</head><p>CogView steps further than DALL-E on finetuning. Especially, we can improve the text-to-image generation via finetuning CogView for super-resolution and self-reranking. All the finetuning tasks can be completed within one day on a single DGX-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Super-resolution</head><p>Since the image tokenizer compresses 256 ? 256-pixel images into 32 ? 32-token sequences before training, the generated images are blurrier than real images due to the lossy compression. However, enlarging the sequence length will consume much more computation and memory due to the O(n 2 ) complex of attention operations. Previous works <ref type="bibr" target="#b12">[13]</ref> about super-resolution, or image restoration, usually deal with images already in high resolution, mapping the blurred local textures to clear ones. They cannot be applied to our case, where we need to add meaningful details to the generated low-resolution images. <ref type="figure" target="#fig_6">Figure 5 (b)</ref> is an example of our finetuning method, and illustrates our desired behavior of super-resolution.</p><p>The motivation of our finetuning solution for super-resolution is a belief that CogView is trained on the most complex distribution in general domain, and the objects of different resolution has already been covered. <ref type="bibr" target="#b5">6</ref> Therefore, finetuning CogView for super-resolution should not be hard.</p><p>Specifically, we first finetune CogView into a conditional super-resolution model from 16 ? 16 image tokens to 32 ? 32 tokens. Then we magnify an image of 32 ? 32 tokens to 64 ? 64 tokens (512 ? 512 pixels) patch-by-patch via a center-continuous sliding-window strategy in <ref type="figure" target="#fig_6">Figure 5</ref> (a). This order performs better that the raster-scan order in preserving the completeness of the central area.</p><p>To prepare data, we crop about 2 million images to 256 ? 256 regions and downsample them to 128 ? 128. After tokenization, we get 32 ? 32 and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Captioning and Self-reranking</head><p>To finetune CogView for image captioning is straightforward: exchanging the order of text and image tokens in the input sequences. Since the model has already learnt the corresponding relationships between text and images, reversing the generation is not hard. We did not evaluate the performance due to that (1) there is no authoritative Chinese image captioning benchmark (2) image captioning is not the focus of this work. The main purpose of finetuning such a model is for self-reranking.</p><p>We propose the Caption Loss (CapLoss) to evaluate the correspondence between images and text. More specifically, CapLoss(x, t) = 1 |t| |t| i=0 ? log p(t i |x, t 0:i?1 ), where t is a sequence of text tokens and x is the image. CapLoss(x, t) is the cross-entropy loss for the text tokens, and this method can be seen as an adaptation of inverse prompting <ref type="bibr" target="#b55">[56]</ref> for text-to-image generation. Finally, images with the lowest CapLosses are chosen.</p><p>Compared to additionally training another constrastive self-supervised model, e.g. CLIP <ref type="bibr" target="#b37">[38]</ref>, for reranking, our method consumes less computational resource because we only need finetuning. The results in <ref type="figure">Figure 9</ref> shows the images selected by our methods performs better in FID than those selected by CLIP. <ref type="figure">Figure 6</ref> shows an example for reranking. <ref type="figure">Figure 6</ref>: 60 generated images for "A man in red shirt is playing video games" (selected at random from COCO), displayed in the order of CapLoss. Most bad cases are ranked in last places. The diversity also eases the concern that CogView might be overfitting a similar image in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Style Learning</head><p>Although CogView is pretrained to cover diverse images as possible, the desire to generate images of a specific style or topic cannot be satisfied well. We finetune models on four styles: Chinese traditional drawing, oil painting, sketch, and cartoon. Images of these styles are automatically extracted from search engine pages including Google, Baidu and Bing, etc., with keyword as "An image of {style} style", where {style} is the name of style. We finetune the model for different styles separately, with 1,000 images each.</p><p>During finetuning, the corresponding text for the images are also "An image of {style} style". When generating, the text is "A {object} of {style} style", where {object} is the object to generate. In this way, CogView can transfer the knowledge of shape of the objects learned from pretraining to the style of finetuning. <ref type="figure" target="#fig_7">Figure 7</ref> shows examples for the styles.  When the generation targets at a single domain, the complexity of the textures are largely reduced. In these scenarios, we can (1) train a VQGAN <ref type="bibr" target="#b14">[15]</ref> instead of VQVAE for the latent variable for more realistic textures, (2) decrease the number of parameters and increase the length of sequences for a higher resolution. Our three-region sparse attention (Appendix B) can speed up the generation of high-resolution images in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Industrial Fashion Design</head><p>We train a 3B-parameter model on about 10 million fashion-caption pairs, using 50?50 VQGAN image tokens and decodes them into 800 ? 800 pixels. <ref type="figure" target="#fig_8">Figure 8</ref> shows samples of CogView for fashion design, which has been successfully deployed to Alibaba Rhino fashion production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine Evaluation</head><p>At present, the most authoritative machine evaluation metrics for general-domain text-to-image generation is the FID on MS COCO, which is not included in our training set. To compare with DALL-E, we follow the same setting, evaluating CogView on a subset of 30,000 captions sampled from the dataset, after applying a Gaussian filter with varying radius to both the ground-truth and generated images. <ref type="bibr" target="#b7">8</ref> The captions are translated into Chinese for CogView by machine translation. To fairly compare with DALL-E, we do not use super-resolution. Besides, DALL-E generates 512 images for each caption and selects the best one by CLIP, which needs to generate about 15 billion tokens. To save computational resource, we select the best one from 60 generated images according to their CapLosses. The evaluation of CapLoss is on a subset of 5,000 images. We finally enhance the contrast of generated images by 1.5. <ref type="table" target="#tab_1">Table 1</ref> shows the metrics for CogView and other methods. Caption Loss as a Metric. FID and IS are designed to measure the quality of unconditional generation from relatively simple distributions, usually single objects. However, text-to-image generation should be evaluated pair-by-pair. <ref type="table" target="#tab_1">Table 1</ref> shows that DM-GAN achieves the best unblurred FID and IS, but is ranked last in human preference <ref type="figure">(Figure 10(a)</ref>). Caption Loss is an absolute (instead of relative, like CLIP) score, so that it can be averaged across samples. It should be a better metrics for this task and is more consistent with the overall scores of our human evaluation in ? 4.2. <ref type="figure">Figure 9</ref>: IS and FID-0 for CLIP and self-ranking.</p><p>Comparing self-reranking with CLIP. We evaluate the FID-0 and IS of CogView-generated images selected by CLIP and self-reranking on MS COCO. <ref type="figure">Figure 9</ref> shows the curves with different number of candidates. Self-reranking gets better FID, and steadily refines FID as the number of candidates increases. CLIP performs better in increasing IS, but as discussed above, it is not a suitable metric for this task.</p><p>Discussion about the differences in performance between CogView and DALL-E. Since DALL-E is pretrained with more data and parameters than CogView, why CogView gets a better FID even without super-resolution? It is hard to know the accurate reason, because DALL-E is not open-source, but we guess that the reasons include: (1) CogView uses PB-relax and Sandwich-LN for a more stable optimization.</p><p>(2) DALL-E uses many cartoon and rendered data, making the texture of generated images quite different from that of the photos in MS COCO. (3) Self-reranking selects images better in FID than CLIP. (4) CogView is trained longer (96B trained tokens in CogView vs. 56B trained tokens in DALL-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Evaluation</head><p>Human evaluation is much more persuasive than machine evaluation on text-to-image generation. Our human evaluation consists of 2,950 groups of comparison between images generated by AttnGAN, DM-GAN, DF-GAN, CogView, and recovered ground truth, i.e., the ground truth blurred by our image tokenizer. Details and example-based comparison between models are in Appendix E.</p><p>Results in <ref type="figure">Figure 10</ref> show that CogView outperforms GAN-based baselines at a large margin. CogView is chosen as the best one with probability 37.02%, competitive with the performance of recovered ground truth (59.53%). <ref type="figure">Figure 10</ref>(b)(c) also indicates our super-resolution model consistently improves the quality of images, especially the clarity, which even outperforms the recovered ground truth. <ref type="figure">Figure 10</ref>: Human Evaluation results. The recovered ground truth is obtained by first encoding the ground truth image and then decoding it, which is theoretically the upper bound of CogView.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>Limitations. A disadvantage of CogView is the slow generation, which is common for auto-regressive model, because each image is generated token-by-token. The blurriness brought by VQVAE is also an important limitation. These problems will be solved in the future work.</p><p>Ethics Concerns. Similar to Deepfake, CogView is vulnerable to malicious use <ref type="bibr" target="#b48">[49]</ref> because of its controllable and strong capacity to generate images. The possible methods to mitigate this issue are discussed in a survey <ref type="bibr" target="#b4">[5]</ref>. Moreover, there are usually fairness problems in generative models about human <ref type="bibr" target="#b8">9</ref> . In Appendix D, we analyze the situation about fairness in CogView and introduce a simple "word replacing" method to solve this problem.</p><p>We systematically investigate the framework of combining VQVAE and Transformers for text-toimage generation. CogView demonstrates promising results for scalable cross-modal generative pretraining, and also reveals and solves the precision problems probably originating from data heterogeneity. We also introduce methods to finetune CogView for diverse downstream tasks. We hope that CogView could advance both research and application of controllable image generation and cross-modal knowledge understanding, but need to prevent it from being used to create images for misinformation. <ref type="bibr" target="#b8">9</ref> https://thegradient.pub/pulse-lessons</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Collection and Details about the Tokenizers</head><p>We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB). The dataset is an extension of project WudaoCorpora <ref type="bibr" target="#b51">[52]</ref>  <ref type="bibr" target="#b9">10</ref> . About 50% of the text is in English, including Conceptual Captions <ref type="bibr" target="#b43">[44]</ref>. They are translated into Chinese by machine translation. In addition, we did not remove the watermarks and white edges in the dataset even though they affect the quality of generated images, because we think it will not influence the conclusions of our paper from the perspective of research.</p><p>The sources of data are basically classified into the following categories: (1) Professional image websites (both English and Chinese). The images in the websites are usually with captions. Data from this channel constitute the highest proportion.</p><p>(2) Conceptual Captions <ref type="bibr" target="#b43">[44]</ref> and ImageNet <ref type="bibr" target="#b10">[11]</ref>.</p><p>(3) News pictures online with their surrounding text. (4) A small part of item-caption pairs from Alibaba . (5) Image search engines. In order to cover as many common entities as possible, we made a query list consist of 1,200 queries. Every query was an entity name extracted from a large-scale knowledge graph. We choose seven major categories: food, regions, species, people names, scenic, products and artistic works. We extracted top-k entities for each category based on their number of occurrences in the English Wikipedia, where k is manually selected for each category. We collected the top-100 images returned by every major search engine website for each query.</p><p>We have already introduced tokenizers in section 2.2, and here are some details. The text tokenizer are directly based on the SentencePiece package at https://github.com/google/sentencepiece. The encoder in the image tokenizer is a 4-layer convolutional neural network (CNN) with 512 hidden units and ReLU activation each layer. The first three layers have a receptive field of 4 and stride of 2 to half the width and height of images, and the final layer is a 1 ? 1 convolution to transform the number of channels to 256, which is the hidden size of embeddings in the dictionary. The decoder have the same architecture with the encoder except replacing convolution as deconvolution. The embeddings in the dictionary are initialized via Xavier uniform initialization <ref type="bibr" target="#b17">[18]</ref>. <ref type="table">O   Text  Text  Text  Text  Text  Text  Text  Text</ref> All texts and some random "pivot" attention + Blockwise window attention <ref type="figure">Figure 11</ref>: Illustration about our threeregion sparse attention. The sequence is shown as a H ? W image and some text tokens in front. Colored grids are all the tokens attended to by the token marked "O". In this case, each block consists of four consecutive tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sparse Attention</head><p>As shown in <ref type="figure">Figure 11</ref>, we design the three-region sparse attention, an implementation-friendly sparse attention for text-to-image generation. Each token attends to all text tokens, all pivots tokens and tokens in the blocks in an adjacent window before it.</p><p>The pivot tokens are image tokens selected at random, similar to big bird <ref type="bibr" target="#b52">[53]</ref>. They are re-sampled every time we enter a new layer. We think they can provide global information about the image.</p><p>The blockwise window attention provides local information, which is the most important region. The forward computation of 1-D window attention can be efficiently implemented inplace by carefully padding and altering the strides of tensors, because the positions to be attended are already continguous in memory. However, we still need extra memory for backward computation if without customized CUDA kernels. We alleviate this problem by grouping adjacent tokens into blocks, in which all the tokens attend to the same tokens (before causally masking). More details are included in our released codes.</p><p>In our benchmarking on sequences of 4096 tokens, the three-region sparse attention (768 text and pivot tokens, 768 blockwise window tokens) is 2.5? faster than vanilla attention, and saves 40% GPU memory.</p><p>The whole training is 1.5? faster than that with vanilla attention and saves 20% GPU memory. With the same hyperparameters, data and random seeds, their loss curves are nearly identical, which means the sparse attention will not influence the convergence.</p><p>However, we did not use three-region sparse attention during training the 4-billion-parameter CogView, due to the concern that it was probably not compatible with finetuning for super-resolution in section 3.1. But it successfully accelerated the training of CogView-fashion without side effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Attention Analysis</head><p>To explore the attention mechanism of CogView, we visualize the attention distribution during inference by plotting heat maps and marking the most attended tokens. We discover that our model's attention heads exhibit strong ability on capturing both position and semantic information, and attention distribution varies among different layers. The analysis about the scale of attention scores is in section C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Positional Bias</head><p>The attention distribution is highly related to images' position structures. There are a lot of heads heavily attending to fixed positional offsets, especially multiple of 32 (which is the number of tokens a row contains) <ref type="figure" target="#fig_1">(Figure 12 (a)</ref>). Some heads are specialized to attending to the first few rows in the image <ref type="figure" target="#fig_1">(Figure12 (b)</ref>) . Some heads' heat maps show checkers pattern <ref type="figure" target="#fig_1">(Figure 12 (c)</ref>), indicating tokens at the boundary attends differently from that at the center. Deeper layers also show some broad structural bias. For example, some heads attend heavily on tokens at top/lower half or the center of images ( <ref type="figure" target="#fig_1">Figure 12 (d)</ref>(e)). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Semantic Segmentation</head><p>The attention in CogView also shows that it also performs implicit semantic segmentation. Some heads highlight major items mentioned in the text. We use "There is an apple on the table, and there is a vase beside it, with purple flowers in it." as input of our experiment. In <ref type="figure" target="#fig_3">Figure 13</ref> we marked pixels corresponding to the most highly attended tokens with red dots, and find that attention heads successfully captured items like apple and purple flowers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Attention Varies with Depth</head><p>Attention patterns varies among different layers. Earlier layers focus mostly on positional information, while later ones focus more on the content. Interestingly, we observe that attention become sparse in the last few layers (after layer 42), with a lot of heads only attend to a few tokens such as separator tokens <ref type="figure" target="#fig_1">(Figure 12 (f)</ref>). One possible explanation is that those last layers tend to concentrate on current token to determine the output token, and attention to separator tokens may be used as a no-op for attention heads which does not substantially change model's output, similar to the analysis in BERT <ref type="bibr" target="#b9">[10]</ref>. As the result, the last layers' heads disregard most tokens and make the attention layers degenerate into feed-forward layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Value Scales of Attention</head><p>As a supplement to section 2.4, we visualize the value scales of attention in the 38-th layer, which has the largest scale of attention scores Q T K/ ? d in CogView. The scales varies dramatically in different heads, but the variance in each single head is small (that is why the attention does not degenerate, even though the scores are large). We think the cause is that the model wants different sensitiveness in different heads, so that it learns to multiply different constants to get Q and K. As a side effect, the values may have a large bias. The PB-relax for attention is to remove the bias during computation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Fairness in CogView: Situation and Solution</head><p>Evaluation of the situation of fairness in CogView. We examine the bias in the proportion of different races and genders. Firstly, if given the detailed description in the text, e.g. a black man or an Asian woman, CogView can generate correctly for almost all samples. We also measure the proportion of the generated samples without specific description by the text "a face, photo". The figure of proportions in different races and genders are in <ref type="figure" target="#fig_6">Figure 15</ref>. The (unconditional) generated faces are relatively balanced in races and ages, but with more men than women due to the data distribution.</p><p>CogView is also beset by the bias in gender due to the stereotypes if not specifying the gender. However if we specify the gender, almost all the gender and occupation are correct. We tested the examples introduced in <ref type="bibr" target="#b5">[6]</ref>, and generated images for the text {male, female} ? {"science", mathematics", "arts", "literature"}. Results are showed in this outer link to reduce the size of our paper.</p><p>Word Replacing Solution. Different from the previous unconditional generative models, we have a very simple and effective solution for racial and gender fairness.</p><p>We can directly add some adjective words sampled from "white", "black", "Asian", ..., and "male", "female" (if not specified) in the front of the words for human, like "people" or "person", in the text. The sampling is according to the real proportion in the whole population. We can train an additional NER model to find the words about human.</p><p>Since CogView will predict correctly according to the results above, if given description, this method will greatly help solve the fairness problem in generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details about Human Evaluation</head><p>To evaluate the performance, we conduct a human evaluation to make comparisons between various methods, similar to previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>. In our designed evaluation, 50 images and their captions are randomly selected from the MS COCO dataset. For each image, we use the caption to generate images based on multiple models including AttnGAN, DM-GAN, DF-GAN and CogView. We do not generate images with DALL-E as their model has not been released yet. For each caption, evaluators are asked to give scores to 4 generated images and the recovered ground truth image respectively. The recovered ground truth image refers to the image obtained by first encoding the ground truth image (the original image in the MS COCO dataset after cropped into the target size) and then decoding it.</p><p>For each image, evaluators first need to give 3 scores (1 ? 5) to evaluate the image quality from three aspects: the image clarity, the texture quality and the relevance to the caption. Then, evaluators will give an overall score (1 ? 10) to the image. After all 5 images with the same caption are evaluated, evaluators are required to select the best image additionally. 72 anonymous evaluators are invited in the evaluation. To ensure the validity of the evaluation results, we only collect answers from evaluators who complete all questions and over 80% of the selected best images are accord with the one with the highest overall quality score. Finally, 59 evaluators are kept. Each evaluator is awarded with 150 yuan for the evaluation. There is no time limit for the answer.</p><p>To further evaluate the effectiveness of super-resolution, we also introduced a simple A-B test in the human evaluation. Evaluators and captions are randomly divided into two groups E a , E b and C a , C b respectively. For evaluators in E a , the CogView images with captions from C a are generated without super-resolution while those from C b are generated with super-resolution. The evaluators in E b do the reverse. Finally, we collected equal number of evaluation results for CogView images with and without super-resolution.</p><p>The average scores and their standard deviation are plotted in <ref type="figure">Figure 10</ref>. Several examples of captions and images used in the human evaluation are listed in <ref type="figure" target="#fig_5">Figure 16</ref>. The evaluation website snapshots are displayed in <ref type="figure" target="#fig_7">Figure 17</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>????????</head><p>The reflection of the house in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?????????? ??</head><p>A picture of the pier with birds flying above.</p><p>?????????? ????? Three plush bears hug and sit on blue pillows</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?????????? ????</head><p>A city bus driving on the city street</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?????????? ??????</head><p>A woman is skiing on a white mountain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?????????? ??</head><p>A cat is standing in the dresser drawer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?????????? ?????????? ??</head><p>A very cute stuffed animal with a funny hat.</p><p>?????????? ????????? Close-up of a man eating a piece of pizza while holding a plate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Show Cases for captions from MS COCO</head><p>In <ref type="figure" target="#fig_8">Figure 18</ref>, we provide further examples of CogView on MS COCO. <ref type="figure" target="#fig_7">Figure 17</ref>: Snapshots of the human evaluation website. The left side is the scoring page for images and the right side is the best-selection page for all images with the same caption. <ref type="figure" target="#fig_8">Figure 18</ref>: More generated images for COCO captions (after super-resolution).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and then each d?dimensional vector is quantized to a nearby embedding in a learnable codebook {v 0 , ..., v |V |?1 }, ?v k ? R d . The quantized result can be represented by h ? w indices of embeddings, and then we get the latent variable z ? {0, ..., |V | ? 1} h?w . The Decoder ? maps the quantized vectors back to a (blurred) image to reconstruct the input. In our 4B-parameter CogView, |V | = 8192, d = 256, H = W = 256, h = w = 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>L 2 loss curves during training image tokenizers. All the above methods finally converge to a similar loss level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, four seperator tokens, [ROI1] (reference text of image), [BASE], [BOI1] (beginning of image), [EOI1] (end of image) are added to each sequence to indicate the boundaries of text and image. All the sequences are clipped or padded to a length of 1088.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The framework of CogView. [ROI1], [BASE1], etc., are seperator tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(a) Illustration of different LayerNorm structures in Transformers. Post-LN is from the original paper; Pre-LN is the most popular structure currently; Sandwich-LN is our proposed structure to stabilize training. (b) The numerical scales in our toy experiments with 64 layers and a large learning rate. Trainings without Sandwich-LN overflow in main branch; trainings without PB-relax overflow in attention; Only the training with both can continue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>16 ?</head><label>16</label><figDesc>16 sequence pairs for different resolution. The pattern of finetuning sequence is "[ROI1] text tokens [BASE][BOI1] 16 ? 16 image tokens [EOI1] [ROI2][BASE] [BOI2] 32 ? 32 image tokens [EOI2]", longer than the max position embedding index 1087. As a solution, we recount the position index from 0 at [ROI2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>(a) A 64 ? 64-token image are generated patch-by-patch in the numerical order. The overlapping positions will not be overwritten. The key idea is to make the tokens in the 2nd and 4th regions -usually regions of faces or other important parts -generated when attending to the whole region.(b) The finetuned super-resolution model does not barely transform the textures, but generates new local structures, e.g. the open mouth or tail in the example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Generated images for "The Oriental Pearl" (a landmark of Shanghai) in different styles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Generated images for fashion design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>(a)(b)(c) Our model's attention is highly related to images' positional structures. (d)(e) Our model's attention show some broad structural bias. (f) Some heads only attend to a few tokens such as separator token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Our model's attention heads successfully captured items like apple and purple flowers. Pixels corresponding to the most highly attended tokens are marked with red dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Illustration of scales of attention scores in the 38-th layer. Only half are heads are shown for display reasons. The error bar is from the minimum to the maximum of scores. The values of text-to-text attention scores are smaller, indicating the scales are related to the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>The distribution of different genders, races and ages of the generation of "a face, photo".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Human evaluation examples. The captions for evaluation are selected at random from MS COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Text Tokenizer (sentence pieces) Image Tokenizer (Discrete AutoEncoder)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Input Text:</cell><cell></cell><cell cols="2">Input Image:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell>Discretize</cell><cell>Recover</cell><cell>Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Flattern</cell><cell></cell><cell></cell></row><row><cell>[ROI1]</cell><cell>Text Token</cell><cell>Text Token</cell><cell>[BASE]</cell><cell>[BOI1]</cell><cell>Image Token</cell><cell cols="2">Image Token</cell><cell>[EOI1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Metrics for machine evaluation. Statistics about DALL-E and GANs are extracted from their figures. FID-k means that all the images are blurred by a Gaussian Filter with radius k.</figDesc><table><row><cell>Model</cell><cell cols="5">FID-0 FID-1 FID-2 FID-4 FID-8</cell><cell>IS</cell><cell>CapLoss</cell></row><row><cell>AttnGAN</cell><cell>35.2</cell><cell>44.0</cell><cell>72.0</cell><cell cols="3">108.0 100.0 23.3</cell><cell>3.01</cell></row><row><cell>DM-GAN</cell><cell>26.5</cell><cell>39.0</cell><cell>73.0</cell><cell cols="3">119.0 112.3 32.2</cell><cell>2.87</cell></row><row><cell>DF-GAN</cell><cell>26.5</cell><cell>33.8</cell><cell>55.9</cell><cell>91.0</cell><cell>97.0</cell><cell>18.7</cell><cell>3.09</cell></row><row><cell>DALL-E</cell><cell>27.5</cell><cell>28.0</cell><cell>45.5</cell><cell>83.5</cell><cell>85.0</cell><cell>17.9</cell><cell>-</cell></row><row><cell>CogView</cell><cell>27.1</cell><cell>19.4</cell><cell>13.9</cell><cell>19.4</cell><cell>23.6</cell><cell>18.2</cell><cell>2.43</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">meaning that all computation, including forwarding and backwarding are in FP16 without any conversion, but the optimizer states and the master weights are FP32.<ref type="bibr" target="#b2">3</ref> In this paper, bold font denotes a random variable, and regular font denotes a concrete value. See this comprehensive tutorial<ref type="bibr" target="#b11">[12]</ref> for the basics of VAE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We cannot directly divide x by a large constant, which will lead to underflow in the early stage of training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The max must be at least head-wise, because the values vary greatly in different heads.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">An evidence to support the belief is that if we append "close-up view" at the end of the text, the model will generate details of a part of the object.<ref type="bibr" target="#b6">7</ref> One might worry about that the reuse of position indices could cause confusions, but in practice, the model can distinguish the two images well, probably based on whether they can attend to a [ROI2] in front.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We use the same evaluation codes with DM-GAN and DALL-E, which is available at https://github. com/MinfengZhu/DM-GAN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://wudaoai.cn/data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank Zhao Xue, Zhengxiao Du, Hanxiao Qu, Hanyu Zhao, Sha Yuan, Yukuo Cen, Xiao Liu, An Yang, Yiming Ju for their help in data, machine maintaining or discussion. We would also thank Zhilin Yang for presenting this work at the conference of BAAI.</p><p>Funding in direct support of this work: a fund for GPUs donated by BAAI, a research fund from Alibaba Group, NSFC for Distinguished Young Scholar (61825602), NSFC (61836013).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal feature integration in the angular gyrus during episodic and semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Bonnici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Simons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5462" to="5471" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The malicious use of artificial intelligence: Forecasting, prevention, and mitigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eckersley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garfinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dafoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scharre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeitzoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Filar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07228</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The road from MLE to EM to VAE: A brief tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<ptr target="https://www.researchgate.net/profile/Ming-Ding-2/publication/342347643_The_Road_from_MLE_to_EM_to_VAE_A_Brief_Tutorial/links/5f1e986792851cd5fa4b2290/The-Road-from-MLE-to-EM-to-VAE-A-Brief-Tutorial.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gasquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>C?zanne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1926" />
			<biblScope unit="page" from="159" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The human visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grill-Spector</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Malach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="649" to="677" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lagging inference networks and posterior collapse in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text-to-image generation grounded by fine-grained user attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12174" to="12182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00823</idno>
		<title level="m">A chinese multimodal pretrainer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Gpt understands, too. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00446</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The emergence of deepfake technology: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Westerlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology Innovation Management Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Wudaocorpora: A super large-scale chinese corpora for pre-training language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Controllable generation from pre-trained language models via inverse prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10685</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
