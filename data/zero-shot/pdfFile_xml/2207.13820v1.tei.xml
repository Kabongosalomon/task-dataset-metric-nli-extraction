<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyeong</forename><surname>Cho</surname></persName>
							<email>junhyeong99@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Youwang</surname></persName>
							<email>youwang.kim@postech.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of EE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of EE</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of AI Pohang</orgName>
								<orgName type="institution">University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human mesh recovery</term>
					<term>transformer</term>
					<term>encoder-decoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer encoder architectures have recently achieved state-of-the-art results on monocular 3D human mesh reconstruction, but they require a substantial number of parameters and expensive computations. Due to the large memory overhead and slow inference speed, it is difficult to deploy such models for practical use. In this paper, we propose a novel transformer encoder-decoder architecture for 3D human mesh reconstruction from a single image, called FastMETRO. We identify the performance bottleneck in the encoder-based transformers is caused by the token design which introduces high complexity interactions among input tokens. We disentangle the interactions via an encoder-decoder architecture, which allows our model to demand much fewer parameters and shorter inference time. In addition, we impose the prior knowledge of human body's morphological relationship via attention masking and mesh upsampling operations, which leads to faster convergence with higher accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency, and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore, we validate its generalizability on FreiHAND.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose and shape estimation models aim to estimate 3D coordinates of human body joints and mesh vertices. These models can be deployed in a wide range of applications that require human behavior understanding, e.g., human motion analysis and human-computer interaction. To utilize such models for practical use, monocular methods <ref type="bibr">[2, 9, 16, 17, 21-23, 25, 26, 36, 39, 43, 47]</ref> estimate the 3D joints and vertices without using 3D scanners or stereo cameras. This task is essentially challenging due to complex human body articulation, and becomes more difficult by occlusions and depth ambiguity in monocular settings.</p><p>To deal with such challenges, state-of-the-art methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> exploit non-local relations among human body joints and mesh vertices via transformer encoder architectures. This leads to impressive improvements in accuracy by consuming a substantial number of parameters and expensive computations as trade-offs; efficiency is less taken into account, although it is crucial in practice.  <ref type="figure">Fig. 1</ref>. Comparison with encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and our models on Human3.6M <ref type="bibr" target="#b14">[15]</ref>. Our FastMETRO substantially improves the Pareto-front of accuracy and efficiency. ? indicates training for 60 epochs, and * denotes training for 200 epochs.</p><p>In this paper, we propose FastMETRO which employs a novel transformer encoder-decoder architecture for 3D human pose and shape estimation from an input image. Compared with the transformer encoders <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, FastMETRO is more practical because it achieves competitive results with much fewer parameters and faster inference speed, as shown in <ref type="figure">Figure 1</ref>. Our architecture is motivated by the observation that the encoder-based methods overlook the importance of the token design which is a key-factor in accuracy and efficiency.</p><p>The encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> share similar transformer encoder architectures. They take K joint and N vertex tokens as input for the estimation of 3D human body joints and mesh vertices, where K and N denote the number of joints and vertices in a 3D human mesh, respectively. Each token is constructed by the concatenation of a global image feature vector x ? R C and 3D coordinates of a joint or vertex in the human mesh. This results in the input tokens of dimension R (K+N )?(C+3) which are fed as input to the transformer encoders. <ref type="bibr" target="#b0">1</ref> This token design introduces the same sources of the performance bottleneck: 1) spatial information is lost in the global image feature x, and 2) the same image feature x is used in an overly-duplicated way. The former is caused by the average pooling operation to obtain the global image feature x. The latter leads to considerable inefficiency, since expensive computations are required to process mostly duplicated information, where distinctively informative signals are only in 0.15% of the input tokens. 2 Furthermore, the computational complexity of each transformer layer is quadratic as O(L 2 C + LC 2 ), where L ? K + N . Once either L or C is dominantly larger, it results in unfavorable efficiency. Both methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> are such undesirable cases. In contrast, our FastMETRO does not concatenate an image feature vector for the construction of input tokens. As illustrated in <ref type="figure">Figure 2</ref>, we disentangle the image encoding part and mesh estimation part via an encoder-decoder architecture. Our joint and vertex tokens focus on certain image regions through cross-attention modules in the transformer decoder. In this way, the proposed method efficiently estimates the 3D coordinates of human body joints and mesh vertices from a 2D image. To effectively capture non-local joint-vertex relations and local vertex-vertex relations, we mask self-attentions of non-adjacent vertices according to the topology of human triangle mesh. To avoid the redundancy caused by the spatial locality of human mesh vertices, we perform coarse-to-fine mesh upsampling as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. By leveraging the prior knowledge of human body's morphological relationship, we substantially reduce optimization difficulty. This leads to faster convergence with higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flattened Image Features</head><p>We present the proposed method with model-size variants by changing the number of transformer layers: FastMETRO-S, FastMETRO-M, FastMETRO-L. Compared with the encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, FastMETRO-S requires only about 9% of the parameters in the transformer architecture, but shows competitive results with much faster inference speed. In addition, the large variant (FastMETRO-L) achieves the state of the art on the Human3.6M <ref type="bibr" target="#b14">[15]</ref> and 3DPW <ref type="bibr" target="#b33">[34]</ref> datasets among image-based methods, which also demands fewer parameters and shorter inference time compared with the encoder-based methods. We demonstrate the effectiveness of the proposed method by conducting extensive experiments, and validate its generalizability by showing 3D hand mesh reconstruction results on the FreiHAND <ref type="bibr" target="#b49">[50]</ref> dataset.</p><p>Our contributions are summarized as follows:</p><p>? We propose FastMETRO which employs a novel transformer encoder-decoder architecture for 3D human mesh recovery from a single image. Our method resolves the performance bottleneck in the encoder-based transformers, and improves the Pareto-front of accuracy and efficiency. ? The proposed model converges much faster by reducing optimization difficulty.</p><p>Our FastMETRO leverages the prior knowledge of human body's morphological relationship, e.g., masking attentions according to the human mesh topology. ? We present model-size variants of our FastMETRO. The small variant shows competitive results with much fewer parameters and faster inference speed. The large variant clearly outperforms existing image-based methods on the Human3.6M and 3DPW datasets, which is also more lightweight and faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our proposed method aims to estimate the 3D coordinates of human mesh vertices from an input image by leveraging the attention mechanism in the transformer architecture. We briefly review relevant methods in this section. Human Mesh Reconstruction. The reconstruction methods belong to one of the two categories: parametric approach and non-parametric approach. The parametric approach learns to estimate the parameters of a human body model such as SMPL <ref type="bibr" target="#b29">[30]</ref>. On the other hand, the non-parametric approach learns to directly regress the 3D coordinates of human mesh vertices. They obtain the 3D coordinates of human body joints via linear regression from the estimated mesh.</p><p>The reconstruction methods in the parametric approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref> have shown stable performance in monocular 3D human mesh recovery. They have achieved the robustness to environment variations by exploiting the human body prior encoded in a human body model such as SMPL <ref type="bibr" target="#b29">[30]</ref>. However, their regression targets are difficult for deep neural networks to learn; the pose space in the human body model is expressed by the 3D rotations of human body joints, where the regression of the 3D rotations is challenging <ref type="bibr" target="#b32">[33]</ref>.</p><p>Recent advances in deep neural networks have enabled the non-parametric approach with promising performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. Kolotouros et al . <ref type="bibr" target="#b22">[23]</ref> propose a graph convolutional neural network (GCNN) <ref type="bibr" target="#b18">[19]</ref> to effectively learn local vertex-vertex relations, where the graph structure is based on the topology of SMPL human triangle mesh <ref type="bibr" target="#b29">[30]</ref>. They extract a global image feature vector through a CNN backbone, then construct vertex embeddings by concatenating the image feature vector with the 3D coordinates of vertices in the human mesh. After iterative updates via graph convolutional layers, they estimate the 3D locations of human mesh vertices. To improve the robustness to partial occlusions, Lin et al . <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> propose transformer encoder architectures which effectively learn the non-local relations among human body joints and mesh vertices via the attention mechanism in the transformer. Their models, METRO <ref type="bibr" target="#b24">[25]</ref> and Mesh Graphormer <ref type="bibr" target="#b25">[26]</ref>, follow the similar framework with the GCNN-based method <ref type="bibr" target="#b22">[23]</ref>. They construct vertex tokens by attaching a global image feature vector to the 3D coordinates of vertices in the human mesh. After several updates via transformer encoder layers, they regress the 3D coordinates of human mesh vertices.</p><p>Among the reconstruction methods, METRO <ref type="bibr" target="#b24">[25]</ref> and Mesh Graphormer <ref type="bibr" target="#b25">[26]</ref> are the most relevant work to our FastMETRO. We found that the token design in those methods leads to a substantial number of unnecessary parameters and computations. In their architectures, transformer encoders take all the burdens to learn complex relations among mesh vertices, along with the highly non-linear mapping between 2D space and 3D space. To resolve this issue, we disentangle the image-encoding and mesh-estimation parts via an encoder-decoder architecture. This makes FastMETRO more lightweight and faster, and allows our model to learn the complex relations more effectively. Transformers. Vaswani et al . <ref type="bibr" target="#b43">[44]</ref> introduce a transformer architecture which effectively learns long-range relations through the attention mechanism in the transformer. This architecture has achieved impressive improvements in diverse computer vision tasks <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. Dosovitskiy et al . <ref type="bibr" target="#b7">[8]</ref> present a transformer encoder architecture, where a learnable token aggregates image features via self-attentions for image classification. Carion et al . <ref type="bibr" target="#b2">[3]</ref> propose a transformer encoder-decoder architecture, where learnable tokens focus on certain image regions via cross-attentions for object detection. Those transformers have the most relevant architectures to our model.</p><p>Our FastMETRO employs a transformer encoder-decoder architecture, whose decoupled structure is favorable to learn the complex relations between the heterogeneous modalities of 2D image and 3D mesh. Compared with the existing transformers <ref type="bibr">[3-5, 8, 13, 18, 28, 32, 41, 46, 48]</ref>, we progressively reduce hidden dimension sizes in the transformer architecture as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Our separate decoder design enables FastMETRO to easily impose the human body prior by masking self-attentions of decoder input tokens, which leads to stable optimization and higher accuracy. This is novel in transformer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a novel method, called Fast MEsh TRansfOrmer (FastMETRO). FastMETRO has a transformer encoder-decoder architecture for 3D human mesh recovery from an input image. The overview of our method is shown in <ref type="figure">Figure 2</ref>. The details of our transformer encoder and decoder are illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extractor</head><p>Given a single RGB image, our model extracts image features X I ? R H?W ?C through a CNN backbone, where H ? W denotes the spatial dimension size and C denotes the channel dimension size. A 1 ? 1 convolution layer takes the image features X I as input, and reduces the channel dimension size to D. Then, a flatten operation produces flattened image features X F ? R HW ?D . Note that we employ positional encodings for retaining spatial information in our transformer, as illustrated in <ref type="figure">Figure 3</ref>.  <ref type="figure">Fig. 3</ref>. Details of our transformer architecture and 3D human body mesh. For simplicity, we illustrate the transformer without progressive dimensionality reduction. Note that the camera feature is not fed as input to the decoder. We mask attentions using the adjacency matrix obtained from the human triangle mesh of SMPL <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer with Progressive Dimensionality Reduction</head><p>Following the encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, FastMETRO progressively reduces the hidden dimension sizes in the transformer architecture via linear projections, as illustrated in <ref type="figure">Figure 2</ref>. Transformer Encoder. Our transformer encoder ( <ref type="figure">Figure 3a</ref>) takes a learnable camera token and the flattened image features X F as input. The camera token captures essential features to predict weak-perspective camera parameters through the attention mechanism in the transformer; the camera parameters are used for fitting the 3D estimated human mesh to the 2D input image. Given the camera token and image features, the transformer encoder produces a camera feature and aggregated image features X A ? R HW ?D . Transformer Decoder. In addition to the image features X A obtained from the encoder, our transformer decoder <ref type="figure">(Figure 3a</ref>) takes the set of learnable joint tokens and the set of learnable vertex tokens as input. Each token in the set of joint tokens</p><formula xml:id="formula_0">T J = {t J 1 , t J 2 , . . . , t J K } is used to estimate 3D coordinates of a human body joint, where t J i ? R D .</formula><p>The joint tokens correspond to the body joints in <ref type="figure">Figure 3b</ref>. Each token in the set of vertex tokens</p><formula xml:id="formula_1">T V = {t V 1 , t V 2 , . . . , t V N } is used to estimate 3D coordinates of a human mesh vertex, where t V j ? R D .</formula><p>The vertex tokens correspond to the mesh vertices in <ref type="figure">Figure 3b</ref>. Given the image features and tokens, the transformer decoder produces joint features X J ? R K?D and vertex features X V ? R N ?D through self-attention and cross-attention modules. Our transformer decoder effectively captures non-local relations among human body joints and mesh vertices via self-attentions, which improves the robustness to environment variations such as occlusions. Regarding the joint and vertex tokens, each focuses on its relevant image region via cross-attentions. Attention Masking based on Mesh Topology. To effectively capture local vertex-vertex and non-local joint-vertex relations, we mask self-attentions of non-adjacent vertices according to the topology of human triangle mesh in <ref type="figure">Figure 3b</ref>. Although we mask the attentions of non-adjacent vertices, the coverage of each vertex token increases as it goes through decoder layers in the similar way with iterative graph convolutions. Note that GraphCMR <ref type="bibr" target="#b22">[23]</ref> and Mesh Graphormer <ref type="bibr" target="#b25">[26]</ref> perform graph convolutions based on the human mesh topology, which demands additional learnable parameters and computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regressor and Mesh Upsampling</head><p>3D Coordinates Regressor. Our regressor takes the joint features X J and vertex features X V as input, and estimates the 3D coordinates of human body joints and mesh vertices. As a result, 3D joint coordinates? 3D ? R K?3 and 3D vertex coordinatesV 3D ? R N ?3 are predicted. Coarse-to-Fine Mesh Upsampling. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, our FastMETRO estimates a coarse mesh, then upsample the mesh. In this way, we avoid the redundancy caused by the spatial locality of human mesh vertices. As in <ref type="bibr" target="#b22">[23]</ref>, FastMETRO obtains the fine mesh outputV ? 3D ? R M ?3 from the coarse mesh outputV 3D by performing matrix multiplication with the upsampling matrix U ? R M ?N , i.e.,V ? 3D = UV 3D , where the upsampling matrix U is pre-computed by the sampling algorithm in <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training FastMETRO</head><p>3D Vertex Regression Loss. To train our model for the regression of 3D mesh vertices, we use L1 loss function. This regression loss L V 3D is computed by</p><formula xml:id="formula_2">L V 3D = 1 M ?V ? 3D ?V 3D ? 1 ,<label>(1)</label></formula><p>whereV 3D ? R M ?3 denotes the ground-truth 3D vertex coordinates. 3D Joint Regression Loss. In addition to the estimated 3D joints? 3D , we also obtain 3D joints? ? 3D ? R K?3 regressed from the fine meshV ? 3D , which is the common practice in the literature <ref type="bibr">[6, 9, 17, 21-23, 25, 26, 36, 43, 47]</ref>. The regressed joints? ? 3D are computed by the matrix multiplication of the joint regression matrix R ? R K?M and the fine meshV ? 3D , i.e.,? ? 3D = RV ? 3D , where the regression matrix R is pre-defined in SMPL <ref type="bibr" target="#b29">[30]</ref>. To train our model for the regression of 3D body joints, we use L1 loss function. This regression loss L J 3D is computed by</p><formula xml:id="formula_3">L J 3D = 1 K (?? 3D ?J 3D ? 1 + ?? ? 3D ?J 3D ? 1 ),<label>(2)</label></formula><p>whereJ 3D ? R K?3 denotes the ground-truth 3D joint coordinates. 2D Joint Projection Loss. Following the literature <ref type="bibr">[9, 17, 21-23, 25, 26, 43, 47]</ref>, for the alignment between the 2D input image and the 3D reconstructed human mesh, we train our model to estimate weak-perspective camera parameters {s, t}; a scaling factor s ? R and a 2D translation vector t ? R 2 . The weak-perspective camera parameters are estimated from the camera feature obtained by the transformer encoder. Using the camera parameters, we get 2D body joints via an orthographic projection of the estimated 3D body joints. The projected 2D body joints are computed by?</p><formula xml:id="formula_4">2D = s?(? 3D ) + t,<label>(3)</label></formula><formula xml:id="formula_5">J ? 2D = s?(? ? 3D ) + t,<label>(4)</label></formula><p>where ?(?) denotes the orthographic projection; 1 0 0</p><formula xml:id="formula_6">0 1 0 T ? R 3?2</formula><p>is used for this projection in FastMETRO. To train our model with the projection of 3D body joints onto the 2D image, we use L1 loss function. This projection loss L J 2D is computed by</p><formula xml:id="formula_7">L J 2D = 1 K (?? 2D ?J 2D ? 1 + ?? ? 2D ?J 2D ? 1 ),<label>(5)</label></formula><p>whereJ 2D ? R K?2 denotes the ground-truth 2D joint coordinates. Total Loss. Following the literature <ref type="bibr">[6, 9, 17, 20-23, 25, 26, 36, 43, 47]</ref>, we train our model with multiple 3D and 2D training datasets to improve its accuracy and robustness. This total loss L total is computed by</p><formula xml:id="formula_8">L total = ?(? V 3D L V 3D + ? J 3D L J 3D ) + ?? J 2D L J 2D ,<label>(6)</label></formula><p>where ? V 3D , ? J 3D , ? J 2D &gt; 0 are loss coefficients and ?, ? ? {0, 1} are binary flags which denote the availability of ground-truth 3D and 2D coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>We implement our proposed method with three variants: FastMETRO-S, FastMETRO-M, FastMETRO-L. They have the same architecture with a different number of layers in the transformer encoder and decoder. <ref type="table" target="#tab_1">Table 1</ref> shows the configuration for each variant. Our transformer encoder and decoder are initialized with Xavier Initialization <ref type="bibr" target="#b9">[10]</ref>. Please refer to the supplementary material for complete implementation details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PA-MPJPE (mm)</head><p>FastMETRO-L-H64</p><p>MeshGraphormer-H64</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METRO-H64</head><p>Number of Parameters (millions) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FastMETRO-M-R50</head><p>FastMETRO-L-R50 <ref type="figure">Fig. 4</ref>. Comparison with encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and our proposed models on Human3.6M <ref type="bibr" target="#b14">[15]</ref>. The small variant of our FastMETRO shows much faster inference speed, and its large variant converges faster than the transformer encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Following the encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, we train our FastMETRO with Human3.6M <ref type="bibr" target="#b14">[15]</ref>, UP-3D <ref type="bibr" target="#b23">[24]</ref>, MuCo-3DHP <ref type="bibr" target="#b34">[35]</ref>, COCO <ref type="bibr" target="#b26">[27]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref> training datasets, and evaluate the model on P2 protocol in Human3.6M. Then, we fine-tune our model with 3DPW <ref type="bibr" target="#b33">[34]</ref> training dataset, and evaluate the model on its test dataset.</p><p>Following the common practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>, we employ the pseudo 3D human mesh obtained by SMPLify-X <ref type="bibr" target="#b37">[38]</ref> to train our model with Human3.6M <ref type="bibr" target="#b14">[15]</ref>; there is no available ground-truth 3D human mesh in the Human3.6M training dataset due to the license issue. For fair comparison, we employ the ground-truth 3D human body joints in Human3.6M during the evaluation of our model. Regarding the experiments on 3DPW <ref type="bibr" target="#b33">[34]</ref>, we use its training dataset for fine-tuning our model as in the encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We evaluate our FastMETRO using three evaluation metrics: MPJPE <ref type="bibr" target="#b14">[15]</ref>, PA-MPJPE <ref type="bibr" target="#b48">[49]</ref>, MPVPE <ref type="bibr" target="#b38">[39]</ref>. The unit of each metric is millimeter. MPJPE. This metric denotes Mean-Per-Joint-Position-Error. It measures the Euclidean distances between the predicted and ground-truth joint coordinates. PA-MPJPE. This metric is often called Reconstruction Error. It measures MPJPE after 3D alignment using Procrustes Analysis (PA) <ref type="bibr" target="#b10">[11]</ref>. MPVPE. This metric denotes Mean-Per-Vertex-Position-Error. It measures the Euclidean distances between the predicted and ground-truth vertex coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>We evaluate the model-size variants of our FastMETRO on the 3DPW <ref type="bibr" target="#b33">[34]</ref> and Human3.6M <ref type="bibr" target="#b14">[15]</ref> datasets. In this paper, the inference time is measured using a single NVIDIA V100 GPU with a batch size of 1. Comparison with Encoder-Based Transformers. In <ref type="table" target="#tab_2">Table 2</ref>, we compare our models with METRO <ref type="bibr" target="#b24">[25]</ref> and Mesh Graphormer <ref type="bibr" target="#b25">[26]</ref> on the Human3.6M <ref type="bibr" target="#b14">[15]</ref> dataset. Note that encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> are implemented with ResNet-50 <ref type="bibr" target="#b13">[14]</ref> (R50) or HRNet-W64 <ref type="bibr" target="#b44">[45]</ref> (H64). FastMETRO-S outperforms METRO when both models employ the same CNN backbone (R50), although our model demands only 8.99% of the parameters in the transformer architecture. Regarding the overall inference speed, our model is 1.86? faster. It is worth noting that FastMETRO-L-R50 achieves similar results with METRO-H64, but our model is 2.58? faster. FastMETRO-L outperforms Mesh Graphormer when both models employ the same CNN backbone (H64), while our model demands only 25.30% of the parameters in the transformer architecture. Also, our model converges much faster than the encoder-based methods as shown in <ref type="figure">Figure 4</ref>.  <ref type="bibr" target="#b14">[15]</ref> and 3DPW <ref type="bibr" target="#b33">[34]</ref>. We visualize the 3D human mesh estimated by FastMETRO-L-H64. By leveraging the attention mechanism in the transformer, our model is robust to partial occlusions.</p><p>Comparison with Image-Based Methods. In <ref type="table" target="#tab_3">Table 3</ref>, we compare our FastMETRO with the image-based methods for 3D human mesh reconstruction on 3DPW <ref type="bibr" target="#b33">[34]</ref> and Human3.6M <ref type="bibr" target="#b14">[15]</ref>. Note that existing methods are implemented with ResNet-50 <ref type="bibr" target="#b13">[14]</ref> (R50) or HRNet-W32 <ref type="bibr" target="#b44">[45]</ref> (H32) or HRNet-W64 <ref type="bibr" target="#b44">[45]</ref> (H64). When all models employ R50 as their CNN backbones, FastMETRO-S achieves the best results without iterative fitting procedures or test-time optimizations. FastMETRO-L-H64 achieves the state of the art in every evaluation metric on the 3DPW dataset and PA-MPJPE metric on the Human3.6M dataset. Visualization of Self-Attentions. In <ref type="figure" target="#fig_4">Figure 6</ref>, the first and second rows show the visualization of the attention scores in self-attentions between a specified body joint and mesh vertices. We obtain the scores by averaging attention scores from all attention heads of all multi-head self-attention modules in our transformer decoder. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, our FastMETRO effectively captures the non-local relations among joints and vertices via self-attentions in the transformer. This improves the robustness to environment variations such as occlusions. Visualization of Cross-Attentions. In <ref type="figure" target="#fig_4">Figure 6</ref>, the third and fourth rows show the visualization of the attention scores in cross-attentions between a specified body joint and image regions. We obtain the scores by averaging attention scores from all attention heads of all multi-head cross-attention modules in our transformer decoder. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the input tokens used in our transformer decoder focus on their relevant image regions. By leveraging the cross-attentions between disentangled modalities, our FastMETRO effectively learns to regress the 3D coordinates of joints and vertices from a 2D image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We analyze the effects of different components in our FastMETRO as shown in <ref type="table">Table 4</ref>. Please refer to the supplementary material for more experiments. Attention Masking. To effectively learn the local relations among mesh vertices, GraphCMR <ref type="bibr" target="#b22">[23]</ref> and Mesh Graphormer <ref type="bibr" target="#b25">[26]</ref> perform graph convolutions based on the topology of SMPL human triangle mesh <ref type="bibr" target="#b29">[30]</ref>. For the same goal, we mask self-attentions of non-adjacent vertices according to the topology. When we evaluate our model without masking the attentions, the regression accuracy drops as shown in the first row of <ref type="table">Table 4</ref>. This demonstrates that masking the attentions of non-adjacent vertices is effective. To compare the effects of attention masking with graph convolutions, we train our model using graph convolutions without masking the attentions. As shown in the second row of <ref type="table">Table 4</ref>, we obtain similar results but this requires more parameters. We also evaluate our model when we mask the attentions in half attention heads, i.e., there is no attention masking in other half attention heads. In this case, we get similar results using the same number of parameters as shown in the third row of <ref type="table">Table 4</ref>. <ref type="table">Table 4</ref>. Ablation study of our FastMETRO on Human3.6M <ref type="bibr" target="#b14">[15]</ref>. The effects of different components are evaluated. Coarse-to-Fine Mesh Upsampling. The existing transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> also first estimate a coarse mesh, then upsample the mesh to obtain a fine mesh. They employ two learnable linear layers for the upsampling. In our FastMETRO, we use the pre-computed upsampling matrix U to reduce optimization difficulty as in <ref type="bibr" target="#b22">[23]</ref>; this upsampling matrix is a sparse matrix which has only about 25K non-zero elements. When we perform the mesh upsampling using learnable linear layers instead of the matrix U, the regression accuracy drops as shown in the fourth row of <ref type="table">Table 4</ref>, although it demands much more parameters. Progressive Dimensionality Reduction. Following the existing transformer encoders <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, we also progressively reduce the hidden dimension sizes in our transformer via linear projections. To evaluate its effectiveness, we train our model using the same number of transformer layers but without progressive dimensionality reduction, i.e., hidden dimension sizes in all transformer layers are the same. As shown in the fifth row of <ref type="table">Table 4</ref>, we obtain similar results but this requires much more parameters. This demonstrates that the dimensionality reduction is helpful for our model to achieve decent results using fewer parameters. Generalizability. Our model can reconstruct any arbitrary 3D objects by changing the number of input tokens used in the transformer decoder. Note that we can employ learnable layers for coarse-to-fine mesh upsampling without masking attentions. For 3D hand mesh reconstruction, there is a pre-computed upsampling matrix and a human hand model such as MANO <ref type="bibr" target="#b41">[42]</ref>. Thus, we can leverage the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand Joints</head><p>Vertices &amp; Edges <ref type="figure">Fig. 7</ref>. Hand Joints and Mesh Topology.</p><p>matrix for mesh upsampling and mask self-attentions of non-adjacent vertices in the same way with 3D human mesh recovery. As illustrated in <ref type="figure">Figure 7</ref>, we can obtain an adjacency matrix and construct joint and vertex tokens from the human hand mesh topology.</p><p>To validate the generalizability of our method, we train FastMETRO-L-H64 on the FreiHAND <ref type="bibr" target="#b49">[50]</ref> training dataset and evaluate the model. As shown in <ref type="table" target="#tab_5">Table 5</ref>, our proposed model achieves competitive results on FreiHAND.  <ref type="bibr" target="#b49">[50]</ref>. We visualize the 3D hand mesh estimated by FastMETRO-L-H64. By leveraging the attention mechanism in the transformer, our model is robust to partial occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We identify the performance bottleneck in the encoder-based transformers is due to the design of input tokens, and resolve this issue via an encoder-decoder architecture. This allows our model to demand much fewer parameters and shorter inference time, which is more appropriate for practical use. The proposed method leverages the human body prior encoded in SMPL human mesh, which reduces optimization difficulty and leads to faster convergence with higher accuracy. To be specific, we mask self-attentions of non-adjacent vertices and perform coarse-to-fine mesh upsampling. We demonstrate that our method improves the Pareto-front of accuracy and efficiency. Our FastMETRO achieves the robustness to occlusions by capturing non-local relations among body joints and mesh vertices, which outperforms image-based methods on the Human3.6M and 3DPW datasets. A limitation is that a substantial number of samples are required to train our model as in the encoder-based transformers.</p><p>In this supplementary material, we present more implementation details (Section A), quantitative evaluations (Section B) and qualitative evaluations (Section C), which are not included in the main paper due to its limited space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>PyTorch <ref type="bibr" target="#b36">[37]</ref> is used to implement our FastMETRO. We employ ResNet-50 <ref type="bibr" target="#b13">[14]</ref> or HRNet-W64 <ref type="bibr" target="#b44">[45]</ref> as our CNN backbone, where each backbone is initialized with ImageNet <ref type="bibr" target="#b6">[7]</ref> pre-trained weights. For the initialization of our transformer, we use Xavier Initialization <ref type="bibr" target="#b9">[10]</ref>. Given an input image of size 224 ? 224 ? 3, our CNN backbone produces the image features X I ? R H?W ?C , where H = W = 7 and C = 2048. The hidden dimension size of the camera token is D = 512, which is also same for each joint token t J i and vertex token t V j . Following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, the number of joint tokens is K = 14 and that of vertex tokens is N = 431. The number of vertices in the fine mesh outputV ? 3D is M = 6890, which is same with that of SMPL <ref type="bibr" target="#b29">[30]</ref>. The number of heads in multi-head attention modules is 8. We employ two linear layers with a ReLU activation function for the MLPs in our transformer layers. Regarding the 3D coordinates regressor or camera predictor, we use a linear layer. To retain spatial information for the flattened image features X F , we use fixed sine positional encodings as in <ref type="bibr" target="#b2">[3]</ref>. Note that the pre-computed matrix for mesh upsampling and the adjacency matrix for attention masking are sparse matrices in our implementation; only about 25K elements are non-zeros in the upsampling matrix and about 3K elements are non-zeros in the adjacency matrix. We leverage these sparse matrices for memory-efficient implementation.</p><p>We use AdamW optimizer <ref type="bibr" target="#b30">[31]</ref> with the learning rate of 10 ?4 , weight decay of 10 ?4 , ? 1 = 0.9 and ? 2 = 0.999. For stable training of our transformer, we apply gradient clipping and set the maximal gradient norm value to 0.3. The loss coefficients are ? V 3D = ? J 2D = 100 and ? J 3D = 1000. We train our FastMETRO with a batch size of 16 for 60 epochs, which takes about 4 days on 4 NVIDIA V100 GPUs (16GB RAM). Note that METRO <ref type="bibr" target="#b24">[25]</ref> and Mesh Graphormer <ref type="bibr" target="#b25">[26]</ref> are trained with a batch size of 32 for 200 epochs, which takes about 5 days on 8 NVIDIA V100 GPUs (32GB RAM). During training, we apply the standard data augmentation for this task as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Quantitative Evaluations</head><p>Baseline. To validate the effectiveness of our method, we evaluate a na?ve transformer encoder-decoder architecture. Following encoder-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, we employ a 3D human mesh for input tokens and learnable layers for mesh upsampling, and do not perform attention masking. For the construction of joint and vertex tokens, we linearly project the 3D coordinates of joints and vertices in the human mesh. To simplify this model, we do not progressively reduce hidden dimension sizes in the transformer. This baseline shows lower regression accuracy as shown in the first row of <ref type="table" target="#tab_1">Table B1</ref>, although it demands much more parameters. This demonstrates that our FastMETRO effectively improves the accuracy and reduces the number of parameters required in the architecture. Positional Encodings. Following <ref type="bibr" target="#b2">[3]</ref>, we employ fixed sine positional encodings for retaining spatial information in our transformer. When we use learnable embeddings as positional encodings, we obtain similar results but this demands more parameters as shown in the second row of <ref type="table" target="#tab_1">Table B1</ref>. Weak-Perspective Camera Parameters. We estimate these parameters using a camera token in our transformer encoder. On the other hand, GraphCMR <ref type="bibr" target="#b22">[23]</ref> estimates the camera parameters from the vertex features obtained by graph convolutional layers, and encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> predict the camera parameters from the 3D coordinates of mesh vertices estimated by transformer layers. As shown in the third and fourth rows of <ref type="table" target="#tab_1">Table B1</ref>, we also evaluate our FastMETRO using different methods to predict the camera parameters. When we employ a camera token in our transformer decoder, we obtain similar results. When we predict the camera parameters using the 3D mesh estimated by transformer layers, the regression accuracy drops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Evaluations</head><p>Comparison with Encoder-Based Transformers. <ref type="figure">Figure C1</ref> shows the qualitative comparison of transformer encoders <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> with our method. As shown in <ref type="figure">Figure C1</ref>, FastMETRO-L-H64 achieves competitive results, although our model requires only about 25% of the parameters in the transformer architecture compared with the encoder-based transformers. Note that FastMETRO captures more detailed body pose especially for knees and ankles. <ref type="figure">Fig. C1</ref>. Comparison with encoder-based transformers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and FastMETRO-L-H64 on 3DPW <ref type="bibr" target="#b33">[34]</ref>. Our model achieves competitive results using much fewer parameters, and shows more favorable body pose especially for knees and ankles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METRO Mesh Graphormer FastMETRO</head><p>SMPL Parameters from Estimated Mesh. As in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref>, we can optionally regress SMPL <ref type="bibr" target="#b29">[30]</ref> parameters from the output mesh estimated by our model. To be specific, we first regress the 3D coordinates of human mesh vertices via our model, then predict SMPL pose and shape coefficients via a SMPL parameter regressor which takes the estimated 3D mesh vertices as input. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref>, we employ fully connected layers with skip connections as the SMPL parameter regressor. In this way, we can reconstruct 3D human mesh using the predicted SMPL parameters. <ref type="figure">Figure C2</ref> shows the visualization of the estimation results obtained by our FastMETRO and the SMPL parameter regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Non?Parametric Side View Parametric Side View <ref type="figure">Fig. C2</ref>. Qualitative results of FastMETRO-L-H64 on Human3.6M <ref type="bibr" target="#b14">[15]</ref> and 3DPW <ref type="bibr" target="#b33">[34]</ref>. We can optionally learn to regress SMPL <ref type="bibr" target="#b29">[30]</ref> parameters from the 3D coordinates of mesh vertices estimated by our FastMETRO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Human Body Joints and Mesh Topology! ? (a) Transformer Encoder-Decoder Architecture"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of our FastMETRO on Human3.6M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results of FastMETRO-L-H64 on COCO<ref type="bibr" target="#b26">[27]</ref>. We visualize the attentions scores in self-attentions (top two rows) and cross-attentions (bottom two rows). The brighter lines or regions indicate higher attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Transformer Decoder?1 ? Adjacency Matrix (Mesh Topology) Transformer Encoder?2 Transformer Decoder?2 CNN 3D Coordinates Regressor Dimensionality Reduction Positional Encodings UP Coarse Mesh Output Fine Mesh Output Body Joint Output Transformer Encoder?1 Dimensionality Reduction 2D Image 3D Human Mesh (Multi Views) Cam Token ? Joint Tokens Vertex Tokens</head><label></label><figDesc></figDesc><table /><note>Fig. 2. Overall architecture of FastMETRO. Our model estimates 3D coordinates of human body joints and mesh vertices from a single image. We extract image features via a CNN backbone, which are fed as input to our transformer encoder. In addition to image features produced by the encoder, our transformer decoder takes learnable joint and vertex tokens as input. To effectively learn non-local joint-vertex relations and local vertex-vertex relations, we mask self-attentions of non-adjacent vertices according to the topology of human triangle mesh. Following [25, 26], we progressively reduce the hidden dimension sizes via linear projections in our transformer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Configurations for the variants of FastMETRO. Each has the same transformer architecture with a different number of layers. Only transformer parts are described.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Enc-1 &amp; Dec-1</cell><cell cols="2">Enc-2 &amp; Dec-2</cell></row><row><cell>Model</cell><cell cols="4">#Params Time (ms) #Layers Dimension</cell><cell cols="2">#Layers Dimension</cell></row><row><cell>FastMETRO-S</cell><cell>9.2M</cell><cell>9.6</cell><cell>1</cell><cell>512</cell><cell>1</cell><cell>128</cell></row><row><cell cols="2">FastMETRO-M 17.1M</cell><cell>15.0</cell><cell>2</cell><cell>512</cell><cell>2</cell><cell>128</cell></row><row><cell>FastMETRO-L</cell><cell>24.9M</cell><cell>20.8</cell><cell>3</cell><cell>512</cell><cell>3</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with transformers for monocular 3D human mesh recovery on Human3.6M<ref type="bibr" target="#b14">[15]</ref>. ? and * indicate training for 60 epochs and 200 epochs, respectively.</figDesc><table><row><cell></cell><cell cols="2">CNN Backbone</cell><cell cols="2">Transformer</cell><cell>Overall</cell><cell></cell></row><row><cell>Model</cell><cell cols="6">#Params Time (ms) #Params Time (ms) #Params FPS PA-MPJPE ?</cell></row><row><cell>METRO-R50  *  [25]</cell><cell>23.5M</cell><cell>7.5</cell><cell>102.3M</cell><cell>24.2</cell><cell>125.8M 31.5</cell><cell>40.6</cell></row><row><cell>METRO-H64  ? [25]</cell><cell>128.1M</cell><cell>49.0</cell><cell>102.3M</cell><cell>24.2</cell><cell>230.4M 13.7</cell><cell>38.0</cell></row><row><cell>METRO-H64  *  [25]</cell><cell>128.1M</cell><cell>49.0</cell><cell>102.3M</cell><cell>24.2</cell><cell>230.4M 13.7</cell><cell>36.7</cell></row><row><cell cols="2">MeshGraphormer-H64  ? [26] 128.1M</cell><cell>49.0</cell><cell>98.4M</cell><cell>24.5</cell><cell>226.5M 13.6</cell><cell>35.8</cell></row><row><cell cols="2">MeshGraphormer-H64  *  [26] 128.1M</cell><cell>49.0</cell><cell>98.4M</cell><cell>24.5</cell><cell>226.5M 13.6</cell><cell>34.5</cell></row><row><cell>FastMETRO-S-R50  ?</cell><cell>23.5M</cell><cell>7.5</cell><cell>9.2M</cell><cell>9.6</cell><cell>32.7M 58.5</cell><cell>39.4</cell></row><row><cell>FastMETRO-M-R50  ?</cell><cell>23.5M</cell><cell>7.5</cell><cell>17.1M</cell><cell>15.0</cell><cell>40.6M 44.4</cell><cell>38.6</cell></row><row><cell>FastMETRO-L-R50  ?</cell><cell>23.5M</cell><cell>7.5</cell><cell>24.9M</cell><cell>20.8</cell><cell>48.4M 35.3</cell><cell>37.3</cell></row><row><cell>FastMETRO-L-H64  ?</cell><cell>128.1M</cell><cell>49.0</cell><cell>24.9M</cell><cell>20.8</cell><cell>153.0M 14.3</cell><cell>33.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>52 52</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>48 48</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>44 44</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>40 40</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>36 36</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 10 20 30 40 50 60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-the-art monocular 3D human pose and mesh recovery methods on 3DPW<ref type="bibr" target="#b33">[34]</ref> and Human3.6M<ref type="bibr" target="#b14">[15]</ref> among image-based methods.</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell cols="2">Human3.6M</cell></row><row><cell>Model</cell><cell cols="3">MPVPE ? MPJPE ? PA-MPJPE ?</cell><cell cols="2">MPJPE ? PA-MPJPE ?</cell></row><row><cell>HMR-R50 [17]</cell><cell>-</cell><cell>130.0</cell><cell>76.7</cell><cell>88.0</cell><cell>56.8</cell></row><row><cell>GraphCMR-R50 [23]</cell><cell>-</cell><cell>-</cell><cell>70.2</cell><cell>-</cell><cell>50.1</cell></row><row><cell>SPIN-R50 [22]</cell><cell>116.4</cell><cell>96.9</cell><cell>59.2</cell><cell>62.5</cell><cell>41.1</cell></row><row><cell>I2LMeshNet-R50 [36]</cell><cell>-</cell><cell>93.2</cell><cell>57.7</cell><cell>55.7</cell><cell>41.1</cell></row><row><cell>PyMAF-R50 [47]</cell><cell>110.1</cell><cell>92.8</cell><cell>58.9</cell><cell>57.7</cell><cell>40.5</cell></row><row><cell>ROMP-R50 [43]</cell><cell>105.6</cell><cell>89.3</cell><cell>53.5</cell><cell>-</cell><cell>-</cell></row><row><cell>ROMP-H32 [43]</cell><cell>103.1</cell><cell>85.5</cell><cell>53.3</cell><cell>-</cell><cell>-</cell></row><row><cell>PARE-R50 [21]</cell><cell>99.7</cell><cell>82.9</cell><cell>52.3</cell><cell>-</cell><cell>-</cell></row><row><cell>METRO-R50 [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.5</cell><cell>40.6</cell></row><row><cell>DSR-R50 [9]</cell><cell>99.5</cell><cell>85.7</cell><cell>51.7</cell><cell>60.9</cell><cell>40.3</cell></row><row><cell>METRO-H64 [25]</cell><cell>88.2</cell><cell>77.1</cell><cell>47.9</cell><cell>54.0</cell><cell>36.7</cell></row><row><cell>PARE-H32 [21]</cell><cell>88.6</cell><cell>74.5</cell><cell>46.5</cell><cell>-</cell><cell>-</cell></row><row><cell>MeshGraphormer-H64 [26]</cell><cell>87.7</cell><cell>74.7</cell><cell>45.6</cell><cell>51.2</cell><cell>34.5</cell></row><row><cell>FastMETRO-S-R50</cell><cell>91.9</cell><cell>79.6</cell><cell>49.3</cell><cell>55.7</cell><cell>39.4</cell></row><row><cell>FastMETRO-M-R50</cell><cell>91.2</cell><cell>78.5</cell><cell>48.4</cell><cell>55.1</cell><cell>38.6</cell></row><row><cell>FastMETRO-L-R50</cell><cell>90.6</cell><cell>77.9</cell><cell>48.3</cell><cell>53.9</cell><cell>37.3</cell></row><row><cell>FastMETRO-L-H64</cell><cell>84.1</cell><cell>73.5</cell><cell>44.6</cell><cell>52.2</cell><cell>33.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The default model is FastMETRO-S-R50.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Human3.6M</cell></row><row><cell>Model</cell><cell cols="3">#Params MPJPE ? PA-MPJPE ?</cell></row><row><cell>w/o attention masking</cell><cell>32.7M</cell><cell>58.0</cell><cell>40.7</cell></row><row><cell>w/o attention masking + w/ graph convolutions</cell><cell>33.1M</cell><cell>56.6</cell><cell>39.4</cell></row><row><cell>w/ attention masking in half attention heads</cell><cell>32.7M</cell><cell>55.8</cell><cell>39.4</cell></row><row><cell>w/ learnable upsampling layers</cell><cell>45.4M</cell><cell>58.1</cell><cell>41.1</cell></row><row><cell>w/o progressive dimensionality reduction</cell><cell>39.5M</cell><cell>55.5</cell><cell>39.6</cell></row><row><cell>FastMETRO-S-R50</cell><cell>32.7M</cell><cell>55.7</cell><cell>39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison with transformers for monocular 3D hand mesh recovery on FreiHAND<ref type="bibr" target="#b49">[50]</ref>. Test-time augmentation is not applied to these transformers.</figDesc><table><row><cell></cell><cell>Transformer</cell><cell>Overall</cell><cell></cell><cell>FreiHAND</cell></row><row><cell>Model</cell><cell>#Params</cell><cell>#Params</cell><cell cols="2">PA-MPJPE ? F@15mm ?</cell></row><row><cell>METRO-H64 [25]</cell><cell>102.3M</cell><cell>230.4M</cell><cell>6.8</cell><cell>0.981</cell></row><row><cell>FastMETRO-L-H64</cell><cell>24.9M</cell><cell>153.0M</cell><cell>6.5</cell><cell>0.982</cell></row></table><note>Fig. 8. Qualitative results of our FastMETRO on FreiHAND</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table B1 .</head><label>B1</label><figDesc>Ablation study of our FastMETRO on Human3.6M<ref type="bibr" target="#b14">[15]</ref>. The effects of different components are evaluated. The default model is FastMETRO-S-R50.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Human3.6M</cell></row><row><cell>Model</cell><cell cols="3">#Params MPJPE ? PA-MPJPE ?</cell></row><row><cell>w/ baseline setting</cell><cell>51.9M</cell><cell>59.0</cell><cell>41.8</cell></row><row><cell>w/ learnable positional encodings</cell><cell>32.8M</cell><cell>56.2</cell><cell>39.7</cell></row><row><cell>w/ camera token in transformer decoder</cell><cell>32.7M</cell><cell>56.1</cell><cell>39.5</cell></row><row><cell>w/ camera prediction using estimated mesh</cell><cell>32.8M</cell><cell>58.0</cell><cell>39.6</cell></row><row><cell>FastMETRO-S-R50</cell><cell>32.7M</cell><cell>55.7</cell><cell>39.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, we discuss the input tokens mainly based on METRO<ref type="bibr" target="#b24">[25]</ref>. Mesh Graphormer<ref type="bibr" target="#b25">[26]</ref> has subtle differences, but the essence of the bottleneck is shared.<ref type="bibr" target="#b1">2</ref> 3-dimensional coordinates out of (C + 3)-dimensional input tokens, where C = 2048.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2D Human Pose Estimation: New Benchmark and State of the Art Analysis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collaborative Transformers for Grounded Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grounded Situation Recognition with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) (2020) 4, 7</title>
		<meeting>the European Conference on Computer Vision (ECCV) (2020) 4, 7</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to Regress Bodies from Images using Differentiable Semantic Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating Human Shape and Pose from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalized and Geometry-Aware Self-Attention Network for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coherent Reconstruction of Multiple Humans from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end Recovery of Human Shape and Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HOTR: End-to-End Human-Object Interaction Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VIBE: Video Inference for Human Body Pose and Shape Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PARE: Part Attention Regressor for 3D Human Body Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 1, 4, 7</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 1, 4, 7</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the Loop Between 3D and 2D Human Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-End Human Pose and Mesh Reconstruction with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mesh Graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Paint Transformer: Feed Forward Neural Painting with Stroke Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SMPL: A Skinned Multi-Person Linear Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context-Aware Scene Graph Generation with Seq2Seq Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Mixed Classification-Regression Framework for 3D Pose Estimation from 2D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single-Shot Multi-Person 3D Pose Estimation from Monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) (2020)</title>
		<meeting>the European Conference on Computer Vision (ECCV) (2020)<address><addrLine>1, 4, 7, 8, 9</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating 3D Faces using Convolutional Mesh Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embodied Hands: Modeling and Capturing Hands and Bodies Together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monocular, One-Stage, Regression of Multiple 3D People</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-End Video Instance Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 1, 4, 7</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation with Spatial and Temporal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MonoCap: Monocular Human Motion Capture using a CNN Coupled with a Geometric Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TPAMI)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
