<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data Storage and Intelligent Vision Technical Research Dept</orgName>
								<orgName type="institution">Huawei Cloud</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
							<email>xjia@dlut.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijun</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
							<email>liu.jianzhuang@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-source unsupervised domain adaptation (MSDA) aims at adapting models trained on multiple labeled source domains to an unlabeled target domain. In this paper, we propose a novel multi-source domain adaptation framework based on collaborative learning for semantic segmentation. Firstly, a simple image translation method is introduced to align the pixel value distribution to reduce the gap between source domains and target domain to some extent. Then, to fully exploit the essential semantic information across source domains, we propose a collaborative learning method for domain adaptation without seeing any data from target domain. In addition, similar to the setting of unsupervised domain adaptation, unlabeled target domain data is leveraged to further improve the performance of domain adaptation. This is achieved by additionally constraining the outputs of multiple adaptation models with pseudo labels online generated by an ensembled model. Extensive experiments and ablation studies are conducted on the widely-used domain adaptation benchmark datasets in semantic segmentation. Our proposed method achieves 59.0% mIoU on the validation set of Cityscapes by training on the labeled Synscapes and GTA5 datasets and unlabeled training set of Cityscapes. It significantly outperforms all previous state-of-the-arts single-source and multi-source unsupervised domain adaptation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation as one of the core tasks in computer vision community, aims to assign semantic label to each pixel of images, e.g., person, car, road and etc.. With the development of convolutional neural networks (CNNs), semantic segmentation has made great progress recently. For example, recent deep methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref>, have achieved superior performance on almost all public bench- <ref type="figure">Figure 1</ref>. Multi-source domain adaptation for semantic segmentation. The left shows synthetic images and corresponding labels generated from different simulators, which suffer domain shift between each other but share similar semantic contexts. The right part shows unlabeled target images sampled from real scenes. marks. However, their success is based on the large numbers of densely annotated images which used to train the networks. Dense pixel-level annotation for semantic segmentation is very laborious and expensive, e.g., annotating one image in the Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref> takes about 90 minutes, which makes it difficult and sometimes even impossible to collect large amounts of densely annotated images for semantic segmentation. Thanks to the recent progress in graphics and simulation infrastructure, simulators can generate lots of images with dense annotation for semantic segmentation, such as recent proposed large-scale dense labeled datasets SYNTHIA <ref type="bibr" target="#b30">[31]</ref>, GTA5 <ref type="bibr" target="#b29">[30]</ref> and Synscapes <ref type="bibr" target="#b37">[38]</ref>. Although the huge amounts of annotated synthesized images are very close to the real scene, there is still great domain gap between synthetic datasets and real scene datasets. The domain gap causes another problem that networks trained on synthetic datasets often perform poorly on real target scenes. To handle this issue, many un-/semi-supervised domain adaptation (UDA) approaches are proposed, like <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> and etc., with the purpose of mitigating the gap between synthetic source and real target domain. Over the past years, UDA has made a great progress.</p><p>Although existing works have greatly boosted the performance of UDA for semantic segmentation, most of them focus on single source. Seldom works consider a more practical setting where labeled datasets from multiple sources with different distributions are available, e.g., SYNTHIA and GTA5. Training with multiple sources can further alleviate the problem on lack of annotated data. Moreover, multiple sources sampled from different distribution can also encourage networks to learn more essential knowledge for semantic segmentation. A straightforward approach is to simply combine all source domains into a single one, and then trains a UDA model on the combined sources and target domain dataset. This simple method can indeed boost the performance, but it does not fully exploit the abundant information across multiple source domains. Domain shift across multiple sources restricts the power within them in learning a more powerful domain adaptation model.</p><p>There are several multi-source deep UDA methods are proposed recently to exploit multiple source domains for better adaptation. They align different domains by translating images from source domains to the target style via generative adversarial networks (GAN). However, most of them <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17</ref>] work on image classification task except for MADAN <ref type="bibr" target="#b44">[45]</ref> which works on semantic segmentation, a pixel-wise prediction task. In this paper, we propose an approach based on collaborative learning and image translation to address multi-source domain adaptation for semantic segmentation.</p><p>Our observation shows that appearance discrepancy especially color discrepancy between source domains and target domain has a great impact on the performance of adaptation. Existing works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11]</ref> demonstrate that style transfer could reduce this discrepancy in some extent. However, most of them are complicated to plug in networks during training process. Therefore, we propose a simple image translation method to first mitigate domain gap between sources and target. Unlike MADAN <ref type="bibr" target="#b44">[45]</ref>, FDA <ref type="bibr" target="#b38">[39]</ref> and GAN-based translation methods, we propose to translate source domain images to the target style by aligning different distributions to the target domain in LAB color space. In addition, we observe that apart from discrepancy in appearance, images from different domains do still share much similarity in semantic contexts as shown in <ref type="figure">Fig 1.</ref> The shape of instances (person, car, bike and etc.) and spatial layout of different instances (cars always on the road, sidewalk adjacent to the road, sky on the top and etc.) are almost the same in all domains. Two collaborative learning strategies are proposed to explore essential and domain-invariant semantic contexts across different domains. First we propose a collaborative learning between source domains to investigate the case of domain adaptation without seeing any data from target domain, which is also called domain generalization in previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>. For each source domain, we have a semantic segmentation network supervised by annotation maps, and an additional soft supervision coming from other models trained on a different source domain. In addition, similar to previous UDA methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b38">39]</ref>, we also consider making full use of the unlabeled data of the target domain to further boost the performance. A collaborative learning based on target domain is proposed, in which an ensemble of models trained on source domains is used to produce pseudo labels for data from target domain in an online fashion. In turn, each model can be additionally supervised by the generated pseudo labels. Such two collaborations help constantly improve each model's adaptation capability to target domain during the training process.</p><p>The performance of our method significantly outperforms other state-of-the-art single-source and multi-source UDA methods. This success of proposed method is mainly attributed to the effective image translation and domaininvariant feature learning. Note that, our method can be trained in both end-to-end and stage-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, we briefly review some related works in the literature, i.e., semantic segmentation, domain generalization and unsupervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>Semantic segmentation plays a vital role in computer vision community and is beneficial to many practical applications, such as autonomous driving, virtual reality and medical imaging and etc.. It has developed several decade years and is well researched. Since Long et al. <ref type="bibr" target="#b17">[18]</ref> propose to transform the classification CNNs to fully convolutional network for semantic segmentation, large numbers of deep learning based methods have been proposed and greatly boost the advances of this task. For example, Chen et al.propose the DeepLab series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> approaches which utilize?trous spatial pyramid pooling (ASPP) to capture different scale of context information. Fu et al. <ref type="bibr" target="#b5">[6]</ref> involve non-local attention block into the CNNs architecture to exploit the global context of image and relation of objects. Hou et al. <ref type="bibr" target="#b9">[10]</ref> propose a new effective strip pooling to model long-range dependencies. However, the advanced performance of these semantic segmentation methods often build on the large amounts of densely annotated images which are usually hard and sometimes impossible to collect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain Generalization</head><p>Domain generalization is a particular case of transfer learning. It's purpose is to enhance the generalization ability of models on new domains that have not been seen during the training process. Currently, most of the domain generalization methods can be categorized to three parts: data-based, feature-based and meta-learning based. Most data-based methods employ GANs, Variational Autoencoder (VAE) or other image edit methods to generate new data for network training to enhance robustness and generalization of the models, e.g., Dlow <ref type="bibr" target="#b6">[7]</ref> and STRG <ref type="bibr" target="#b40">[41]</ref>. Then, the feature-based methods mainly aim to learn representations invariant to different domains by adversarial learning to align features or employing normalization to eliminate the style information, like CADAG <ref type="bibr" target="#b27">[28]</ref> and IBN-Net <ref type="bibr" target="#b24">[25]</ref>. Meta-learning based methods aim to enhance the generalization ability by using meta learning, such as Zhang et al. <ref type="bibr" target="#b41">[42]</ref> deal with the domain generalization from the training scheme perspective and develop a target-specific normalization method to further boost the generalization ability in unseen target domain. Domain generalization do not access the data in target domain which may not learn the optimal feature for target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Unsupervised Domain Adaptation</head><p>In contrast to domain generalization, unsupervised domain adaptation involves in unlabeled data of target domain during training process to learn knowledge of it. UDA has developed many years and large numbers of methods have been proposed. From the perspective of the number of source domains, domain adaptation methods can be split into two categories: single-source and multi-source.</p><p>Single-source domain adaptation focuses on single-source-single-target setting. Most of existing UDA works are for classification, like MMD <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, ADDA <ref type="bibr" target="#b33">[34]</ref> and et al.. With the synthetic dataset GTA5, SYNTHIA and Synscapes proposed , UDA for semantic segmentation has also achieved great advances in recent. For example, Tsai et al. <ref type="bibr" target="#b32">[33]</ref> propose AdaptSeg that based on adversarial learning to aligns scene layout and local context of images between source and target domain. Yang et al. <ref type="bibr" target="#b38">[39]</ref> draw on the image-to-image translation and propose to using Fourier Transform to translate images in source domain to the style of target domain. While Li et al. <ref type="bibr" target="#b14">[15]</ref> propose a bidirectional framework for domain adaptation and learn a image translation model with perceptual loss to translate images to target style. These methods have significantly boost the performance of UDA in semantic segmentation. However, they do not take the existing multiple different source domains into account which is a great waste of labeled resources.</p><p>Multi-source domain adaptation aims to make full use of existing labeled source for adaptation. Compared to singlesource UDA, it is relatively more challenging because of the shift between source domains. There are also some multi-source UDA works but most of them focus on image classification. Directly extending these methods for semantic segmentation may not work. Zhao et al. <ref type="bibr" target="#b44">[45]</ref> propose MADAN for semantic segmentation. They first learns a cycle GAN <ref type="bibr" target="#b45">[46]</ref> for each source domain to translate images to target, and then combine different adapted sources with specific weights. Finally, training a adaptation network on Source Image Trans. on RGB Trans. on LAB Target Image <ref type="figure">Figure 3</ref>. The qualitative comparison of image translation on different color space.</p><p>the combined data in a way similar to single-source UDA. In contrast to <ref type="bibr" target="#b44">[45]</ref>, we not only draw on the style-transfer for image-to-image translation, also aim to learn the shared semantic information of source domains and explore the knowledge of unlabeled target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LAB-based Image Translation</head><p>We observe that the domain discrepancy between domains mainly lies in appearance of images, i.e., color and texture. For example, the appearance discrepancy between source domains (GTA5 and SYNTHIA), and between source domains and target domain (GTA5 and Cityscapes) as demonstrated in <ref type="figure">Fig. 1</ref>. This discrepancy would further increase the difficulty of multi-source domain adaptation. An effective style transfer could reduce this discrepancy to some extent. For simplicity and efficiency, we proposed an image translation method that translates the style of images in source domains to the style of target domain by aligning the distribution of pixel values, like <ref type="bibr" target="#b28">[29]</ref>. Specifically, due to the gamut of LAB color space is larger than RGB color space, and the style of images translated on LAB color space are closer to the style of images in target domain than directly operated on RGB color space as shown in <ref type="figure">Fig. 3</ref>, we involve the image translation based on LAB color space to achieve the target of reducing domain discrepancy.</p><p>Specifically, for a RGB image X RGB S in source domains, it is firstly converted to LAB color space to generate LAB image X LAB S , X LAB S = rgb2lab(X RGB S ). Then, we calculate the mean ? S and standard deviation values ? S of each channel of the generated LAB image X LAB S . At the same time, an image from target domain is randomly selected and converted to LAB color space in same way as image from source. After that, we also calculate the mean ? T and standard deviation values ? T of this converted target image. Finally, we translate the converted LAB image from source domains to the style of target by shifting the distribution of pixel values to the image of target domain, ie.,X</p><formula xml:id="formula_0">LAB S = (X LAB S ? ? S ) ? S * ? T + ? T .<label>(1)</label></formula><p>After aligning distribution in LAB color space, we then convert the translated LAB imageX LAB S from source domains back to the RGB color spaceX RGB S ,X RGB S = lab2rgb(X LAB S ), for the subsequent training. The rgb2lab and lab2rgb are the library conversion function in python, and also easy to implement it with CUDA to accelerate computation. <ref type="figure">Fig 3 shows</ref> the qualitative comparison between RGB-and LAB-based translation. We can see that the images after translation are closer to the style of target domain, which the gap between domains is reduced to some extent. Moreover, the LAB-based translated images are closer to the style of target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Collaborative Learning between Labeled Source Domains</head><p>As shown in <ref type="figure">Fig. 1</ref>, although the images of different source domains are sampled from different simulators, i.e., from different i.i.d distributions, they are highly structured and share many similarities, e.g., the spatial layout of different categories and local context of objects. Based on this observation and to encourage models learn these essential similar properties for semantic segmentation, we propose a collaborative learning (Co-Learning) method which fully takes advantage of the capacity of different models learned on different source domains. This approach utilizes a model learn from one source domain to teach the model trained on another one, which allows that model to learn the knowledge from current source domain. Assume that there are N different labeled source domains S = {S 1 , S 2 , ? ? ? , S N } which are sampled from N different i.i.d distributions, and N deep neural networks M = {M S1 , M S2 , ? ? ? , M S N } of the same architecture but different weights learned on these source domains.</p><p>Then, for an model M Si , the learning process of model M Si is supervised by segmentation loss on labeled data from source S i and collaborative loss on output from source S k,k =i . That is, for model M Si , the object function is</p><formula xml:id="formula_1">L i = L seg Si (F Si Si , Y Si ) + ? col S L col S ({(F S k Si , F S k S k ) k =i }),<label>(2)</label></formula><p>where the loss L seg is the cross entropy loss, i.e.,</p><formula xml:id="formula_2">L seg S (F S , Y S ) = ? 1 |X S | h,w c?C Y (h,w,c) S log(?(F (h,w,c) S )),<label>(3)</label></formula><p>and the loss L col is the average of Kullback-Leibler (KL) divergence loss, i.e.,</p><formula xml:id="formula_3">L col S ({(F S k Si , F S k S k ) k =i }) = 1 N ? 1 k,k =i L kl k?i (F S k S k F S k Si ),<label>(4)</label></formula><formula xml:id="formula_4">L kl k?i (F S k S k F S k Si ) = ? 1 |X S k | ?(F S k S k ) log( ?(F S k Si ) ?(F S k S k )</formula><p>).</p><p>(5) X , F, Y are the input image, outputs of networks and the corresponding ground truth respectively. For F S k Si , the subscript indicates that F is generated by M Si while the superscript indicates F is the feature computed for images from domain S k , i.e., F S k Si = M Si (X S k ). C is the number of categories to be segmented, ?(?) indicates the softmax function, |X | represents the number of pixels in image X . Collaborative learning can allow the model M Si learn the knowledge of other models M S k,k =i learned from corresponding source S k . Thus, rather than stuck in the source domain S i , model M Si also tries to learn essential properties between all the source domains for better generalization and segmentation on target domain.</p><p>Difference with existing works: In the co-training of CLAN <ref type="bibr" target="#b20">[21]</ref> and CT <ref type="bibr" target="#b26">[27]</ref>, two diversified classifiers are produced to make predictions and their ensemble with either summation or multiplication is taken as the final prediction; however, in the collaborative learning framework, two segmentation models trained based on each source domain teach each other to extract essential semantic information across domains. CrDoCo <ref type="bibr" target="#b3">[4]</ref> uses bi-directional KL divergence to encourage segmentation consistency between images in unlabeled target domain and their sourcestyle transformed correspondences. However, in our work, images from both source domains are first transformed to the same style, i.e., target domain style. Then an arbitrary image is sent to two models that are trained for source 1 and source 2 respectively. The prediction of the model trained for the source domain that image comes from works as teacher to supervise the other model's prediction on the same image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Collaborative Learning on Unlabeled Target Domain</head><p>In practice, unlabeled data is often relatively easy and cheap to collect. Moreover, models trained on the data of target domain can learn better features and perform well on target domain. Therefore, we propose a collaborative learning method to fully take advantage of the unlabeled images in target domain, and further boost the performance of models on target domain.</p><p>Denote target domain as T which has collected large amounts of unlabeled images {X j T } N T j=1 . For an image X T in the target domain T , we first feed it to all the N models M = {M Si , i = 1, 2, ? ? ? , N } and compute the corresponding outputs of models M Si for the input image X T as F T Si , i.e., F T Si = M Si (X T ). Outputs of these networks are ensembled and a softmax function ?(?) is used to compute the probability map, i.e.,</p><formula xml:id="formula_5">P = ?( 1 N i F T Si ).<label>(6)</label></formula><p>Finally, we generate one-hot pseudo labels? for the target image X T by utilizing the probability map in a way similar to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47]</ref>, and the detail process is described in the </p><formula xml:id="formula_6">L i = L seg Si (F Si Si , Y Si ) + cur it max its ? seg T L seg T (F T Si ,? T ),<label>(7)</label></formula><p>where cross entropy loss is employed to compute the loss L seg T and is same as Equ. 3, the only difference is that the target labels are generated pseudo labels. Here the the weight term cur it max its is designed to prevent poor predictions of the target image in early training process collapsing the network training, cur it and max its represent current and maximum iterations of training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture and Training</head><p>The overall architecture of proposed approach are shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. In summary, it consists of three parts, i.e., imageto-image translation based on LAB color space, collaborative learning between labeled source domains and on unlabeled target domain. Firstly, images of source domains are translated to the style of target domain as described in Sec. 3.1 during training process. Note that for simplicity, we just showcase the collaborative learning approach for multi-source domain adaptation with two source domains. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we train a segmentation network, e.g., DeepLab v2 <ref type="bibr" target="#b0">[1]</ref>, for each source domain. These networks can be trained end-to-end which initialized from ImageNet pretrained model, or trained with stage-wise which initialized from source fine-tuned model. For each network, the final overall object function is as follows,</p><formula xml:id="formula_7">L = L seg S + ? col S L col S + cur it max its ? seg T L seg T .<label>(8)</label></formula><p>The loss L seg S ensures the correct functionality of networks for segmentation, while the loss L col S allows the network to learn the similarities that shared by source domains. In addition, the loss L seg T allows the network learn the property of target data. When network training get converged, we have two strategies to obtain the final model. One is to choose the model which achieves the best performance on target domain. The other is to keep all models, and ensemble their outputs when inference. Unless otherwise specified, all refer to the last case. The following section tests the validity of proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In this subsection, we briefly introduce the datasets used to validate our adaptation method, i.e., the widely used dataset Cityscapes <ref type="bibr" target="#b4">[5]</ref>, and recent proposed synthetic datasets GTA5 <ref type="bibr" target="#b29">[30]</ref>, SYNTHIA <ref type="bibr" target="#b30">[31]</ref>, and Synscapes <ref type="bibr" target="#b37">[38]</ref>.</p><p>Cityscapes <ref type="bibr" target="#b4">[5]</ref> consists of 5,000 real-world urban traffic scene images with 2048?1024 resolution and dense-pixel annotation. This dataset is split to 2,975 for training, 500 for validation and 1,525 for testing. Cityscapes annotates 33 categories and 19 of them are used for training and evaluation. The training set without ground truth is used for training adaptation models and validation set for evaluation.</p><p>GTA5 <ref type="bibr" target="#b29">[30]</ref> includes 24,966 dense annotated images that are synthesized from a game engine with the resolution of 1914?1052. Its ground-truth labels are consistent with Cityscapes <ref type="bibr" target="#b4">[5]</ref>. In a way similar to previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref>, common categories between GTA5 and Cityscapes are used in all experiments.</p><p>SYNTHIA <ref type="bibr" target="#b30">[31]</ref> is a large synthetic dataset that consists of photo-realistic frames rendered from a virtual city. In experiments, we use the SYNTHIA-RAND-CITYSCAPES <ref type="bibr" target="#b30">[31]</ref> set for adaptation. It contains 9,400 images with a resolution of 1280?760 which are annotated into 16 categories. Similar to GTA5, its annotation are also automatically produced and compatible with Cityscapes. Following previous works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>, we evaluate performance on 16 and 13 common categories between Cityscapes and SYNTHIA when SYNTHIA is used.</p><p>Synscapes <ref type="bibr" target="#b37">[38]</ref> is a synthetic dataset that created using photo-realistic rendering techniques. It consists of 25,000 images at 1440?720 resolution with 33 categories dense annotation and only 19 of them are used. Similarly, its </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement our proposed approach with PyTorch and conduct experiments by adopting DeepLab-v2 <ref type="bibr" target="#b0">[1]</ref> with ResNet-101 <ref type="bibr" target="#b8">[9]</ref> as backbone. We report the performance of both initialization from ImageNet pretrained model and initialization from source pretrained model, which are respectively denoted as end-to-end and stage-wise training stratgies. Following prior works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33]</ref>, all the networks are trained with stochastic gradient descent (SGD) optimizer. The initial learning rate and momentum are set to 2.5 ? e ?4 and 0.9, respectively, and the polynomial decay policy with power of 0.9 is adopted to adjust the learning rate. We set the batch size to 1 for all datasets during training because of memory limitation. The hyperparameters in collaborative learning on target domain, ? and ? are set to 50% and 0.9, respectively. It indicates that we keep pixels that prediction probability within the top 50% or higher than 0.9 as true labels, and reminder are ignored. The weights ? col S and ? seg T for collaborative learning losses are set to 0.5, 0.1 for end-to-end training and 9.5, 0.1 for stage-wise training. Following Advent <ref type="bibr" target="#b36">[37]</ref>, we set the number of maximum iterations to 250,000 but early stop at 120,000 iterations. Same as previous works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24]</ref>, the metric mean intersection-over-union (mIoU) is used to evaluate the performance of our proposed adaptation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this subsection, extensive experiments on adaptation from GTA+Synscapes to Cityscapes are conducted to study the effectiveness of each component in the proposed approach.</p><p>Firstly, LAB-based image translation is applied to the state-of-the-art UDA methods in semantic segmentation, i.e., AdaptSeg <ref type="bibr" target="#b32">[33]</ref> and Advent <ref type="bibr" target="#b36">[37]</ref>, to investigate the effectiveness of proposed image translation method. <ref type="table" target="#tab_0">Table 1</ref> is the performance comparison of previous UDA methods training on original and translated images. From the results, <ref type="table">Table 3</ref>. Ablation studies of proposed methods. Note that, the performances are achieved by end-to-end training strategy for comparison with simple combination of sources. Because our method can be trained in different strategies and there are N models in the framework, we then test the performance of each single model and the final ensemble one with different training strategies. All these results are based on the setting that adapting from GTA5+Synscapes to Cityscapes. As shown in <ref type="table" target="#tab_1">Table 2</ref> (a), the performance of all models are significantly improved and the ensembled model achieves the best performance. Therefore, we only report the ensembled model's performance later. The collaborative learning between source domains does not access to the unlabeled target data and therefore can be used to address the task of domain generalization. <ref type="table" target="#tab_1">Table 2 (b)</ref> shows the performance comparison with the recent multisource domain generalization method MLDG <ref type="bibr" target="#b41">[42]</ref> and a baseline based on simple data combination. We can see that collaborative learning achieves better generalization performance. For example, MLDG only marginally outperforms the simple baseline about 1.17%, while collaborative learning boosts the performance about 4.23%. Note that, collaborative learning here is only applied to source domains without image translation for fair comparison. <ref type="table">Table 3</ref> shows different contribution of each component to performance of our proposed approach. As the results shown, training on combination of source domains can improve the performance on target domain to some extent, which achieves 51.59% and 54.38% respectively. Collaborative learning between different source domains further  boosts the performance to 56.03%. Collaborative learning on target domain also boosts the performance on target which achieves 54.03%. This result shows that collaborative learning between source domains brings more improvements than on target domain. Moreover, full version of our approach achieves the best performance, achieving 58.55% on target domain. Thus, we can conclude that the proposed approach is effective for unsupervised domain adaptation in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTA5 + Synscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with SOTA</head><p>In this subsection, our proposed approach is compared to the recent state-of-the-art single-source and multi-source UDA methods on the GTA5+Synscapes to Cityscapes, including a baseline of source-only direct transfer (DT), single-source UDA methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>, the multi-source baseline that simple combination of source domains and multi-source UDA methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. We further validate the effectiveness of our proposed method by conducting experiments based on adapting different number of source domains to target domain. <ref type="table" target="#tab_4">Table 6</ref> shows the results comparison with other methods and <ref type="figure" target="#fig_3">Fig. 4</ref> demonstrates the visual comparison with baseline and MADAN <ref type="bibr" target="#b44">[45]</ref>. From the results of <ref type="table" target="#tab_4">Table 6</ref>, we can see that simply combining the images of source domains for networks training can already greatly boost the generalization performance on target domain, which advances from the 44.1% of GTA5?Cityscapes and 45.3% of Synscapes?Cityscapes to 51.6%. Simply adapting UDA methods to train on combination of source domains does not bring much improvements, such as AdaptSeg has only 1% improvement and Advent only 0.5%. By employing adaptation strategy, multi-source UDA further boost the performance, i.e., MDAN <ref type="bibr" target="#b43">[44]</ref> achieves 55.2% and MADAN <ref type="bibr" target="#b44">[45]</ref> achieves 55.7%. By further integrating collaborative learning on the source domains and target do- <ref type="table">Table 4</ref>. The quantitative comparison with the state-of-the-art methods. DT is the abbreviation of direct transfer. G, S and A indicate GTA5, Synscapes and All respectively. Adv, CL, ST and RL indicate Adversarial learning, Curriculum Learning, Self Training and Reconstruction Learning respectively. Ours-E and Ours-S represent end-to-end training and stage-wise training of our proposed method respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Appr <ref type="table">.  Source  road  sidewalk  building   wall  fence  pole  light  sign  veg  terrain  sky  person  rider  car  truck  bus  train  mbike  bike</ref>  main, we achieve 58.6% mIoU with the end-to-end training strategy and 59.0% mIoU with stage-wise training strategy, which greatly outperforms all the previous methods. When compared to single-source UDA, we observed that our approach achieves more significant improvement on categories such as train, truck, bus and et al.. These objects have rigid body and share much similarity in shape among different source domains. These results also validate the effectiveness of our proposed method. It is noteworthy that our approach does not employ any adversarial learning or any other sophisticated tricks, such as curriculum learning or self-training. More results see supplementary. <ref type="table">Table 5</ref> shows the performance comparison that adapting different source domains to Cityscapes. As the results shown, our approach can make full use of the labeled source domains and significantly improve the performance on target domain. We can see that adapting from GTA5 and Synscapes achieves 59.04% mIoU on 19 categories and 61.25% mIoU on 16 categories, which brings about 15% and 13% improvements w.r.t to the best model training on single-source. When adapting from all the three labeled source domains, our method further improve the performance (mIoU on 16 categories) from 61.25% to 62.24%. These results further illustrates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an effective multi-source domain adaptation framework for semantic segmentation based on collaborative learning. A simple image translation method is proposed to reduce the gap between domains. A collaborative learning method based on both labeled source domains and unlabeled target domain is proposed to fully explore essential semantic contexts across domains. Extensive experiments and ablation studies show that the proposed framework is able to significantly outperform all previous state-of-the-arts single-source and multi-source unsupervised domain adaptation methods, by effectively taking advantage of labeled data from multiple source domains and unlabeled data from target domain. Mapillary and IDD are another two widely used benchmarks for autonomous driven scene. They are have more images sampled from more various scenes. Tab. 7 shows the statistics comparison of different datasets.</p><p>Mapillary Vistas dataset (M) is a large-scale diverse street-level image dataset that containing 25,000 high resolution images with densely pixel-level annotated into 66 object categories. It is designed and compiled to cover diversity, richness of detail and geographic extent. The images are from all around the world, captured at various conditions regarding weather, season and daytime. Moreover, these images come from different imaging devices (mobile phones, tablets, action cameras, professional capturing rigs) and differently experienced photographers. To evaluation our proposed method, we train models with the common 19 categories with Cityscapes <ref type="bibr" target="#b4">[5]</ref> training labels.</p><p>IDD (India Driving Dataset) <ref type="bibr" target="#b35">[36]</ref> (I) consists of 20,000 images, which are obtained from a front facing camera attached to a car and finely annotated with 34 classes collected from 182 drive sequences on Indian roads. Most of images are 1080p resolution with some are 720p. Their label set is expanded in comparison to Cityscapes <ref type="bibr" target="#b4">[5]</ref>, to account for new classes. We train all the models based on the common 19 classes with Cityscapes for adaptation setting. Note that, IDD has another 10k version and here we use thus 20k version one for evalutaion of our proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Results</head><p>Tab. 8 shows the performance comparison of proposed collaborative learning between sources trained on the original images which is not translated with baseline that simple combination and domain generalization method MLDG <ref type="bibr" target="#b41">[42]</ref>. From the results, we can see that our proposed collaborative learning can achieve better or comparable performance compared with the state-of-the-art domain generalization method. For example, we achieve 47.80% and 47.16% on the IDD and Mapillary dataset, respectively. Both of them are better or comparable to the MLDG.</p><p>Tab. 6 shows the comparison of i): the reproduce of AdaptSeg <ref type="bibr" target="#b32">[33]</ref> and Advent <ref type="bibr" target="#b36">[37]</ref> that adapting from GTA5, Synscapes and combination of GTA5 and Synscapes to IDD and Mapillary, and ii): Direct Transfer from GTA5, Synscapes and GTA5+Synscapes to IDD and Mapillary, and iii): each model and ensemble of our proposed method that adapting from GTA5 + Synscapes to IDD and Mapillary. Note that, the network architecture and hyperparameters for different losses are same as the setting to Cityscapes.</p><p>From Tab. 6, we can see that our proposed method achieve the best performance no matter what the target dataset, i.e., achieving 50.19% and 53.44% on IDD and Mapillary respectively. Moreover, directly adopting UDA methods on combined sources data sometimes could not achieve better performance than direct transfer. For example, AdaptSeg only achieves 46.65% when IDD as target domain which is lower the performance of directly transfer based on combined data. All these results further validate the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The overall framework of proposed approach consists of three components, including that image-to-image translation based on LAB color space, collaborative learning between source domains and collaborative learning on target domain. The solid arrows represent the forward data flow and different colors indicate different source domains or target domain data flow. The dash arrows represent the supervision to the network outputs. For illustration, we just show the case of two source domains as an example to explain our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 6 mask1 ?? == c; 7 mask2</head><label>167</label><figDesc>Pseudo Labels GenerationData: The probability mapP ? R C?H?W , keep proportion ?, maximum thresh ? , the ignore label l ig Result: one-hot hard pseudo labels? 1? ? argmax(P , dim = 0),? ? R H?W 2 for c ? 0 to C ? 1 do 3P c ? sort(P {c,?,?} , order = Descending); 4 get the number of pixels n c which are predicted to category c: n c ? sum(? == c);<ref type="bibr" target="#b4">5</ref> get the threshold t that used to filter the prediction: t ? min(P c [n c ? ?], ? ); ?P {c,?,?} &lt;= t;8? [mask1 &amp; mask2] ? l ig . 9 endAlgorithm 1. After then, we can train models M Si with both source domains and target domain. Thus, the object function for model M Si becomes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual Comparison with baseline and other methods. Left to right: Input image from Cityscapes, corresponding groundtruth, segmentation results of baseline that simple combination of source domains, segmentation results of MADAN<ref type="bibr" target="#b44">[45]</ref> and proposed method. Note that, all these results are adapting from GTA5+Synscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Validity of the proposed image translation method. The performance comparison with the recent single-source UDA methods trained on images that before and after translation.</figDesc><table><row><cell cols="3">GTA5?Cityscapes</cell><cell></cell></row><row><cell>Methods</cell><cell>Before</cell><cell>+Trans</cell><cell>Diff.</cell></row><row><cell>Direct Transfer</cell><cell>39.53</cell><cell>43.36</cell><cell>? 3.83</cell></row><row><cell>AdaptSeg [33]</cell><cell>41.32</cell><cell>43.66</cell><cell>? 2.43</cell></row><row><cell>AdaptSeg-LS [33]</cell><cell>43.11</cell><cell>45.95</cell><cell>? 2.84</cell></row><row><cell>Advent [37]</cell><cell>44.30</cell><cell>45.96</cell><cell>? 1.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The validity of model selection and the proposed collaborative learning on the GTA5 + Synscapes to Cityscapes. (a) shows the performance of each single model and the final ensemble, (b) shows the comparison of proposed collaborative learning between source domains (Co-Learning-Src) with baseline and MLDG<ref type="bibr" target="#b41">[42]</ref>. E: End-to-End, S: Stage-Wise.</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell>Model</cell><cell>E</cell><cell>S</cell><cell>Methods</cell><cell cols="2">mIoU Diff.</cell></row><row><cell>MS GT A5</cell><cell cols="2">56.90 57.72</cell><cell cols="2">Data Combination 51.56</cell><cell>-</cell></row><row><cell>MS Syns</cell><cell cols="2">56.65 57.81</cell><cell cols="3">MLDG+TN [42] 52.73 ? 1.17</cell></row><row><cell cols="3">M Ensemble 58.55 59.04</cell><cell cols="3">Co-Learning-Src 55.79 ? 4.23</cell></row><row><cell cols="6">annotations are compatible with Cityscapes. The style</cell></row><row><cell cols="6">of Syncscapes is closer to Cityscapes than GTA5 and</cell></row><row><cell>Synscapes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>86.7 47.7 34.1 39.3 44.6 34.2 87.2 49.6 89.7 65.6 38.1 88.2 48.1 63.0 41.9 39.2 59.2 58.6 Ours-S -93.6 59.6 87.1 44.9 36.7 42.1 49.9 42.5 87.7 47.6 89.9 63.5 40.3 88.2 41.0 58.3 53.1 37.9 57.7 59.0</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mIoU</cell></row><row><cell>DT [33]</cell><cell>-</cell><cell></cell><cell cols="3">81.8 40.6 76.1 23.3 16.8 36.9 36.8 40.1 83.0 34.8 84.9 59.9 37.7 78.5 20.4 20.5 7.8 27.3 52.5 45.3</cell></row><row><cell cols="2">AdaptSeg [33] Adv</cell><cell></cell><cell cols="3">94.2 60.9 85.1 29.1 25.2 38.6 43.9 40.8 85.2 29.7 88.2 64.4 40.6 85.8 31.5 43.0 28.3 30.5 56.7 52.7</cell></row><row><cell>FDA [39]</cell><cell>ST</cell><cell>S</cell><cell cols="3">93.6 58.1 84.0 30.4 29.2 39.0 43.1 51.7 85.9 28.8 86.9 64.0 45.7 84.7 30.4 36.5 28.5 34.4 62.4 53.5</cell></row><row><cell>Advent [37]</cell><cell>Adv</cell><cell></cell><cell cols="3">92.2 51.3 85.0 40.8 31.2 39.0 42.5 42.5 86.5 46.1 84.8 65.2 39.0 87.0 32.6 49.0 29.5 28.6 50.0 53.8</cell></row><row><cell>UIA [24]</cell><cell>Adv</cell><cell></cell><cell cols="3">94.0 60.0 84.9 29.5 26.2 38.5 41.6 43.7 85.3 31.7 88.2 66.3 44.7 85.7 30.7 53.0 29.5 36.5 60.2 54.2</cell></row><row><cell>DT [33]</cell><cell>-</cell><cell></cell><cell cols="3">75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6</cell></row><row><cell cols="2">AdaptSeg [33] Adv</cell><cell></cell><cell cols="3">86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4</cell></row><row><cell>Advent [37]</cell><cell>Adv</cell><cell></cell><cell cols="3">89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5</cell></row><row><cell>UIA [24] PyCDA [16]</cell><cell>Adv CL</cell><cell>G</cell><cell cols="3">90.6 36.1 82.6 29.5 21.3 27.6 31.4 23.1 85.2 39.3 80.2 59.3 29.4 86.4 33.6 53.9 0.0 32.7 37.6 46.3 90.5 36.3 84.4 32.4 28.7 34.6 36.4 31.5 86.8 37.9 78.5 62.3 21.5 85.6 27.9 34.8 18.0 22.9 49.3 47.4</cell></row><row><cell>BDL [15]</cell><cell>ST</cell><cell></cell><cell cols="3">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5</cell></row><row><cell>FDA [39]</cell><cell>ST</cell><cell></cell><cell cols="3">92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4 50.5</cell></row><row><cell>PIT [22]</cell><cell>RL</cell><cell></cell><cell cols="3">87.5 43.4 78.8 31.2 30.2 36.3 39.9 42.0 79.2 37.1 79.3 65.4 37.5 83.2 46.0 45.6 25.7 23.5 49.9 50.6</cell></row><row><cell>Data Comb.</cell><cell>-</cell><cell></cell><cell cols="3">85.1 36.9 84.1 39.0 33.3 38.7 43.1 40.2 84.8 37.1 82.4 65.2 37.8 69.4 43.4 38.8 34.6 33.2 53.1 51.6</cell></row><row><cell cols="2">AdaptSeg [33] Adv</cell><cell></cell><cell cols="3">89.3 47.3 83.6 40.3 27.8 39.0 44.2 42.5 86.7 45.5 84.5 63.1 38.0 79.4 34.9 48.3 42.1 30.7 52.3 53.7</cell></row><row><cell>Advent [37]</cell><cell>Adv</cell><cell></cell><cell cols="3">91.8 49.0 84.6 39.4 31.5 39.9 42.9 43.5 86.3 45.1 84.6 65.3 41.0 87.1 37.9 49.2 31.0 30.3 48.8 54.2</cell></row><row><cell cols="2">MDAN [44] Adv</cell><cell>A</cell><cell cols="3">92.4 56.1 86.8 42.7 32.9 39.3 48.0 40.3 87.2 47.2 90.5 64.1 35.9 87.8 33.8 48.6 39.0 27.6 49.2 55.2</cell></row><row><cell cols="2">MADAN [45] Adv</cell><cell></cell><cell cols="3">94.1 61.0 86.4 43.3 32.1 40.6 49.0 44.4 87.3 47.7 89.4 61.7 36.3 87.5 35.5 45.8 31.0 33.5 52.1 55.7</cell></row><row><cell cols="6">Ours-E 94.2 61.8 Table 5. The performance of our proposed method that uses -</cell></row><row><cell cols="6">different source domains for adaptation.</cell><cell>G: GTA5, S:</cell></row><row><cell cols="6">Synscapes, Y: SYNTHIA. mIoU19, mIoU16 and mIoU13</cell></row><row><cell cols="6">indicate performance on different number of categories.</cell></row><row><cell></cell><cell cols="5">sources mIoU19 mIoU16 mIoU13</cell></row><row><cell></cell><cell></cell><cell cols="2">G</cell><cell>39.53</cell><cell>43.28</cell><cell>48.25</cell></row><row><cell>Source-Only</cell><cell></cell><cell>S</cell><cell></cell><cell>44.43</cell><cell>48.74</cell><cell>54.09</cell></row><row><cell></cell><cell></cell><cell cols="2">Y</cell><cell>-</cell><cell>32.31</cell><cell>37.41</cell></row><row><cell></cell><cell></cell><cell cols="2">G+S</cell><cell>59.04</cell><cell>61.25</cell><cell>65.87</cell></row><row><cell cols="2">Multi-Sources</cell><cell cols="2">G+Y S+Y</cell><cell>--</cell><cell>54.03 58.19</cell><cell>59.42 63.18</cell></row><row><cell></cell><cell cols="3">G+S+Y</cell><cell>-</cell><cell>62.24</cell><cell>67.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The quantitative results that adapting from GTA5 + Synscapes to IDD and Mapillary respectively. Here, Our-M* means the performance of model MS * , and Ours-Ensemble means the results that ensemble of all outputs of models MS * . ? means training our proposed approach with stage-wise.<ref type="bibr" target="#b22">23</ref>.6 63.6 14.8 12.0 25.8 30.7 32.7 75.2 41.2 89.4 36.2 22.0 73.0 19.5 17.2 0.2 27.7 31.1 37.18 AdaptSeg [33] 85.9 24.2 73.2 17.7 27.4 26.4 33.0 39.0 75.4 44.6 94.3 34.7 27.8 77.4 25.8 16.5 1.2 29.9 31.2 41.35 Advent [37] 86.2 23.9 74.6 17.8 26.8 29.5 35.9 39.8 79.4 43.6 96.2 37.3 27.5 78.4 26.3 16.1 1.4 29.1 29.1 42.04 DT G 82.2 28.6 74.2 23.4 27.2 35.3 36.4 18.6 73.8 29.2 89.6 58.9 39.2 74.5 35.0 17.2 12.5 31.3 27.8 42.89 AdaptSeg [33] 86.5 31.6 78.2 24.6 30.0 36.1 35.8 31.6 73.4 33.2 93.7 59.2 44.5 78.6 41.2 39.3 14.8 36.5 32.3 47.44 Advent [37] 86.6 28.3 77.9 24.7 30.6 36.1 36.0 32.5 75.8 34.9 94.4 58.8 44.1 79.9 41.3 42.3 15.7 35.6 32.6 47.79 DT S+G 77.7 30.9 75.2 27.0 27.5 33.4 37.2 37.3 76.9 43.1 93.3 55.8 38.0 72.5 38.4 40.2 2.8 36.9 42.3 46.64 AdaptSeg [33] 84.2 33.4 78.0 27.9 34.0 38.0 41.6 39.4 78.6 34.5 92.7 46.9 41.6 81.9 38.3 39.0 3.6 41.5 40.5 48.19 Advent [37] 87.2 36.2 78.0 27.1 31.2 38.4 40.8 40.2 80.8 44.2 96.0 47.1 43.5 82.3 39.0 39.3 5.0 42.0 40.3 49.40 Ours-M1 88.2 32.5 81.0 29.1 37.5 39.9 41.7 39.6 80.4 44.6 95.8 58.7 40.2 83.1 48.1 40.7 2.3 40.1 43.2 50.89 Ours-M2 87.8 31.6 81.0 30.0 37.8 34.8 38.3 41.3 78.1 39.1 95.1 60.1 49.5 82.2 42.7 39.0 19.2 45.9 48.0 51.67 Ours-Ensemble 88.5 34.3 81.9 31.9 41.1 39.0 40.1 41.5 79.7 45.0 95.7 62.7 51.1 83.3 49.9 45.9 8.5 46.4 47.5 53.37 Ours-M1 ? 87.5 40.1 80.9 31.0 37.4 40.0 42.5 40.6 79.6 42.4 95.2 55.5 46.5 84.5 45.1 40.3 16.5 41.6 39.1 51.92 Ours-M2 ? 88.6 36.5 81.4 29.7 38.2 41.3 43.0 43.4 80.2 45.8 95.6 58.3 43.8 84.5 42.5 42.0 10.1 46.2 43.9 52.37 Ours-Ensemble ? 88.4 40.1 81.9 32.4 39.8 41.4 42.2 42.7 80.1 46.4 95.6 58.2 48.5 84.7 46.6 45.5 11.7 46.9 42.4 53.44</figDesc><table><row><cell>Methods</cell><cell>Source</cell><cell>Target</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>DT</cell><cell></cell><cell></cell><cell cols="20">80.5 7.8 51.1 17.8 6.4 23.4 4.0 22.4 77.5 9.2 90.4 41.4 37.3 68.6 32.0 27.9 0.0 55.7 18.6 35.37</cell></row><row><cell>AdaptSeg [33]</cell><cell>S</cell><cell></cell><cell cols="20">92.5 19.4 58.1 23.2 8.9 20.4 5.0 25.7 77.2 9.5 93.9 49.6 42.7 72.0 37.1 30.6 0.0 59.6 20.0 39.23</cell></row><row><cell>Advent [37]</cell><cell></cell><cell></cell><cell cols="20">93.2 19.5 59.1 21.9 8.4 23.9 5.6 24.8 79.1 9.4 94.7 48.2 40.2 71.4 37.1 29.7 0.0 58.9 21.3 39.28</cell></row><row><cell>DT</cell><cell></cell><cell></cell><cell cols="20">90.2 27.9 56.3 23.4 20.4 27.8 4.9 26.0 74.4 29.6 87.8 46.4 39.1 65.1 47.3 36.6 0.0 49.1 26.9 41.01</cell></row><row><cell>AdaptSeg [33]</cell><cell>G</cell><cell></cell><cell cols="20">92.8 21.4 64.7 25.0 23.3 26.9 6.0 40.7 76.7 30.5 92.5 45.7 34.0 70.9 50.5 37.5 0.0 47.6 26.2 42.78</cell></row><row><cell>Advent [37]</cell><cell></cell><cell></cell><cell cols="20">93.0 25.1 66.2 31.9 22.3 29.1 10.0 38.1 73.7 26.4 93.2 49.4 43.2 72.1 52.5 40.0 0.0 50.7 26.6 44.40</cell></row><row><cell>DT AdaptSeg [33]</cell><cell></cell><cell>IDD</cell><cell cols="20">92.2 19.1 66.0 32.1 19.4 29.4 9.5 45.1 80.3 35.7 94.8 59.4 40.5 76.4 49.3 46.6 0.0 59.9 38.4 47.06 92.0 18.9 66.2 23.9 17.6 30.6 5.8 45.8 81.7 30.1 94.4 57.3 47.5 75.2 51.5 53.6 0.0 58.9 35.4 46.65</cell></row><row><cell>Advent [37]</cell><cell></cell><cell></cell><cell cols="20">93.9 28.8 68.2 32.1 20.0 32.1 8.8 44.9 77.1 23.1 95.0 58.8 47.1 74.3 57.4 49.4 0.0 61.0 32.8 47.61</cell></row><row><cell>Ours-M1</cell><cell></cell><cell></cell><cell cols="20">95.4 38.5 70.0 36.7 21.2 25.0 14.2 43.9 78.6 28.5 94.8 58.9 45.0 70.8 56.1 48.3 0.0 63.4 38.8 48.86</cell></row><row><cell>Ours-M2</cell><cell>S+G</cell><cell></cell><cell cols="20">95.1 35.2 71.2 39.0 19.3 27.2 11.5 48.1 77.8 26.3 95.3 57.6 39.2 69.7 52.2 46.1 0.0 60.0 34.0 47.63</cell></row><row><cell>Ours-Ensemble</cell><cell></cell><cell></cell><cell cols="20">95.8 41.8 72.9 39.5 21.5 26.4 18.2 44.5 78.1 28.1 95.5 62.2 43.0 70.6 58.9 49.5 0.0 63.5 38.9 49.94</cell></row><row><cell>Ours-M1 ?</cell><cell></cell><cell></cell><cell cols="20">95.6 39.6 71.5 38.4 19.9 30.1 12.8 47.8 78.3 31.5 95.3 55.6 47.5 74.6 48.9 54.9 0.0 64.5 39.9 49.83</cell></row><row><cell>Ours-M2 ?</cell><cell></cell><cell></cell><cell cols="20">95.3 37.5 71.5 36.4 21.1 31.2 13.1 44.6 79.4 33.0 95.2 55.4 46.9 73.4 51.6 44.8 0.0 64.8 41.5 49.30</cell></row><row><cell>Ours-Ensemble ?</cell><cell></cell><cell></cell><cell cols="20">95.8 39.9 73.1 38.8 21.0 31.0 14.1 43.8 78.2 32.2 95.5 58.2 47.2 74.2 52.6 50.7 0.0 65.8 41.4 50.19</cell></row><row><cell cols="4">DT 70.4 A. More Experiments S Mapillary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.1. Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>The comparison of different datasets for semantic segmentation in autonomous driving.</figDesc><table><row><cell>Dataset</cell><cell>Num. of</cell><cell>Num. of</cell><cell>Cats.</cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell>Images</cell><cell>Scenes</cell><cell cols="2">(Train/All)</cell><cell>Resolution</cell></row><row><cell>Cityscapes [5]</cell><cell>5K</cell><cell>50</cell><cell>19/30</cell><cell></cell><cell>2048?1024</cell></row><row><cell cols="2">Mapillary [23] 25K</cell><cell>-</cell><cell cols="3">19/66 ?1920?1080</cell></row><row><cell>IDD [36]</cell><cell>20k</cell><cell>180</cell><cell>19/34</cell><cell></cell><cell>1678?968</cell></row><row><cell cols="6">Table 8. The domain generalization ability comparison of</cell></row><row><cell cols="6">Collaborative Learning Between Sources (Co-Learning-Src) with</cell></row><row><cell cols="4">baseline and domain generalization method.</cell><cell></cell></row><row><cell></cell><cell cols="3">GTA5+Synscapes</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Target</cell><cell cols="2">mIoU</cell></row><row><cell cols="2">Data Combination</cell><cell></cell><cell></cell><cell cols="2">47.06</cell></row><row><cell cols="2">MLDG+TN [42]</cell><cell></cell><cell>I</cell><cell cols="2">47.42</cell></row><row><cell cols="2">Co-Learning-Srcs</cell><cell></cell><cell></cell><cell cols="2">47.80</cell></row><row><cell cols="2">Data Combination</cell><cell></cell><cell></cell><cell cols="2">46.64</cell></row><row><cell cols="2">MLDG+TN [42]</cell><cell></cell><cell>M</cell><cell cols="2">47.11</cell></row><row><cell cols="2">Co-Learning-Srcs</cell><cell></cell><cell></cell><cell cols="2">47.16</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation based on dual-level domain mixing for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bdcn: Bi-directional cascade network for perceptual edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-target domain adaptation with collaborative consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid based graph reasoning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A closed-form solution to photorealistic image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="453" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-preserved multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1059" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR, 2015. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generatively inferential co-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correlation-aware adversarial domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Mohammad Mahfujur Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">107124</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A two-stage weighting framework for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Idd: A dataset for exploring problems of autonomous navigation in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbumani</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Synscapes: A photorealistic synthetic dataset for street scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Wrenninge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08705</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Photorealistic style transfer via wavelet transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongkyu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9036" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2100" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Generalizable semantic segmentation via model-agnostic learning and target-specific normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12296</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7287" to="7300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
