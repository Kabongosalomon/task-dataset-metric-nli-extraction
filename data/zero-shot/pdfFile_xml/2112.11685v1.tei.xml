<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cost Aggregation Is All You Need for Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisu</forename><surname>Nam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cost Aggregation Is All You Need for Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel cost aggregation network, dubbed Volumetric Aggregation with Transformers (VAT), to tackle the few-shot segmentation task by using both convolutions and transformers to efficiently handle high dimensional correlation maps between query and support. In specific, we propose our encoder consisting of volume embedding module to not only transform the correlation maps into more tractable size but also inject some convolutional inductive bias and volumetric transformer module for the cost aggregation. Our encoder has a pyramidal structure to let the coarser level aggregation to guide the finer level and enforce to learn complementary matching scores. We then feed the output into our affinity-aware decoder along with the projected feature maps for guiding the segmentation process. Combining these components, we conduct experiments to demonstrate the effectiveness of the proposed method, and our method sets a new state-of-the-art for all the standard benchmarks in few-shot segmentation task. Furthermore, we find that the proposed method attains state-of-the-art performance even for the standard benchmarks in semantic correspondence task although not specifically designed for this task. We also provide an extensive ablation study to validate our architectural choices. The trained weights and codes are available at: https: //seokju-cho.github.io/VAT/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is one of the fundamental Computer Vision tasks which aims to label each pixel in an image with a corresponding class. With the advent of deep networks and the availability of large-scale datasets with ground-truth segmentation annotations, substantial progress has been made in this task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b59">60]</ref>. However, as such advances can be attributed to abundant pixel-wise segmentation maps made by manual annotation, which is often labor-intensive, few-shot segmentation task <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b53">54]</ref> has been introduced to address this, where only a handful of <ref type="bibr">Figure 1</ref>. Our VAT reformulates few-shot segmentation task to semantic correspondence task. VAT not only sets state-of-the-art in few-shot segmentation (top), but also attains highly competitive performance for semantic correspondence (bottom). support samples are provided to make a mask prediction for a query, which mitigates the reliance on the labeled data.</p><p>For a decade, numerous methods for few-shot segmentation have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref>, and most methods follow a learning-to-learn paradigm <ref type="bibr" target="#b47">[48]</ref> to avoid the risk of overfitting due to insufficient training data. Since the prediction for query image should be conditioned on support images and corresponding masks, the key to this task is how to effectively utilize provided support samples. Although formulated in various ways, most early efforts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b75">76]</ref> attempted to utilize a prototype extracted from support samples. However, such approaches disregard pixel-wise relationships between support and query features or spatial structure of features, which may lead to sub-optimal results.</p><p>In light of this, we argue that few-shot segmentation task can be reformulated as semantic correspondence that aims to find pixel-level correspondences across semantically similar images, which poses some challenges from large intraclass appearance and geometric variations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref>. Recent approaches for this task <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63]</ref> carefully designed their models analogously to the classical matching pipeline <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53]</ref>, i.e., feature extraction, cost aggregation and flow estimation. Especially, the latest works focused on cost aggregation stage <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> and showed outstanding performance.</p><p>Taking similar approaches, recent few-shot segmentation methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81]</ref> also attempted to leverage pixelwise information by refining features either by using crossattention <ref type="bibr" target="#b80">[81]</ref> or graph attention <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79]</ref>. However, as proven in semantic correspondence literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref>, without aggregating the matching scores, and solely relying on raw correlation maps between features may suffer from the challenges posed due to ambiguities generated by repetitive patterns or background clutters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b62">63]</ref>. To address this, one of the latest work, HSNet <ref type="bibr" target="#b37">[38]</ref>, attempts to aggregate the matching scores with 4D convolutions, but it lacks an ability to consider an interaction among the matching scores due to the inherent nature of convolutions.</p><p>In this paper, we introduce a novel cost aggregation network, dubbed Volumetric Aggregation with Transformers (VAT), to tackle the few-shot segmentation task by using both convolutions and transformers to efficiently handle high dimensional correlation maps between query and support. Specifically, within our encoder, we propose volumetric embedding module consisting of a series of 4D convolutions to not only transform high-dimensional correlation maps into more tractable size, but also inject some convolutional bias to aid the subsequent processing by transformers <ref type="bibr" target="#b72">[73]</ref>. The output then undergoes volumetric transformer module for the cost aggregation with 4D swin transformer. With these combined, our encoder processes the input in a pyramidal manner to expedite the learning by letting the aggregated correlation maps at coarser level play as a guidance to finer level, enforcing to learn complementary matching scores. Subsequent to encoder, our affinity-aware decoder refines the aggregated costs with the help from appearance affinity and makes a prediction.</p><p>We demonstrate the effectiveness of our method on several benchmarks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref>. Although not specifically designed for semantic correspondence task, our work attains state-of-the-art performance on all the benchmarks for fewshot segmentation and achieves highly competitive results even for semantic correspondence, showing its superiority over the recently proposed methods. We also include a detailed ablation study to justify our choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Few-shot Segmentation. Inspired by few-shot learning paradigm <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b56">57]</ref>, which aims to learn-to-learn a model for a novel task with only a limited number of samples, fewshot segmentation has received considerable attention. Following the success of <ref type="bibr" target="#b53">[54]</ref>, prototypical networks <ref type="bibr" target="#b56">[57]</ref> and numerous other works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b81">82]</ref> proposed to utilize a prototype extracted from support samples, which is used to refine the query features to contain the relevant support information. In addition, inspired by <ref type="bibr" target="#b79">[80]</ref> that observed the use of high-level features leads to a performance drop, <ref type="bibr" target="#b61">[62]</ref> proposed to utilize high-level features by computing a prior map which takes maximum score within a correlation map. Many variants <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b77">78]</ref> extended this idea of utilizing prior maps to guide the feature learning.</p><p>However, as methods based on prototypes or prior maps have apparent limitations, e.g., disregarding pixel-wise relationships between support and query features por spatial structure of feature maps, numerous recent works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79]</ref> attempted to fully utilize a correlation map to leverage the pixel-wise relationships between source and query features. Specifically, <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79]</ref> use graph attention, <ref type="bibr" target="#b37">[38]</ref> proposes efficient 4D convolutions to fully exploit the multi-level features, and <ref type="bibr" target="#b28">[29]</ref> formulates the task as optimal transport problem. However, these approaches either do not provide a means to aggregate the matching scores or lack an ability to consider interactions of matching scores.</p><p>As concurrent works, <ref type="bibr" target="#b80">[81]</ref> utilizes transformers and proposes to use a cycle-consistent attention mechanism to refine the feature maps to be more discriminative, without considering aggregation of matching scores. <ref type="bibr" target="#b58">[59]</ref> propose global and local enhancement module to refine the features using transformers and convolutions in the decoder, respectively. <ref type="bibr" target="#b36">[37]</ref> focuses solely on the transformer-based classifier by freezing the encoder and decoder, and adapting only the classifier to the task. Unlike them, we take different approach, focusing on aggregation of the high dimensional correlation maps in a novel and efficient way.</p><p>Semantic Correspondence. The objective of semantic correspondence is to find correspondences between semantically similar images with additional challenges posed by large intra-class appearance and geometric variations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>. This is highly similar to few-shot segmentation setting in that few-shot segmentation also aims to label the objects of same class with large intra-class variations, and thus the recent works in both tasks have been taking similar approaches. The latest approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> in semantic correspondence focused on cost aggregation stage to find reliable correspondences, and proved its importance. Among those, <ref type="bibr" target="#b38">[39]</ref> proposed to use 4D convolutions for cost aggregation while it showed apparent limitations which include limited receptive fields of convolutions. CATs <ref type="bibr" target="#b5">[6]</ref> resolve this issue and sets a new state-of-the-art by leveraging transformers <ref type="bibr" target="#b64">[65]</ref> to aggregate the cost volume. However, it suffers from high computation due to high dimensional nature of correlation map. In this paper, we pro- Projection</p><formula xml:id="formula_0">Prediction . ! Decoder (? ! " , ! " , ? # " , # " , " ) (? ! $ , ! $ , ? # $ , # $ , $ ) (? ! % , ! % , ? # % , # % , % ) ( ' ? ! $ , ( ! $ , ' ? # $ , ( # $ , ) ( ' ? ! " , ( ! " , ' ? # " , ( # " , ) ( ' ? ! % , ( ! % , ' ? # % , ( # % , ) ( ' ? ! % , ( ! % , ' ? # % , ( # % , )</formula><p>Affinity-aware Decoder Block Affinity-aware <ref type="figure" target="#fig_3">Figure 2</ref>. Overall network architecture. Our networks consist of feature extraction and cost computation, pyramidal transformer encoder, and affinity-aware transformer decoder. Given query and support images, we first extract all the intermediate features extracted from backbone network and compute multi-level correlation maps. They then undergo the encoder to aggregate the matching scores with transformers in a pyramidal fashion. The decoder finally predicts a mask label for a query image.</p><formula xml:id="formula_1">Decoder Block Support Dim. Avg. Pool Projection $ % # $ % ( ' ? ! % , ( ! % , + ?) (2 ' ? ! % , 2 ( ! % , + ?) (4 ' ? ! % , 4 ( ! % , + ?) %</formula><p>pose to resolve the aforementioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>The goal of few-shot segmentation is to segment an object of unseen classes from a query image given only a few annotated examples <ref type="bibr" target="#b53">[54]</ref>. To mitigate the overfitting caused by insufficient training data, we follow the common protocol called episodic training <ref type="bibr" target="#b65">[66]</ref>. Let us denote training and test sets as D train and D test , respectively, where object classes of both sets do not overlap. Under K-shot setting, multiple episodes are formed from both sets, each consisting of a support set S = {(x k s , m k s )} K k=1 , where (x k s , m k s ) is k-th support image and its corresponding mask pair and a query sample Q = (x q , m q ), where x q and m q are a query image and mask, respectively. During training, our model takes a sampled episode from D train , and learn a mapping from S and x q to a prediction m q . At inference, our model predictsm q for randomly sampled S and x q from D test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation and Overview</head><p>The key to few-shot segmentation is how to effectively utilize provided support samples for a query image. While conventional methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81]</ref> attempted to utilize global-or part-level prototypes extracted from support features, recent methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81]</ref> attempted to leverage pixel-wise relationships between query and support. One of the latest work, HSNet <ref type="bibr" target="#b37">[38]</ref>, attempts to aggregate the matching scores with 4D convolutions, but it lacks an ability to consider interactions among the matching scores due to the inherent nature of convolutions.</p><p>To overcome these, we present a novel design, dubbed volumetric aggregation with transformers (VAT), to effec-tively integrate information present in all pixel-wise matching costs between query and support with transformers <ref type="bibr" target="#b64">[65]</ref>. VAT is an encoder-decoder architecture. To compensate for huge complexity of standard transformers <ref type="bibr" target="#b64">[65]</ref> that prevents from directly applying to correlation maps, for the encoder, we present volume embedding module to effectively reduce the number of tokens while injecting some convolutional inductive bias <ref type="bibr" target="#b72">[73]</ref> and volume transformer module based on swin transformer <ref type="bibr" target="#b32">[33]</ref>. We design our encoder in a pyramidal fashion to let the output from coarser level cost aggregation to guide the finer level. We then present our decoder that utilizes appearance affinity to resolve the ambiguities in the correlation map and expedite the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction and Cost Computation</head><p>We first extract features from query and support images and compute initial cost between them. In specific, we follow <ref type="bibr" target="#b37">[38]</ref> to exploit rich semantics present in different feature levels, and build multi-level correlation maps. Given query and support images, x q and x s , we use CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b55">56]</ref> to produce a sequence of L feature maps, {(F l q , F l s )} L l=1 , where F l q and F l s denote query and support feature maps at l-th level, respectively. We utilize a support mask, m s , to encode segmentation information and filter out the background information as done in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b77">78]</ref>. We obtain a masked support feature such thatF l s = F l s ? l (m s ), where denotes Hadamard product and ? l (?) denotes a function that resizes the given tensor followed by expansion along channel dimension of l-th layer.</p><p>Given a pair of feature maps, F l q and F l s , we compute a correlation map using the inner product between L-2 nor- malized features such that</p><formula xml:id="formula_2">Query dim. Support dim. ? ? ? ? ?? ? ? Volume Embedding Module (VEM) Volume Transformer Module (VTM) ? ( , , , , ) (1, 1, 1, 1, ) Query dim. ? Support dim. ? ? ? ? ? ? ? ? ? ? ? ? (1, 1, 1, 1, ) Query dim. ? ? ? ? ? ? ? ? ? ? ? ? ? Support dim.</formula><formula xml:id="formula_3">C l (i, j) = ReLU F l q (i) ?F l s (j) F l q (i) F l s (j) ,<label>(1)</label></formula><p>where i and j denote 2D spatial positions of feature maps, respectively. As done in <ref type="bibr" target="#b37">[38]</ref>, we collect correlation maps computed from all the intermediate features of same spatial size and stack them to obtain a hypercorrelation C p = {C l } l?Lp ? R hq?wq?hs?ws?|Lp| , where h q , w q and h s , w s are height and width of feature maps of query and support, respectively, and L p is subset of CNN layer indices {1, ..., L} at some pyramid layer p, indicating the correlation maps of identical spatial size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pyramidal Transformer Encoder</head><p>In this section, we show how to effectively aggregate the hypercorrelation with proposed Transformers-based architecture and how to extend this to pyramidal architecture.</p><p>Volume Embedding Module. Aggregating the hypercorrelation by considering all the query and support spatial dimensions, i.e., h q ? w q ? h s ? w s , as tokens requires extremely large computation, which has to be resolved to fully exploit all the pixel-wise interactions present in the correlation maps. Perhaps the most straightforward way to reduce the resolutions is to use 4D spatial pooling across query and support spatial dimensions, but this strategy risks losing some information. As an alternative, one can split the hypercorrelation into non-overlapping tensors and embed with a large learnable kernel similarly to a patch embedding in ViT <ref type="bibr" target="#b8">[9]</ref>, but this demands a substantial amount of resources due to the curse of dimensionality. Furthermore, unlike image classification task <ref type="bibr" target="#b8">[9]</ref> which finds a global representation of an image, segmentation aiming for dense prediction needs to consider overlapping neighborhood information.</p><p>To alleviate these limitations, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, we introduce Volume Embedding Module (VEM) to not only reduce the computation by effectively decreasing the number of tokens, but also inject some convolutional inductive bias <ref type="bibr" target="#b72">[73]</ref> to help the subsequent transformer-based model to improve the ability of learning interactions among the hypercorrelation. Concretely, we sequentially reduce support and query spatial dimensions by applying 4D spatial max-pooling, overlapping 4D convolutions, RELU, and Group Normalization (GN), where we project the multilevel similarity vector at each 4D position, i.e., projecting a vector size of |L p | to a arbitrary fixed dimension denoted as D. Considering receptive fields of VEM as 4D window size, i.e., m ? m ? m ? m, we build a tensor M ? R? q ??q??s??s?D , where? and? are the processed sizes. Note that different size of output can be made for source and target spatial dimensions by varying the hyperparameters.</p><p>Overall, we define such a process as following:</p><formula xml:id="formula_4">M p = VEM(C p ).<label>(2)</label></formula><p>Volumetric Transformer Module. Although VEM reduces the number of tokens to some extent, directly applying standard transformers <ref type="bibr" target="#b8">[9]</ref> that has quadratic complexity with respect to number of tokens is still challenging. Our Volumetric Transformer Module (VTM) tackles this by extending swin transformer <ref type="bibr" target="#b32">[33]</ref>. By setting a local 4D window, the computation for computing self-attention can be significantly reduced, without significant drop in performance. While <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71]</ref> could also handle a long sequence of tokens with linear complexity, we argue that as proven in optical flow and semantic correspondence literature <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b57">58]</ref> that neighboring pixels tend to have similar correspondences, computing self-attention over local regions and allowing cross-window interaction can help to find reliable correspondences, which in turn yields better segmentation performance in our framework. It should be noted that use of 4D convolutions <ref type="bibr" target="#b37">[38]</ref> also considers local consensus, but it lacks an ability to consider pixel-wise interactions due to the use of fixed kernels during convolution while transformers attentively explore pixel-wise interactions, which we validate our choice in later section. In specific, for the design of VTM, we extend the original 2D version of swin transformer <ref type="bibr" target="#b32">[33]</ref>. As illustrated in <ref type="figure">Fig</ref>  <ref type="figure" target="#fig_5">Figure 4</ref>. Illustration of the shifted 4D window Our VTM computes self-attention within the partitioned windows in high dimensional space, and it also provides a means to consider inter-window interactions by shifting the 4D window.</p><p>4, we first evenly partition query and support spatial dimensions of M p into non-overlapping sub-hypercorrelations M p ? R n?n?n?n?D . We compute self-attention within each partitioned sub-hypercorrelation. Subsequently, we shift the windows by displacement of ( n 2 , n 2 , n 2 , n 2 ) pixels from the previously partitioned windows, which we perform self-attention within the newly created windows. Then as done in original swin transformer <ref type="bibr" target="#b32">[33]</ref>, we simply roll it back to its original form without adopting any complex implementation. In computing self-attention, we use relative position bias and take the values from an expanded parameterized bias matrix, following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>. We leave other components in swin transformer blocks unchanged, e.g., Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref> and MLP layers.</p><p>In addition, to stabilize the learning, we enforce our networks to estimate the residual matching scores as complementary details. We add residual connection in order to expedite the learning process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b82">83]</ref>, accounting for fact that at the initial phase when the input M p is fed, erroneous matching scores are inferred due to randomly-initialized parameters of transformers, which could complicate the learning process as the networks need to learn the complete matching details from random matching scores.</p><p>To summarize, the overall process is defined as:</p><formula xml:id="formula_5">A p = VTM(M p ) = T (M p ) + M p ,<label>(3)</label></formula><p>where T denotes transformer module.</p><p>Pyramidal Processing. Analogous to <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b58">59]</ref>, we also utilize the coarse-to-fine approach through a pyramidal processing as illustrated in <ref type="figure" target="#fig_3">Fig. 2</ref>. Note that although it was claimed that utilizing the high-level features has a negative effect on performance <ref type="bibr" target="#b79">[80]</ref>, e.g., conv5 x, numerous recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b80">81]</ref> in both semantic matching and few-shot segmentation task demonstrated that leveraging multi-level features results in performance boost by large margin. Motivated by this, we also use pyramidal hypercorrelation.</p><p>For the coarse-to-fine approach, we let the finer level aggregated correlation map A p to be guided by the aggregated correlation map of previous (or deeper) levels A p+1 . Concretely, aggregated correlation map A p+1 is up-sampled, which is denoted as up(A p+1 ), and added to next level's correlation map A p to play as a guidance. This process is repeated until the finest level prior to decoder. This differs from <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b82">83]</ref>, which independently aggregates each correlation map and fuses later, that at each level, they have to learn the matching details from scratch while we are guided by the previous level scores, which dramatically boosts the performance. The pyramidal process is defined as:</p><formula xml:id="formula_6">A p = VTM(M p + up(A p+1 )),<label>(4)</label></formula><p>where up(?) denotes a bilinear upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Affinity-Aware Transformer Decoder</head><p>Given the hypercorrelation processed by pyramidal aggregation, we propose to additionally utilize the appearance embedding obtained from query feature maps to effectively decode the hypercorrelation to a query mask. From the perspective of correlation maps, this setting can help finding more accurate correspondence as the appearance affinity information helps to filter out the erroneous matching scores, as proven in stereo matching literature, e.g., Cost Volume Filtering (CVF) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b57">58]</ref>. Note that HSNet <ref type="bibr" target="#b37">[38]</ref> only decodes the matching costs with a series of 2D convolutions, thus often suffering from ambiguities in the matching costs.</p><p>For the design of our decoder, we first take the average over support dimensions of A p , which is then concatenated with appearance embedding from query feature maps and processed by swin transformer <ref type="bibr" target="#b32">[33]</ref> followed by bilinear interpolation. The process is defined as following:</p><formula xml:id="formula_7">A = Decoder([?, P(F q )]),<label>(5)</label></formula><p>where? ? R? q ??q?D extracted by average-pooling A p in its support spatial dimensions, P(?) is linear projection, P(F q ) ? R? q ??q?h , and [ ?, ? ] denotes concatenation. We sequentially refine the output when immediately after bilinear upsampling to maximize preserving fine details and integrating appearance information.? is bilinearly upsampled and undergoes Eq 5 until the projection head which outputs the predicted maskm q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Extension to K-Shot Setting</head><p>For K &gt; 1, given K pairs of support image and mask {(x i s , m i s )} K i=1 and a query image x q , our model forwardpasses K times to obtain K different query maskm k q . We sum up all the K predictions, and find the maximum number of predictions labelled as foreground across all the spatial locations. If the output divided by k is above threshold ? , we label it as foreground, otherwise background.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>For backbone feature extractor, we use ResNet50 and ResNet101 <ref type="bibr" target="#b13">[14]</ref> pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>, which are frozen during training, following <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b79">80]</ref>. We set the threshold ? to 0.5. We use data augmentation used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> for training. We use AdamW <ref type="bibr" target="#b34">[35]</ref> with learning rate set to 5e ? 4. We set D to 128. We use feature maps from conv3 x (p = 3), conv4 x (p = 4) and conv5 x (p = 5) for cost computation. We use last layers from conv2 x, conv3 x and conv4 x for appearance affinity when trained on FSS-1000 <ref type="bibr" target="#b26">[27]</ref> and conv4 x is excluded when trained on PASCAL-5 i <ref type="bibr" target="#b53">[54]</ref> and COCO-20 i <ref type="bibr" target="#b27">[28]</ref>. The aforementioned hyperparameters are set with cross-validation. More details can be found in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>In this section, we conduct comprehensive experiments for few-shot segmentation, by evaluating our approach through comparisons to recent state-of-the-art methods including PMM <ref type="bibr" target="#b75">[76]</ref>, RPMM <ref type="bibr" target="#b75">[76]</ref>, FWB <ref type="bibr" target="#b42">[43]</ref>, OSLSM <ref type="bibr" target="#b53">[54]</ref>, PANet <ref type="bibr" target="#b67">[68]</ref>, CANet <ref type="bibr" target="#b79">[80]</ref>, PFENet <ref type="bibr" target="#b61">[62]</ref> DAN <ref type="bibr" target="#b66">[67]</ref>, RePRI <ref type="bibr" target="#b1">[2]</ref>, SAGNN <ref type="bibr" target="#b73">[74]</ref>, FSOT <ref type="bibr" target="#b28">[29]</ref>, CyCTR <ref type="bibr" target="#b80">[81]</ref>, CWT <ref type="bibr" target="#b36">[37]</ref>, ASGNet <ref type="bibr" target="#b24">[25]</ref>, and HSNet <ref type="bibr" target="#b37">[38]</ref>. In Section 4.3, we provide a detailed analysis on our segmentation results evaluated on several benchmarks, and we conduct an extensive ablation study to justify our architectural choices and include an analysis of each component in Section 4.4.</p><p>Datasets. We evaluate our approach on three standard few-shot segmentation datasets, PASCAL-5 i <ref type="bibr" target="#b53">[54]</ref>, COCO-20 i <ref type="bibr" target="#b27">[28]</ref>, and FSS-1000 <ref type="bibr" target="#b26">[27]</ref>. PASCAL-5 i is created from images from PASCAL VOC 2012 <ref type="bibr" target="#b9">[10]</ref> and extra mask annotations <ref type="bibr" target="#b12">[13]</ref>, Originally, 20 object classes are available, but for cross-validation, as done in OSLM <ref type="bibr" target="#b53">[54]</ref>, they are evenly divided into 4 folds i ? {0, 1, 2, 3}, and this makes each fold contains 5 classes. COCO-20 i contains 80 object classes, and as done for PASCAL-5 i , the dataset is evenly divided into 4 folds, which results 20 classes for each fold. FSS-1000 is a more diverse dataset consisting of 1000 object classes. Following <ref type="bibr" target="#b26">[27]</ref>, we divide 1000 categories into 3 splits for training, validation and testing, which consist of 520, 240 and 240 classes, respectively. For PASCAL-5 i and COCO-20 i , we follow the common evaluation practice <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b61">62]</ref> and standard cross-validation protocol; for each i-th fold, the target fold i is used for evaluation and other folds are used for training.  <ref type="table">Table 4</ref>. Ablation study of VAT.</p><p>Different aggregators FSS-1000 <ref type="bibr" target="#b26">[27]</ref> mIoU (%)</p><p>Standard transformer <ref type="bibr" target="#b64">[65]</ref> OOM Center-pivot 4D convolutions <ref type="bibr" target="#b37">[38]</ref> 88.1 Linear transformer <ref type="bibr" target="#b20">[21]</ref> 87.7 Fastformer <ref type="bibr" target="#b70">[71]</ref> 87.8 Volumetric transformer (ours) 90.0 <ref type="table" target="#tab_9">Table 5</ref>. Ablation study of cost aggregators. OOM: Out of Memory.</p><p>Backbones feature FSS-1000 <ref type="bibr" target="#b26">[27]</ref> mIoU (%) 1-shot 5-shot ResNet50 <ref type="bibr" target="#b13">[14]</ref> 89.5 90.3 ResNet101 <ref type="bibr" target="#b13">[14]</ref> 90.0 90.6 PVT <ref type="bibr" target="#b69">[70]</ref> 89.5 89.9 Swin transformer <ref type="bibr" target="#b32">[33]</ref> 89.1 89.4 <ref type="table">Table 6</ref>. Ablation study of different feature backbone.</p><p>Evaluation Metric. Following common practice <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81]</ref>, we adopt mean intersection over union (mIoU) and foreground-background IoU (FB-IoU) as our evaluation metric. The mIoU averages over all IoU values for all object classes such that mIoU = 1 C C c=1 IoU c , where C is the number of classes in each fold, e.g., C = 20 for COCO-20 i . FB-IoU disregards the object classes and averages over foreground and background IoU, IoU F and IoU B , such that FBIoU = 1 2 (IoU F + IoU B ). As stated in <ref type="bibr" target="#b79">[80]</ref>, we mainly focus on mIoU since it better accounts for generalization power of a model than FB-IoU. <ref type="table" target="#tab_1">Table 1</ref> summarizes quantitative results on the PASCAL-5 i <ref type="bibr" target="#b53">[54]</ref>. We denote the type of backbone feature extractor and number of parameters for comparison. For PASCAL-5 i , we tested with two backbone networks, ResNet50 and ResNet101 <ref type="bibr" target="#b13">[14]</ref>. The proposed method outperforms other methods for almost all the folds with respect to both mIoU and FB-IoU. Consistent with this, VAT also attains stateof-the-art performance on COCO-20 i [28] for both 1-shot and 5-shot. Interestingly, for the most recently introduced dataset deliberately created for few-shot segmentation task, FSS-1000 <ref type="bibr" target="#b26">[27]</ref>, VAT outperforms HSNet <ref type="bibr" target="#b37">[38]</ref> and FSOT <ref type="bibr" target="#b28">[29]</ref> by a large margin, almost 4% increase in mIoU compared to HSNet with ResNet50. Overall, VAT sets a new state-of-the-art for all the benchmarks and this is confirmed at both <ref type="table" target="#tab_2">Table 2 and Table 3</ref>. The qualitative results are shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Segmentation Results</head><p>Another remarkable point is that, we find that even with larger number of learnable parameter than <ref type="bibr" target="#b37">[38]</ref>, our method still outperforms. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the models with larger number of parameters tend to perform worse. It is well known that the number of parameters has inverse relation to generalization power <ref type="bibr" target="#b60">[61]</ref>. However, VAT successfully avoids this by focusing on cost aggregation, showing that simply reducing the number of parameters may not be the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>In this section, we show an ablation analysis to justify the architectural choices we made, explore impact of feature backbone and investigate whether VAT is transferable to semantic correspondence task. Throughout this section, all</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Ours Ground Truth Support Set <ref type="figure">Figure 5</ref>. Qualitative results on FSS-1000 <ref type="bibr" target="#b26">[27]</ref>.</p><p>the experiments are conducted on FSS-1000 <ref type="bibr" target="#b26">[27]</ref> datasets with ResNet-101 as backbone feature extractor unless specified. Each ablation experiment is conducted under same experimental setting for a fair comparison. For semantic correspondence ablation study, we use percentage of correct keypoints (PCK) as our evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of each component in VAT.</head><p>We consider these as core components: volume embedding module, volumetric transformer module, residual connection and appearance affinity. We define the baseline for this ablation study as a model that replaces VEM with simple 4D maxpooling, skips VTM to show the importance of cost aggregation, excludes residual connection around cost aggregation module and disregards appearance affinity. We evaluate this by adding components progressively.</p><p>As summarized in <ref type="table">Table 4</ref>, each component helps to boost the performance. Especially, from (I) to (III) and (IV) to (V), we observe significant improvement. We find that leveraging appearance affinity helps to find accurate correspondences, which in turn yields better segmentation performance, and VEM followed by VTM not only eases the computational burden but also boosts the performance.</p><p>Is VTM better than other aggregators? As summarized in <ref type="table" target="#tab_9">Table 5</ref>, we provide an ablation study to justify the use of our pyramidal transformers for cost aggregation.  <ref type="table">Table 7</ref>.</p><p>Quantitative results on SPair-71k <ref type="bibr" target="#b40">[41]</ref>, PF-PASCAL <ref type="bibr" target="#b11">[12]</ref> and PF-WILLOW <ref type="bibr" target="#b10">[11]</ref>. end, it is perhaps necessary to compare with other methods including 4D convolutions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52]</ref> and efficient transformers <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b70">71]</ref>. As 4D convolution as in <ref type="bibr" target="#b37">[38]</ref> also aggregates the matching scores in a local manner similar to swin transformer <ref type="bibr" target="#b32">[33]</ref>, we conjecture that it should perform similarly or even better. We investigate this and report the results to confirm that VTM exceeds other aggregators.</p><p>For a fair comparison, we only replace VTM with other aggregators and leave all the other components in our architecture unchanged. We observe that our proposed method outperforms other aggregators by a large margin. Note that the use of standard transformers for cost aggregation is inapplicable due to high dimensional space of hypercorrelation. Interestingly, although center-pivot 4D convolutions <ref type="bibr" target="#b37">[38]</ref> also focus on locality as swin transformer <ref type="bibr" target="#b32">[33]</ref>, the performance gap indicates that the ability to attentively consider pixel-wise interactions during the self-attention computation is critical. Another interesting point is that linear transformer <ref type="bibr" target="#b20">[21]</ref> and fastformer <ref type="bibr" target="#b70">[71]</ref> that exploit from global receptive fields of transformers achieve similar performance. This confirms that for the cost aggregation, our approach may be more suitable.</p><p>Does different feature extractor matter? Conventional few-shot segmentation methods only utilized CNN-based feature backbones <ref type="bibr" target="#b13">[14]</ref> for extracting features. <ref type="bibr" target="#b79">[80]</ref> observed that high-level features contains semantics of objects which could lead to overfitting and not suitable to use for the task of few-shot segmentation. Then the question naturally arises. What about transformer-based backbone networks? As addressed in many works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b46">47]</ref>, CNN and transformers see images differently, which means that the kinds of backbone networks may affect the performance significantly, but this has never been explored in this task. We thus exploit several well-known vision transformer architectures to explore the potential differences that probably exist.</p><p>The results are summarized in <ref type="table">Table 6</ref>. We were surprised to find that both convolution-and transformer-based backbone networks attain similar performance. We conjecture that although it has been widely studied that convolutions and transformers see differently <ref type="bibr" target="#b46">[47]</ref>, as they are pretrained on the same dataset <ref type="bibr" target="#b6">[7]</ref>, the representations learned by models are almost alike. Note that we only utilized backbones with pyramidal structure, and the results may differ Support Query (a) CATs <ref type="bibr" target="#b5">[6]</ref> Support Query (b) Ours <ref type="figure">Figure 6</ref>. Qualitative results on PF-PASCAL <ref type="bibr" target="#b11">[12]</ref>.</p><p>if other backbone networks are used, which we leave this exploration for the future work.</p><p>Can VAT also perform well in semantic correspondence task? To tackle the few-shot segmentation task, we reformulated it as finding semantic correspondences under conditions of several challenges posed by large intra-class variations and geometric deformations. This means that the proposed method should be able to perform well in semantic correspondence task at least to some extent. Here, we compare our method with other state-of-the-art methods in semantic correspondence task and investigate the key aspect:</p><p>To what extent and how well can VAT perform in semantic correspondence task given similar formulation?</p><p>For this ablation study, we made minor modifications to our model: i.e., output spatial resolution and embedded features. We refer the readers to either Appendix or github page: https://seokju-cho.github.io/VAT/ for details. Note that this is possible as our projection head also outputs the same dimension as if a flow field is inferred. Following common protocol <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, we use standard benchmarks for this task, which we trained our model on training split of PF-PASCAL <ref type="bibr" target="#b11">[12]</ref> when evaluated on test split of PF-PASCAL <ref type="bibr" target="#b11">[12]</ref> and PF-WILLOW <ref type="bibr" target="#b10">[11]</ref>, and trained on SPair-71k <ref type="bibr" target="#b40">[41]</ref> when evaluated on SPair-71k <ref type="bibr" target="#b40">[41]</ref>. We fine-tuned the feature backbone for fair comparison. As shown in <ref type="table">Table 7</ref> and <ref type="figure">Fig. 6</ref>, although not specifically designed for semantic correspondence task, VAT either sets a new state-of-the-art <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref> or attain the second highest PCK <ref type="bibr" target="#b11">[12]</ref> for this task. This results show that cost aggregation is a prime importance in both few-shot segmentation and semantic correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed, for the first time, to aggregate the high dimensional correlation maps with both convolutions and transformers for few-shot segmentation. For our pyramidal encoder, to handle the high computation caused by direct aggregation of correlation maps, we introduced volume embedding module, and for the cost aggregation, we proposed volumetric transformer module. Then the output undergoes the proposed affinity-aware decoder for prediction. We have shown that although the proposed method is not specifically designed for semantic correspondence task, it attains state-of-the-art performance for all the standard benchmarks for both few-shot segmentation and semantic correspondence. Also, we have conducted extensive ablation study.  <ref type="table" target="#tab_2">Table 2</ref>. Quantitative evaluation on standard benchmarks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different aggregators Memory Run-time (GB) (ms)</head><p>Standard transformer <ref type="bibr" target="#b64">[65]</ref> OOM N/A Center-pivot 4D convolutions <ref type="bibr" target="#b37">[38]</ref> 3.5 52.7 Linear transformer <ref type="bibr" target="#b20">[21]</ref> 3.5 56.8 Fastformer <ref type="bibr" target="#b70">[71]</ref> 3.5 122.9 Volumetric transformer (ours) 3.8 57.3 more efficient method while enabling to process at higher spatial resolutions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. More Results</head><p>For few-shot segmentation, VAT clearly sets new stateof-the-art for all the benchmarks, demonstrating its effec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Ours Ground Truth Support Set Query Ours Ground Truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Ours Ground Truth Support Set Query Ours Ground Truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Set</head><p>Query Ours Ground Truth Support Set Query Ours Ground Truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support</head><p>Query Support Query Support Query Support Query <ref type="figure">Figure 5</ref>. Qualitative results on PF-PASCAL <ref type="bibr" target="#b11">[12]</ref> (left) and PF-WILLOW <ref type="bibr" target="#b10">[11]</ref> (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Query Support</head><p>Query Support Query <ref type="figure">Figure 6</ref>. Qualitative results on SPair-71k <ref type="bibr" target="#b40">[41]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Overview of volumetric embedding and transformer modules. Our VEM aims to ease the subsequent processing of correlation maps by projecting the correlation maps to reduce the spatial dimensions. The output of VEM then undergoes VTM for cost aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Qualitative results on PASCAL-5 i [54].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on COCO-20 i<ref type="bibr" target="#b27">[28]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on FSS-1000<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.</figDesc><table><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Query</cell><cell>Support</cell><cell>Query</cell><cell>Support</cell></row><row><cell>dim.</cell><cell>dim.</cell><cell>dim.</cell><cell>dim.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4D local</cell></row><row><cell></cell><cell></cell><cell></cell><cell>window</cell></row><row><cell></cell><cell></cell><cell></cell><cell>A token</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">Original 4D Window</cell><cell cols="2">Shifted 4D Window</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance on PASCAL-5 i [54] in mIoU and FB-IoU. Numbers in bold indicate the best performance and underlined ones are the second best.</figDesc><table><row><cell cols="2">Backbone feature</cell><cell cols="2">Methods</cell><cell></cell><cell>5 0</cell><cell>5 1</cell><cell>5 2</cell><cell>1-shot 5 3</cell><cell cols="4">mIoU FB-IoU</cell><cell>5 0</cell><cell>5 1</cell><cell>5 2</cell><cell>5-shot 5 3</cell><cell>mIoU FB-IoU</cell><cell># learnable params</cell></row><row><cell></cell><cell></cell><cell cols="3">PANet [68]</cell><cell cols="6">44.0 57.5 50.8 44.0 49.1</cell><cell></cell><cell>-</cell><cell>55.3 67.2 61.3 53.2 59.3</cell><cell>-</cell><cell>23.5M</cell></row><row><cell></cell><cell></cell><cell cols="3">PFENet [62]</cell><cell cols="6">61.7 69.5 55.4 56.3 60.8</cell><cell cols="2">73.3</cell><cell>63.1 70.7 55.8 57.9 61.9</cell><cell>73.9</cell><cell>10.8M</cell></row><row><cell></cell><cell></cell><cell cols="9">ASGNet [25] 58.8 67.9 56.8 53.7 59.3</cell><cell cols="2">69.2</cell><cell>63.4 70.6 64.2 57.4 63.9</cell><cell>74.2</cell><cell>10.4M</cell></row><row><cell cols="2">ResNet50 [14]</cell><cell cols="2">CWT [37] RePRI [2]</cell><cell></cell><cell cols="6">56.3 62.0 59.9 47.2 56.4 59.8 68.3 62.1 48.5 59.7</cell><cell></cell><cell>--</cell><cell>61.3 68.5 68.5 56.6 63.7 64.6 71.4 71.1 59.3 66.6</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell cols="3">HSNet [38]</cell><cell cols="6">64.3 70.7 60.3 60.5 64.0</cell><cell cols="2">76.7</cell><cell>70.3 73.2 67.4 67.1 69.5</cell><cell>80.6</cell><cell>2.6M</cell></row><row><cell></cell><cell></cell><cell cols="3">CyCTR [81]</cell><cell cols="6">67.8 72.8 58.0 58.0 64.2</cell><cell></cell><cell>-</cell><cell>71.1 73.2 60.5 57.5 65.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">VAT (ours)</cell><cell cols="6">67.6 71.2 62.3 60.1 65.3</cell><cell cols="2">77.4</cell><cell>72.4 73.6 68.6 65.7 70.0</cell><cell>80.9</cell><cell>3.2M</cell></row><row><cell></cell><cell></cell><cell cols="2">FWB [43]</cell><cell></cell><cell cols="6">51.3 64.5 56.7 52.2 56.2</cell><cell></cell><cell>-</cell><cell>54.8 67.4 62.2 55.3 59.9</cell><cell>-</cell><cell>43.0M</cell></row><row><cell></cell><cell></cell><cell cols="2">DAN [67]</cell><cell></cell><cell cols="6">54.7 68.6 57.8 51.6 58.2</cell><cell cols="2">71.9</cell><cell>57.9 69.0 60.1 54.9 60.5</cell><cell>72.3</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">PFENet [62]</cell><cell cols="6">60.5 69.4 54.4 55.9 60.1</cell><cell cols="2">72.9</cell><cell>62.8 70.4 54.9 57.6 61.4</cell><cell>73.5</cell><cell>10.8M</cell></row><row><cell></cell><cell></cell><cell cols="9">ASGNet [25] 59.8 67.4 55.6 54.4 59.3</cell><cell cols="2">71.7</cell><cell>64.6 71.3 64.2 57.3 64.4</cell><cell>75.2</cell><cell>10.4M</cell></row><row><cell cols="2">ResNet101 [14]</cell><cell cols="2">CWT [37]</cell><cell></cell><cell cols="6">56.9 65.2 61.2 48.8 58.0</cell><cell></cell><cell>-</cell><cell>62.6 70.2 68.8 57.2 64.7</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">RePRI [2]</cell><cell></cell><cell cols="6">59.6 68.6 62.2 47.2 59.4</cell><cell></cell><cell>-</cell><cell>66.2 71.4 67.0 57.7 65.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">HSNet [38]</cell><cell cols="6">67.3 72.3 62.0 63.1 66.2</cell><cell cols="2">77.6</cell><cell>71.8 74.4 67.0 68.3 70.4</cell><cell>80.6</cell><cell>2.6M</cell></row><row><cell></cell><cell></cell><cell cols="3">CyCTR [81]</cell><cell cols="6">69.3 72.7 56.5 58.6 64.3</cell><cell cols="2">72.9</cell><cell>73.5 74.0 58.6 60.2 66.6</cell><cell>75.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">VAT (ours)</cell><cell cols="6">68.4 72.5 64.8 64.2 67.5</cell><cell cols="2">78.8</cell><cell>73.3 75.2 68.4 69.5 71.6</cell><cell>82.0</cell><cell>3.3M</cell></row><row><cell>Backbone feature</cell><cell>Methods</cell><cell></cell><cell>20 0</cell><cell>20 1</cell><cell>20 2</cell><cell cols="4">1-shot 20 3 mean FB-IoU 20 0</cell><cell>20 1</cell><cell>20 2</cell><cell cols="2">5-shot 20 3 mean FB-IoU</cell></row><row><cell></cell><cell cols="2">PMM [76]</cell><cell cols="4">29.3 34.8 27.1 27.3 29.6</cell><cell></cell><cell>-</cell><cell cols="5">33.0 40.6 30.3 33.3 34.3</cell><cell>-</cell></row><row><cell></cell><cell cols="2">RPMM [76]</cell><cell cols="4">29.5 36.8 28.9 27.0 30.6</cell><cell></cell><cell>-</cell><cell cols="5">33.8 42.0 33.0 33.3 35.5</cell><cell>-</cell></row><row><cell></cell><cell cols="6">PFENet [62] 36.5 38.6 34.5 33.8 35.8</cell><cell></cell><cell>-</cell><cell cols="5">36.5 43.3 37.8 38.4 39.0</cell><cell>-</cell></row><row><cell>ResNet50 [14]</cell><cell cols="2">ASGNet [25] RePRI [2]</cell><cell cols="4">-32.0 38.7 32.7 33.1 34.1 ---34.6</cell><cell></cell><cell>60.4 -</cell><cell cols="5">-39.3 45.4 39.7 41.8 41.6 ---42.5</cell><cell>67.0 -</cell></row><row><cell></cell><cell cols="2">HSNet [38]</cell><cell cols="4">36.3 43.1 38.7 38.7 39.2</cell><cell></cell><cell>68.2</cell><cell cols="5">43.3 51.3 48.2 45.0 46.9</cell><cell>70.7</cell></row><row><cell></cell><cell cols="2">CyCTR [81]</cell><cell cols="4">38.9 43.0 39.6 39.8 40.3</cell><cell></cell><cell>-</cell><cell cols="5">41.1 48.9 45.2 47.0 45.6</cell><cell>-</cell></row><row><cell></cell><cell cols="2">VAT (ours)</cell><cell cols="4">39.0 43.8 42.6 39.7 41.3</cell><cell></cell><cell>68.8</cell><cell cols="5">44.1 51.1 50.2 46.1 47.9</cell><cell>72.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance on COCO-20 i<ref type="bibr" target="#b27">[28]</ref> in mIoU and FB-IoU.</figDesc><table><row><cell>Backbone feature</cell><cell>Methods</cell><cell cols="2">mIoU 1-shot 5-shot</cell></row><row><cell></cell><cell>FSOT [29]</cell><cell>82.5</cell><cell>83.8</cell></row><row><cell>ResNet50 [14]</cell><cell>HSNet [38]</cell><cell>85.5</cell><cell>87.8</cell></row><row><cell></cell><cell>VAT (ours)</cell><cell>89.5</cell><cell>90.3</cell></row><row><cell></cell><cell>DAN [67]</cell><cell>85.2</cell><cell>88.1</cell></row><row><cell>ResNet101 [14]</cell><cell>HSNet [38]</cell><cell>86.5</cell><cell>88.5</cell></row><row><cell></cell><cell>VAT (ours)</cell><cell>90.0</cell><cell>90.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Mean IoU comparison on FSS-1000<ref type="bibr" target="#b26">[27]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Per-class quantitative evaluation on SPair-71k<ref type="bibr" target="#b40">[41]</ref> benchmark.</figDesc><table><row><cell></cell><cell cols="4">PF-PASCAL [12] PF-WILLOW [11]</cell></row><row><cell>Methods</cell><cell cols="2">PCK @ ? img</cell><cell cols="2">PCK @ ? bbox</cell></row><row><cell></cell><cell>0.1</cell><cell>0.15</cell><cell>0.1</cell><cell>0.15</cell></row><row><cell>CNNGeo [49]</cell><cell>69.5</cell><cell>80.4</cell><cell>69.2</cell><cell>77.8</cell></row><row><cell cols="2">WeakAlign [50] 74.8</cell><cell>84.0</cell><cell>70.2</cell><cell>79.9</cell></row><row><cell>SFNet [24]</cell><cell>81.9</cell><cell>90.6</cell><cell>74.0</cell><cell>84.2</cell></row><row><cell>NC-Net [52]</cell><cell>78.9</cell><cell>86.0</cell><cell>67.0</cell><cell>83.7</cell></row><row><cell>DCC-Net [19]</cell><cell>82.3</cell><cell>90.5</cell><cell>73.8</cell><cell>86.5</cell></row><row><cell>HPF [40]</cell><cell>84.8</cell><cell>92.7</cell><cell>74.4</cell><cell>85.6</cell></row><row><cell>GSF [20]</cell><cell>87.8</cell><cell>95.9</cell><cell>78.7</cell><cell>90.2</cell></row><row><cell>DHPF [42]</cell><cell>90.7</cell><cell>95.0</cell><cell>77.6</cell><cell>89.1</cell></row><row><cell>SCOT [31]</cell><cell>85.4</cell><cell>92.7</cell><cell>76.0</cell><cell>87.1</cell></row><row><cell>CHM [39]</cell><cell>91.6</cell><cell>94.9</cell><cell>79.4</cell><cell>87.5</cell></row><row><cell>MMNet [83]</cell><cell>91.6</cell><cell>95.9</cell><cell>-</cell><cell>-</cell></row><row><cell>PMNC [23]</cell><cell>90.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CATs [6]</cell><cell>92.6</cell><cell>96.4</cell><cell>79.2</cell><cell>90.3</cell></row><row><cell>VAT (ours)</cell><cell>92.3</cell><cell>96.1</cell><cell>81.0</cell><cell>91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Memory and Run-time comparison between different aggregators. The memory and run-time are measured by simply replacing the aggregator module within VAT. Memory and Run-time comparison to HSNet [38].</figDesc><table><row><cell></cell><cell cols="2">Memory Run-time</cell></row><row><cell></cell><cell>(GB)</cell><cell>(ms)</cell></row><row><cell>HSNet [38]</cell><cell>2.2</cell><cell>51.7</cell></row><row><cell>VAT (ours)</cell><cell>3.8</cell><cell>57.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Augmentation type</figDesc><table><row><cell>Support Set</cell><cell>Query</cell><cell>Ours</cell><cell>Ground Truth</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this document, we provide detailed implementation details, more quantitative results on semantic correspondence benchmarks, including SPair-71k <ref type="bibr" target="#b40">[41]</ref>, PF-PASCAL <ref type="bibr" target="#b11">[12]</ref>, and PF-WILLOW <ref type="bibr" target="#b10">[11]</ref>, and more qualitative results on all the benchmarks we used.</p><p>Appendix A. Implementation details Implementation Details. We first extract backbone features using ResNet50 <ref type="bibr" target="#b13">[14]</ref> and ResNet101 <ref type="bibr" target="#b13">[14]</ref> from conv3 x, conv4 x and conv5 x. This feature extracting scheme results in 3 pyramidal layers. To compose VEM, we stack a series of 3 ? 3 ? 3 ? 3 convolutions followed by ReLU and Group Normalization. The coarsest level correaltion map has a size of 8 ? 8 ? 8 ? 8 after undergoing VEM. Finer levels, p = 4 and p = 3, have volume embeddings of size 16 ? 16 ? 8 ? 8 and 32 ? 32 ? 8 ? 8, respectively. We set the number of heads in VTM as 4. At p = 5 level, we set the depth of VTM as 4 and rest are set to 2. Window size M is set to 4 at all levels of VTM. We project the feature maps of query at last layer of level p = 4, p = 3 and p = 2 to have channels 64, 32 and 16 from 1024, 512 and 256, respectively. We implemented our work with PyTorch <ref type="bibr" target="#b44">[45]</ref>. We used learning rate of 5e-4. We used augmentation introduced in <ref type="bibr" target="#b2">[3]</ref> for training as shown in <ref type="table">Table 5</ref>. We adopted early-stopping for all the training, which we picked the model with best validation score. We set the batch size as 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components of VAT.</head><p>For the ablation study of components of VAT, we defined the baseline model which substitutes VEM with 4D max-pooling. As VTM is a transformer module that takes an input and outputs the same shape to input, it can simply be skipped. Residual connections are simply removed to test its efficiency, and appearance affinity is excluded at the decoder. This experiment is conducted in a sequential manner, where each component is added progressively.</p><p>Different Cost Aggregators. For the ablation study of cost aggregators, we used codes released by authors of works, including standard transformer <ref type="bibr" target="#b64">[65]</ref>, center-pivot 4D convolutions <ref type="bibr" target="#b37">[38]</ref>, linear transformer <ref type="bibr" target="#b20">[21]</ref>, and fastformer <ref type="bibr" target="#b70">[71]</ref>. We simply replaced VTM with those in order to make a fair comparison.</p><p>Different backbone networks. Ablation study on different feature backbone follows similar procedure, as PVT <ref type="bibr" target="#b69">[70]</ref> and swin transformer <ref type="bibr" target="#b32">[33]</ref> have pyramidal architecture that outputs features of size and dimensions similar to ResNet <ref type="bibr" target="#b13">[14]</ref>.</p><p>VAT in Semantic Correspondence. For the experiments to validate effectiveness of VAT on semantic correspondence, we used the three most widely used datasets in this task, which include SPair-71k <ref type="bibr" target="#b40">[41]</ref>, PF-PASCAL <ref type="bibr" target="#b11">[12]</ref> and PF-WILLOW <ref type="bibr" target="#b10">[11]</ref>. We made minor modifications to our model: we excluded bilinear upsampling in the affinityaware decoder, which the original output shape is 128?128 but with the modification, now the output shape is 32 ? 32 as two x2 bilinear upsampling is excluded. We used embedded features from conv4 and conv5 x, which is different from the training setting for the experiments on few-shot segmentation datasets. Note that we do not utilize segmentation mask for this task. We only use keypoints provided in the datasets for training similarly to <ref type="bibr" target="#b5">[6]</ref>. We used the same data augmentation scheme <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> as <ref type="table">Table 5</ref>. We used learning rate of 3e-6 for the feature backbone and 3e-4 for VAT. We used batch size of 8. For the evaluation, we followed the standard protocol <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Failure Cases</head><p>Overall, we observe that our method may lack an ability to preserve fine-details and address multi-correspondence as shown in <ref type="figure">Figure 1</ref>. Given an object in the support image and multiple corresponding objects in query image, VAT sometimes seem to fail finding correspondence. From these, perhaps, it could be argued that the proposed method is better at encoding matching information and effectively aggregating them than others, but it might lack an ability to better find multi-objects or preserve the fine-details. Better means to consider both factors either by introducing some confidence module <ref type="bibr" target="#b63">[64]</ref> or preserve fine-details when increasing the resolutions of the predicted mask during the process in decoder can be a promising direction future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Memory and Run-time</head><p>We additionally provide memory and run-time comparison to other aggregators and HSNet <ref type="bibr" target="#b37">[38]</ref>. The results are obtained using a single NVIDIA GeForce RTX 3090 GPU and Intel Core i7-10700 CPU. As shown in <ref type="table">Table 3</ref>, we observe that VAT is relatively slower and comsumes more memory than other aggregators. About 0.3 GB of more memory consumption and 5 ms slower run-time occur with the proposed VAT in return for the performance. From the perspective of aggregators, 0.3 GB and 5 ms gap seem quite trivial to us.</p><p>We also provide direct comparison to the current stateof-the-art method, HSNet <ref type="bibr" target="#b37">[38]</ref>, in <ref type="table">Table 4</ref>. We observe that memory consumption gap is quite large, which is an apparent limitation to the proposed method. Run-time gap is quite trivial, enabling real-time inference. Note that standard transformer can not be used due to OOM. In light of this, although we managed to propose a method to aggregate a raw correlation map without arbitrarily changing its resolutions prior to feeding into networks, we observe relatively large memory consumption gap to HSNet <ref type="bibr" target="#b37">[38]</ref>, which suggests a promising direction for future work, i.e., even</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Few-shot segmentation without meta-learning: A good transductive inference is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoel</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Ziko Imtiaz Masud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Albumentations: fast and flexible image augmentations. Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr A</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalinin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cats: Cost aggregation transformers for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep matching prior: Test-time optimization for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmaa</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Guided semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Patchmatch-based neighborhood consensus for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Yong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Degol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fragoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fss-1000: A 1000-class dataset for fewshot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Yau Pun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Few-shot segmentation with optimal transport matching and message flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Yi</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08518</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Crnet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11945,2021.4</idno>
		<title level="m">Soft: Softmax-free transformer with linear complexity</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simpler is better: Few-shot semantic segmentation with classifier weight transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01538</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Convolutional hough matching networks for robust and efficient visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05221</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Spair-71k: A large-scale benchmark for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10543</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XV 16</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08810</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-toend weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10510</idno>
		<title level="m">Neighbourhood consensus networks</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03410</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Adaptive masked proxies for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11123</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Boosting few-shot semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02266</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Neural network studies, 1. comparison of overfitting and overtraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Luik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="826" to="833" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Glunet: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6258" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning accurate dense correspondences and when to trust them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Fastformer: Additive attention can be all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09084</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning meta-class memory for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scaleaware graph neural network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Guo-Sen Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5475" to="5484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with cyclic memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Guo-Sen Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Mining latent classes for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15402</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Self-guided and cross-guided learning for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiushuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Few-shot segmentation via cycle-consistent transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02320</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Prototypical matching and open set rejection for zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multi-scale matching networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Methods aero. bike bird boat bott. bus car cat chai. cow dog hors. mbik. pers. plan. shee. trai</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Also as VAT outperforms other SOTA methods by large margin for semantic correspondence, we argue that the cost aggregation is indeed a prime importance, even for few-shot segmentation as well. We argue that this performance boost in few-shot segmentation can be attributed to VAT&apos;s ability to find accurate semantic correspondences when tackling the segmentation task</title>
		<imprint/>
	</monogr>
	<note>In light of this, we are suggesting a paradigm shift for the few-shot segmentation task</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">As shown in Table 1 and Table 2, we provide more quantitative results on SPair-71k [41], PF-PASCAL [12], and PF-WILLOW [11] in comparison to other semantic correspondence methods</title>
		<imprint/>
	</monogr>
	<note>Quantitative Results on Semantic Correspondence. including CNNGeo [49], WeakAlign [50], NC-Net [52], HPF [40], SFNet [24], DCC-Net [19], GSF [20], SCOT [31], DHPF [42], CHM [39], MM-Net [83], PMNC [23] and CATs [6</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">As shown in Figure 2, Figure 3, Figure 4, Figure 5 and Figure 6, we provide qualitative results on all the benchmarks</title>
		<idno>PF- WILLOW [11] and SPair-71k [41</idno>
		<imprint/>
	</monogr>
	<note>Qualitative Results. which includes PASCAL-5 i [54], COCO-20 i [28</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
