<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransBoost: Improving the Best ImageNet Performance using Deep Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Belhasin</surname></persName>
							<email>omer.be@cs.technion.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Bar-Shalom</surname></persName>
							<email>guy.b@cs.technion.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science Technion -Israel Institute of Technology, Deci</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransBoost: Improving the Best ImageNet Performance using Deep Transduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper deals with deep transductive learning, and proposes TransBoost as a procedure for fine-tuning any deep neural model to improve its performance on any (unlabeled) test set provided at training time. TransBoost is inspired by a large margin principle and is efficient and simple to use. Our method significantly improves the ImageNet classification performance on a wide range of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and ConvNext-T, leading to state-of-the-art transductive performance. Additionally we show that TransBoost is effective on a wide variety of image classification datasets. The implementation of TransBoost is provided at: https://github.com/omerb01/TransBoost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Alternative learning frameworks that make use of unlabeled data have gained considerable attention in recent years. These include semi-supervised learning and transductive learning, both of which were extensively studied in classical and statistical machine learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> before the advent of deep learning. In this article we focus on deep transductive classification. In this setting, both a labeled training sample and an (unlabeled) test sample are provided at training time. The goal is to predict only the labels of the given test instances as accurately as possible. In contrast, in standard inductive classification, the goal is to train a general model capable of predicting the labels of unseen test instances.</p><p>In semi-supervised learning (SSL) we are also given a set of unlabeled examples, which is used for selecting a model. Despite a commonly held misconception, however, the goal of SSL remains inductive rather than transductive, and the trained model must predict labels for new instances (i.e., the unlabeled instances are used for training only).</p><p>Transductive models deliver higher accuracy gains compared with traditional inductive models, as we demonstrate in this paper in the context of image classification. Much of the power of transductive learning is achieved through analyzing the given test set as a group (see <ref type="bibr">Section 5.4)</ref>. Transductive prediction is thus especially useful whenever we can accumulate a test set of instances and then train a specialized model to predict their labels. While there are many possible relevant use cases, one that stands out is medical diagnosis. Here, daily or weekly medical records can be accumulated and sent to the transductive predictor as a set. In Appendix A, we highlight a number of additional use cases.</p><p>Perhaps the best-known transductive classification method in classical machine learning is the transductive support vector machine (TSVM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. While a support vector machine (SVM) <ref type="bibr" target="#b5">[6]</ref> seeks to achieve a general decision function, a TSVM attempts to reduce misclassifications of just the target test instances. To the best of our knowledge, in deep learning, transductive settings have only been addressed in the case of few-shot learning (t-FSL).</p><p>In this paper we address deep transductive learning and introduce TransBoost, a novel procedure that takes a pretrained neural classification model, along with a (labeled) training set, and an (unlabeled) test set. The procedure fine-tunes the model to improve its performance on this particular test set. The TransBoost optimization procedure is inspired by TSVM and its objective attempts to approximate a large margin principle. TransBoost is implemented through a simple transductive loss component, which is combined with the standard cross-entropy loss.</p><p>We examine the effectiveness of TransBoost by using a variety of pretrained neural networks, datasets, and baseline models. Our extensive study examines a spectrum of ResNet architectures and a variety of modern architectures such as ConvNext <ref type="bibr" target="#b6">[7]</ref>, MobileNetv3 <ref type="bibr" target="#b7">[8]</ref> and ViT <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> highlights some of TransBoost's results on the ImageNet dataset, which demonstrate impressive and consistent accuracy gains, leading to state-of-the-art performance compared to standard (inductive) classification as well as versus SSL and t-FSL methods (see full results in Section 5.2).</p><p>To summarize, the contributions of this paper are: (1) A novel deep transductive loss function, inspired by a large margin principle, which optimizes performance on a specific test set. (2) A simple and efficient transductive procedure for fine-tuning a pretrained neural model in order to boost its performance over an inductive setting. (3) A comprehensive empirical study of the proposed fine-tuning procedure showing great benefits across a large number of architectures, datasets and baseline techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>As transductive learning has not been extensively researched in the deep learning community, we begin by defining the transductive learning problem within the context of deep learning classification. We follow Vapnik's standard formulation from statistical learning theory <ref type="bibr" target="#b3">[4]</ref>. Let P (X, Y ) be a probability distribution over X ? Y, where X represents an input space (e.g., raw image data), and Y represents a label set corresponding to C classes. The learner is provided with a set of labeled instances, S l {(x 1 , y 1 ), . . . , (x L , y L )}, where x i ? X , y i ? Y, and a finite set of unlabeled instances, X u {x L+1 , . . . , x L+U }, where x i ? X . The objective is to label the unlabeled instances based on this data. Given a deep neural network f ? : X ? Y, where ? denotes its parameters, and a loss function : Y ? Y ? R, we follow <ref type="bibr" target="#b0">[1]</ref> (Setting 2) and define the (true) risk over the unlabeled set,</p><formula xml:id="formula_0">E f ? (X U ) 1 U U i=1 (f ? (x L+i ), y L+i ).<label>(1)</label></formula><p>Thus, we are given a labeled training set S l (X l , Y l ) selected i.i.d. according to P (X, Y ), as well as a pretrained classification model f ? , and we presume it was trained with the labeled set S l . An independent test set S u (X u , Y u ), of U samples is randomly and independently selected in the same manner, and we are required to revise the given model based on both S l and X u , so as to minimize Equation (1) without knowing the labels Y u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Transductive learning (or transduction) was first formulated by Vapnik <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> who also introduced the first transductive algorithm -TSVM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, which learns a large margin hyperplane classifier from labeled training data while simultaneously forcing it to take into account the (unlabeled) test data. Whereas transduction received considerable attention in the context of classical machine learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, it has only been briefly addressed in the context of deep neural networks.</p><p>Recall that in (deep) transductive classification we are provided with both a labeled training sample and an unlabeled test sample. A related but different setting is semi-supervised learning (SSL), where in addition to the labeled training set, we are also given a (typically large) unlabeled training sample. The objective is inductive -namely, to guess the label of any unseen test sample (that was drawn from the same distribution). A simple observation is that any SSL algorithm can be applied in transduction by using the given (unlabeled) test sample as the unlabeled SSL training sample. Thus, algorithms such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> can be used to solve transductive learning. This fact may explain a common confusion between SSL and transduction. Nevertheless, an inductive SSL algorithm does not need to use (and cannot rely on) the fact that it will only be asked about the given test points. In contrast, a (meaningful) transductive algorithm must rely on this knowledge, as we contend in this paper. In fact, in Section 5.2 we demonstrate that our proposed transductive procedure substantially outperforms the SimCLR <ref type="bibr" target="#b10">[11]</ref> and SimCLRv2 <ref type="bibr" target="#b11">[12]</ref> algorithms, where SimCLRv2 currently achieves state-of-the-art SSL performance according to <ref type="bibr" target="#b13">[14]</ref>.</p><p>Recently, there has been a flurry of research concerning transductive few-shot learning (t-FSL) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. In t-FSL we are given a classifier for a certain classification task, such as cats versus dogs, as well as a training set (called the support set) for unseen classes (referred to as ways), such as elephants, tigers and lizards, containing only few labeled samples per unseen class. A test set (called the query set), containing instances from the unseen classes, is given and the goal is to learn the additional classes so as to correctly classify these test instances. Clearly, this setting is transductive; however, the goal here is to learn from a few labeled examples (e.g., one-shot, five-shot). Formally, a t-FSL algorithm can be applied to transductive classification (by treating the training set as a support set). For the most part, however, methods for t-FSL cannot be effectively applied to large (non few-shot) training sets (e.g., iLPC <ref type="bibr" target="#b16">[17]</ref>). Nevertheless, in order to conduct a complete study in transductive classification, we chose two t-FSL representative algorithms as baselines for our experiments (see <ref type="bibr">Section 5)</ref>. Specifically, we consider the entropy minimization (Ent-min) method of <ref type="bibr" target="#b14">[15]</ref>, and the TIM-GD method of <ref type="bibr" target="#b15">[16]</ref>. With Ent-min, the Shannon entropy is reduced by fine-tuning, whereas TIM-GD optimizes the mutual information of test instances using only the classifier weights with fine-tuning. Ent-min is a common t-FSL baseline while TIM-GD is a top performing 5-shot t-FSL algorithm according to the few-shot leaderboard <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TransBoost: Fine-Tuning via Transductive Learning</head><p>We now present TransBoost, a procedure that takes a pretrained neural model along with its (labeled) training set as well as an (unlabeled) test set. The procedure then fine-tunes the model to improve its performance for this specific test set. The TransBoost optimization objective, L, is driven by Equation (3) and approximated by Algorithm 1, which includes a novel transductive loss component in addition to the standard cross-entropy loss. Our transductive component, L TransBoost , is introduced in Equation (3) as a regularization term and is defined in Equation <ref type="bibr" target="#b1">(2)</ref>. L TransBoost is inspired by TSVMs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and follows a large margin principle.</p><p>Let us elaborate on our loss function, L TransBoost (2), which is applied on the set of test samples, X u . This loss function includes an unsupervised non-negative symmetric pairwise similarity function, S : X ? X ? R, which measures the similarity of two input instances. In our implementation, we simply used an L 2 -based score function applied on prediction (softmax) vectors generated by f ? ; see Equation <ref type="bibr" target="#b3">(4)</ref>. S is only applied on pairs of test instances that are likely to belong to different classes. The likelihood is determined by using an implementation of the Kronecker delta function, ? : X ? X ? {0, 1}, which obtains 1 iff the instances are predicted to belong to different classes; LTransBoost ? LTransBoost + ?(xi)?(xi ? )?(xi, xi ? )S(xi, xi ? ) 10 end for <ref type="bibr" target="#b10">11</ref> LTransBoost</p><formula xml:id="formula_1">? LTransBoost/ U i=1 ?(xi, xi ? ) if U i=1 ?(xi, xi ? ) = 0 else 0 12 // Apply an optimization step 13 ? ? optimize 1 L L i=1 (f ? (xi), yi) + ? ? LTransBoost 14</formula><p>end for <ref type="bibr" target="#b14">15</ref> end for see Equation <ref type="formula" target="#formula_5">(5)</ref> for our implementation. Additionally, for each pair we intensify the loss using the model's confidence in its predictions. We define ? : X ? R + to be a confidence function, which for each instance, x, gives its class prediction confidence; see Equation <ref type="formula" target="#formula_6">(6)</ref> for our implementation. Finally, our transductive loss function, L TransBoost , is</p><formula xml:id="formula_2">L TransBoost (X u |f ? , S, ?, ?) 1 U ? 1?i&lt;j?U ?(x i )?(x j )?(x i , x j )S(x i , x j ),<label>(2)</label></formula><p>where U ? 1?i&lt;j?U ?(x i , x j ). The final loss function of our fine-tuning procedure includes the transductive loss term (2) as a regularization term as follows,</p><formula xml:id="formula_3">L(X l , Y l , X u |f ? , S, ?, ?) 1 L L i=1 (f ? (x i ), y i ) labeled/inductive (standard) loss +? ? L TransBoost (X u |f ? , S, ?, ?) unlabeled/transductive loss ,<label>(3)</label></formula><p>where is a standard (inductive) pointwise loss, e.g., the cross-entropy loss and ? ? R is a regularization hyperparameter.</p><p>Using the optimization objective in Equation <ref type="formula" target="#formula_3">(3)</ref>, we incentivize test sample pairs that are likely to be different in their classes to also be different in their empirical class probabilities while preserving the prior knowledge of f ? . Accordingly, given two test samples, (x i , x j ), which are predicted to be different in their classes by ?, we obtain a higher cost (by S) if x i is close (similar) to x j in its prediction vector than if x i is far (dissimilar) from x j , depending on the model's confidence in its predictions utilizing ?.</p><p>The computation of the L TransBoost loss component <ref type="formula" target="#formula_2">(2)</ref> is quadratic in U -namely, its time complexity is O( U 2 ). For large test sets (e.g., ImageNet), this is prohibitively large. Therefore, we propose to approximate L TransBoost by using random subsets of pairs from the test set. This random sampling is applied for each minibatch. The pseudo-code of the optimization procedure is presented in Algorithm 1. As can be seen, the size of the set of sampled test pairs used to approximate (2) is taken to be the batch size (U ), which allows for very fast computation of L TransBoost but could be sub-optimal (this size was not optimized).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TransBoost Implementation</head><p>While S, ? and ? in Equation <ref type="formula" target="#formula_2">(2)</ref> can be implemented in many ways, we now describe our proposed implementation that was used to obtain all the results described in Section 5. As can be seen below, we attempted to instantiate Equation (2) in the simplest possible manner. The consideration of more sophisticated methods is left for future work (and an alternative choice is discussed in Section 5.5).</p><p>The unsupervised symmetric pairwise similarity function S f was straightforwardly implemented using the L 2 norm as follows,</p><formula xml:id="formula_4">S f (x i , x j ) ? 2 ? p(x i |f ? ) ?p(x j |f ? ) 2 ,<label>(4)</label></formula><p>wherep(x|f ? ) denotes the empirical class probability of f ? applied on x. It is easy to show that S f is non-negative * (see Appendix B for a proof).</p><p>Additionally, we implemented ? f (5) and ? f (6) based on the pretrained weights of the model before it was fine-tuned with our procedure, which we refer to as ? 0 . Thereafter, the unsupervised symmetric pairwise selection function ? f is implemented using pseudo-labels computed by the given pretrained model f ?0 ,</p><formula xml:id="formula_5">? f (x i , x j ) 1 f ?0 (x i ) = f ?0 (x j ) 0 otherwise,<label>(5)</label></formula><p>and the confidence function ? f is implemented using the standard softmax response, i.e.,</p><formula xml:id="formula_6">? f (x) max{p j (x|f ?0 )} C j=1 ,<label>(6)</label></formula><p>wherep(x|f ?0 ) is the empirical class probabilities of f ?0 applied to x, and C is the number of classes.</p><p>Finally, the optimization objective <ref type="formula" target="#formula_3">(3)</ref> is implemented using the standard cross-entropy loss,</p><formula xml:id="formula_7">CE(X l , Y l |f ? ) 1 L L i=1 ? logp yi (x i |f ? ), as well the proposed S f , ? f and ? f , L(X l , Y l , X u |f ? , S f , ? f , ? f ) = CE(X l , Y l |f ? ) + ? ? L TransBoost (X u |f ? , S f , ? f , ? f ),<label>(7)</label></formula><p>where ? ? R is a regularization hyperparameter. TransBoost is inspired by TSVM, in which a large margin principle is applied to the given (unlabeled) test sample to leverage the knowledge of this specific test set. In <ref type="figure" target="#fig_1">Figure 2</ref> we present a toy quantitative (graphical) example indicating that our transductive loss component, L TransBoost , encourages behavior similar to TSVM. Consider <ref type="figure" target="#fig_1">Figure 2</ref> (left) showing the decision boundary obtained by SVM for the given training set (and ignoring the test points). Using SVM to implement ? (to determine if two test points are in the same class), we get that L TransBoost = 0.1796 (shown at the bottom). Clearly, this loss value is significantly larger than the corresponding loss (0.05167) obtained when using TSVM (right), where a large margin principle of test instances is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Large Margin Analogy of TransBoost</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Study</head><p>In this section we present a comprehensive empirical study of TransBoost. We begin by describing our experimental design and then present our studies in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Design and Details</head><p>This section describes the experimental details: datasets used, architectures used, and our TransBoost's procedure details and hyperparameters.</p><p>Datasets and Preprocessing. Most of our study of TransBoost is done using the well-known ImageNet-1k ILSVRC-2012 dataset <ref type="bibr" target="#b18">[19]</ref>, which contains 1,281,167 training instances and 50,000 test instances in 1,000 categories. Preprocessing for training is standard and includes resizing and random cropping to 224?224, followed by a random horizontal flip. For testing, the images are resized and center cropped. In some of our experiments involving advanced architectures from the Timm repository <ref type="bibr" target="#b19">[20]</ref>, we applied the test augmentations (at test time only) using its code from the Timm repository. Additionally, we investigated the effectiveness of TransBoost on the Food-101 <ref type="bibr" target="#b20">[21]</ref>, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b22">[22]</ref>, SUN-397 <ref type="bibr" target="#b23">[23]</ref>, Stanford Cars <ref type="bibr" target="#b24">[24]</ref>, FGVC Aircraft <ref type="bibr" target="#b25">[25]</ref>, the Describable Textures Dataset (DTD) <ref type="bibr" target="#b26">[26]</ref> and Oxford 102 Flowers <ref type="bibr" target="#b27">[27]</ref>. For each of these datasets, we applied the same preprocessing as in ImageNet, with the exception of CIFAR-10 and CIFAR-100, where we used random cropping with padding (cropping to 32?32), followed by a random horizontal flip, a single RandAugment <ref type="bibr" target="#b28">[28]</ref> operation and MixUp <ref type="bibr" target="#b29">[29]</ref>.</p><p>Architectures. We examined various inductive (standard) pretrained architectures as baselines for TransBoost. Specifically, we used a variety of ResNet <ref type="bibr" target="#b30">[30]</ref> architectures: ResNet18, ResNet34, ResNet50, ResNet101 and ResNet152, which were pretrained using the standard PyTorch training recipe <ref type="bibr" target="#b31">[31]</ref>. We also considered ResNet50-StrikesBack <ref type="bibr" target="#b32">[32]</ref>, which is the ResNet50 architecture trained with Timm's <ref type="bibr" target="#b19">[20]</ref> advanced training recipe. Additionally, we show results with more advanced architectures spanning various families: EfficientNetB0 <ref type="bibr" target="#b33">[33]</ref> and MobileNetV3-L <ref type="bibr" target="#b7">[8]</ref>, ConvNext-T <ref type="bibr" target="#b6">[7]</ref>, and the Transformer models ViT-S <ref type="bibr" target="#b8">[9]</ref> and Swin-T <ref type="bibr" target="#b34">[34]</ref>, which were pretrained with advanced procedures of Timm's library <ref type="bibr" target="#b19">[20]</ref> in accordance to their original papers.</p><p>Details on the TransBoost Fine-Tuning Procedure. Throughout all TransBoost's experiments, we followed Algorithm 1, and our functions were implemented as described in Section 4.1. We used the SGD optimizer (Nesterov) with a momentum of 0.9, weight decay of 10 ?4 , and a batch size of 1024 (consisting of 512 labeled training instances and 512 unlabeled test instances). Unless otherwise specified, the learning rate was fixed to 10 ?3 with no warmup for 120 epochs. The regularization hyperparameter of our loss in all our experiments was fixed to ? = 2; see Equation <ref type="formula" target="#formula_7">(7)</ref>. Our hyperparameters were fine-tuned based on a validation set that was sampled from the training set over ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ImageNet Experiments: Transductive vs. Inductive</head><p>This study examines the transductive performance of TransBoost applied to various pretrained models (all described in Section 5.1), and the transductive performance of the other baselines (SSL and t-FSL) versus the inductive performance of all baselines. <ref type="table" target="#tab_0">Table 1</ref> presents all the results. We note that some of these results have been already highlighted in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="table" target="#tab_0">Table 1</ref> is divided into several horizontal sections. In the first section we present TransBoost applied to a number of ResNet architectures. In the second section, we discuss modern architectures, including two types of Vision Transformers, and also ConvNext-T as representative of the ConvNext family, which presently achieves state-of-the-art results over ImageNet <ref type="bibr" target="#b13">[14]</ref>. Next we present the SimCLR and SimCLRv2 SSL methods (see Appendix C for more details on how we trained them in transduction). We note that SimCLRv2 presently dominates SSL performance over ImageNet according to <ref type="bibr" target="#b13">[14]</ref>. The last table section considers two popular t-FSL methods (which are described in Section 3). The column structure of <ref type="table" target="#tab_0">Table 1</ref> is straightforward but the most important column that deserves some explanation is the Inductive / Transductive column (4 th column) in which we compare the transductive performance of each experiment to the relevant inductive fully supervised performance. For example, TransBoost is capable of increasing the ImageNet top-1 accuracy of ViT-S by 2.28%. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_0">Figure 1</ref> demonstrate that fine-tuning with TransBoost consistently and significantly improves the inductive top-1 accuracy performance in all cases. Even the tiny version of the top performing ConvNext architecture family (presently the state-of-the-art according to <ref type="bibr" target="#b13">[14]</ref>) is improved by TransBoost. Surprisingly, ViT-S performed better than ConvNext-T when taking advantage of the given test samples using TransBoost, indicating that members of the ViT family could be the best transductive performers on ImageNet. A second interesting fact is that ViT-S and Swin-T ResNet <ref type="bibr" target="#b30">[30]</ref> architectures that were pretrained using standard simple procedures: perform similarly in the inductive setting while the ViT-S top-1 accuracy gain is improved by almost x3 relative to Swin-T. Could this advantage be related to the architecture? Interestingly, TransBoost also improves the performance of ResNet50-StrikesBack, which is the standard ResNet50 architecture trained using a sophisticated training procedure <ref type="bibr" target="#b32">[32]</ref> that boosts its top-1 accuracy by +4.2%. Surprisingly, TransBoost adds almost +1% on top of this phenomenal improvement. Another striking related result is that TransBoost improves the standard ResNet50 so that it achieves performance close to ResNet50-StrikesBack despite the fact that our procedure is much simpler than the sophisticated training procedure of <ref type="bibr" target="#b32">[32]</ref>.</p><p>Consider the first section of <ref type="table" target="#tab_0">Table 1</ref> (the ResNets section). While TransBoost always improves the inductive baselines, it is evident that its relative advantage monotonically decreases with the model size/performance. For example, the best transductive improvement is achieved for ResNet18 yielding a top-1 accuracy gain of +3.6% and resulting in a top-1 accuracy of 73.36% * . With respect to the other related baselines, TransBoost consistently outperforms both the SSL and t-FSL baseline algorithms. Comparing the results of SimCLR to the results of SimCLRv2, we see that SimCLR outperforms SimCLRv2 in the transductive setting (they mainly differ by the additional stage of knowledge distillation <ref type="bibr" target="#b36">[36]</ref> that is added to SimCLRv2), which suggests that the stage of knowledge distillation can hinder the transductive performance. We hypothesize that distilling knowledge from test instances leads to overfitting of incorrect class predictions that lead to model confusion. Finally, when evaluating the t-FSL representatives, we observe that Ent-min improves its inductive baseline while TIM-GD (which is an upgraded version of Ent-min) is inferior to its inductive baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">TransBoost's Performance on Additional Vision Datasets</head><p>We now examine the performance of TransBoost across eight other datasets (all described in Section 5.1). Throughout this study, we start with a ResNet50 architecture pretrained on ImageNet (using PyTorch's standard training recipe). Then we apply inductive fine-tuning for each specific dataset and, finally, we apply our TransBoost procedure (see Appendix D.1 for more details). * This ResNet18 transductive performance even outperforms the current state-of-the-art inductive performance <ref type="bibr" target="#b13">[14]</ref> for ResNet18 on ImageNet (73.19%) <ref type="bibr" target="#b35">[35]</ref>, which is achieved with a fancy training recipe. The results are visually depicted in the graph of <ref type="figure" target="#fig_3">Figure 3</ref> and are also shown in <ref type="table" target="#tab_3">Table 3</ref> in the Appendix. Clearly, TransBoost outperforms its inductive baseline in all cases. The improvement is large and significant in all datasets with the exception of CIFAR-10 where the improvement is minor. The most prominent result is obtained for the FGVC Aircraft dataset, where we observe a striking +5.34% top-1 accuracy gain from 78.46% to 83.8%. For a more in-depth discussion, we refer the reader to Appendix D. In this study, we consider various transductive settings, where each setting is characterized by a certain training set size and a certain test set size. We are interested in examining the final ImageNet transductive behavior of ResNet50 as a function of these sizes. The sizes of our training sets are: 5%, 10%, 20%, 100%; and for testing sets are: 10%, 25%, 50%, 100%, all taken as a fraction of the original train/validation set sizes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Transductive Performance Improves with Test Set Size</head><p>Consider <ref type="figure" target="#fig_4">Figure 4</ref> showing the top-1 accuracy gains for all combinations (4 ? 4 experiments in total). Importantly, we observe that whenever TransBoost uses the entire training set, its performance increases as the test set size grows. This strongly indicates that TransBoost leverages the test set as a whole, a desirable property of a transductive learning algorithm. On the other hand, and quite surprisingly, when TransBoost only used 5% of the training set, increasing the test set size worsened its performance. We hypothesize that this type of behavior is a result of poor prior knowledge (very small training set) that leads the models to wrongly analyze the relationships (similarities in our case) between pairs. This overall bad effect increases as the test set size expands. For a detailed description/discussion of this section's experiments, see Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Bringing the Like Together or Separating the Difference?</head><p>TransBoost is driven by Equation (2) whose informal desideratum is: revise the model so as to take apart the representations of two instances that are likely (?) to belong to different classes (?), which currently appear to be similar (S). Rather than, or in addition to separating instances that are likely to be different, we could consider grouping together instances that are likely to belong to the same class. In this section we summarize our experiments to apply TransBoost for this alternative objective and for a combination of both objectives. In our study we used the ResNet50 architecture, which was pretrained on ImageNet (using PyTorch's standard training recipe). The transductive performance of TransBoost was analyzed using three variants of the transductive loss component: <ref type="bibr" target="#b0">(1)</ref> the proposed separation component, L TransBoost (X u |S f , ? f , ? f ), as in Equation <ref type="formula" target="#formula_7">(7)</ref>; (2) a "reciprocal" instantiation of this objective -namely, L TransBoost (X u | ? 2 ? S f , 1 ? ? f , ? f ), which brings together similar instances; and (3) a variant that considers both these objectives together. The results are presented in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transductive Loss</head><p>Inductive / Transductive Top1 Acc. (%) Improv. (%) Clearly (but not significantly) the best performance is achieved using our proposed implementation (row highlighted in green). We chose our proposed loss component over the combination of both objectives because it is simpler. Interestingly, the reciprocal variation significantly degrades the top-1 performance.</p><formula xml:id="formula_8">LTransBoost(Xu|S f , ? f , ? f ) 76.15 / 79.03 +2.88 LTransBoost(Xu| ? 2 ? S f , 1 ? ? f , ? f ) 76.15 / 74.64 -1.51 LTransBoost(Xu|S f , ? f , ? f ) + LTransBoost(Xu| ? 2 ? S f , 1 ? ? f , ? f ) 76.15 / 79.00 +2.85</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>We presented TransBoost, a novel and powerful transductive fine-tuning procedure that can be efficiently applied on any pretrained model. TransBoost is a deep transductive classification algorithm that is inspired by a large margin principle such as the TSVM algorithm. Strikingly, TransBoost consistently and significantly improves the inductive ImageNet top-1 performance of many architectures including the most advanced ones, leading to state-of-the-art transductive performance. Moreover, TransBoost is effective on a broad range of image classification datasets.</p><p>As described in Algorithm 1, TransBoost is a general optimization procedure that can be implemented in a range of ways depending on the similarity function (S), the selection function (?), and the confidence function (?). We instantiated these functions simply and straightforwardly. A question for future research may be to ask whether more sophisticated choices will lead to better transduction performance. The ViT-S architecture achieved the best transductive classification performance in our experiments and accrued the largest gain relative to induction among advanced architectures. It would be interesting to explore the question of how this Transformer architecture facilitates this gain. In general, we can ask what is the best architecture for transductive classification. Finally, it would be very interesting to see whether such large performance gains are also sustained in NLP applications.</p><p>Finally, our results clearly demonstrate that transductive learning can lead to huge accuracy gains. Thus, it would be wasteful to use standard induction in settings where transduction is applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TransBoost Applications</head><p>In general TransBoost is particularly useful when we are able to accumulate a test set of instances and then finetune a specialized model to predict their labels. This setting has numerous use cases in various application fields including:</p><p>Medicine Medical diagnosis is one possible meaningful use case. In this case, medical records can be gathered on a daily or weekly basis. TransBoost can then be used to finetune transductive models on top of existing inductive models in order to provide more reliable results for these specific records.</p><p>FinTech A possible use case is portfolio management (not high frequency trading), where the model should recommend stocks for trading based on weekly or monthly activities. In order to predict actions better than an inductive model, TransBoost can train specialized models based on up-to-date activities and then provide better trading recommendations. Another example is risk assessment for loans or insurances, where many financial companies are already using machine learning (inductive) models to predict the risk. When it comes to loans, financial records can be gathered and then a specialized model can be finetuned by TransBoost to assess the risk for particular clients. In this context, TransBoost can be used to lower insurance premiums based on client-specific profiles.</p><p>Targeted Advertising Recommendation systems for marketing are another strong use case. Here, users' recent activities are used to recommend products in active campaigns. TransBoost can be applied to build a specialized model on a daily or weekly basis in order to recommend accurately user-specific products based on their activities.</p><p>Homeland Security Hazard prediction for governments is another strong application. If, for instance, we need to predict whether a dangerous event will occur based on some relevant documents provided by open or closed sources. TransBoost can be used to provide this data-specific predictions. It is worth mentioning that training a general inductive model to predict this task is much more difficult.</p><p>Data Analytics An example of a possible use case is the analysis of photos or videos in Google Photos (or similar systems). The photos can be accumulated by their location (for instance) and then a TransBoost model can be trained to produce a photo/video summary that is biased towards these specific photos (e.g. photos-specific effects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The Similarity Function is Bounded</head><p>In the following, we claim and prove that S f is bounded.</p><p>Claim 1. Let f : X ? {1, . . . C} be a classification model and x 1 , x 2 ? X input instances, where C is the number of classes. Accordingly, our implementation of TransBoost's symmetric pairwise similarity function, S f (4), satisfies the following:</p><formula xml:id="formula_9">0 ? S f (x 1 , x 2 ) ? ? 2.</formula><p>Proof.</p><p>Recalling our implementation of S f (4),</p><formula xml:id="formula_10">S f (x 1 , x 2 ) ? 2 ? p(x 1 |f ) ?p(x 2 |f ) 2 ,</formula><p>wherep(x|f ) is the empirical class probabilities of f applied on x. Since || ? || is non-negative, we obtain S f (x 1 , x 2 ) ? ? 2. Now we need to show that S f (x 1 , x 2 ) ? 0, which holds iff,</p><formula xml:id="formula_11">p(x 1 |f ) ?p(x 2 |f ) 2 ? ? 2, iff p(x 1 |f ) ?p(x 2 |f ) 2 2 ? 2.</formula><p>Considering the left-hand side of the inequality,</p><formula xml:id="formula_12">p(x 1 |f ) ?p(x 2 |f ) 2 2 = p(x 1 |f ) 2 2 + p(x 2 |f ) 2 2 ? 2 p(x 1 |f ),p(x 2 |f ) .</formula><p>Following a sub case of H?lder inequality, ?v ? R n ; ||v|| 2 ? ||v|| 1 . Therefore, using the last equation we obtain:</p><formula xml:id="formula_13">p(x 1 |f ) ?p(x 2 |f ) 2 2 = p(x 1 |f ) 2 2 + p(x 2 |f ) 2 2 ? 2 p(x 1 |f ),p(x 2 |f ) ? p(x 1 |f ) 2 1 + p(x 2 |f ) 2 1 ? 2 p(x 1 |f ),p(x 2 |f ) = 2 ? 2 p(x 1 |f ),p(x 2 |f ) (8) ? 2.</formula><p>(9) Where <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref> hold sincep is a probability vector. Hence for any x ? X , p(x|f</p><formula xml:id="formula_14">) 1 = C i=1 |p i (x|f )| = 1 and ?1 ? i ? C ; 0 ?p i (x|f ) ? 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Transduction Training Details of SimCLR and SimCLRv2</head><p>The SimCLR <ref type="bibr" target="#b10">[11]</ref> algorithm consists of self-supervised pretraining and supervised fine-tuning. SimCLRv2 <ref type="bibr" target="#b11">[12]</ref> incorporates approximately the same steps, but adds a final knowledge distillation <ref type="bibr" target="#b36">[36]</ref> stage that has been found powerful in inductive SSL (see <ref type="bibr" target="#b11">[12]</ref> for further details).</p><p>SimCLR (and SimCLRv2), however, are designed for SSL rather than transduction; therfore, we will now elaborate how they were trained for transduction. As a first point, we note that both SimCLR and SimCLRv2 were applied to the ResNet50 and ResNet152 architectures using pretrained weights provided by the authors. Next, we fine-tuned the model, using both the labeled training set as well as the unlabeled test set, for another 100 epochs. We followed the pretraining scheme as suggested by the authors, except for the batch size which was set to 1024 due to lack of computational resources (they used 4096). In the second step of supervised fine-tuning, we followed the SimCLRv2 instructions given by the authors. For the final stage (self-distillation), SimCLRv2 optimizes a weighted loss function based on labeled training instances applied to standard CE and unlabeled training instances applied to the CE distillation loss. In our case, the unlabeled set is the transductive set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Supplement for the Additional Vision Datasets Experiments</head><p>Here we discuss the details in the experiments on TransBoost using the additional vision datasets that were introduced in Section 5.3, as well as an additional observation offered in Appendix D.2. <ref type="table" target="#tab_3">Table 3</ref> compares the TransBoost performance to the standard inductive (fully supervised) performance and highlights some relevant properties of the datasets we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Train (+Validation)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test</head><p>Classes Resolution Inductive / Transductive Top1 Acc. (%) Improv. (%) CIFAR-10 <ref type="bibr" target="#b22">[22]</ref> 50,000 10,000 10 32 97.60 / 97.61 +0.01 CIFAR-100 <ref type="bibr" target="#b22">[22]</ref> 50,000 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Training Details on The Additional Vision Datasets</head><p>We now elaborate on the training details (inductive pre-training and transductive fine-tuning) of the additional vision datasets experiments. We note that we used the ResNet50 architecture in all the experiments.</p><p>In the case of non-CIFAR datasets, we started with a model pre-trained on ImageNet, then fine-tuned it on each dataset. We chose to start with pre-trained weights on ImageNet, in order to achieve superior results on each dataset for later comparisons. The training procedure on ImageNet was conducted using PyTorch's standard training recipe. Then, the inductive fine-tuning followed PyTorch's standard training recipe with the exception of the learning rate, which was set to 10 ?3 at the beginning of the procedure. Finally, transductive fine-tuning was applied as detailed in Section 5.1, except for the batch size, which was set to 128 (rather than 1024).</p><p>With the CIFAR-10 and CIFAR-100 datasets, we started with a random initialization of ResNet50, in which we disabled the max-pooling layer as standard. The model was trained inductively using the SGD optimizer with a momentum of 0.9, weight decay of 10 ?4 , and nesterov. We used a learning rate of 0.1 and a cosine schedule for 250 epochs with a batch size of 128, while in the first 75 epochs we applied a linear warmup. Transductive fine-tuning was performed using the SGD optimizer with a learning rate of 10 ?4 based on a validation set. Other details follow our standard training scheme, as described in Section 5.1, except for the batch size which was set to 128 (rather than 1024). <ref type="table" target="#tab_3">Table 3</ref> displays an interesting pattern regarding the number of classes. For experiments conducted using both non-CIFAR and CIFAR groups, we observe that TransBoost performs better on datasets with many classes than on datasets with a few classes. This pattern becomes blurrier as the number of classes increases. Specifically, among non-CIFAR experiments, it can be seen that the experiment on the DTD dataset attained the smallest improvement. This effect is also observed among the CIFAR group while the CIFAR-10 dataset was achieved the smallest accuracy gain. On the basis of these observations, we hypothesize that the more classes the dataset has, the more likely similar representations, which belong to different classes, will be found and, therefore, there are more target test instances to improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Transductive Performance Gain Improves as the Number of Classes Grows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Supplement for the ImageNet Subsets Experiments</head><p>This section provides a supplementary in-depth discussion of our experiments on ImageNet training and test subsets, which are varying in their sizes (introduced in Section 5.4). In this study, we used the ResNet50 architecture that was pretrained on ImageNet combinations of training and test subsets (using PyTorch's standard training recipe). For experiments that do not utilize the entire training set, we applied the TransBoost procedure with a batch size of 128. Although TransBoost was optimized for a specific test set, here we evaluate its performance in various inductive settings, where unseen test instances are examined at test time. Additionally, we present a comprehensive analysis of TransBoost's performance in various transductive settings as well as a standard inductive (fully supervised) performance analysis as a baseline. The complete results are presented in <ref type="table" target="#tab_4">Table 4</ref>. As can be seen, TransBoost consistently outperforms the baselines in transductive settings while the baselines almost consistently outperform TransBoost in inductive settings. This behavior is indeed what one may expect from real transductive models, which will only be asked for the given test set. Interestingly, TransBoost's best top-1 accuracy gain -an impressive improvement of +4.00% -was achieved by the experiment that used 20% of the training set and 25% of the test set. Our best top-1 accuracy (79.03%) was attained in the experiment that used the entire training and test sets. For further details, we refer the reader to Appendix E.1 and Appendix E.2, below.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 TransBoost's Inductive Performance Increases with Test Set Size</head><p>In <ref type="figure" target="#fig_5">Figure 5</ref>, we present an interesting trend of TransBoost's top-1 accuracy gains in various inductive settings (derived from <ref type="table" target="#tab_4">Table 4</ref>). Note that the fractions described in the figure legend refer to the test instances that participated in the training procedure, while the inductive performance was examined using the rest of the instances. We have already discussed the fact that almost all inductive baselines outperform TransBoost in all inductive settings. This can be expected from a transductive model that specializes in a particular test set. <ref type="figure" target="#fig_5">Figure 5</ref>, however, reveals an interesting pattern: the more test instances are provided at training time, the better TransBoost's performance in the inductive setting. This observation implies that our proposed optimization procedure learns better to generalize unseen instances when the ratio of test instances (out of the training and test instances combined) is significant. Moreover, it is apparent that performance deteriorates with increasing training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 TransBoost's Loss is Consistently Improved</head><p>Here we provide an additional analysis of the relative improvement in our proposed loss component, L TransBoost (see Section 4.1), across the experiments that we described above. Specifically, we approximated TransBoost's loss following Algorithm 1 (the approximation stage only) on the inductive baselines as well as on TransBoost's output models in the transductive setting. Then, we calculated their relative improvements in (%) and presented the results in <ref type="figure" target="#fig_6">Figure 6</ref>. For example, our best relative improvement in L TransBoost was given by the experiment that used 20% of the training set and 10% of the test set with a relative improvement of +97.2%.</p><p>As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, the larger the test set that TransBoost uses, the smaller its improvement on our loss component L TransBoost . Accordingly, larger test sets may be harder to optimize than smaller ones, and may require more optimization steps. The larger the training set, the greater the improvement on the L TransBoost . This suggests that the greater the prior knowledge of the model is, the easier it is to optimize L TransBoost .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ImageNet top-1 accuracy gains of TransBoost (green) using representatives of CNNs (blue), Mobile-CNNs (yellow) and Transformers (red), in comparison to the standard inductive (fully supervised) performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A toy quantitative example indicates that our LTransBoost encourages behavior similar to TSVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>TransBoost's transductive performance using ResNet50 (green) on various vision datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>TransBoost's transductive performance using ResNet50 on ImageNet training and test subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>TransBoost's inductive performance using ResNet50 on ImageNet training and test subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Relative improvements in our proposed loss component, LTransBoost, using ResNet50 on ImageNet training and test subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>TransBoost Procedure Input : Labeled train sample S l (X l , Y l ), unlabeled test sample Xu, a model f ? that was pretrained on S l , a supervised pointwise loss function , implementations of TransBoost's functions: S, ? and ?. Hyperparameters: Number of epochs E, labeled batch size L , unlabeled batch size U , regularization parameter ?.</figDesc><table><row><cell cols="3">1 for epoch ? 1 to E do</cell></row><row><cell>2</cell><cell cols="2">// Sample labeled and unlabeled batches in a cyclical manner</cell></row><row><cell>3</cell><cell cols="2">for S l (X l , Y l ) ? S l , X u ? Xu to max{ L L , U U } do</cell></row><row><cell>4</cell><cell cols="2">// Prepare random test sample pairs</cell></row><row><cell>5</cell><cell>X u</cell><cell>? ? generate a random permutation of X u</cell></row><row><cell>6</cell><cell cols="2">// Approximate the TransBoost loss</cell></row><row><cell>7</cell><cell cols="2">LTransBoost ? 0</cell></row><row><cell>8</cell><cell cols="2">for i ? 0 to U do</cell></row><row><cell>9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of transductive loss variants using ResNet50. Our proposed loss is highlighted in green.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>A study comparing TransBoost transductive performance to standard inductive (fully supervised) performance on various vision datasets (green) using ResNet50. TransBoost's best result is highlighted in yellow.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>presents 16 experiments of TransBoost's procedure performed on all combinations of the ImageNet training set fractions: 5%, 10%, 20%, 100%; and the ImageNet test set fractions: 10%, 25%, 50%, 100%. The table's sections represent training set fractions, and the rows in each section represent test set fractions. There are two main column groups highlighted in green and blue. Green columns present comprehensive results in transductive settings, whereas blue columns present comprehensive results in inductive settings. Across the two settings (transductive / inductive), TransBoost's performance is compared to the standard inductive (fully supervised) performance. For instance, using 20% of the training set and 25% of the test set, we obtained the best top-1 accuracy gain of +4.00% in the transductive setting while the performance in the inductive setting degraded by -1.72%. We note that the performance in the inductive setting (highlighted in blue) is evaluated using the test instances that were not used at training time.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>A comprehensive analysis of TransBoost's performance on ImageNet subsets, both in transduction and induction, compared to the standard inductive (fully supervised) performance using ResNet50. The performance of TransBoost in transduction is highlighted in green and the performance of TransBoost in induction is highlighted in blue.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* S f (xi, xj) = 0 iffp(xi|f ? ) andp(xj|f ? ) are one-hot vectors where the ones indicate different classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">,000 100</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by the Israel Science Foundation, grant No. 710/18.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explicit learning curves for transduction and application to clustering and compression algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Derbeko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transductive rademacher complexity and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Pechyony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="193" to="234" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large margin vs. large volume in transductive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Pechyony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="188" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Estimation of dependences based on empirical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8443" to="8452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Papers with code: The latest in machine learning</title>
		<ptr target="https://paperswithcode.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transductive information maximization for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2445" to="2457" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iterative label cleaning for transductive and semi-supervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Lazarou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Stathaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8751" to="8760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Few-shot classification leaderboard</title>
		<ptr target="https://fewshot.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08453</idno>
		<title level="m">Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without tricks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
