<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing Relevance Maps of Vision Transformers Improves Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing Relevance Maps of Vision Transformers Improves Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It has been observed that visual classification models often rely mostly on the image background, neglecting the foreground, which hurts their robustness to distribution changes. To alleviate this shortcoming, we propose to monitor the model's relevancy signal and manipulate it such that the model is focused on the foreground object. This is done as a finetuning step, involving relatively few samples consisting of pairs of images and their associated foreground masks. Specifically, we encourage the model's relevancy map (i) to assign lower relevance to background regions, (ii) to consider as much information as possible from the foreground, and (iii) we encourage the decisions to have high confidence. When applied to Vision Transformer (ViT) models, a marked improvement in robustness to domain shifts is observed. Moreover, the foreground masks can be obtained automatically, from a self-supervised variant of the ViT model itself; therefore no additional supervision is required. Our code is available at https: //github.com/hila-chefer/RobustViT.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The reliance on simple image-level classification supervision, together with the sampling biases of object recognition datasets, leads to vision models that exhibit unintuitive behavior, as depicted in <ref type="figure" target="#fig_0">Fig 1.</ref> First, the models we tested (ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref>, and DeiT <ref type="bibr" target="#b48">[48]</ref>) tend to give disproportional high weight to the background of the image in the decision-making process. Second, the tested models occasionally regard a sparse subset of the pixels in the foreground object for the classification, disregarding much of the object's data. As argued by Geirhos et al. <ref type="bibr" target="#b17">[18]</ref>, and stated in <ref type="bibr" target="#b23">[24]</ref> "image classification datasets contain 'spurious cues' or 'shortcuts' . For instance, cows tend to co-occur with green pastures, and even though the background is inessential to the identity of the object, models may predict 'cow', using primarily the green pasture background cue."</p><p>There is considerable evidence that context is a useful cue <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>. However, many of the associated background elements and foreground shortcuts are only relevant to the specific data distribution, which leads to lack of robustness to distribution shifts <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">38]</ref>. There are many methods for overcoming domain shifts, including domain adaptation techniques <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1]</ref> and methods that augment the training set or the training procedure <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref>. In this work, however, we opt for a direct approach, which monitors the relevancy score of the model for each image region, and manipulates the relevancy map to be focused on the regions within the foreground mask.</p><p>The method is based on a finetuning procedure, which is applied to a pretrained Vision Transformer (ViT) model. A relatively small set of samples, for which the foreground is given, is employed during this phase. In most of our experiments, we use three samples for half the classes, following work that examined the effect of transfer learning on half of the classes <ref type="bibr" target="#b54">[54]</ref>. The ground-truth foreground mask is either human annotated <ref type="bibr" target="#b15">[16]</ref>, or estimated from a self-supervised ViT model <ref type="bibr" target="#b52">[52]</ref>.  <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and DeiT <ref type="bibr" target="#b48">[48]</ref>, even if the confidence of the model is above 90%.</p><p>The finetuning procedure employs three loss terms. The first encourages the relevance map to assign lower values to the background of the image. The second term aims to remedy the sparse relevance issue by making larger parts of the foreground part of the relevancy map. The third term is a regularizer that ensures that the classification accuracy of the original model remains unimpaired.</p><p>Unsurprisingly, applying this method leads to a (modest) drop in accuracy on the original training dataset and on datasets with very similar distributions. In an extensive battery of experiments we show that (i) the classification accuracy on datasets from shifted domains increases considerably. This includes real-world unbiased and adversarial datasets, as well as synthetic ones that were created specifically to measure the robustness of the classification model, (ii) the resulting relevance maps demonstrate a significant improvement in focusing on the foreground of the image, i.e. the object, rather than on its background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image classification datasets are becoming increasingly challenging, while at the same time models are growing more complex <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b21">22]</ref>. With the rapid advancements in object recognition, the measuring stick is being narrowed down to a single number, accuracy <ref type="bibr" target="#b47">[47]</ref>. By relying solely on accuracy, classifiers introduce biases, since they utilize shortcuts to select the right class <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>. The shortcomings of models that rely on shortcuts have been demonstrated for domains other than vision, such as natural language processing <ref type="bibr" target="#b13">[14]</ref> and multi-modal learning <ref type="bibr" target="#b16">[17]</ref>.</p><p>One way to assess the salient behavior of a model is to study Sufficient Input Subsets (SIS), i.e., the minimal number of pixels necessary for a confident prediction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. Finding an SIS for a class can imply that the classifier has overinterpreted its input, since it can make a confident accurate decision using a small, sparse subset of pixels, which does not seem meaningful to humans. We study the SIS with gradients approach for ViT models, and find that it can be misleading. Specifically, SIS can be regarded as an adversarial method that can lead to high-confidence classification of any label from a sparse set of pixels, see Appendix A. Therefore, we opt to use datasets designed specifically for evaluating accuracy when facing distribution shifts, to assess the model's resilience and ability to generalize. Several alternatives have been proposed to ImageNet <ref type="bibr" target="#b36">[37]</ref>: (i) ImageNet-v2 <ref type="bibr" target="#b33">[34]</ref>, a new test set sampled from the same distribution, which reduces adaptive overfitting; (ii) ImageNet-A <ref type="bibr" target="#b23">[24]</ref>, a test set of natural adversarial samples; (iii) ImageNet-R <ref type="bibr" target="#b22">[23]</ref> and ImageNet-Sketch <ref type="bibr" target="#b51">[51]</ref>, which contain renditions of objects (e.g., art, sculptures, sketches); (iv) ObjectNet <ref type="bibr" target="#b3">[4]</ref>, a real-world set with controls on object locations and view points, and (v) SI-Score <ref type="bibr" target="#b11">[12]</ref>, which is a synthetic dataset designed specifically for testing robustness to object location, rotation and size. Explainability methods may be used to determine the reasons for the decisions made by classifiers.</p><p>As an example, we may find that models tend to overlook objects with relevance maps (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Gradients are a dominant and useful signal for model interpretation <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. By adding input signals to the gradients, relevance maps were refined <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>. Alternatives to gradient propagation for explanation include attribution propagation -a theory-driven method based on axioms, and permutation based on Shapley values <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b28">29]</ref>. For transformer architectures, the combination of gradients and attention values has been shown to produce a viable interpretation of the model's prediction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Our method optimizes the relevancy maps of the model as a regularization term. Other works that investigated the use of relevancy to alleviate overfitting include Ross et al. <ref type="bibr" target="#b35">[36]</ref>, who introduced a regularization term for the input gradient, which reduces reliance on irrelevant cues, e.g., background pixels. Additional work in this vein has been conducted on medical data, studying how doctors classified a disease <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b50">50]</ref>. Singh et al. <ref type="bibr" target="#b42">[42]</ref> regularize feature representations of a category from its co-occurring context. Zhu et al. <ref type="bibr" target="#b55">[55]</ref> enrich classifier representations by mimicking detection models. Importantly, unlike all the above methods, our method incorporates the foreground features and the classifier confidence, rather than considering only the background features. Furthermore, we apply our method not during training, but as a short finetuning process that is feasible for large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach aims to direct vision models such that their decision will be based on the features of the object rather than on other supportive background features. To achieve this, we employ additional supervision to distinguish between the foreground and background features. The method finetunes the model in a way that encourages the class relevance map, obtained through a relevance computation method, to roughly resemble the segmentation map. This way, the decision-making process is focused on the foreground. The relevance map employed by our method is calculated using a recent advancement in explainability for transformer-based architectures <ref type="bibr" target="#b7">[8]</ref>. A brief introduction of the explainability method used is provided in Appendix B.</p><p>The method employs a small set of labeled segmentation maps for distinguishing between the foreground and background of the input image. Our first loss term discourages the model from considering mostly the background:</p><formula xml:id="formula_0">L bg = MSE R(i) S (i), 0 ,<label>(1)</label></formula><p>where i is the input image, R(i) is the relevance map produced for i,S(i) is the inverse of the segmentation map for i, and is the Hadamard product. Put differently, L bg extracts the relevance values assigned to the background using the provided segmentation, and encourages those values to be close to 0, which is the minimal possible relevance value.</p><p>Our second loss term encourages the model to consider as much information as possible from the foreground of the image:</p><formula xml:id="formula_1">L fg = MSE (R(i) S(i), 1) ,<label>(2)</label></formula><p>where S(i) is the foreground mask. This loss encourages the relevance of pixels inside the segmentation to be higher (1 is the maximal achievable relevance value). The overall explainability loss is constructed as follows:</p><formula xml:id="formula_2">L relevance = ? bg ? L bg + ? fg ? L fg ,<label>(3)</label></formula><p>where ? bg , ? fg are hyperparameters. All our experiments apply the same choice of ? bg = 2, ? fg = 0.3. Note that the coefficient ? fg is much smaller than ? bg . The reason is two-fold: (i) we find that the issue of overinterpreting the background is more common than the issue of partial relevance of foreground pixels, and (ii) L fg implies a uniform relevance of 1 for all foreground pixels, which may be detrimental, as we wish to allow the model to be able to focus on specific features of the object.</p><p>Finally, in the absence of an additional regularization loss, the finetuning results in explanations that resemble the ground-truth segmentation, while the accuracy plummets due to the absence of encouragement to maintain high accuracy. Therefore, one must apply an additional loss term to ensure that the output distribution of the model remains similar to the original model. We opt to use a confidence-boosting loss for this purpose, which is constructed as follows:  <ref type="figure">Figure 2</ref>: Examples from the ImageNet validation set of cases where our method does not change the prediction, corrects the prediction, and ruins the prediction. Even in cases where our method changes a correct prediction, there is often a rationale behind the modified prediction. The "Pred" row specifies the predictions before and after our finetuning. The examples are presented for the base, large models of ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and the base model of DeiT <ref type="bibr" target="#b48">[48]</ref>.</p><formula xml:id="formula_3">L classification = CE (M(i), arg max(M(i))) ,<label>(4)</label></formula><p>where M notates the vision model, and arg max(M(i)) is the class predicted by M for the input image i. L classification calculates the cross-entropy loss between the output distribution of M and the one-hot distribution where the predicted class is assigned a probability of 1. In other words, this loss encourages the confidence of the predicted class to increase.</p><p>The overall loss for the finetuning process is, therefore:</p><formula xml:id="formula_4">L = ? relevance ? L relevance + ? classification ? L classification ,<label>(5)</label></formula><p>where ? relevance = 0.8, and ? classification = 0.2 remain constant in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The main hypothesis of this work is that improving the salient maps of ViTs trained on ImageNet will result in reduced overfitting, and better generalization to data from unseen distributions. We present a wide range of tests to confirm our hypothesis.</p><p>First, we evaluate the improvement in robustness, i.e., the ability to maintain high accuracy under distribution shifts. The datasets with shifted distributions are only used for evaluation and contain both real-world datasets and synthetic ones. Second, we conduct segmentation tests following <ref type="bibr" target="#b8">[9]</ref> to assess the effect of our method on the level of agreement between the relevancy maps and the foreground segmentation maps. Third, following <ref type="bibr" target="#b54">[54]</ref>, our method employs samples from a subset of the classes during training, so that we can check whether the training classes (set A of the labels)  <ref type="figure">Figure 3</ref>: Examples of cases where our method corrects wrong predictions, alongside the original and modified (after finetuning) explainability maps. The "Pred" row specifies the predictions before and after our finetuning. The original classifiers focus on sparse or irrelevant data (e.g. the presence of snow leads DeiT-B to predict that a bus is a Snowplow, a porcupine is classified by ViT-L as a sea lion due to the presence of the ocean, a tank is classified by AR-B as a tram due to the presence of tram cables in the image, etc.). The examples are presented for the base, large models of ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and DeiT <ref type="bibr" target="#b48">[48]</ref>. differ from the set of classes not used during training (set B). Ideally, the method would have a positive effect on test samples from both sets A and B. Furthermore, in Appendix G we evaluate how sensitive the method is to the number of samples per class, and to the number of classes in set A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>We focus on methods that resemble ours, i.e. methods that strive to correct overfitting by manipulating the saliency maps of the model. Our baselines include GradMask <ref type="bibr" target="#b41">[41]</ref>, and Right for the Right Reasons <ref type="bibr" target="#b35">[36]</ref> (RRR). Both GradMask and RRR were originally applied during training; in this work, however, we focus on large vision models that require significant resources for training, which makes this approach computationally impossible for us. Therefore, we apply their accuracy and relevancy losses within a finetuning process, similar to ours, in order to test their success in a fair way, while adhering to computational limitations.</p><p>Both GradMask and RRR employ two loss functions. First, a classic cross-entropy loss with the ground-truth labels to ensure correct labeling, and second, a loss limiting the values of the gradients of irrelevant parts of the input. The latter resembles our background loss (Eq. 1), with gradients as the relevance map. We refer the reader to Appendix C for the full description of the applied losses.</p><p>We note that while using the gradient of the output w.r.t. the input is common practice for interpreting CNNs, these gradients are less stable for transformer-based models. For example, results presented in <ref type="bibr" target="#b27">[28]</ref> demonstrate that for transformer-based models the classic Input?Gradient method violates faithfulness. We found it difficult to grid-search hyperparameters to fit both accuracy and relevancy losses simultaneously for the baselines. Furthermore, we had to tune the hyperparameters for each model separately to obtain an improved relevance loss. Our method, on the other hand, uses the same hyperparameter choice (see Sec. 3) for all models, which makes it far more stable to use, thus allowing us to run experiments on large models as well (i.e. ViT-L, ViT AugReg-L). We refer the reader to Appendix D for the full description of hyperparameters used in ours experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Training</head><p>To demonstrate the effectiveness of our method, we experiment on three types of ViT-based models: vanilla ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg (AR) <ref type="bibr" target="#b45">[45]</ref>, and DeiT <ref type="bibr" target="#b49">[49]</ref> which presents techniques for efficiently training ViTs. For each model type, we experiment with different model sizes, in order to learn whether our method improves robustness across training techniques and model sizes. All models use 224 resolution images with a patch size of 16 ? 16. We use the implementation and pre-trained weights from <ref type="bibr" target="#b53">[53]</ref>. The small, base models are finetuned on a single RTX 2080 Ti GPU, and the large models on a single Tesla V100 GPU. All models are finetuned as described in Sec. 3 for 50 epochs, with a batch size of 8. We use 3 training images from 500 ImageNet classes for our finetuning (overall-1500 samples), and another 414 images as a validation set. In most experiments, we simply select the first 500 classes from ImageNet-S to be used for training (set A).</p><p>For results with multiple random seeds and multiple choices of the 500 classes to use for training, see Appendix E. The learning rate of each model is determined using a grid search between the values 5e ? 7 and 5e ? 6. As a rule of thumb, we select the highest learning rate for which the validation accuracy does not decrease by more than 2 ? 3% and L classification does not increase. We find that our method is not sensitive to small shifts in the learning rate, thus this rule applies well to all models.</p><p>As mentioned in Sec. 3, we use segmentation maps to distinguish between the foreground and the background of an input image. Our experiments employ two options of obtaining segmentation maps for ImageNet training images. In the first option, we use human-annotated segmentation maps from the ImageNet-S dataset <ref type="bibr" target="#b15">[16]</ref>. The ImageNet-S dataset contains 10 training samples with their segmentation maps for 919 of the ImageNet classes. We employ a considerably smaller subset, as detailed above. The second option does not employ extra supervision in the form of manually labeled images; instead, it employs the Tokencut object localization method presented in <ref type="bibr" target="#b52">[52]</ref> to produce foreground segmentation maps.</p><p>Results Tab. 1 presents the results of our method and the baseline methods applied to different ViT-based models. As can be seen, for real-world datasets, both adversarial datasets (INet-A) and datasets with random and controlled background, rotations, and view points (ObjectNet), our method significantly and consistently improves performance (average of 5.8%, 5.0% top-1 improvement on INet-A, ObjectNet, respectively). For datasets that contain art, sculptures, sketches etc. (INet-R, INet-Sketch) the increase in accuracy is less steep (2.7%, 0.9% averaged top-1 improvement, respectively). This can be intuitively attributed to the fact that art and sketches often feature the object without a background, or with a uniform background. Additionally, as can be seen, although the baseline methods preserve accuracy on the datasets from the original ImageNet distribution (INet val set and INet-v2), they fall behind our method on real-world out-of-distribution datasets (INet-A and ObjectNet), indicating that the baselines are less successful in alleviating overfitting.</p><p>Following our method, there is a slight decrease in the performance of the models on data from the ImageNet distribution (INet val set and INet-v2), which can be attributed to the fact that we reduce some of the overfitting on the ImageNet data. <ref type="figure">Fig. 2</ref> presents examples of all three possible prediction cases on the ImageNet validation set: cases where our method preserves the original classification, corrects the original prediction, and changes a correct prediction. As can be seen, while performance sometimes decreases with respect to the assigned label, in most cases the rationale of the class proposed by the modified model is clear, often indicating a more natural labeling option. In addition, in the cases where the prediction remains the same or is corrected, our method produces improved relevance maps.</p><p>Additionally, as shown in Tab. 2 for the SI-Score dataset, which is a synthetic dataset designed for testing resilience for shifts in object locations, sizes, and rotations, there is a very steep improvement in performance across all models, while, once again, the baselines fall behind on all models and sizes.</p><p>Evidently, in both Tab. 1,2 our method works just as well and often better when using the unsupervised segmentation maps. This means that our method may be applied without requiring any manual supervision, except for the image label.  <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and DeiT <ref type="bibr" target="#b49">[49]</ref> with our method, and the baseline methods GradMask <ref type="bibr" target="#b41">[41]</ref> and Right for the Right Reason (RRR) <ref type="bibr" target="#b35">[36]</ref>. "Annotated segmentation" indicates whether we used annotated segmentation <ref type="bibr" target="#b15">[16]</ref> or unsupervised localization <ref type="bibr" target="#b52">[52]</ref>. "Original" stands for the model without finetuning. The bottom rows indicate the average change caused by our method across all architectures (on some models the baselines could not be run successfully; therefore, we do not compute their average change).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Method Avg.</p><formula xml:id="formula_5">Annotated INet val INet-A INet-R Sketch INet-v2 ObjNet segmentation R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 R@1</formula><formula xml:id="formula_6">Ours -0.8 0.0 +5.8 +7.8 +2.7 +3.0 +0.9 +1.3 -0.3 +0.2 +5.0 +6.4 change Ours -0.5 0.0 +6.1 +8.2 +2.8 +2.9 +1.1 +1.4 -0.1 +0.4 +5.0 +6.3 Fig.</formula><p>3 presents example cases in which our method is able to correct the prediction of the original model on images from various robustness datasets. As can be seen, the original models tend to overinterpret the background, and therefore produce false classifications based on it. For example, a lemon is classified as a golf ball due to the grass in the background (third example in the first row), a tank is classified as a tram due to the tram cables at the top of the image (first example in the third row), and so on. Additional examples can be found in Appendix F.</p><p>Segmentation Tests Since our motivation is to encourage the relevance to focus less on the background and more on as much of the foreground as possible, we test the resemblance of the resulting relevance maps to the segmentation maps following <ref type="bibr" target="#b8">[9]</ref>. As can be seen in Tab. 3, our method significantly and consistently improves segmentation metrics on all models, indicating that our finetuning indeed achieves its goal. <ref type="table">Table 2</ref>: Robustness evaluation on the synthetic SI-Score dataset <ref type="bibr" target="#b11">[12]</ref>, which tests changes in object position, rotation, and size using our method and the baseline methods GradMask <ref type="bibr" target="#b41">[41]</ref>, Right for the Right Reasons (RRR) <ref type="bibr" target="#b35">[36]</ref>. The models tested are ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and DeiT <ref type="bibr" target="#b49">[49]</ref>. "Annotated segmentation" indicates if we used annotated segmentation <ref type="bibr" target="#b15">[16]</ref> or unsupervised localization <ref type="bibr" target="#b52">[52]</ref>. "Original" stands for the model without finetuning.  <ref type="table">Table 3</ref>: Evaluation of segmentation performance from relevance maps on the ImageNet-segmentation dataset <ref type="bibr" target="#b20">[21]</ref> for ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and DeiT <ref type="bibr" target="#b49">[49]</ref> before and after finetuning with our method. Metrics and dataset are taken from <ref type="bibr" target="#b8">[9]</ref>. Comparing training classes to the other classes To ensure that the effects of our finetuning generalize to classes that were not included in the training set, we test the increase in robustness resulting from our method separately on the training classes and non-training classes. <ref type="figure" target="#fig_1">Fig. 4</ref> presents the average improvement across the base models of ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref>(AR), and DeiT <ref type="bibr" target="#b48">[48]</ref>.</p><formula xml:id="formula_7">Model ViT-B ViT-L AR-S AR-B AR-L DeiT-S DeiT-B Orig</formula><p>As can be seen, both subsets of classes demonstrate a very similar increase in accuracy for the robustness datasets, with the classes that belong to the training set demonstrating better performance on the datasets from the original ImageNet distribution (INet val, INet-v2), as can be expected due to the fact that they were represented in the training set. The full results of this experiment are presented in Appendix H.  Ablation Study We conduct an ablation study to test the effect of each of our loss terms on the result of the finetuning process, by studying the impact of removing each loss term L classification , L bg , L fg . Our ablation study is conducted on the base models of ViT and DeiT, as they demonstrate different advantages for each loss term. Additionally, we perform an ablation to test the choice of confidenceboosting as the classification loss (Eq. 4), by replacing it with the classic cross-entropy loss with the ground-truth label. For brevity, Tab. 4 presents the top-1 accuracy results for each version of our method; the complementary table for top-5 accuracy can be found in Appendix I. As can be seen, validation accuracy is lost mostly due to our background loss (Eq. 1), as when we remove it, the accuracy remains intact, and yet, as we hypothesized, it is more effective than the foreground loss (Eq. 2) in increasing the robustness, since when removing it, the accuracy on the out-of-distribution datasets drops significantly. Additionally, the DeiT ablation demonstrates that in some cases, the classification loss does not contribute to the increase in robustness (other than for INet-A), nor is it necessary for preserving the original ImageNet validation accuracy. However, for ViT, the classification loss is crucial for avoiding a significant loss of accuracy.</p><p>Finally, our ablation study demonstrates the benefit of using confidence boosting over the ground-truth for the classification loss. While the ground-truth variant preserves the original accuracy better (more so for ViT than for DeiT), using confidence boosting often improves robustness more significantly over using the ground-truth labels (see for example INet-A for ViT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and limitations</head><p>A surge of works have explored the benefits of using large-scale datasets of unlabeled samples from the internet, and many techniques have been proposed to train vision models in an unsupervised or self-supervised manner <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. As a result, it is increasingly important to develop methods for boosting model robustness without requiring any labels. We note that our method is compatible with such frameworks. As can be seen from Tab. 1, 2, our method performs well, even when applied with foreground masks obtained in an unsupervised manner, using Tokencut. Additionally, all of our losses can be applied without knowledge of the ground-truth label, since our classification loss L classification uses the predicted label, and the relevance maps could be propagated w.r.t. the predicted class.</p><p>Recently, it has been discovered that the effect of augmentation-based regularization is extremely uneven across classes <ref type="bibr" target="#b2">[3]</ref>. While such a regularization improves performance on average, some classes benefit significantly from it, while other classes suffer from a large drop in accuracy. This variation is also apparent in the effect of our method. Above we have established that the method helps both classes which are included in the training set of the finetuning and classes which are not. However, in both sets there is considerable variability in the effect of individual classes. This makes sense, since some classes present more well-localized objects and some classes require more reliance on the context of the object. See Appendix J for these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Can segmentation information help image categorization? Intuitively, the answer has to be positive. However, despite some effort to manually segment images of classification datasets, the obtained improvement, if any, is soon overtaken by better image-level methods.</p><p>Here, we propose a generic way to improve classification accuracy that can be applied to virtually any image classifier. Applied to transformers, we present evidence to show that while the accuracy on the original dataset does not improve, there is an increase in accuracy for out-of-distribution test sets. The method optimizes the relevancy maps directly based on intuitive desiderata. It opens a new way for improving accuracy using explainability methods, which are currently seldom used for improving downstream tasks other than seeding weakly supervised segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reinterpreting overinterpretation</head><p>One way to assess the salient behavior of a model is to study Sufficient Input Subsets (SIS), i.e., the minimal number of pixels necessary for a confident prediction <ref type="bibr" target="#b6">[7]</ref>. A gradient signal can be utilized to find the sufficient pixels <ref type="bibr" target="#b5">[6]</ref>. Finding an SIS for a class can imply that the classifier has overinterpreted its input since it can make a confident accurate decision using a small, sparse subset of pixels, which does not appear meaningful to humans.</p><p>We study the SIS with gradients approach for ViT models, and find that it can be misleading. Specifically, SIS can be regarded as an adversarial method that can lead to high-confidence classification of any label from a sparse set of pixels. <ref type="figure">Fig. 5</ref> demonstrates the resulting SIS pixels for 4 randomly selected images from the ImageNet validation set, with 5 randomly selected ImageNet labels-Gibbon, Black-widow, Common Iguana, Shovel, and Australian Terrier. As can be seen, we were able to find a subset of pixels for each random image and random label that made the classifier predict the random label with a confidence higher than 90%.</p><p>This can indicate that there is no guarantee that the classifier used the unnatural cues to decide for a given class based on the existence of a corresponding SIS, i.e. the subset of pixels considered an SIS is not necessarily indicative of the pixels used for the original decision, and resembles an adversarial attack in the sense that it can make the model predict unexpected outputs from a given input. Thus, we opt to use datasets designed specifically for testing robustness <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIS subset</head><p>Original  <ref type="figure">Figure 5</ref>: Examples of SIS subsets for 4 random images from the ImageNet validation set with 5 random ImageNet classes. As can be seen, an SIS subset can be found for all images with all classes with confidence higher than 90%, demonstrating that SIS subsets do not necessarily explain the prediction of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Background: explainability for Vision Transformers</head><p>The Vision Transformer architecture (ViT) splits an image i into n i fixed-size patches.</p><p>Our method employs the Generic Attention Explainability (GAE) method <ref type="bibr" target="#b7">[8]</ref> to create a relevance map for the input image tokens. In essence, GAE produces a relevance map? for each self-attention layer. The final relevance map is the result of aggregating all the self-attention layers' relevance maps into a single map R(i) for the entire network in a forward pass using matrix multiplication.</p><p>The aggregated map is initialized using identity matrix: R(i) := I ni . The relevance map of each self-attention layer of the Vision Transformer is calculated as follows:</p><formula xml:id="formula_8">?A := ?M(i) ?A ;? = E h ((?A A) + ),<label>(6)</label></formula><p>where A is the attention map of the current layer, is the Hadamard product, M is the Vision Transformer, M(i) is the logit that corresponds to the class we wish to visualize, and E h is the mean across the heads dimension. Put differently, the method integrates the gradients of the desired output logit w.r.t. the attention map, in order to average across the attention heads and produce a single unified attention relevance map. The unified attention map? is considered the relevance map of the attention layer.</p><p>Finally, to incorporate each layer's explainability map into the accumulated relevance maps the following propagation rule for self-attention layers is applied during a forward pass on the attention layers:</p><formula xml:id="formula_9">R(i) ? R(i) +? ? R(i),<label>(7)</label></formula><p>where? is the attention relevance map for the self-attention layers, which is calculated using Eq. 6.</p><p>To extract the relevance of each image token, the row of R(i) that corresponds to the [CLS] token is used since the [CLS] token alone determines the classification.</p><p>In this work, rather than using the relevance map as a form of explainability for a fixed model, we regularize the network to obtain the desired relevance map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline description</head><p>Both GradMask and RRR employ two loss functions. First, a classic cross-entropy loss with the ground-truth labels to ensure correct labeling, and second, gradients of the output of the model w.r.t. the input are used as a form of explanation, to limit the relevance scores of irrelevant parts of the input. This notion resembles our background loss (Eq. 1) with gradients as the relevance map.</p><p>In the following, we describe the loss terms applied in each method. For both methods, a standard cross-entropy loss with the ground truth class is applied to maintain accuracy, i.e.:</p><formula xml:id="formula_10">L classification = CE (M(i), y i ) ,<label>(8)</label></formula><p>where i is the input image, and y i is a one-hot vector, where the ground truth classification of i is assigned the value 1.</p><p>GradMask applies a gradient-based loss to ensure that the gradients of the background are close to 0:</p><formula xml:id="formula_11">L bg = ?? i ?i ?S(i) 2 ,<label>(9)</label></formula><p>where? i is the predicted output for the ground-truth class, andS(i) is, as before, the reversed segmentation map for the ground-truth class. Eq. 9 resembles our background loss (Eq. 1) with simple gradients w.r.t. the input image instead of the relevance map produced by GAE.</p><p>Similarly, Right for the Right Reasons (RRR) applies a loss to restrain the magnitude of explanations outside the relevant information. Their relevance loss is obtained as follows:</p><formula xml:id="formula_12">L bg = ? K k=1 log(p i,k ) ?i ?S(i) 2 ,<label>(10)</label></formula><p>where k = 1, ..., K are the possible output classes, and p i,k is the probability assigned to the k-th class for image i by the model.</p><p>For both GradMask and RRR, the following loss is used L final = ? bg ? L bg + ? classification ? L classification , where ? bg , ? classification are hyperparameters. We note that while using the gradient of the output w.r.t. the input is common practice for interpreting CNNs, these gradients are less stable for transformerbased models. For example, results presented in <ref type="bibr" target="#b27">[28]</ref> demonstrate that for transformer-based models the classic Input?Gradient method violates faithfulness.</p><p>In our experiments, we found it difficult to grid-search hyperparameters to fit L bg and L classification . Furthermore, we had to tune ? bg , ? classification for each model separately to obtain an improvement for L bg . Our method, on the other hand, uses the same hyperparameter choice (see Sec. 3), which makes it far more stable to use, thus allowing us to run experiments on large models as well. We refer the reader to Appendix D for the full description of hyperparameters used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters</head><p>In Tab. 5 we present the hyperparameter selection for all our experiments. Our method is stable and uses the same selection in all cases, other than the learning rate. The learning rates range from 6e ? 7 to 3e ? 6, allowing for a quick and easy grid search.</p><p>Tab. 6, 7 represent the hyperparameters of RRR, GradMask, respectively. For RRR, we had to tune the parameters per model, and the method is sensitive to the specific selection. For GradMask, we had to carefully tune the learning rate for each model. We found that the results of GradMask are sensitive to the specific learning rate selection, and to minor changes in the learning rate. We found it difficult to get the background loss to converge in both cases. <ref type="table">Table 5</ref>: Hyperparameter selection for our method for all models-ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref>, and DeiT <ref type="bibr" target="#b48">[48]</ref>. All hyperparameters are fixed except for the learning rate.</p><p>Model ? classification ? relevance ? bg ? fg Learning rate </p><formula xml:id="formula_13">ViT-B 0.2 0.8 2 0.3 3e ? 6 ViT-L 0.2 0.8 2 0.3 9e ? 7 AR-S 0.2 0.8 2 0.3 2e ? 6 AR-B 0.2 0.8 2 0.3 6e ? 7 AR-L 0.2 0.8 2 0.3 9e ? 7 DeiT-S 0.2 0.8 2 0.3 1e ? 6 DeiT-B 0.2 0.8 2 0.3 8e ? 7</formula><formula xml:id="formula_14">AR-S 2e ? 8 1e ? 8 1e ? 5 AR-B 2e ? 7 1e ? 8 5e ? 7 DeiT-S 2e ? 6 1e ? 10 1e ? 5</formula><p>DeiT-B 2e ? 6 1e ? 10 5e ? 6 As can be seen, the Standard Deviation is not large, especially in comparison to the performance gap. In all shifted-distribution datasets, the original result, before our intervention, is outside the standard error range for the multiple-seed experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional qualitative results</head><p>Fig <ref type="figure">. 6</ref> provides more examples of cases where the salient behavior of the models prevents them from producing correct predictions. For example, an artichoke is classified as a green mamba due to partial consideration of the foreground (third example in the first row), a bagel is classified as a horsechestnut due to the leaves in the background (third example in the third row), a racket is classified as a strainer due to the kitchen setting (second example in the second row), and a grasshopper is classified as a rock crab due to the rocks in the background (first example in the second row). By correcting the relevance maps, our method assists the models to achieve an accurate prediction.  <ref type="figure">Figure 6</ref>: Examples of cases where our method corrects wrong predictions, alongside the original and modified (after finetuning) explainability maps (please zoom in for a better view). The "Pred" row specifies the predictions before and after our finetuning. The examples demonstrate cases where the original classifier relies on partial or irrelevant data, while our method rectifies the classification to be based on the object. The examples are presented for the large and base models of ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and the base model of DeiT <ref type="bibr" target="#b48">[48]</ref>. <ref type="figure" target="#fig_2">Fig. 7</ref>, presents sensitivity tests evaluating our robustness results on increasing number of samples per class (panel a), and increasing number of classes (panel b). Evidently, three samples for half the classes suffice to achieve the maximal improvement in robustness, while minimally harming the performance on ImageNet-based datasets (ImageNet val, ImagNet-v2). Using only two samples or considerably fewer classes does not harm performance much. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INet-A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Sensitivity tests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Comparing training classes to the other classes</head><p>Tab. 10, 11 contain the full results of our experiment comparing the effect of our method on the classes in the training set and the classes outside of it. As can be seen, both the training and non-training classes benefit very similarly from applying our method when evaluated in terms of robustness on both real-world and synthetic datasets.  <ref type="table" target="#tab_2">Table 11</ref>: Robustness evaluation on the classes that were included in the training set of our finetuning and classes that were not for the synthetic datasets. The results are presented for the base models of ViT <ref type="bibr" target="#b12">[13]</ref>, ViT AugReg <ref type="bibr" target="#b45">[45]</ref> (AR), and DeiT <ref type="bibr" target="#b48">[48]</ref>. The last row indicates the average change for the training and non-training classes across the models for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Train SI-loc. SI-rot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SI-size classes R@1 R@5 R@1 R@5 R@1 R@5</head><p>ViT-B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Ablation study</head><p>Tab. 12 presents the top-5 accuracy results of our ablation study to complement Tab. 4. As can be seen, the top-5 results are consistent with the top-1 results from Tab. 4. <ref type="table" target="#tab_2">Table 12</ref>: Ablation study for our method on the ViT <ref type="bibr" target="#b12">[13]</ref> and DeiT <ref type="bibr" target="#b48">[48]</ref> base models. The table presents top-5 accuracy. We check the impact of each term of our loss on the robustness of the model, as well as the choice of confidence boosting versus cross-entropy with the ground-truth label (labeled as w/ ground-truth). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Per class robustness analysis</head><p>There is considerable variability in the effect of our method between the different classes. This is similar to the varying effect of regularization reported by Balestriero et al. <ref type="bibr" target="#b2">[3]</ref> and it is reasonable to assume that it is a common phenomenon among regularization techniques. <ref type="figure" target="#fig_0">Fig. 8-14</ref> depict the classes that benefited the most by our finetune process on ViT-B and those that were harmed the most. For datasets with 1, 000 classes, we present the 50 classes with the most beneficial or harmful change for readability. Other datasets are presented with all their classes.</p><p>Inspecting the classes with the largest amount of change, we can observe a few trends. In some cases, classes with relatively small objects considerably benefit, and classes with objects that either reside in the background or benefit from the context are harmed, as can be expected. In some cases, classes with a small number of test samples, regardless of the type of object, lead to a higher absolute change, due to statistical reasons.</p><p>In order to study the limitations of our method, we inspect the classes where it fails the most for From each class, we show the effect of our method on the first three samples in which a correct classification before finetuning turned into a wrong classification (so we avoid cherry-picking).</p><p>The pool table samples tend to show other objects in the foreground. In the top-left sample, the object is identified as a bucket (it is a hat, which is the 2nd top prediction). In the other two samples for pool tables and INet-A, the foreground object is correctly identified. In all three cases, the heatmap explains the result. In the breastplate case, there are only 11 test samples, two of which were wrongly classified by our method as other, reasonable classes.</p><p>For INet-v2, following our method, some breastplates are identified as cuirass (oxford dictionary: a piece of armor consisting of breastplate and backplate fastened together), which is a similar class. In the 2nd most affected class, miniature poodles are identified as toy poodles, or as a Lhasa Apso, which are similar dog breeds.        <ref type="figure" target="#fig_0">Figure 11</ref>: The effect of the finetune procedure on each class for ImageNet-R (there are some classes with zero effect). <ref type="figure" target="#fig_0">Figure 12</ref>: The 50 classes which most benefited and the 50 classes that were most harmed by the finetune procedure for ImageNet-sketch.   <ref type="figure" target="#fig_0">Figure 13</ref>: The effect of the finetune procedure on each class for SI-rotation (there are some classes with zero effect).   <ref type="figure" target="#fig_0">Figure 14</ref>: The effect of the finetune procedure on each class for SI-size (there are some classes with zero effect).  <ref type="figure" target="#fig_0">Figure 15</ref>: Examples of the 2 classes most harmed by our method for a dataset that is out of distribution (INet-A), and a dataset that has a similar distribution to ImageNet (INet-v2). For each dataset, we show the first three examples where our method modified a correct prediction and made it wrong (by showing the first three samples, we demonstrate that these samples are typical and not cherry-picked). As can be seen, in most cases the heatmaps are improved by our method, and the wrong prediction has a rationale (see text for examples). For INet-A there are only 2 mistakes in the Breastplate class (out of a total of 11 examples in the dataset), therefore the corresponding row (second row) only contains 2 examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of the salient issues with ViTs. Each pair depicts an input image and its corresponding relevance map. The first row demonstrates examples of background-centered relevance, the second row shows examples of sparse foreground relevance. Both issues occur in all models-ViT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>INet val INet-A INet-R Sketch INet-v2 ObjtNet SI-loc SI-rot SI-classes INet val INet-A INet-R Sketch INet-v2 ObjtNet SI-loc SI-rot SI-Evaluation of the average change produced by our method on the training classes and on the classes that were not in the training set. Changes are averaged across the base models of ViT [13], ViT AugReg [45], and DeiT [48]. (a) top-1 average change, (b) top-5 average change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Evaluation of our method's sensitivity to (a) the number of samples used per class in the training set (we use n = 3), and (b) number of training classes (we use c = 500), on ViT-B [13]. As can be seen, all the presented combinations of hyperparameters result in a significant increase in robustness, and a relatively modest decrease in INet val, INet-v2 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>The 50 classes which most benefited and the 50 classes that were most harmed by the finetune procedure for ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>The 50 classes which most benefited and the 50 classes that were most harmed by the finetune procedure for ImageNet-v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>30</head><label>30</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>The effect of the finetune procedure on each class for ImageNet-A (there are some classes with zero effect).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Robustness evaluation for ViT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>96.0 16.0 37.0 33.8 48.5 35.4 57.4 71.1 89.9 35.1 56.4 GradMask 81.8 96.1 17.5 39.8 34.5 49.4 35.8 57.8 71.4 90.5 36.7 58.2 RRR 81.9 96.2 18.9 41.9 34.8 49.7 35.8 57.8 71.4 90.5 38.1 60.0 Ours 80.3 95.4 24.1 48.0 36.3 51.4 36.2 58.5 70.0 89.4 42.2 65.1 13.7 35.1 31.6 47.4 32.9 54.2 70.3 90.1 35.1 56.7 Ours 79.8 95.7 18.2 40.6 33.9 50.2 33.5 55.4 69.6 90.0 38.7 61.1 Ours 80.3 95.8 19.1 42.2 33.8 49.7 33.8 55.5 69.6 90.1 39.3 61.7 97.2 23.9 49.2 41.0 57.8 43.1 65.7 73.8 92.3 41.4 63.7 GradMask 84.5 97.3 25.1 51.4 41.5 58.1 43.1 65.7 74.0 92.6 42.7 64.8 RRR 84.6 97.3 26.8 53.0 41.9 58.5 43.2 65.7 74.3 92.6 43.7 65.9 Ours 83.1 96.9 31.3 57.1 44.7 61.5 44.6 67.4 73.5 92.0 47.1 70.0 Ours 83.6 97.1 31.2 57.2 44.5 60.9 44.7 67.4 73.7 92.4 46.5 69.1 Original 85.6 97.8 34.7 61.0 48.8 64.9 51.8 73.6 75.8 93.4 46.5 68.3 AR-L Ours 85.1 97.5 42.1 67.5 54.0 69.1 54.2 75.8 75.8 93.4 51.6 73.2 Ours 85.4 97.6 42.4 68.0 53.8 69.0 54.1 75.8 76.1 93.6 52.0 73.5 .3 23.5 28.2 41.9 28.8 46.7 66.5 86.6 28.3 47.3 GradMask 77.0 93.6 7.9 24.7 26.6 40.5 26.0 43.5 64.5 85.6 28.2 48.6 RRR 78.1 94.1 9.0 26.9 26.9 40.6 26.9 44.4 66.0 86.7 29.3 49.9 Ours 78.6 94.5 10.1 29.0 29.3 43.6 29.1 47.8 67.3 87.3 31.6 53.0 Ours 78.6 94.4 11.0 30.3 29.9 44.4 29.4 48.0 67.1 87.4 31.6 52.9 94.2 12.9 31.0 30.9 44.2 31.2 48.6 69.7 86.8 31.4 48.5 GradMask 81.1 95.3 15.1 36.9 31.0 45.5 31.2 49.1 69.7 88.7 33.5 53.1 RRR 81.0 95.2 14.8 37.0 30.7 45.1 30.9 48.8 69.5 88.6 33.6 53.3 Ours 80.5 94.9 17.2 40.0 32.4 47.0 30.9 49.2 69.1 88.3 35.9 56.2 Ours 80.5 95.0 18.3 40.9 32.8 47.5 31.5 49.9 69.3 88.5 36.3 56.6</figDesc><table><row><cell></cell><cell></cell><cell>R@5</cell></row><row><cell>ViT-B</cell><cell cols="2">Original 81.5 Ours 80.4 95.4 23.0 45.7 35.4 50.0 35.8 58.2 69.8 89.4 40.8 64.0</cell></row><row><cell></cell><cell>Original</cell><cell>82.9 96.4 19.0 41.5 36.6 52.0 40.4 63.4 71.8 90.7 37.4 59.5</cell></row><row><cell>ViT-L</cell><cell>Ours</cell><cell>82.0 96.2 25.2 49.6 38.8 54.6 41.2 64.3 71.3 90.6 42.5 65.4</cell></row><row><cell></cell><cell>Ours</cell><cell>82.7 96.4 25.2 50.0 39.8 55.1 41.8 64.8 72.1 91.2 43.2 65.8</cell></row><row><cell>AR-S</cell><cell></cell><cell></cell></row><row><cell cols="3">AR-B 84.4 DeiT-S Original Original 78.1 93.7 8DeiT-B Original 80.8</cell></row></table><note>Original 81.4 96.1 13.0 33.9 31.2 47.1 32.8 54.2 69.9 90.1 34.3 55.8GradMask 81.3 96.1 16.4 39.2 32.3 48.3 32.5 53.7 70.1 90.3 37.6 60.2RRR 81.5 96.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(+1.3) 53.9 (+1.7) 40.7 (+1.6) 60.3 (+2.0) 57.0 (+1.4) 77.5 (+1.3) RRR 35.6 (+2.3) 55.0 (+2.8) 41.9 (+2.8) 61.8 (+3.5) 58.0 (+2.4) 78.4 (+2.2) Ours 38.6 (+5.3) 57.8 (+5.6) 46.2 (+7.1) 67.0 (+8.7) 61.0 (+5.4) 81.4 (+5.2) Ours 38.4 (+5.1) 57.0 (+4.8) 44.8 (+5.7) 65.2 (+6.9) 60.2 (+4.6) 80.6 (+4.4) (+1.9) 53.9 (+2.2) 43.3 (+2.7) 63.0 (+3.4) 58.0 (+2.6) 78.3 (+2.6) RRR 32.9 (+0.5) 52.3 (+0.6) 41.4 (+0.8) 60.6 (+1.0) 56.0 (+0.6) 76.3 (+0.6) Ours 36.8 (+4.4) 56.6 (+4.9) 47.6 (+7.0) 67.8 (+8.2) 61.3 (+5.9) 81.2 (+5.5) Ours 36.3 (+3.9) 55.6 (+3.9) 46.6 (+6.0) 66.7 (+7.1) 60.7 (+5.3) 80.4 (+4.7) (+1.0) 61.8 (+1.0) 49.3 (+1.2) 69.5 (+1.2) 61.4 (+0.8) 81.3 (+0.9) RRR 42.4 (+1.9) 62.7 (+1.9) 50.4 (+2.3) 70.7 (+2.4) 62.1 (+1.5) 82.0 (+1.6) Ours 43.2 (+2.7) 62.8 (+2.0) 54.0 (+5.9) 74.6 (+6.3) 64.1 (+3.5) 83.9 (+3.5) Ours 44.3 (+3.8) 64.0 (+3.2) 54.6 (+6.5) 74.7 (+6.4) 64.5 (+3.9) 84.6 (+4.2) +4.5) 68.5 (+4.3) 57.0 (+4.6) 77.2 (+4.7) 66.4 (+4.1) 86.0 (+3.8) Ours 47.4 (+3.6) 67.4 (+3.2) 58.0 (+5.6) 78.1 (+5.6) 66.5 (+4.2) 85.6 (+3.4) (+1.3) 50.7 (+0.3) 38.9 (+2.2) 56.7 (+2.4) 54.1 (+2.5) 74.0 (+2.0) RRR 32.0 (+1.3) 51.0 (+0.6) 38.5 (+1.8) 56.3 (+2.0) 53.9 (+2.3) 73.8 (+1.8) Ours 32.3 (+1.6) 51.5 (+1.1) 40.6 (+3.9) 59.4 (+5.1) 55.8 (+4.2) 76.3 (+4.3) Ours 32.5 (+1.8) 51.4 (+1.0) 41.0 (+4.3) 59.6 (+5.3) 56.0 (+4.4) 76.1 (+4.1) (+0.3) 39.1 (-0.2) 58.3 (+2.0) 55.2 (+0.6) 75.8 (+2.4) RRR 34.4 (-0.1) 55.2 (+0.6) 40.4 (+1.1) 58.5 (+2.2) 55.3 (+0.7) 75.8 (+2.4) Ours 36.6 (+2.1) 57.0 (+2.4) 42.9 (+3.6) 61.5 (+5.2) 58.0 (+3.4) 78.2 (+4.8) Ours 37.8 (+3.3) 58.1 (+3.5) 44.2 (+4.9) 62.7 (+6.4) 59.3 (+4.7) 79.0 (+5.6)</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell>Annotated segmentation</cell><cell cols="2">SI-location R@1 R@5</cell><cell cols="2">SI-rotation R@1 R@5</cell><cell>R@1</cell><cell>SI-size</cell><cell>R@5</cell></row><row><cell></cell><cell>Original</cell><cell></cell><cell>33.3</cell><cell>52.2</cell><cell>39.1</cell><cell>58.3</cell><cell>55.6</cell><cell>76.2</cell></row><row><cell>ViT-B</cell><cell cols="3">GradMask 34.6 Original 31.6</cell><cell>50.3</cell><cell>40.7</cell><cell>60.1</cell><cell>54.8</cell><cell>75.6</cell></row><row><cell>ViT-L</cell><cell>Ours</cell><cell></cell><cell cols="6">36.3 (+4.7) 56.2 (+5.9) 45.3 (+4.6) 66.2 (+6.1) 58.6 (+3.8) 80.3 (+4.7)</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell cols="6">36.7 (+5.1) 56.3 (+6.0) 45.3 (+4.6) 66.6 (+6.5) 59.1 (+4.3) 80.5 (+4.9)</cell></row><row><cell></cell><cell>Original</cell><cell></cell><cell>32.4</cell><cell>51.7</cell><cell>40.6</cell><cell>59.6</cell><cell>55.4</cell><cell>75.7</cell></row><row><cell cols="4">AR-S 34.3 AR-B GradMask Original 40.5 GradMask 41.5 Original 43.8</cell><cell>60.8 64.2</cell><cell>48.1 52.4</cell><cell>68.3 72.5</cell><cell>60.6 62.3</cell><cell>80.4 82.2</cell></row><row><cell cols="5">AR-L 48.3 (DeiT-S Ours Original 30.7 GradMask Original 34.5 32.0 DeiT-B GradMask 34.1 (-0.4) 54.9 50.4 54.6</cell><cell>36.7 39.3</cell><cell>54.3 56.3</cell><cell>51.6 54.6</cell><cell>72.0 73.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Ours Orig Ours Orig Ours Orig Ours Orig Ours Orig Ours Orig Ours Pixel acc. 76.3 82.1 73.4 82.5 76.7 83.3 76.6 81.2 65.2 78.9 78.7 80.8 79.0 81.3 mIoU 58.3 65.8 54.4 66.4 57.7 67.7 57.1 64.6 43.6 61.0 60.7 64.0 61.6 64.7 mAP 85.3 87.5 82.7 86.9 84.2 87.7 84.4 86.8 78.6 85.4 85.0 86.4 85.7 86.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Original</cell><cell>81.5</cell><cell>16.0</cell><cell>33.8</cell><cell>35.4</cell><cell>71.1</cell><cell>35.1</cell><cell>33.3</cell><cell>39.1</cell><cell>55.6</cell></row><row><cell>ViT-B</cell><cell>Ours w/o Lclassification (Eq. 4) w/o Lbg (Eq. 1)</cell><cell>80.3 77.8 81.5</cell><cell>24.1 17.9 19.1</cell><cell>36.3 34.2 35.9</cell><cell>36.2 34.8 36.4</cell><cell>70.0 67.4 71.4</cell><cell>42.2 37.6 39.7</cell><cell>38.6 37.2 34.9</cell><cell>46.2 43.0 42.3</cell><cell>61.0 58.4 58.4</cell></row><row><cell></cell><cell>w/o Lfg (Eq. 2)</cell><cell>80.2</cell><cell>24.1</cell><cell>34.3</cell><cell>35.2</cell><cell>69.6</cell><cell>41.8</cell><cell>39.2</cell><cell>45.6</cell><cell>60.7</cell></row><row><cell></cell><cell>w/ ground-truth</cell><cell>81.7</cell><cell>21.5</cell><cell>35.5</cell><cell>35.8</cell><cell>71.2</cell><cell>40.3</cell><cell>37.8</cell><cell>44.5</cell><cell>60.1</cell></row><row><cell></cell><cell>Original</cell><cell>80.8</cell><cell>12.9</cell><cell>30.9</cell><cell>31.2</cell><cell>69.7</cell><cell>31.4</cell><cell>34.5</cell><cell>39.3</cell><cell>54.6</cell></row><row><cell>DeiT-B</cell><cell>Ours w/o Lclassification (Eq. 4) w/o Lbg (Eq. 1)</cell><cell>80.5 81.0 81.0</cell><cell>17.2 15.5 13.2</cell><cell>32.4 33.3 30.5</cell><cell>30.9 32.1 30.2</cell><cell>69.1 69.8 69.6</cell><cell>35.9 36.1 33.0</cell><cell>36.6 39.0 32.7</cell><cell>42.9 44.0 39.7</cell><cell>58.0 58.3 54.5</cell></row><row><cell></cell><cell>w/o Lfg (Eq. 2)</cell><cell>80.3</cell><cell>17.9</cell><cell>32.6</cell><cell>31.0</cell><cell>69.0</cell><cell>35.8</cell><cell>37.3</cell><cell>43.0</cell><cell>58.2</cell></row><row><cell></cell><cell>w/ ground-truth</cell><cell>80.7</cell><cell>17.2</cell><cell>31.9</cell><cell>31.3</cell><cell>69.2</cell><cell>35.6</cell><cell>36.7</cell><cell>42.8</cell><cell>57.6</cell></row></table><note>Ablation study for our method on ViT [13] and DeiT [48] base models. The table presents top-1 accuracy. We check the impact of each term of our loss on robustness, as well as the choice of confidence boosting versus cross-entropy with the ground-truth label (labeled as w/ ground-truth).Model Method INet val INet-A INet-R Sketch INet-v2 ObjNet SI-loc. SI-rot. SI-size</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter selection for the Right for the Right Reasons<ref type="bibr" target="#b35">[36]</ref> method for all models-ViT<ref type="bibr" target="#b12">[13]</ref>, ViT AugReg<ref type="bibr" target="#b45">[45]</ref>, and DeiT<ref type="bibr" target="#b48">[48]</ref>. Hyperapramters vary according to the model.</figDesc><table><row><cell cols="2">Model ? classification</cell><cell>? bg</cell><cell>Learning rate</cell></row><row><cell>ViT-B</cell><cell>2e ? 6</cell><cell>1e ? 10</cell><cell>2e ? 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter selection for the GradMask<ref type="bibr" target="#b41">[41]</ref> method for all models-ViT<ref type="bibr" target="#b12">[13]</ref>, ViT AugReg<ref type="bibr" target="#b45">[45]</ref>, and DeiT<ref type="bibr" target="#b48">[48]</ref>. Random seed selection of the training classes Tab. 8, and Tab. 9 present the results for the main experiment for multiple seeds, extending Tab. 1 and Tab. 2 of the main text. Each seed changes the 500 random classes used for finetuning with exactly the same hyperparameters.</figDesc><table><row><cell cols="4">Model ? classification ? bg Learning rate</cell></row><row><cell>ViT-B</cell><cell>3e ? 9</cell><cell>50</cell><cell>0.0002</cell></row><row><cell>AR-S</cell><cell>3e ? 9</cell><cell>50</cell><cell>0.0005</cell></row><row><cell>AR-B</cell><cell>3e ? 9</cell><cell>50</cell><cell>1e ? 5</cell></row><row><cell>DeiT-S</cell><cell>3e ? 9</cell><cell>50</cell><cell>0.005</cell></row><row><cell>DeiT-B</cell><cell>3e ? 9</cell><cell>50</cell><cell>0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results using 3 random seeds. The table presents the average and standard deviation of the top-1 accuracy per dataset. 23.6?0.7 36.1?0.3 36.3?0.1 70.1?0.3 41.7?0.6 38.6?0.2 46.1?0.4 60.9?0.4 25.0?0.3 38.6?0.4 41.1?0.1 71.2?0.2 42.6?0.2 36.6?0.4 45.3?0.4 58.6?0.5 18.3?0.8 34.1?0.2 33.6?0.1 69.4?0.2 39.1?0.4 36.4?0.9 47.6?0.9 61.0?0.8 30.9?0.3 45.3?0.7 44.9?0.4 73.3?0.2 47.0?0.6 43.8?0.7 54.7?0.6 64.6?0.5 42.2?0.4 53.9?0.3 54.0?0.3 75.7?0.1 51.7?0.2 48.6?0.6 57.8?0.8 66.6?0.3 Ours 78.7?0.1 10.4?0.3 29.3?0.1 29.0?0.2 67.3?0.3 31.5?0.1 32.2?0.2 40.6?0.4 55.6?0.4 Ours 80.6?0.2 17.2?0.2 32.7?0.3 31.2?0.3 69.3?0.2 35.9?0.2 37.0?0.4 43.3?0.4 58.3?0.3</figDesc><table><row><cell cols="4">Model Method INet val INet-A</cell><cell>INet-R</cell><cell>Sketch</cell><cell>INet-v2</cell><cell>ObjNet</cell><cell>SI-loc.</cell><cell>SI-rot.</cell><cell>SI-size</cell></row><row><cell cols="3">Original Ours Original 80.3?0.1 ViT-L 81.5 ViT-B 82.9 Ours Original 81.4 82.0?0.1 AR-S Ours Original 84.4 79.8?0.2 AR-B Ours Original 85.6 83.0?0.1 AR-L Ours 84.8?0.3 DeiT-S Original 78.1</cell><cell>16.0 19.0 13.0 23.9 34.7 8.3</cell><cell>33.8 36.6 31.2 41.0 48.8 28.2</cell><cell>35.4 40.4 32.8 43.1 51.8 28.8</cell><cell>71.1 71.8 69.9 73.8 75.8 66.5</cell><cell>35.1 37.4 34.3 41.4 46.5 28.3</cell><cell>33.3 31.6 32.4 40.5 43.8 30.7</cell><cell>39.1 40.7 40.6 48.1 52.4 36.7</cell><cell>55.6 54.8 55.4 60.6 62.3 51.6</cell></row><row><cell>DeiT-B</cell><cell>Original</cell><cell>80.8</cell><cell>12.9</cell><cell>30.9</cell><cell>31.2</cell><cell>69.7</cell><cell>31.4</cell><cell>34.5</cell><cell>39.3</cell><cell>54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Results using 3 random seeds. The table presents the average and standard deviation of the top-5 accuracy per dataset. 47.1?0.8 51.3?0.2 58.7?0.3 89.4?0.1 64.7?0.5 57.5?0.3 66.6?0.5 81.2?0.3 49.2?0.6 54.6?0.3 64.3?0.1 90.6?0.3 65.5?0.2 56.4?0.3 66.3?0.5 80.3?0.3 40.9?1.1 50.2?0.4 55.3?0.1 89.9?0.1 61.8?0.6 56.1?0.8 67.6?0.8 81.2?0.5 56.8?0.3 62.0?0.7 67.9?0.6 91.9?0.1 69.9?0.5 63.5?0.8 75.1?0.6 84.4?0.5 Ours 97.4?0.1 67.3?0.6 69.2?0.4 75.7?0.2 93.3?0.1 73.5?0.4 68.8?0.7 78.1?0.8 86.3?0.3 Ours 94.5?0.1 29.0?0.5 43.6?0.0 47.6?0.4 87.4?0.1 53.0?0.1 51.3?0.2 59.3?0.5 76.1?0.3 Ours 95.0?0.1 40.1?0.2 47.4?0.4 49.7?0.4 88.4?0.1 56.3?0.2 57.4?0.4 61.9?0.5 78.4?0.3</figDesc><table><row><cell cols="4">Model Method INet val INet-A</cell><cell>INet-R</cell><cell>Sketch</cell><cell>INet-v2</cell><cell>ObjNet</cell><cell>SI-loc.</cell><cell>SI-rot.</cell><cell>SI-size</cell></row><row><cell cols="3">Original Ours Original 95.4?0.0 ViT-L 96.0 ViT-B 96.4 Ours Original 96.1 96.2?0.0 AR-S Ours Original 97.2 95.6?0.1 AR-B Ours 96.8?0.1 AR-L Original 97.8</cell><cell>37.0 41.5 33.9 49.2 61.0</cell><cell>48.5 52.0 47.1 57.8 64.9</cell><cell>57.4 63.4 54.2 65.7 73.6</cell><cell>89.9 90.7 90.1 92.3 93.4</cell><cell>56.4 59.5 55.8 63.7 68.3</cell><cell>52.2 50.3 51.7 60.8 64.2</cell><cell>58.3 60.1 59.6 68.3 72.5</cell><cell>76.2 75.6 75.7 80.4 82.2</cell></row><row><cell>DeiT-S</cell><cell>Original</cell><cell>93.7</cell><cell>23.5</cell><cell>41.9</cell><cell>46.7</cell><cell>86.6</cell><cell>47.3</cell><cell>50.4</cell><cell>54.3</cell><cell>72.0</cell></row><row><cell>DeiT-B</cell><cell>Original</cell><cell>94.2</cell><cell>31.0</cell><cell>44.2</cell><cell>48.6</cell><cell>86.8</cell><cell>48.5</cell><cell>54.6</cell><cell>56.3</cell><cell>73.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Robustness evaluation on the classes that were included in the training set of our finetuning and classes that were not. The results are presented for the base models of ViT<ref type="bibr" target="#b12">[13]</ref>, ViT AugReg<ref type="bibr" target="#b45">[45]</ref>, and DeiT<ref type="bibr" target="#b48">[48]</ref>. The last row indicates the average change for the training and non-training classes across the models for each dataset. Original 85.0 97.3 22.6 46.7 42.3 58.7 41.5 64.9 73.3 92.4 42.6 62.7 Ours 85.0 97.2 30.3 54.9 44.8 62.2 42.0 66.1 72.1 91.9 48.4 69.2 94.4 11.7 29.4 29.6 42.9 32.0 49.3 70.3 86.5 32.7 48.8 Ours 81.7 95.2 15.2 37.5 30.9 45.5 31.5 49.6 69.4 88.4 36.6 55.3 Original 79.9 94.0 14.1 32.5 32.4 45.7 30.4 48.0 69.0 87.0 29.8 48.1 Ours 79.3 94.7 19.1 42.5 33.9 48.5 30.3 48.8 68.9 88.3 34.8 57.3</figDesc><table><row><cell>Model</cell><cell>Train classes</cell><cell>INet val INet-A R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5 INet-R Sketch INet-v2 ObjNet</cell></row><row><cell></cell><cell></cell><cell>Original 82.0 96.0 14.8 35.2 32.9 47.4 36.8 58.4 71.3 89.8 36.0 55.8</cell></row><row><cell>ViT-B</cell><cell></cell><cell>Ours 82.0 95.7 22.9 46.1 35.5 50.5 38.1 59.7 71.4 89.9 43.5 64.6</cell></row><row><cell></cell><cell></cell><cell>Original 81.0 95.9 17.2 38.7 34.7 49.6 34.0 56.4 70.9 90.1 33.9 57.0</cell></row><row><cell></cell><cell></cell><cell>Ours 78.6 95.1 25.3 49.8 37.1 52.3 34.4 57.4 68.6 88.9 40.3 65.7</cell></row><row><cell></cell><cell></cell><cell>Original 83.8 97.2 25.3 51.6 39.8 56.9 44.7 66.5 74.4 92.3 39.8 64.9</cell></row><row><cell>AR-B</cell><cell></cell><cell>Ours 81.2 96.7 32.2 59.1 44.6 60.9 47.2 68.7 74.9 92.2 45.2 70.9</cell></row><row><cell cols="3">DeiT-B Original 81.7 Avg. 0 +0.1 +6.4 +9.1 +2.9 +3.2 +1.1 +1.3 +0.5 +0.6 +5.7 +7.3</cell></row><row><cell>change</cell><cell></cell><cell>-1.9 -0.2 +6.7 +9.5 +2.1 +3.0 +0.3 +1.0 -1.2 -0.1 +5.6 +8.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>INet-A and INet-V2. These two test sets represent different amounts of domain shift from the original ImageNet (INet-V2 is very close to ImageNet, while INet-A is out-of-distribution). As shown in the paper, our method increases robustness to domain shifts.Fig. 15 depicts the two classes, out of each of the two test sets (INet-A and INet-V2) that were harmed the most: pool table and breastplate for INet-A and breastplate and miniature poodle for INet-v2.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">Invariant risk minimization. stat</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The effects of regularization and data augmentation are class dependent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03632</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overinterpretation reveals image classification model pathologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><forename type="middle">W</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What made you do this? understanding black-box decisions with sufficient input subsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6970" to="6979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On robustness and transferability of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvan</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16453" to="16463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07781</idno>
		<title level="m">Pathologies of neural models make interpretations difficult</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Large-scale unsupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03149</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual score: What data modalities does your model perceive?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wichmann</surname></persName>
		</author>
		<idno>abs/2004.07780</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vision models are more robust and fair when pretrained on uncurated images without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Seessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno>abs/2202.08360</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet auto-annotation with segmentation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>K?ttel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="328" to="348" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unmasking clever hans predictors and assessing what machines really learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>W?ldchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking attentionmodel explainability through faithfulness violation test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2201.12114</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing deep convolutional neural networks using natural pre-images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="255" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep taylor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1897</idno>
		<title level="m">Deep neural networks are easily fooled: high confidence predictions for unrecognizable images</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Permuted adain: reducing the bias towards global statistics in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Nuriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9482" to="9491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03305</idno>
		<title level="m">The elephant in the room</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Right for the right reasons: Training differentiable models by constraining their explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno>abs/1703.03717</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Not using the car to see the sidewalkquantifying and controlling the effects of context in classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8218" to="8226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations. Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gradmask: Reduce overfitting by regularizing saliency. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Becks</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Don&apos;t judge an object by its context: learning to overcome contextual bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11070" to="11078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Full-gradient representation for neural network visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4126" to="4135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno>abs/2106.10270</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv&amp;apos;e J&amp;apos;egou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Becks</forename><surname>Viviano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00199</idno>
		<title level="m">Saliency is a possible red herring when diagnosing poor generalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-supervised transformers for unsupervised object discovery using normalized cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Vaufreydaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
