<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Singular Value Fine-tuning: Few-shot Segmentation requires Few-parameters Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu VIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu VIS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu VIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu VIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu VIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu VIS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Singular Value Fine-tuning: Few-shot Segmentation requires Few-parameters Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Freezing the pre-trained backbone has become a standard paradigm to avoid overfitting in few-shot segmentation. In this paper, we rethink the paradigm and explore a new regime: fine-tuning a small part of parameters in the backbone. We present a solution to overcome the overfitting problem, leading to better model generalization on learning novel classes. Our method decomposes backbone parameters into three successive matrices via the Singular Value Decomposition (SVD), then only fine-tunes the singular values and keeps others frozen. The above design allows the model to adjust feature representations on novel classes while maintaining semantic clues within the pre-trained backbone. We evaluate our Singular Value Fine-tuning (SVF) approach on various few-shot segmentation methods with different backbones. We achieve state-of-the-art results on both Pascal-5 i and COCO-20 i across 1-shot and 5-shot settings. Hopefully, this simple baseline will encourage researchers to rethink the role of backbone finetuning in few-shot settings. The source code and models will be available at https://github.com/syp2ysy/SVF. * Equal Contribution. ? Corresponding author. <ref type="bibr" target="#b2">3</ref> In this paper, we consider two settings of few-shot segmentation: 1-shot and 5-shot, which contain only one and five support images and masks, respectively.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Benefiting from the large amounts of annotated data, deep learning has achieved noticeable improvements in the field of semantic segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. In contrast, their performances dramatically degrade when novel classes arrive or label data is insufficient. Thus, few-shot segmentation (FSS) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b53">54]</ref> was proposed to address these challengs. In FSS, one needs to segment novel class objects in query images given only a few densely-annotated samples (i.e., support images and support masks). <ref type="bibr" target="#b2">3</ref> Due to the extremely limited data in FSS, over-fitting has become a critical problem that needs to be carefully handled.</p><p>One feasible solution is to restrict the model's learning capacity so that it can not overfit the small dataset. Most recent works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> follow this idea by freezing the pre-trained backbone. Then, different feature fusion methods and prototypes are introduced to enhance the generalization ability. Although this paradigm has achieved promising results, it is still suboptimal to directly adopt parameters pre-trained on image classification to image segmentation. The semantic clues contained in the pre-trained backbone can be irrelated to objects shown in support images, bringing unexpected obstacles to segmenting novel class objects in FSS.</p><p>In this paper, we rethink the paradigm of freezing the pre-trained backbone and show that finetuning a small part of parameters in the backbone is free from overfitting, leading to better model <ref type="bibr">(</ref>  <ref type="figure" target="#fig_0">Figure 1</ref>: Previous paradigm vs. SVF. (a) Previous paradigm introduces different segmentation heads based on the frozen pre-trained backbone. (b) SVF uses SVD to decompose the pre-trained parameters into three consecutive matrices, then only fine-tune the singular values and keep others frozen. Compared to the previous paradigm, SVF shows that fine-tuning a small part of parameters in the backbone is invulnerable to over-fitting, leading to better model generalization in learning novel classes.</p><p>generalization in learning novel classes. Our method is illustrated in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. First, to find such a small part of parameters for fine-tuning, we decompose pre-trained parameters into three successive matrices via the Singular Value Decomposition (SVD). Second, we then fine-tune the singular value matrices and keep others frozen. The above design, called Singular Value Fine-tuning (SVF), follows two principles: (i) maintaining rich semantic clues in the pre-trained backbone and (ii) adjusting feature map representations when learning to segment novel classes.</p><p>We evaluate our SVF on two few-shot segmentation benchmarks, Pascal-5 i and COCO-20 i . Extensive experiments show that SVF is invulnerable to overfitting and works well with various FSS methods using different backbones. It is significantly better than the freezing backbone counterpart, leading to new state-of-the-art results on both Pascal-5 i and COCO-20 i . Moreover, we provide quantitative and qualitative analyses on how singular values change during fine-tuning. Results show that SVF helps models focus more on the objects to be segmented instead of the noisy background. Our experiments highlight that proper backbone fine-tuning consistently outperforms backbone freezing on several leading methods. We hope our simple method will encourage researchers to rethink the role of backbone fine-tuning for few-shot segmentation.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot Segmentation</head><p>The purpose of few-shot segmentation is to segment the unseen class in query image with a few densely-annotated samples. In this task, a semantically rich representation and a nice matching approach have a particularly large impact on the results. Therefore, mainstream methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5]</ref> focus on obtaining excellent prototypes from support images, and obtaining accurate segmentation results by improving the quality of prototype features. CANet <ref type="bibr" target="#b49">[50]</ref>, PFENet <ref type="bibr" target="#b38">[39]</ref> and PANet <ref type="bibr" target="#b40">[41]</ref> filters class irrelevant information by global average pooling to obtain foreground and background prototypes. ASGNet <ref type="bibr" target="#b16">[17]</ref> pointed out that increasing the number of prototypes can further improve the segmentation results. CyCTR <ref type="bibr" target="#b51">[52]</ref> believes that pixel-level features in support image are important for segmentation tasks, and proposes to use pixel-level prototypes to predict query images. On the other hand, some methods focus on designing better matching methods to improve segmentation performance. SG-One <ref type="bibr" target="#b53">[54]</ref> uses cosine similarity to match prototype and query feature for segmentation results. CANet <ref type="bibr" target="#b49">[50]</ref> proposes an additive alignment module to iteratively refine the network output. HSNet <ref type="bibr" target="#b24">[25]</ref> exploits neighborhood consensus to disambiguate semantics by analyzing patterns of local neighborhoods in matching tensors. In addition to the above work, BAM <ref type="bibr" target="#b15">[16]</ref> utilizes the segmentation results of the base class to guide the generation of unseen classes, and achieves SOTA results. However, the above methods are all based on backbone freeze, and freezing backbone not only reduces the representational ability of the model, but also does not fit distribution to data better. Unlike previous work, in this paper we focus on the prospect of fine-tuning backbone in FSS. Therefore, instead of proposing a new model, we adopt the classic PFENet <ref type="bibr" target="#b38">[39]</ref> and BAM <ref type="bibr" target="#b15">[16]</ref> as our baselines. Our SVF enables these methods to further improve segmentation results.  <ref type="figure">Figure 2</ref>: The mIoU curve of PFENet <ref type="bibr" target="#b38">[39]</ref> with different fine-tune strategies on Pascal-5 i Fold-0. (a) is the result of freezing backbone, (b) and (c) represent the result of fine-tuning the entire backbone and SVF, respectively. Compared with the direct fine-tuning (b), SVF not only avoids the overfitting problem, but also brings positive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Backbone Fine-tuning</head><p>Fine-tuning backbone in downstream tasks has become a common approach in deep learning. The initial breakthroughs in vision tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref> were achieved by fine-tuning classification networks based ImageNet pre-trained weight, such as R-CNN <ref type="bibr" target="#b6">[7]</ref> for detection and FCN <ref type="bibr" target="#b21">[22]</ref> for segmentation. However, the direct application of fine-tuing the entire backbone in few-shot scenarios will lead to over-fitting of the model <ref type="bibr" target="#b33">[34]</ref>. Therefore, fine-tuning part parameters of the backbone in few-shot learning may avoid model over-fitting. P-Transfer <ref type="bibr" target="#b33">[34]</ref> utilizes NAS to search parameters of backbone that require fine-tune in few-shot classification tasks. However, this method is very complicated and cannot be directly applied to small sample segmentation. And some works <ref type="bibr" target="#b14">[15]</ref> borrow the idea of prompt <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref> in NLP to fine-tuning part parameters in the visual transformer <ref type="bibr" target="#b5">[6]</ref>. The above methods are proposed in a transformer-based model, but most few-shot segmentation models use CNN-based backbones. Applying prompt-based methods to various few-shot segmentation methods may need further adjustments. Different from the above methods, our SVF borrows the commonly used SVD <ref type="bibr" target="#b0">[1]</ref> in model compression and constructs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b54">55]</ref> a novel part fine-tune method for few-shot segmentation task. In addition, some approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref> introduce highly constrained subset of parameters to fine-tuning. However, these methods are not applied on few-shot segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">From Freezing Backbone to Singular Value Fine-tuning</head><p>In this section, we start with the preliminaries on the few-shot segmentation (FSS) setting. Then, we revisit the overfitting problem in FSS when fine-tuning the backbone in Section 3.1. In Section 3.2, we propose a novel Singular Value Fine-tuning (SVF) method for FSS instead of freezing the pre-trained backbone as proposed in previous methods. Section 3.3 provides a discussion on the differences between SVF and other fine-tuning methods. FSS Setup. Few-shot segmentation (FSS) aims to segment novel class objects given only a few densely-annotated samples. In this task, datasets are split into the training set (D train ) with base classes (C train ) and the testing set (D test ) with novel classes (C test ), where C train ? C test = ?.</p><p>Following previous works, we adopt episode training. Each episode consists of k support images and one query image to construct a k-shot segmentation task (k = 1 or k = 5 in this paper). Then, FSS methods are trained with episodes to segment novel class objects in the query image given the knowledge of k support images and support masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Model Overfitting in FSS</head><p>As presented in Section 1, model overfitting is a critical problem in extremely limited data scenarios (1-shot and 5-shot), especially when the model has large amounts of learnable parameters. We validate this problem and design experiments with a typical FSS method, PFENet <ref type="bibr" target="#b38">[39]</ref>. <ref type="figure">Figure 2</ref> shows that as model training moves on, fine-tuning backbone leads to better performance on the training dataset while it does not improve results on the validation set. It is a typical overfitting phenomenon. In contrast, freezing backbone can achieve steady improvements on the validation set during training.Therefore, existing methods in FSS turn to freezing the pre-trained backbone to avoid the overfitting problem.</p><p>Although this strategy has achieved promising results, it is clear that directly adopting an ImageNet pre-trained backbone to image segmentation can be suboptimal. One need to extract the most related semantic clues within the backbone instead of involving too much noise coming from the irrelevant categories learned from upstream tasks. In light of this, we rethink the paradigm of freezing the pre-trained backbone and try to find a new solution to the overfitting problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Singular Value Fine-tuning</head><p>According to the analysis above, fine-tuning all parameters in the backbone can be unsatisfactory. One feasible solution is to restrict the backbone's learning capacity so that it is suitable for the few-shot circumstance. Instead of freezing the whole pre-trained backbone as in previous works, we consider exploring a new regime, which is only fine-tuning a small part of the parameter in the backbone. However, it is nontrivial to find such a small part of parameters to be fine-tuned in the backbone. Simply splitting the backbone's parameters into learnable and freezing ones results in negative results. <ref type="table" target="#tab_7">Table 5</ref> and <ref type="table" target="#tab_8">Table 6</ref> show the inferior performances no matter splitted by layers or convolution types. We attribute this to the adjustment of the backbone, making the model biased towards base class objects shown in the training set, yet leads to worse model generalization in segmenting novel classes. <ref type="figure" target="#fig_1">Figure 3</ref> also provides evidences of the overfitting problem. Given that the backbone is pre-trained on a large-scale dataset with a classification task, it contains rich semantic clues but it is suboptimal to adopt for a segmentation task directly. We deliver two principles for finding a small part of fine-tuning parameters in the backbone: (i) maintaining rich semantic clues in the pre-trained backbone and (ii) adjusting feature map representations when learning to segment novel classes.</p><p>To fulfill the above goal, we resort to model compression methods in this paper. They are designed to approximate the original pre-trained model with fewer parameters, and also follow the above two principles. Among these methods, low-rank decomposition is a common technique to achieve model compression. It first splits the model weights into multiple subspaces and then compresses each subspace by shrinking its rank. We follow this direction and decompose the backbone parameters into subspaces via the Singular Value Decomposition (SVD). However, we do not shrink subspaces' ranks since our target is to find a small part of parameters to be fine-tuned instead of model compression.</p><p>In detail, for a convolution layer with C i input channels, C o output channels, and a kernel size of K ? K in the pre-trained backbone, we first fold its weight tensor W ? R Co?Ci?K?K into a matrix W ? R Co?CiK 2 , then decompose the obtained matrix by applying SVD with full-rank in subspaces (rank R = min(C o , C i K 2 )). Thus,</p><formula xml:id="formula_0">W = USV T , where U ? R Co?R , S ? R R?R , and V T ? R R?CiK 2 .<label>(1)</label></formula><p>The obtained pair of matrices V T and U construct two new convolution layers, and S is a diagonal matrix with singular values on the diagonal. Then back to convolutions, the results of the Equation 1 corresponds to three successive layers: a R ? C i ? K ? K convolution layer, a scaling layer, and followed by a C o ? R ? 1 ? 1 convolution layer (Algorithm 1 further provides a pseudo-code for SVF). We thus split each convolution layer in the pre-trained backbone into three functionalities: (i) decouple the semantic clues into a subspace with rank R, (ii) re-weight the semantic clues with singular values for the given task, and (iii) project the re-weighted clues back to the original space. Based on the interpretation above, we propose to fine-tune the scaling layer, which is Singular Value Fine-tuning (SVF). SVF does not erase the semantic clues contained in the pre-trained backbone, but it re-weights the representations to help adjust the model for new segmentation tasks. As SVF restricts the learnable parameters to only singular values, which are extremely few (0.25%) compared with the parameters in the whole backbone, SVF is less vulnerable to overfitting and shows better generalization capacity in learning novel classes (shown in <ref type="table" target="#tab_7">Table 5</ref>, <ref type="table" target="#tab_8">Table 6</ref>, <ref type="figure">Figure 2</ref>, and <ref type="figure" target="#fig_1">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion on Fine-tuning Methods</head><p>Fine-tuning pre-trained backbones is a promising way to achieve state-of-the-art results on downstream vision tasks. Many fine-tuning methods have been introduced to transfer pre-trained backbone's knowledge, such as full-model fine-tuning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">51]</ref>, task-specific fine-tuning (freezing the backbone) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref>, residual adapter <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42]</ref>, and bias tuning <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b7">8]</ref>. We compare our SVF with these methods. As presented in previous sections, methods like full-model fine-tuning may not be suitable with extremely limited data, and task-specific fine-tuning can not provide adjustment to the representations in the backbone for downstream tasks. Moreover, methods like residual adapter and bias tuning need prior knowledge to model structure or weights. SVF does not have the above limitations in the few-shot segmentation task.</p><p>Recently, a new fine-tuning method named Vision Prompt Tuning (VPT) <ref type="bibr" target="#b14">[15]</ref> has been proposed to fine-tune vision transformers. It introduces a small number of trainable parameters in the input space while keeping the backbone frozen. From this perspective, our SVF also introduces a small number of trainable parameters but in the singular value space. In SVF, the learned singular value diagonal matrix S can be formulated as a product of a frozen matrix S f rozen and a trainable matrix S trainable , which is S = S f rozen S trainable . We give a detailed explanation of this perspective in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on Pascal-5 i <ref type="bibr" target="#b32">[33]</ref> and COCO-20 i <ref type="bibr" target="#b26">[27]</ref> to discuss fine-tuning approach in FSS. In this section, we first introduce the used representative method and implementation details. Then we discuss the impact of different fine-tune methods on the FSS model, and finally verify the effectiveness and versatility of the proposed fine-tune method. , we remove images from training set containing novel classes of test to prevent potential information leakage. We give a detailed explanation of this setting about train sets in the Appendix.</p><p>Dataset Tricks: The previously methods annotated novel classes in the training set as background during training step. It become a common paradigm in few-shot segmentation. However, based on BAM <ref type="bibr" target="#b15">[16]</ref>, we found a novel dataset trick to improve the performance of FSS models. It simply removes images from the training set that contain the novel classes. For fair comparison with BAM, we use this trick in our experiments. However, we know that previously methods does not use this trick. Therefore, we present the experimental results with and without dataset trick in <ref type="table" target="#tab_2">Table 1</ref>, and more detailed fair comparison results in Appendix. Here, we hope that researchers can make fair comparison under the same setting.   Methods. To quickly verify the effectiveness of SVF, we propose a simple baseline method. Then, two representative methods are used to verify the generality of SVF. Next, we briefly introduce these three methods. Meanwhile, for fair comparison, we unify the three methods into the same framework.</p><p>? Baseline: We replace FEM module on PFENet <ref type="bibr" target="#b38">[39]</ref> with ASPP module to get the baseline method. The baseline only sets main loss, without adding auxiliary loss. ? PFENet <ref type="bibr" target="#b38">[39]</ref>: As a classic method in FSS, it proposed the Prior Guided Feature Enrichment Network, which has a huge impact on the subsequent FSS methods. ? BAM <ref type="bibr" target="#b15">[16]</ref>: As the state-of-the-art method in FSS, it represents the most cutting-edge results in FSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details.</head><p>In our experiments, we use VGG-16 <ref type="bibr" target="#b2">[3]</ref> and ResNet-50 <ref type="bibr" target="#b10">[11]</ref> as the backbone network and initialize it with ImageNet <ref type="bibr" target="#b29">[30]</ref> pre-trained weights. Due to the particularity of BAM, we use the initialization weights provided by author, and SVF does not finetuning base branch in BAM. We use SGD optimizer with cosine Learning rate decay <ref type="bibr" target="#b22">[23]</ref>, the learning rate 0.015 and the random seed 321 when fine-tuning backbone. We keep original settings for the model without fine-tuning. All models are trained 200 epochs on Pascal-5 i with batch size 8 and trained 50 epochs on COCO-20 i with batch size 8. Image is resized to 473 ? 473 on Pascal-5 i and 641 ? 641 on COCO-20 i . Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> , we adopt the mean intersection over union (mIoU) and foreground-background IoU (FB-IoU) as our evaluation metric. Since the middle-level features and high-level features are used in all FSS model, we set the SVF to fine-tuning the parameters of layers 2, 3, and 4 only. All model runs on four NVIDIA A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Art</head><p>In this section, the effectiveness of SVF is validated on three most representative methods. For fair comparison, we rerun all methods with unified framework. Then, we compare different methods with singular value fine-tuning and freeze backbone. The experimental results on Pascal-5 i are in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>It can be seen that the performance of different methods has been significantly improved after SVF. When the backbone is VGG-16, the SOTA method BAM improves miou by 0.46 and 0.35 in 1-shot   and 5-shot respectively after singular value fine-tuning. However, when the backbone is ResNet-50, SVF improves BAM by 1.14 and 1.37 mIoU on 1-shot and 5-shot, respectively. It shows that SVF bring better performance in deeper backbone. Meanwhile, <ref type="table" target="#tab_4">Table 2</ref> shows the effectiveness of SVF on more complex dataset COCO-20 i . Exspecially, SVF improves the performance of BAM by 2.24 and 2.71 mIoU on 1-shot and 5-shot. Furthermore, <ref type="table" target="#tab_5">Table 3</ref> shows the comparison results with FB-IoU on Pascal-5 i . Our SVF can also improve the performance of model. This experiment proves that SVF not only achieve state-of-the-art results, but also is a general method in FSS. In addition, the results (without ? ) in <ref type="table" target="#tab_2">Table 1</ref> prove that the dataset trick can indeed improve the performance of FSS model. It also shows that whether or not the dataset tricks is used does not affect the effectiveness of SVF. And we give more comparative results in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To verify the effectiveness of SVF, we conduct a series of ablation study in this section. We use the baseline method to conduct ablation study on Pascal-5 i 1-shot setting with ResNet-50 as the backbone network. Furthermore, we give the ablation study about hyperparameter in the Appendix.</p><p>Batch Normalization (BN): In <ref type="table" target="#tab_6">Table 4</ref>, we test the effect of BN on SVF. In the case of only finetuning the BN layer, the baseline will greatly reduce the performance. Next, we test SVF (fine-tune subspace S) on the baseline without fine-tuning BN. The results show that SVF achieves the best performance. Finally, we test the performance of the baseline method when fine-tuning subspace S and BN simultaneously. The results show that fine-tuning parameters of BN layer can cause performance of baseline to degrade. Therefore, we freeze the parameters of BN layer when using SVF.</p><p>Traditional fine-tune methods: In this part, we conduct experiments to verify the impact of traditional fine-tuning methods on the FSS model. Traditional fine-tuning methods can be divided into fully fine-tune and part fine-tune. The fully fine-tune method means to fine-tuning all the parameters in the backbone. The part fine-tune methods means to fine-tuning part parameters in the backbone, which includes layer-based and convolution-based fine-tune methods. In table 5, we conduct quantitative experiments with fully and layer-based fine-tune on baseline method. The results show that fully fine-tune brings negative results to baseline method. Meanwhile, we find that the negative results of fully fine-tuning method are mitigated as the number of fine-tuning layers is reduced. However, these methods do not have a positive impact on the baseline. In table 6, we conduct quantitative experiments on convolution-based fine-tune methods. For fair comparison, we only fine-tuning convolutions of 2, 3 and 4 layers. The results show that only fine-tuning 3 ? 3   <ref type="table">Table 9</ref>: Ablation study of different ways of changing semantic cues in weights on Pascal-5 i 1-shot.  convolution or 1 ? 1 convolution can further improve the performance of layer-based fine-tune methods. It show that traditional fine-tune methods cannot bring positive results. However, SVF brings positive results to the baseline method. The success of SVF proves that traditional fine-tune method destroys the rich semantic clues in the pre-trained backbone. In <ref type="figure" target="#fig_1">Figure 3</ref>, we compare the mIoU curves of the training and test sets in different fine-tune methods. The results show that part fine-tune method also produces the over-fitting problem. It proves that disrupting the rich semantic cues in pre-trained backbone will lead to model over-fitting, reducing model generalization. However, SVF solves the over-fitting problem without destroying semantic clues in pre-trained weight. And, it brings a new perspective for fine-tuning backbone.</p><p>Fine-tuning which subspace: To verify the influence of different sub-spaces on SVF, we conduct experiments on the subspace after SVD decomposition. The results are shown in <ref type="table" target="#tab_9">Table 7</ref>. We find that only fine-tuning S subspace brings positive results. Either fine-tuning the U or V subspace returns negative results. It shows that U and V contain rich semantic information in pre-trained weight after SVF. In other words, directly changing the feature distribution of the U or V subspace reduces the generalization ability of the model. To verify the above point, we test the performance of fine-tuning different subspace combinations. The results confirm that changing the distribution of U or V spaces brings negative results. The subspace S represents the weight distribution of different semantic cues. Therefore, fine-tuning the subspace S does not change the semantic cues of pre-trained weights. Meanwhile, adjusting the weights of different semantic cues enables model to better perform downstream tasks.</p><p>Fine-tuning which layers: Since the FSS model directly uses feature maps of layers 2, 3, and 4, we initially fine-tuning the subspace S of layers 2, 3 and 4. However, this setting is unreasonable. To verify which layer S have a greater impact on baseline, we conducted experiments on SVF under different layer combinations. The results are shown in <ref type="table" target="#tab_10">Table 8</ref>. It can be seen that fine-tuning layers 3 and 4 achieves the best performance, while only fine-tuning the layer 4 achieves the lowest performance. It shows that semantic clues in layer3 are the most important for FSS. Next we discuss the reasons why SVF can achieve better performance by visualizing semantic cues in layer3.</p><p>Compare with other parameter-efficient tuning methods: Unlike SVF, the purpose of parameterefficient tuning methods is to obtain performance similar with fully fine-tune by fine-tuning a small number of parameters. To verify the superiority of SVF over parameter-efficient tuning methods in FSS, we compare SVF with adapter <ref type="bibr" target="#b11">[12]</ref> and bias tuning on Pascal-5 i with the 1-shot setting. The details for adapter and bias tuning are given below:</p><p>? Adapter: Adapter is proposed in transformer-based models. When applying it into CNNbased backbone (ResNet), we make simple adjustments. We follow <ref type="bibr" target="#b11">[12]</ref> to build the adapter structures and add them after the stages in the ResNet. ? Bias Tuning: In the ResNet backbone, the convolution layers do not contain bias term. The bias terms that can be used for tuning is the ones in BN layers. We fine-tune the bias terms in all BN layers in this method. The experimental results are given in the table <ref type="bibr" target="#b12">13</ref>. It shows that SVF outperform Adapter and Bias Tuning by large margins. Moreover, we find that the introduction of Adapter will directly lead to over-fitting, while Bias Tuning reduces performance of the baseline model. The larger singular value in subspace S, the more important semantic cues in subspace U and V. We first focus on the changes of singular values during fine-tuning based on the initial distribution of S. In <ref type="figure">Figure 6</ref>, we visualize the variation of Top-30 singular values in pre-trained weights. It can be seen that the singular values of either 1 ? 1 or 3 ? 3 convolution change dramatically after fine-tuning. Next, we visualize the semantic cues of subspace U with the largest variation in singular values. The results are shown in <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref>. We only visualize the semantic cues where the singular value grows and decreases the most for a better view. Notice that semantic cues of decreasing singular values tend to focus on background regions. The semantic cues of increasing singular values always focus on foreground regions. The background-focused semantic cues in pre-trained backbone will damage the performance of FSS model. Since the original distribution of semantic cues in pre-trained backbone is not suitable for downstream tasks, SVF brings positive results to FSS model by increasing the weight of foreground cues and reducing the weight of background cues. It is also important to keep the semantic cues unchanged during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion on Why SVF Works</head><p>Overall, dynamically adjusting the weight of each cue without changing the semantic representation is the key to the success of SVF.</p><p>To verify that changes in the singular value space do not affect the semantic information in pretrained weight, we conduct an interesting experiment to intentionally changing semantic cues in weights. In <ref type="table">Table 9</ref>, We compare different approaches, including introducing a small number of training parameters S , and introducing a random rotation matrix R. It can be seen that changing the semantic cues in the weights negatively affects the FSS model (with or without fine-tuning a small number of parameters). Experimental results demonstrate that fine-tuning the singular value space is non-destructive (without destroy semantic cues). We give a detailed analysis about why SVF work in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Broader Impact</head><p>In this paper, we prove that freeze backbone is not the only paradigm in few-shot segmentation, fine-tune backbone is feasible. Meanwhile, we explore a new mechanism to redistribute the weights of different semantic cues without changing the semantic cues. As a new perspective of few-shot segmentation, it exposes the influence of pre-trained backbone on few-shot segmentation. Moreover, this mechanism not only works on few-shot, but also may be effective when fine-tune very large pre-trained models. This greatly reduces the cost of fine-tuning large models on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Limitations</head><p>Although the above experiments demonstrate the power of SVF, it still has some limitations. For instance, SVF introduces a small number of learning parameters, but the occupancy rate of memory resources is high during training process. Using SVF in ResNet-50 will occupy 16G video memory per image in COCO-20 i 5-shot setting. Furthermore, SVF increase a small amount of training time compared with freeze backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we rethink the paradigm of freezing backbone in FSS and propose a new paradigm Singular Value Fine-tuning (SVF) for fine-tuning backbone. Firstly, SVF decompose pre-trained parameters into three subspaces by SVD, and then only fine-tune the singular value. Our SVF dynamically adjusts the weights of different semantic cues without changing the rich semantic cues in pre-trained backbone. We evaluate the effectiveness of SVF on two commonly used benchmarks, Pascal-5 i and COCO-20 i . Extensive experiments prove that SVF as a new perspective to avoid overfitting and significantly improve the performance of various FSS methods. As a new paradigm of finetune, we will extend it to a variety of vision tasks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A More details</head><p>Training Strategy: Different from the training strategy of previous methods, we set the learning rate to 0.015 and use an SGD optimizer with cosine learning rate decay when fine-tuning the backbone. Therefore, we compared the impact of different training strategies on benchmark datasets. As shown in <ref type="table" target="#tab_2">Table 11</ref>, the new training strategy does not affect the performance of FSS models. Therefore, different training strategies are NOT the key to the success of SVF.  Training Tricks: Following the same setting of BAM <ref type="bibr" target="#b15">[16]</ref>, we remove some images containing novel classes of the test set from the training set. This is a novel trick in FSS to further improve the performance. In <ref type="table" target="#tab_2">Table 12</ref>, we compared the effect of this trick on FSS models. The results show that this trick brings 2.0 mIoU improvement over the original FSS model on average. Especially on Flod-2, the trend of improvement is very obvious. It proves that removing images with novel classes of the test set from the training set prevents potential information leakage.</p><p>Test image of COCO-20 i : We found that the number of test sets used in previous work was different when testing on COCO. For example, BAM <ref type="bibr" target="#b15">[16]</ref>, HSNet <ref type="bibr" target="#b25">[26]</ref> were tested with 1000 images, yet</p><p>Yang <ref type="bibr" target="#b46">[47]</ref> was tested with 4000 images, and CyCTR <ref type="bibr" target="#b51">[52]</ref> was tested with 5000 images. This is very detrimental to the development of the community. In <ref type="table" target="#tab_2">Table 14</ref>, we compare the different number of test images on COCO-20 i to observe changes in model performance. The experimental results show that as the number of test images increases, the performance of the baseline shows a downward trend. Therefore, we call on researchers to use the same training samples for a fair comparison. Meanwhile, SVF brings positive results in different numbers of test sets. It again shows the effectiveness of SVF.</p><p>B Compare with other methods.</p><p>To clear the doubts of dataset, we use the unprocessed training set to make a fair comparison with other SOTA methods, as show in Table15. It can be seen that baseline with SVF achieves best performance on both Pascal-5 i 1-shot and 5-shot settings. The experimental results prove that the advantages of SVF will not disappear due to the introduction of the training trick. Meanwhile, the experimental results prove that finetuning backbone is not only feasible in FSS, but also brings positive results to FSS models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Ablation Study</head><p>Different finetune strategy: In <ref type="figure" target="#fig_4">Figure 7</ref>, we visualize the mIoU curve of different fine-tuning strategies. It can be seen that both layer-based and convolution-based fine-tuning methods bring over-fitting problems. This result shows that traditional fine-tuning methods are not suitable for few-shot segmentation tasks. Directly fine-tuning the parameters of backbone in few-shot learning affects the robustness of FSS models. Therefore, we propose a novel fine-tuning strategy, namely SVF. It decompose pre-trained parameters into three successive matrices via the Singular Value Decomposition (SVD). Then, It only fine-tunes the singular value matrices during the training phase.</p><p>The experimental results show that SVF can effectively avoid over-fitting while bringing positive results to FSS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sigular value subspace:</head><p>In <ref type="figure" target="#fig_5">Figure 8</ref>, we visualize the changes of initial Top-30 largest singular values of all 3 ? 3 convolutional in layer 3 after SVF. The experimental results show that the change of last 3x3 convolution is the most obvious, and the change of singular value gradually moderates as the network becomes shallower. To verify the above point, we visualize the singular value change map of all 3x3 convolutions of layer 2 in <ref type="figure" target="#fig_7">Figure 9</ref>. The variation of singular values in layer2 is more gradual. Furthermore we visualize the singular value changes from the 1 ? 1 convolution of layer 3 and layer 2 in <ref type="figure" target="#fig_0">Figure 10</ref> and <ref type="figure" target="#fig_0">Figure 11</ref>. where the 1 ? 1 convolution is the last 1 ? 1 convolution of each block   in ResNet. This result is the same trend as 3 ? 3 convolution. It shown that the information concerned by deep convolutions in pre-train backbone is not conducive to few-shot segmentation tasks. SVF improves the expressiveness of FSS model by focusing on adjusting distribution of singular value subspace in the deep convolution. Meanwhile, It proves that semantic cues in deep convolutions have the greatest impact on few-shot segmentation. In addition, <ref type="figure" target="#fig_0">Figure 12</ref> shows the variation of all singular values. It can be easy seen that the change of singular values afterward tends to 0. Therefore, the change of top-30 singular values can describe the change of all singular values.</p><p>In <ref type="table" target="#tab_2">Table 16</ref>, <ref type="table" target="#tab_2">Table 17</ref>, <ref type="table" target="#tab_2">Table 18</ref>, <ref type="table" target="#tab_2">Table 19</ref> and Tbale 20, we give more detail ablation study results. It contains the results for each flod in different ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Discussion on other SVD</head><p>In this section, we discuss the differences between other SVD-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> and SVF. Both SVB <ref type="bibr" target="#b13">[14]</ref> and Hanie <ref type="bibr" target="#b31">[32]</ref> constrain the distribution of the singular values s where SVB <ref type="bibr" target="#b13">[14]</ref> forces the singular value around 1 and Hanie <ref type="bibr" target="#b31">[32]</ref> clamps the large singular values into a constant, hence serving - Layer3.0.conv3x3   Layer2.3.conv3x3   Layer3.0. conv1x1        as a regularization term. We did not pose an extra constraint on s, instead, encouraged the fully trainable singular values. As illustrated in SVB's <ref type="figure" target="#fig_0">Figure 1</ref>, the singular values of well-trained weights are widely spread around [0,2]. The strong regularization proposed in SVB <ref type="bibr" target="#b13">[14]</ref> and Hanie <ref type="bibr" target="#b31">[32]</ref> should damage the performance of pre-trained networks. Therefore, they turn to training from scratch, which is infeasible in the circumstance of few-shot segmentation. Our method coupled with pre-trained parameters can further exploit the capacity of the backbone, leading to superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Discussion on different implementation</head><p>In this section, we provide a discussion on our SVF. The main idea of SVF is learning to change singular values in the backbone weights. It has different implementations. We show two possible ways to achieve SVF in <ref type="figure" target="#fig_0">Figure 13</ref>: (i) treat the single value matrix S as trainable parameters directly;</p><p>(ii) freeze the original singular value matrix S and introduce another trainable singular value matrix S (we use exponential function exp to keep it positive and initialize it with zeros), where the final singular value matrix is a product of S (frozen) and S (trainable). In the second implementation, SVF keeps the backbone frozen (as all its weights are frozen) while introducing a small part of extra trainable parameters. It shares similarities with the recently proposed Visual Prompt Tuning (VPT) <ref type="bibr" target="#b14">[15]</ref>. The difference between VPT and SVF is that VPT introduces the trainable parameters in the input space while SVF introduces them in the singular value space. Although SVF and VPT freeze the original backbone, they can produce optimization on the feature maps of the backbone. This property enables SVF to perform better in few-shot segmentation (FSS) and is the essential difference from the properties in previous SSF methods with frozen backbone (they do not change the feature maps of the backbone).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Discussion on success of SVF</head><p>In this section, we discuss the truly responsible for the success of SVF from three question. First, Does fine-tune another small part of parameters in the backbone work? We conduct experiments on Pascal-5 i with the 1-shot setting. We compare our SVF with methods that only fine-tune the parameters in the BN layers. The results in <ref type="table" target="#tab_2">Table 21</ref> show that only fine-tuning the parameters in BN layers does not bring over-fitting in few-shot segmentation methods, but they perform worse than the conventional paradigm (freezing backbone). While our SVF outperform other methods by large margins.</p><p>Second, Is it really necessary to fine-tune the singular values? What if we introduce a new small part of parameters S', which is not in the singular value space, and only fine-tune the S'? To answer this question, we conduction two experiments, where the weight becomes S'W or WS', and only fine-tune the introduced small part of parameters S'. The results in <ref type="table" target="#tab_4">Table 22</ref> are consistence with <ref type="table" target="#tab_2">Table 21</ref>. Both of them can avoid over-fitting but show slightly worse performance than the freezing backbone baseline. The above experimental results suggest that fine-tuning a small part of parameters is a good way to avoid over-fitting when fine-tuning the backbone in few-shot segmentation. But it is non-trivial to find such a small part of parameters that can bring considerable improvements.</p><p>Third, What causes the differences between SVF and WS' or S'W? In this question, we try to provide our understanding of what causes the superior performances of SVF over WS' and S'W. We conjecture that this may be related to the context that S or S' can access when fine-tuning the parameters. Assume that W has the shape of [M, N ]. S and S' are diagonal matrices. S has the shape of [Rank, Rank], and S' has the shape of [M, M ] or [N, N ]. When optimizing the parameters, S' only has relations on dimension M or dimension N in a channel-wise manner, while S can connect all channels on both dimension M and dimension N, as S is in the singular value space. This differences can affect the received gradients when training S or S', which results in different performance. To give more evidences, we design more variants of SVF and provide their results in <ref type="table" target="#tab_4">Table 23</ref>.</p><p>Finaly, To verify whether SVF depends crucially on the singular value space, or simply on the number of effective updated parameters. we design a experiment: let R be a random rotation matrix, and set U=R' and V=RW, where W is the original weight matrix for the given layer. The formulation of the weight becomes RS'R'W. Note that S' is initialized with an identity matrix as done in previous experiments. During the fine-tuning, we only train S' while keep others frozen in the backbone. We provide the results in <ref type="table" target="#tab_4">Table 24</ref>. Random rotation formulation gives poor results. In fact, if we set R as an identity matrix (identity matrix is a rotation matrix), RS'R'W = S'W. As shown in the table, S'W is much better than random RS'R'W. It seems that the selection of the rotation matrix R is critical to the final segmentation performance. Meanwhile, If we consider RS'R' (it is a diagonal matrix in the initialization stage) as a whole, RS'R is only related to one dimension of the weight W. Thus for the middle matrix S', it is also channel-aligned with respect to weight W.</p><p>In addition, if R is random initialized, we can not guarantee that RS'R' is a diagonal matrix when updating S' during training (we verify this phenomenon with the saved checkpoints when we finish the training). Note that the weight W is the one from the pre-trained backbone, which contains semantic clues or learned knowledge. The non-diagonal matrix RS'R' may bring unexpected transformation to the pre-trained weight W, leading to poor results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode of SVF in Python style # Input: Conv2d with weight matrix W, Input feature X # Output: Output feature Y # Previous 3x3 Conv : # Y = F.Conv2d(W, X, kernel=(3,3)) # SVF : U, S, V = svd(W) # decompose weights by SVD U.requires_grad = False # freeze Conv_U V.requires_grad = False # freeze Conv_V Y = F.Conv2d(V, X, kernel=(3,3)) # a new 3x3 conv Y = Y.mul(S) # reconstruct a new affine layer Y = F.Conv2d(U, Y, kernel=(1,1)) # a new 1x1 conv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The mIoU curve of baseline with different finetune strategies on Pascal-5 i Fold-0. (a) represent the results of baseline with freezing backbone, (b) represent directly fine-tuning layers 2, 3 and 4, (c) represent fine-tuning all 1 ? 1 convolution layer in backbone, and (d) represent the proposed method SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>The visualization of segmentation cues with the largest variation in singular values from the last 3 ? 3 convolution in layer 3. (a) represents segmentation clues of subspace U with the largest singular value reduction, (b) represents segmentation clues of subspace U with the largest singular value growth. The visualization of segmentation cues with the largest variation in singular values from the last 1 ? 1 convolution in layer 3. (a) represents segmentation clues of subspace U with the largest singular value reduction, (b) represents segmentation clues of subspace U with the largest singular value growth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Figure 6 :</head><label>16</label><figDesc>Statistics chart about the changes of initial Top-30 largest singular values of the last 1 ? 1 and 3 ? 3 convolution layer in layer3 after fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The mIoU curve of baseline with different finetune strategies on Pascal-5 i Fold-0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Statistics chart about the changes of initial Top-30 largest singular values of the 3 ? 3 convolutional in layer3 after SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Statistics chart about the changes of initial Top-30 largest singular values of the 3 ? 3 convolutional in layer2 after SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Statistics chart about the changes of initial Top-30 largest singular values of the 1 ? 1 convolutional in layer3 after SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Statistics chart about the changes of initial Top-30 largest singular values of the 1 ? 1 convolutional in layer2 after SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Statistics chart about the changes of all singular values of the last 3 ? 3 and 1 ? 1 convolutional in layer3 after SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Different implementations of SVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance on Pascal-5 i<ref type="bibr" target="#b32">[33]</ref> in terms of mIoU for 1-shot and 5-shot segmentation. The best mean results are show in bold. ? indicates that images from training set containing the novel class on test set were removed.</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell cols="4">1-shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell><cell cols="4">5-shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell></row><row><cell>baseline  ?</cell><cell></cell><cell>57.48</cell><cell>66.72</cell><cell>62.66</cell><cell>53.72</cell><cell>60.15</cell><cell>62.98</cell><cell>70.57</cell><cell>68.62</cell><cell>59.60</cell><cell>65.44</cell></row><row><cell>baseline  ? +SVF</cell><cell></cell><cell>63.07</cell><cell>68.40</cell><cell>65.81</cell><cell cols="2">54.28 62.89 (+2.74)</cell><cell>68.52</cell><cell>72.15</cell><cell>69.08</cell><cell cols="2">63.59 68.34 (+2.90)</cell></row><row><cell>PFENet  ? [39] PFENet  ? +SVF</cell><cell>VGG16</cell><cell>61.91 63.43</cell><cell>70.34 71.40</cell><cell>63.77 64.18</cell><cell cols="2">57.38 58.30 64.33 (+0.98) 63.35</cell><cell>67.73 69.11</cell><cell>72.82 73.67</cell><cell>69.31 69.13</cell><cell cols="2">67.59 67.30 69.80 (+0.44) 69.36</cell></row><row><cell>BAM  ? [16]</cell><cell></cell><cell>63.18</cell><cell>70.77</cell><cell>66.14</cell><cell>57.53</cell><cell>64.41</cell><cell>67.36</cell><cell>73.05</cell><cell>70.61</cell><cell>64.00</cell><cell>68.76</cell></row><row><cell>BAM  ? +SVF</cell><cell></cell><cell>64.09</cell><cell>71.07</cell><cell>66.79</cell><cell cols="2">57.54 64.87 (+0.46)</cell><cell>67.75</cell><cell>74.11</cell><cell>70.99</cell><cell cols="2">63.57 69.11 (+0.35)</cell></row><row><cell>baseline  ?</cell><cell></cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell>60.27</cell><cell>65.07</cell><cell>69.89</cell><cell>74.16</cell><cell>67.87</cell><cell>65.73</cell><cell>69.41</cell></row><row><cell>baseline  ? +SVF</cell><cell></cell><cell>67.42</cell><cell>71.57</cell><cell>67.99</cell><cell cols="2">61.57 67.14 (+2.07)</cell><cell>70.37</cell><cell>75.06</cell><cell>71.08</cell><cell cols="2">69.16 71.42 (+2.01)</cell></row><row><cell>baseline</cell><cell></cell><cell>66.36</cell><cell>69.22</cell><cell>57.64</cell><cell>58.73</cell><cell>62.99</cell><cell>70.75</cell><cell>72.92</cell><cell>58.86</cell><cell>65.56</cell><cell>67.02</cell></row><row><cell>baseline + SVF</cell><cell></cell><cell>66.88</cell><cell>70.84</cell><cell>62.33</cell><cell cols="2">60.63 65.17 (+2.18)</cell><cell>71.49</cell><cell>74.04</cell><cell>59.38</cell><cell cols="2">67.43 68.09 (+1.07)</cell></row><row><cell>PFENet  ? [39]</cell><cell></cell><cell>66.61</cell><cell>72.55</cell><cell>65.33</cell><cell>60.91</cell><cell>66.35</cell><cell>70.93</cell><cell>75.32</cell><cell>69.60</cell><cell>68.96</cell><cell>71.20</cell></row><row><cell>PFENet  ? +SVF PFENet</cell><cell>ResNet50</cell><cell>69.27 67.06</cell><cell>73.55 71.61</cell><cell>67.49 55.21</cell><cell cols="2">62.30 68.15 (+1.80) 59.46 63.34</cell><cell>71.82 72.11</cell><cell>74.92 73.67</cell><cell>70.97 61.61</cell><cell cols="2">69.58 71.82 (+0.62) 67.50 68.72</cell></row><row><cell>PFENet + SVF</cell><cell></cell><cell>68.31</cell><cell>71.99</cell><cell>56.25</cell><cell cols="2">61.82 64.59 (+1.25)</cell><cell>72.09</cell><cell>73.99</cell><cell>63.58</cell><cell cols="2">70.03 69.92 (+1.20)</cell></row><row><cell>BAM  ? [16]</cell><cell></cell><cell>68.97</cell><cell>73.59</cell><cell>67.55</cell><cell>61.13</cell><cell>67.81</cell><cell>70.59</cell><cell>75.05</cell><cell>70.79</cell><cell>67.20</cell><cell>70.91</cell></row><row><cell>BAM  ? +SVF</cell><cell></cell><cell>69.38</cell><cell>74.51</cell><cell>68.80</cell><cell cols="2">63.09 68.95 (+1.14)</cell><cell>72.05</cell><cell>76.17</cell><cell>71.97</cell><cell cols="2">68.91 72.28 (+1.37)</cell></row><row><cell>BAM</cell><cell></cell><cell>68.37</cell><cell>72.05</cell><cell>57.55</cell><cell>60.38</cell><cell>64.59</cell><cell>70.72</cell><cell>74.21</cell><cell>63.58</cell><cell>66.18</cell><cell>68.67</cell></row><row><cell>BAM + SVF</cell><cell></cell><cell>68.17</cell><cell>72.86</cell><cell>57.77</cell><cell cols="2">62.04 65.21 (+0.62)</cell><cell>72.30</cell><cell>74.43</cell><cell>65.16</cell><cell cols="2">69.43 70.33 (+1.66)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance on COCO-20 i<ref type="bibr" target="#b26">[27]</ref> in terms of mIoU for 1-shot and 5-shot segmentation. The best mean results are show in bold. ? indicates that images from training set containing the novel class on test set were removed.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">1-shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell><cell cols="4">5-shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell></row><row><cell>baseline  ?</cell><cell></cell><cell>37.56</cell><cell>37.73</cell><cell>38.78</cell><cell>35.66</cell><cell>37.43</cell><cell>43.07</cell><cell>49.42</cell><cell>44.38</cell><cell>46.38</cell><cell>45.81</cell></row><row><cell>baseline  ? +SVF</cell><cell></cell><cell>39.32</cell><cell>39.64</cell><cell>38.63</cell><cell cols="2">35.45 38.26 (+0.83)</cell><cell>46.48</cell><cell>50.72</cell><cell>45.79</cell><cell cols="2">45.63 47.16 (+1.35)</cell></row><row><cell>PFENet [39]</cell><cell></cell><cell>35.40</cell><cell>38.10</cell><cell>36.80</cell><cell>34.70</cell><cell>36.30</cell><cell>38.20</cell><cell>42.50</cell><cell>41.80</cell><cell>38.90</cell><cell>40.40</cell></row><row><cell>PFENet  ? [39]</cell><cell>VGG-16</cell><cell>41.03</cell><cell>44.22</cell><cell>43.74</cell><cell>38.90</cell><cell>41.97</cell><cell>48.66</cell><cell>48.26</cell><cell>45.49</cell><cell>51.02</cell><cell>48.36</cell></row><row><cell>PFENet  ? +SVF</cell><cell></cell><cell>42.68</cell><cell>44.90</cell><cell>42.60</cell><cell cols="2">38.79 42.24 (+0.27)</cell><cell>49.02</cell><cell>53.71</cell><cell>47.59</cell><cell cols="2">47.63 49.49 (+1.13)</cell></row><row><cell>BAM  ? [16]</cell><cell></cell><cell>38.96</cell><cell>47.04</cell><cell>46.41</cell><cell>41.57</cell><cell>43.50</cell><cell>47.02</cell><cell>52.62</cell><cell>48.59</cell><cell>49.11</cell><cell>49.34</cell></row><row><cell>BAM  ? +SVF</cell><cell></cell><cell>40.21</cell><cell>46.62</cell><cell>46.23</cell><cell cols="2">41.97 43.76 (+0.26)</cell><cell>45.05</cell><cell>53.59</cell><cell>48.35</cell><cell cols="2">49.28 49.07 (?0.27)</cell></row><row><cell>baseline  ?</cell><cell></cell><cell>38.91</cell><cell>46.07</cell><cell>42.67</cell><cell>39.71</cell><cell>41.84</cell><cell>50.35</cell><cell>56.78</cell><cell>49.61</cell><cell>50.96</cell><cell>51.93</cell></row><row><cell>baseline  ? +SVF</cell><cell></cell><cell>44.22</cell><cell>46.38</cell><cell>42.65</cell><cell cols="2">41.65 43.72 (+1.88)</cell><cell>51.47</cell><cell>57.48</cell><cell>50.33</cell><cell cols="2">52.29 52.89 (+1.93)</cell></row><row><cell>PFENet  ? [39] PFENet  ? +SVF</cell><cell>ResNet-50</cell><cell>44.93 46.88</cell><cell>50.32 50.86</cell><cell>44.68 47.69</cell><cell cols="2">44.26 46.64 48.02 (+1.97) 46.05</cell><cell>52.29 52.72</cell><cell>59.34 58.14</cell><cell>51.50 52.52</cell><cell cols="2">53.53 54.15 54.38 (+0.21) 54.17</cell></row><row><cell>BAM  ? [16]</cell><cell></cell><cell>43.41</cell><cell>50.59</cell><cell>47.49</cell><cell>43.42</cell><cell>46.23</cell><cell>49.26</cell><cell>54.20</cell><cell>51.63</cell><cell>49.55</cell><cell>51.16</cell></row><row><cell>BAM  ? +SVF</cell><cell></cell><cell>46.87</cell><cell>53.80</cell><cell>48.43</cell><cell cols="2">44.78 48.47 (+2.24)</cell><cell>52.25</cell><cell>57.83</cell><cell>51.97</cell><cell cols="2">53.41 53.87 (+2.71)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance on Pascal-5 i<ref type="bibr" target="#b32">[33]</ref> in terms of FB-IoU for 1-shot and 5-shot segmentation.</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell>FB-IoU (%) 1-shot 5-shot</cell></row><row><cell>baseline  ?</cell><cell></cell><cell>77.11 80.56</cell></row><row><cell>baseline  ? +SVF</cell><cell></cell><cell>78.86 82.66</cell></row><row><cell>PFENet  ? PFENet  ? +SVF</cell><cell>ResNet-50</cell><cell>77.35 82.30 79.07 82.77</cell></row><row><cell>BAM  ?</cell><cell></cell><cell>81.10 82.18</cell></row><row><cell>BAM  ? +SVF</cell><cell></cell><cell>80.13 83.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of BN on Pascal-5 i under 1-shot setting. represents fine-tuning this feature space. The best mean results are show in bold.</figDesc><table><row><cell>Method BN S</cell><cell>Mean</cell></row><row><cell>--</cell><cell>65.07</cell></row><row><cell>baseline  ?</cell><cell>63.12 (?1.95) 64.20 (?0.87)</cell></row><row><cell></cell><cell>67.14 (+2.07)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparative experiment with fine-tuning different layer of backbone on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell>layer</cell><cell>Mean</cell></row><row><cell>baseline  ?</cell><cell>-</cell><cell>65.07</cell></row><row><cell cols="3">+fully fine-tune 1, 2, 3, 4 60.90 (?4.17)</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell>61.15 (?3.92)</cell></row><row><cell>+ part fine-tune</cell><cell>3, 4</cell><cell>61.08 (?3.99)</cell></row><row><cell></cell><cell>4</cell><cell>60.58 (?4.49)</cell></row><row><cell>+SVF</cell><cell>2, 3, 4</cell><cell>67.14 (+2.07)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparative experiment with fine-tuning different convolutional layer of backbone on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell cols="3">layer 3 ? 3 1 ? 1</cell><cell>Mean</cell></row><row><cell>baseline  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.07</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell></cell><cell></cell><cell>61.15 (?3.92)</cell></row><row><cell>+part fine-tune</cell><cell>2, 3, 4</cell><cell></cell><cell></cell><cell>61.86 (?3.21)</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell></cell><cell></cell><cell>61.62 (?3.45)</cell></row><row><cell>+SVF</cell><cell>2, 3, 4</cell><cell>-</cell><cell>-</cell><cell>67.14 (+2.07)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of SVF finetuning different subspace on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell>U</cell><cell>S</cell><cell>V</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.88</cell></row><row><cell>baseline  ?</cell><cell></cell><cell></cell><cell></cell><cell>61.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.42</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of SVF fine-tuning different layer on Pascal-5 i . The best results are show in bold.</figDesc><table><row><cell>Method</cell><cell>layer</cell><cell>Mean</cell></row><row><cell></cell><cell>4</cell><cell>66.21</cell></row><row><cell>baseline  ? + SVF</cell><cell>3, 4 2, 3, 4</cell><cell>67.20 67.14</cell></row><row><cell></cell><cell cols="2">1, 2, 3, 4 67.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table><row><cell cols="3">Compare with parameter-efficient</cell></row><row><cell cols="3">tuning methods on Pascal-5 i 1-shot.</cell></row><row><cell cols="3">Method fine-tune method Mean</cell></row><row><cell></cell><cell cols="2">freeze backbone 65.07</cell></row><row><cell></cell><cell>fully fine-tune</cell><cell>60.90</cell></row><row><cell>baseline</cell><cell>SVF</cell><cell>67.14</cell></row><row><cell></cell><cell>Adapter</cell><cell>20.71</cell></row><row><cell></cell><cell>bias tuning</cell><cell>62.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Compare with different training strategy on Pascal-5 i training set in terms of mIoU for 1-shot segmentation.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone Training Strategy</cell><cell cols="4">1-shot Fold-0 Fold-1 Fold-2 Fold-3 Mean</cell></row><row><cell>baseline</cell><cell></cell><cell>original</cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell>60.27 65.07</cell></row><row><cell>baseline</cell><cell></cell><cell>ours</cell><cell>64.95</cell><cell>69.75</cell><cell>65.91</cell><cell>59.59 65.05</cell></row><row><cell>PFENet [39] PFENet [39]</cell><cell>ResNet50</cell><cell>original ours</cell><cell>66.61 65.58</cell><cell>72.55 72.49</cell><cell>65.33 66.12</cell><cell>60.91 66.35 60.30 66.12</cell></row><row><cell>BAM [16]</cell><cell></cell><cell>original</cell><cell>68.97</cell><cell>73.59</cell><cell>67.55</cell><cell>61.13 67.81</cell></row><row><cell>BAM [16]</cell><cell></cell><cell>ours</cell><cell>68.43</cell><cell>73.66</cell><cell>67.98</cell><cell>61.63 67.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Ablation study on the training trick.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone Training Trick</cell><cell cols="5">1-shot Fold-0 Fold-1 Fold-2 Fold-3 Mean</cell></row><row><cell>baseline</cell><cell></cell><cell>w/o</cell><cell>66.36</cell><cell>69.22</cell><cell>57.64</cell><cell cols="2">58.73 62.99</cell></row><row><cell>baseline</cell><cell></cell><cell>w</cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell cols="2">60.27 65.07</cell></row><row><cell>PFENet [39]</cell><cell></cell><cell>w/o</cell><cell>67.06</cell><cell>71.61</cell><cell>55.21</cell><cell cols="2">59.46 63.34</cell></row><row><cell>PFENet [39] CyCTR [52]</cell><cell>ResNet50</cell><cell>w w/o</cell><cell>66.61 67.80</cell><cell>72.55 72.80</cell><cell>65.33 58.00</cell><cell cols="2">60.91 66.35 58.00 64.20</cell></row><row><cell>CyCTR [52]</cell><cell></cell><cell>w</cell><cell>65.17</cell><cell>72.52</cell><cell>66.60</cell><cell>60.9</cell><cell>66.30</cell></row><row><cell>BAM [16]</cell><cell></cell><cell>w/o</cell><cell>68.37</cell><cell>72.05</cell><cell>57.55</cell><cell cols="2">60.38 64.59</cell></row><row><cell>BAM [16]</cell><cell></cell><cell>w</cell><cell>68.97</cell><cell>73.59</cell><cell>67.55</cell><cell cols="2">61.13 67.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>compare with parameter-efficient tuning methods on Pascal-5 i 1-shot.</figDesc><table><row><cell>Method</cell><cell>fine-tune method</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Mean</cell></row><row><cell></cell><cell>freeze backbone</cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell>60.27</cell><cell>65.07</cell></row><row><cell>baseline</cell><cell>SVF Adapter</cell><cell>67.42 18.41</cell><cell>71.57 20.21</cell><cell>67.99 26.62</cell><cell>61.57 17.62</cell><cell>67.14 20.71</cell></row><row><cell></cell><cell>bias tuning</cell><cell>61.62</cell><cell>70.10</cell><cell>64.80</cell><cell>55.19</cell><cell>62.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Compare with different test image on COCO-20 i in terms of mIoU for 1-shot segmentation.</figDesc><table><row><cell>Method</cell><cell cols="2">backbone test image</cell><cell>1-shot Fold-0 Fold-1 Fold-2 Fold-3 Mean</cell></row><row><cell>baseline</cell><cell></cell><cell>1000</cell><cell>38.91 46.07 42.67 39.71 41.84</cell></row><row><cell>baseline + SVF</cell><cell></cell><cell>1000</cell><cell>44.22 46.38 42.65 41.65 43.72</cell></row><row><cell>baseline baseline + SVF</cell><cell>ResNet50</cell><cell>4000 4000</cell><cell>37.19 45.30 42.90 38.49 40.97 39.80 46.99 42.51 42.06 42.84</cell></row><row><cell>baseline</cell><cell></cell><cell>5000</cell><cell>36.59 45.17 43.34 38.73 40.96</cell></row><row><cell>baseline + SVF</cell><cell></cell><cell>5000</cell><cell>39.49 46.95 42.09 41.15 42.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Compare with SOTA on Pascal-5 i<ref type="bibr" target="#b32">[33]</ref> in terms of mIoU for 1-shot and 5-shot segmentation.</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell cols="7">1-shot Fold-0 Fold-1 Fold-2 Fold-3 Mean Fold-0 Fold-1 Fold-2 Fold-3 Mean 5-shot</cell></row><row><cell>PANet [41]</cell><cell></cell><cell>44.00</cell><cell>57.50</cell><cell>50.80</cell><cell>44.00 49.10 55.30</cell><cell>67.20</cell><cell>61.30</cell><cell>53.20 59.30</cell></row><row><cell>CANet [50]</cell><cell></cell><cell>52.50</cell><cell>65.90</cell><cell>51.30</cell><cell>51.90 55.40 55.50</cell><cell>67.80</cell><cell>51.90</cell><cell>53.20 57.10</cell></row><row><cell>PGNet [49]</cell><cell></cell><cell>56.00</cell><cell>66.90</cell><cell>50.60</cell><cell>50.40 56.00 57.70</cell><cell>68.70</cell><cell>52.90</cell><cell>54.60 58.50</cell></row><row><cell>RPMM [46]</cell><cell></cell><cell>55.20</cell><cell>66.90</cell><cell>52.60</cell><cell>50.70 56.30 56.30</cell><cell>67.30</cell><cell>54.50</cell><cell>51.00 57.30</cell></row><row><cell>PPNet [21] CWT [24]</cell><cell>ResNet50</cell><cell>47.80 56.30</cell><cell>58.80 62.00</cell><cell>53.80 59.90</cell><cell>45.60 51.50 58.40 47.20 56.40 61.30</cell><cell>67.80 68.50</cell><cell>64.90 68.50</cell><cell>56.70 62.00 56.60 63.70</cell></row><row><cell>PFENet [39]</cell><cell></cell><cell>61.70</cell><cell>69.50</cell><cell>55.40</cell><cell>56.30 60.80 63.10</cell><cell>70.70</cell><cell>55.80</cell><cell>57.90 61.90</cell></row><row><cell>CyCTR [52]</cell><cell></cell><cell>67.80</cell><cell>72.80</cell><cell>58.00</cell><cell>58.00 64.20 71.10</cell><cell>73.20</cell><cell>60.50</cell><cell>57.50 65.60</cell></row><row><cell>baseline</cell><cell></cell><cell>66.36</cell><cell>69.22</cell><cell>57.64</cell><cell>58.73 62.99 70.75</cell><cell>72.92</cell><cell>58.86</cell><cell>65.56 67.02</cell></row><row><cell>baseline + SVF</cell><cell></cell><cell>66.88</cell><cell>70.84</cell><cell>62.33</cell><cell>60.63 65.17 71.49</cell><cell>74.04</cell><cell>59.38</cell><cell>67.43 68.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Ablation study of BN on Pascal-5 i under 1-shot setting. represents fine-tuning this feature space. The best mean results are show in bold.</figDesc><table><row><cell>Method</cell><cell>BN</cell><cell>scale</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell>60.27</cell><cell>65.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell>61.93</cell><cell>70.67</cell><cell>62.02</cell><cell>57.86</cell><cell>63.12 (?1.95)</cell></row><row><cell>baseline</cell><cell></cell><cell></cell><cell>63.46</cell><cell>70.66</cell><cell>64.93</cell><cell>57.75</cell><cell>64.20 (?0.87)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.42</cell><cell>71.57</cell><cell>67.99</cell><cell>61.57</cell><cell>67.14 (+2.07)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>Comparative experiment with fine-tuning different layer of backbone on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell>layer</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Mean</cell></row><row><cell>baseline</cell><cell>-</cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell>60.27</cell><cell>65.07</cell></row><row><cell>+fully fine-tune</cell><cell>1, 2, 3, 4</cell><cell>57.97</cell><cell>70.51</cell><cell>61.33</cell><cell>53.80</cell><cell>60.90 (?4.17)</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell>55.34</cell><cell>71.16</cell><cell>62.72</cell><cell>55.38</cell><cell>61.15 (?3.92)</cell></row><row><cell>+ part fine-tune</cell><cell>3, 4</cell><cell>56.85</cell><cell>71.44</cell><cell>61.72</cell><cell>54.32</cell><cell>61.08 (?3.99)</cell></row><row><cell></cell><cell>4</cell><cell>56.19</cell><cell>70.63</cell><cell>59.98</cell><cell>55.50</cell><cell>60.58 (?4.49)</cell></row><row><cell>+SVF</cell><cell>2, 3, 4</cell><cell>67.42</cell><cell>71.57</cell><cell>67.99</cell><cell>61.57</cell><cell>67.14 (+2.07)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 18 :</head><label>18</label><figDesc>Comparative experiment with fine-tuning different convolutional layer of backbone on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell>layer</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Mean</cell></row><row><cell>baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.60</cell><cell>70.28</cell><cell>64.12</cell><cell>60.27</cell><cell>65.07</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell></cell><cell></cell><cell>55.34</cell><cell>71.16</cell><cell>62.72</cell><cell>55.38</cell><cell>61.15 (?3.92)</cell></row><row><cell>+part fine-tune</cell><cell>2, 3, 4</cell><cell></cell><cell></cell><cell>59.57</cell><cell>69.96</cell><cell>61.74</cell><cell>56.16</cell><cell>61.86 (?3.21)</cell></row><row><cell></cell><cell>2, 3, 4</cell><cell></cell><cell></cell><cell>58.30</cell><cell>70.50</cell><cell>62.04</cell><cell>55.63</cell><cell>61.62 (?3.45)</cell></row><row><cell>+SVF</cell><cell>2, 3, 4</cell><cell>-</cell><cell>-</cell><cell>67.42</cell><cell>71.57</cell><cell>67.99</cell><cell>61.57</cell><cell>67.14 (+2.07)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 19 :</head><label>19</label><figDesc>Ablation study of SVF fine-tuning different subspace on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell>U</cell><cell>S</cell><cell>V</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Mean</cell></row><row><cell>baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 20 :</head><label>20</label><figDesc>Ablation study of SVF fine-tuning different layer on Pascal-5 i .</figDesc><table><row><cell>Method</cell><cell>layer</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>Mean</cell></row><row><cell>baseline + SVF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Singular value decomposition (svd) image coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="432" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1538" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spottune: transfer learning through adaptive fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajana</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4805" to="4814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving training of deep neural networks via singular value bounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4344" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12119</idno>
		<title level="m">Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning what not to segment: A new perspective on few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binfei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07615</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ctnet: Context-based tandem network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Few-shot segmentation with global and local contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05293</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simpler is better: Few-shot semantic segmentation with classifier weight transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6941" to="6952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6941" to="6952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The singular values of convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03410</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Partial is better than all: Revisiting fine-tuning strategy for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9594" to="9602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blockmix: Meta regularization and self-calibrated inference for metric-based meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="610" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning attention-guided pyramidal features for few-shot fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page">108792</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scale-aware graph neural network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Guo-Sen Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5475" to="5484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with cyclic memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Guo-Sen Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7293" to="7302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="763" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mining latent classes for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8721" to="8730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiushuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9587" to="9595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature pyramid transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="323" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Few-shot segmentation via cycle-consistent transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting few-sample bert fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3855" to="3865" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Discrimination-aware channel pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Table 22: introduce a new small part of parameters S&apos; to verify the importance of singular values on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Compare with different implementations of SVF on Pascal-5 i 1-shot</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
