<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Accurate Facial Landmark Detection via Cascaded Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
							<email>hui01.li@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute China Xi&apos;an (SRCX)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zidong</forename><surname>Guo</surname></persName>
							<email>zidong.guo@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute China Xi&apos;an (SRCX)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon-Min</forename><surname>Rhee</surname></persName>
							<email>s.rhee@samsung.com</email>
							<affiliation key="aff1">
								<orgName type="department">Samsung Advanced Institute of Technology (SAIT)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Samsung Advanced Institute of Technology (SAIT)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
							<email>jae-joon.han@samsung.com</email>
							<affiliation key="aff1">
								<orgName type="department">Samsung Advanced Institute of Technology (SAIT)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Accurate Facial Landmark Detection via Cascaded Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate facial landmarks are essential prerequisites for many tasks related to human faces. In this paper, an accurate facial landmark detector is proposed based on cascaded transformers. We formulate facial landmark detection as a coordinate regression task such that the model can be trained end-to-end. With self-attention in transformers, our model can inherently exploit the structured relationships between landmarks, which would benefit landmark detection under challenging conditions such as large pose and occlusion. During cascaded refinement, our model is able to extract the most relevant image features around the target landmark for coordinate prediction, based on deformable attention mechanism, thus bringing more accurate alignment. In addition, we propose a novel decoder that refines image features and landmark positions simultaneously. With few parameter increasing, the detection performance improves further. Our model achieves new stateof-the-art performance on several standard facial landmark detection benchmarks, and shows good generalization ability in cross-dataset evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial landmark detection aims to automatically localize fiducial facial landmark points on human faces. It serves as an essential step for several facial analysis tasks, such as face recognition, facial expression analysis, face frontalization and 3D face reconstruction <ref type="bibr" target="#b36">[37]</ref>.</p><p>Facial landmark detection has received significant improvement in recent years. Existing approaches mainly fall into two categories, i.e., coordinate regression-based methods and heatmap-based methods. Coordinate regressionbased methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref> map the input image to landmark coordinates via fully connected prediction layers. To improve accuracy, coordinate regression is usually cascaded as * The first two authors equally contributed to this work. H.Li is the corresponding author. a coarse-to-fine manner <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> or integrated with heatmap regression module <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref>. Heatmap-based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref> usually predict heatmaps by fully convolutional networks and then obtain the landmarks according to the peak probability locations on the heatmaps. Since heatmap-based models can preserve the spatial structure of image features, they have better performance than coordinate regressionbased models generally. Although heatmap-based methods have relatively higher detection accuracy, they suffer from three major issues. 1) The required post-processing step is non-differentiable, which disables the end-to-end training. 2) Considering the computational complexity, the resolution of heatmaps is usually lower than that of the input images, resulting in a quantization error inevitably and limiting the performance.</p><p>3) They concern more on local texture information and neglect global sensing on face shape, making them vulnerable to large appearance variation such as occlusions.</p><p>In contrast, coordinate regression based methods can bypass the aforementioned drawbacks and enable end-to-end model training. However, the fully connected layers destroy the spatial structure of local image features, which deteriorates the localization performance greatly <ref type="bibr" target="#b15">[16]</ref>.</p><p>In this work, we propose a coordinate regressionbased model, Deformable Transformer Landmark Detector (DTLD), for accurate facial landmark detection. On one hand, our model avoids the aforementioned shortcomings of heatmap-based methods, and can be well-trained end-toend, without heuristical post-processing. On the other hand, the model is capable of extracting the most relevant features from multi-level feature maps around the target landmark for coordinate prediction, which preserves the local spatial structure and improves the localization accuracy to a large extent. Moreover, our method helps to exploit the underlying relationship among landmarks and incorporate rich structure knowledge, which enables a robust model to tackle various scenarios such as expression or occlusion. Inspired by the great success of DEtection TRansformer (DETR) in object detection and keypoint detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43]</ref>, we formulate the landmark detection as a gradually refined N-coordinate prediction task, where N is the number of facial landmarks. Self-attention block is adopted to learn potential structural dependencies. Then multi-scale image feature based deformable attention <ref type="bibr" target="#b42">[43]</ref> is employed, where landmark related information is used as the guidance to adaptively extract the most relevant features and refine the coordinates. Different from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref> that define redundant object queries and use bipartite matching to classify objects, here the number of queries is set to be the number of landmarks exactly, following the practice in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>, which simplifies the training process largely. Instead of using randomly initialized query embedding and similar to the DQInit proposed in <ref type="bibr" target="#b14">[15]</ref>, we design a more meaningful image-related query-initialization method, which provides coarse landmark locations rather than a fiducial landmark template. Different from <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>, we further explore a parallel decoder where both image features and landmark coordinates are refined simultaneously in the decoding process. It improves the detection performance further. The entire framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The main contributions can be summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>We propose a coordinate regression-based facial landmark detector DTLD by cascaded deformable transformers, based on Deformable DETR <ref type="bibr" target="#b42">[43]</ref>. DTLD could iteratively capture structural relationships among landmarks and the most relevant visual contextual information to achieve efficient and effective detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>A parallel decoder is further explored to enhance the detection accuracy, with few model parameter increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>We conduct extensive experiments to analyze the effectiveness of the proposed method, by both quantitative evaluations and qualitative visualizations. Our model contributes to tackle landmark detection under various scenarios. It achieves new state-of-the-art (SOTA) accuracy on several facial landmark detection benchmarks, and shows good generalization ability in cross-dataset evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Facial Landmark Detection</head><p>As stated above, the existing approaches on facial landmark detection can be roughly divided into two categories. Heatmap-based methods usually use high-resolution feature maps for precise localization and achieve encouraging performance. Stacked hourglass network <ref type="bibr" target="#b25">[26]</ref> and U-Net <ref type="bibr" target="#b27">[28]</ref> are two typical architectures that perform well in heatmap-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>. Specifically, HSLE <ref type="bibr" target="#b43">[44]</ref> proposes to hierarchically depict holistic and local structures obtained by stacked hourglass network for accurate alignment. LUVLi <ref type="bibr" target="#b18">[19]</ref> investigates U-Net for jointly predicting landmark locations, associated uncertainties of these predicted locations and landmark visibilities. HR-Net <ref type="bibr" target="#b29">[30]</ref> also shows promising results by connecting and exchanging information via fusing multi-scale image features across multiple branches to obtain high-resolution maps. More recently, PIPNet <ref type="bibr" target="#b15">[16]</ref> conducts heatmap and offset predictions simultaneously on low-resolution feature maps, which largely reduces inference time and achieves competitive accuracy. Coordinate regression-based models are mostly fast, but not accurate enough <ref type="bibr" target="#b15">[16]</ref>. In order to improve the accuracy, most algorithms are designed to make predictions in a coarse-to-fine manner through a cascaded structure <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. For instance, Dapogny et al. <ref type="bibr" target="#b6">[7]</ref> proposed DeCaFA that uses fully convolutional U-net to preserve the full spatial resolution throughout the cascaded regression for accurate face alignment. LAB <ref type="bibr" target="#b35">[36]</ref> was proposed by predicting facial boundary as a geometric constraint via heatmap regression to help landmark coordinate prediction. Li et al. <ref type="bibr" target="#b20">[21]</ref> adopt a cascaded Graph Convolutional Network to dynamically leverage global and local features for precise prediction. Although this method shows superior performance, it relies more on high-resolution feature maps which is computationally expensive. A similar work proposed recently is BarrelNet <ref type="bibr" target="#b14">[15]</ref> which adapts DETR to landmark detection and set the number of query to be exactly fixed as the number of landmark points. It also proposes to use dynamic query from the input image features (DQInit) for better performance. The difference between the two work are as follows: 1) Different from our cascaded refinement process, BarrelNet predicts landmarks after the last decoder layer directly. 2) Instead of exploring multi-level image features, BarrelNet adopts the last backbone feature and proposes a QAMem module to improve the accuracy. 3) Bar-relNet calculates the dynamic query after global average pooling on memory feature, whereas our design extracts initial query from the spatial dimension of image feature, and calculates coarse initial landmark coordinates further to constrain them to be landmark related. 4) BarrelNet shows that more encoders harm the detection accuracy. Never- Query Initialization <ref type="figure">Figure 2</ref>. The architecture of our proposed DTLD. Q 0 is obtained from F4, the last layer of backbone features, through a linear projection on spatial dimension, and is further transformed into initial landmark coordinates Y0, which are adjusted by T decoder layers to get the final positions YT .</p><p>theless, our experiments show that both more encoder and more decoder layers contribute to higher detection performance. Hence, we further propose a parallel decoder to keep the effect of encoder while saving parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformers in Vision Tasks</head><p>Attention mechanism in transformer <ref type="bibr" target="#b32">[33]</ref> is able to encode distant dependencies or heterogeneous interactions, and has shown outstanding performance on lots of computer vision tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>. VIT <ref type="bibr" target="#b10">[11]</ref> is the first that employs pure transformer for image classification. PVT <ref type="bibr" target="#b33">[34]</ref> integrates pyramid feature maps and spatial property into the model design. DETR <ref type="bibr" target="#b2">[3]</ref> and Deformable DETR <ref type="bibr" target="#b42">[43]</ref> view object detection as a direct set prediction task and formulate object detection to be trained end-to-end. Yang et al. <ref type="bibr" target="#b37">[38]</ref> introduced transformer for human pose estimation, and employed attention layers to capture long-range spatial dependencies between human body parts. The model is still heatmap-based. Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed pose recognition transformer based on DETR. However, it still needs to perform keypoint detection by finding a match between numerous predictions and the ground-truth. TFPose <ref type="bibr" target="#b24">[25]</ref> proposed to regress keypoint coordinates directly based on deformable DETR. It still inherits the encoder-decoder architecture, while we propose a parallel decoder in addition. With a small amount of parameters and computation, our model achieves the highest accuracy on facial landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The architecture of the proposed DTLD is presented in <ref type="figure">Figure 2</ref>. It is composed of a backbone network for image feature extraction, a query initialization module, and a decoder module for landmark prediction. We adopt a cascaded regression framework where the coordinate offsets are predicted by each decoder layer. The landmark coordinates are refined iteratively during the decoding process. We introduce each part in detail in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone</head><p>The backbone contains an ImageNet <ref type="bibr" target="#b17">[18]</ref> pre-trained ResNet-18 <ref type="bibr" target="#b13">[14]</ref>. Pyramid features are output, which are denoted as F 1 , F 2 , F 3 , F 4 , with down-sampling ratios of 4, 8, 16, 32 relative to the input image. A 1 ? 1 convolution is followed to project the features into the same number of channels. These features are then flattened and concatenated together, and will be used as the memory feature for decoder, denoted as M ? R M ?C , where M is the length of the flattened features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Query Initialization</head><p>A learnable query matrix Q is defined in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>, which is randomly initialized and updated to represent object related information. In our model, the query matrix Q is defined to have the size of N ? C, where N is the number of landmarks and C is the feature dimension. Rather than random initialization, we extract N features from F 4 by a linear projection on spatial dimension, and use them as the initial query features, i.e.</p><formula xml:id="formula_0">Q 0 = F C(F T 4 ) T ,<label>(1)</label></formula><p>We reuse F 4 to denote the flattened feature, Q 0 ? R N ?C . The obtained initial query features are expected to be landmark-related. A landmark predictor (another linear projection layer followed by Sigmoid in this paper) is employed to transform them into N landmark coordinates, i.e.,</p><formula xml:id="formula_1">Y 0 = ?(F C(Q 0 )),<label>(2)</label></formula><p>where Y 0 ? R N ?2 are the initial landmark coordinates, which will be used as the initial reference points for feature sampling in decoding process as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder Module</head><p>The decoder module is composed of T decoder layers. Each layer takes as inputs a query matrix Q, a memory feature M and reference points R, and outputs landmark coordinate offsets in regards to R. Based on whether updating the memory feature, two types of decoders are explored, i.e., a basic one and a parallel one. They work independently. The former is simple and efficient, setting a strong baseline for landmark detection, while the latter presents a slightly higher detection accuracy. Basic Decoder. The configuration of the basic decoder layer is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. It mainly consists of a selfattention layer, a deformable attention layer, and an offset predictor.</p><p>Specifically, the self-attention layer only adopts the query matrix Q as input. It learns the structure dependency among landmarks by dense interactions. This information is image-independent intrinsically, where facial attributes like pose and expression will be captured and these attributes have been proven to be important for landmark localization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. The self-attention layer takes Q P ,Q P ,Q as query, key and value separately, and Q P = Q + P, where P is a learnable position embedding. The output from selfattention layer is denoted as</p><formula xml:id="formula_2">Q S = [q S 1 , . . . , q S N ], where q S i = N j=1 ? ij (W v q j ), i = 1, . . . , N,<label>(3)</label></formula><p>and ? ij are self-attention weights calculated by query and key that exploit the connectivity among landmarks. A residual addition and layer normalization are used as those in normal transformer block. The output is renamed as Q S . The deformable attention layer takes Q S as query and the memory feature M as value. Instead of calculating the relationship between each element of Q S and M, the deformable attention <ref type="bibr" target="#b42">[43]</ref> only attends to a small set of features, obtained by sampling M according to sampling points. The calculation is formulated as</p><formula xml:id="formula_3">q D i = K k=1 ? ik (Wx ik ), i = 1, . . . , N,<label>(4)</label></formula><p>where x ik are image features sampled from M. K is the entire sampling number. The sampling locations for q D i , denoted as p ik ? R 2 , are calculated by p ik = r i + ?p ik , where r i denotes the reference point, which is the i-th landmark coordinate calculated from the previous decoder layer, and ?p ik are sampling offsets, obtained via linear projection over the query feature q S i . ? ik denotes the attention weights over the sampling features, which are calculated by another linear projection over q S i , and a softmax operation. The sampling process extracts more related landmark features from multi-level feature maps, which reduces the feature searching area to a large extent and accelerates model convergence. A residual addition, layer normalization and feed-forward network are followed, and the output is re-denoted as</p><formula xml:id="formula_4">Q D = [q D 1 , . . . , q D N ].</formula><p>The final projection is computed by offset predictor (a 3layer perceptron in this paper). It takes Q D as input and predicts the coordinate offsets Y o with regard to the reference points R. The landmark coordinates are then calculated by,</p><formula xml:id="formula_5">Y t = ?(Y o t + ? ?1 (R t )),<label>(5)</label></formula><p>where t means for the tth decoder layer,</p><formula xml:id="formula_6">t = 1, ..., T . Y o t ? R N ?2 are the predicted coordinate offsets, R t ? R N ?2 are coordinates of reference points and R t = Y t?1 .</formula><p>Note that the input query matrix Q is also updated by each decoder layer. Q = Q 0 for the first decoder layer, and Q = Q D t?1 for others. Q D t?1 is the output from previous deformable attention layer. Parallel Decoder. DETR and deformable DETR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref> employ several layers of encoder to learn more discriminative image features. DTLD removes the encoder module to save parameters and computational costs. However, experiments show that the encoder is indeed beneficial to detection performance. Instead of inheriting the serial encoderdecoder architecture, we propose a parallel decoder, where the memory feature is updated coherently during the decoding process, along with landmark coordinates refinement. The simple variation improves landmark detection accuracy furthermore.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, given the memory feature M, we first add both level embedding and pixel position embedding, denoted together as P , to indicate which level the feature comes from and the spatial location of the feature in feature maps. The embedding added features, denoted as M P , are used as the query for updating image feature, i.e.,</p><formula xml:id="formula_7">f j = K k=1 ? jk (Wx jk ), j = 1, . . . , M.<label>(6)</label></formula><p>f j are updated image features, x jk are sampled features from M according to sampling location p jk = r M j + ?p jk . Similarly, ?p jk and ? jk , which denote the sampling offsets and attention weights, are computed by linear projection over M P . The reference points r M j ? [0, 1] 2 are normalized coordinates of memory feature on each feature map.</p><p>In the parallel decoder, we concatenate M P and Q S as the overall query features, concatenate r M j , j = 1, . . . , M and r i , i = 1, . . . , N as the reference points, and update both image features and landmark query features simultaneously according to Eq 6 and Eq 4. The layer parameters are shared except that we use separate layer normalizations for image and landmark query. It results in only 1.2K more parameters compared to the basic decoder counterpart. The updated image features will be used as the memory feature next, and the updated landmark query features will be used to calculate the offsets Y o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Target</head><p>We simply use L 1 loss between the predicted landmark coordinates and the ground-truth to train the model, i.e.,</p><formula xml:id="formula_8">L = T t=0 Y t ?? ,<label>(7)</label></formula><p>where Y 0 is computed by Eq 2, and Y t , t = 1, . . . , T are from Eq 5.? denotes the ground-truth coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we perform extensive experiments to verify the effectiveness of the proposed method. All the experiments are conducted on an NVIDIA v100 GPU. The models are implemented by PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on a number of popular 2D face landmark detection datasets, including 300W, WFLW, COFW and AFLW. 300W <ref type="bibr" target="#b28">[29]</ref> is collected from five facial datasets, consisting of 3148 training images and 689 test images. The test dataset is further divided into 2 subsets, i.e., common set with 554 images and challenging set with 135 images. Each image is annotated with 68 landmarks.</p><p>WFLW <ref type="bibr" target="#b35">[36]</ref> is collected from WIDER Face, which includes large variations in pose, expression and occlusion. Each face is originally annotated by 98 landmarks, and reannotated by 68 landmarks in <ref type="bibr" target="#b15">[16]</ref>. There are 7, 500 images for training and 2, 500 for test. The test set is further divided into 6 subsets for different scenarios.</p><p>COFW <ref type="bibr" target="#b1">[2]</ref> contains 1345 training images and 507 test images under different occlusion conditions. Each image is annotated by 29 landmarks, and we also use 68 landmarks re-annotated by <ref type="bibr" target="#b12">[13]</ref> for the cross-domain setting.</p><p>AFLW <ref type="bibr" target="#b16">[17]</ref> contains 20000 images for training and 4386 images for test, providing 19 landmarks for each face.</p><p>CelebA <ref type="bibr" target="#b21">[22]</ref> is a large-scale attributes dataset with 202,599 face images in the wild. We only use the images without annotation for training in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For all datasets, the faces are cropped according to the provided bounding boxes firstly, and then resized to 256 ? 256. In order to retain more context information, the bounding boxes on 300W and WFLW are enlarged by 10% and 20%, respectively, following previous work <ref type="bibr" target="#b15">[16]</ref>. Data augmentation is adopted involving translation, horizontal flipping, rotation, occlusion and blurring. The whole model is trained end-to-end by Adam optimizer for 120 epochs in total. The learning rate is set to 1e-4 initially and then reduced to 1e-5 at 100th epoch, where the learning rate for backbone is 10 times smaller than the above. By default, we use 3 decoder layers, with a feature dimension of 256 and 8 heads. For each query, we sample 4 features for each head from each level of the feature maps. The configuration will be analyzed in ablation study. We train the model on 1 v100 GPU with a batch size of 16. The reported results are averaged over three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metric</head><p>we adopt the most widely used metric, normalized mean error (NME), to evaluate our model for fair comparison with previous work. It is calculated by,</p><formula xml:id="formula_9">NME(Y,?) = 1 N N i=1 y i ?? i 2 D .<label>(8)</label></formula><p>We employ the prediction from last decoder layer for evaluation. D is a normalization distance, and we use interocular distance for 300W, WFLW, COFW, and image size for AFLW, following common practice. Failure rate (FR) is also reported which refers to the percentage of failed examples whose NMEs are larger than a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the SOTA</head><p>As presented in <ref type="table">Table 1</ref>, we firstly compare our model with SOTA methods on landmark detection accuracy using the four benchmarks. DTLD is the model with basic decoder, while DTLD-s has all model parameters trained from scratch. DTLD+ is equipped with the parallel decoder. The models are trained and tested separately, with the default configuration.</p><p>The results show that our models consistently outperform all the other methods on all test datasets with a simple backbone. To be specific, our DTLD achieves the NME of 2.96%, 3.04% and 1.38% on 300W-Full, COFW and AFLW respectively. In addition, with the NME threshold of 8%, the failure rates are 0.29%, 0.20% and 0.25% separately. On WFLW-Full which contains various scenarios, DTLD obtains NME of 4.08%, leading to a relative decrease of 2.86% compared to the second best (4.20% NME), and 7.69% relative to BarrelNet-18 (4.42% NME), the previous best method using the same backbone. The failure rates are 2.76% at the threshold of 10% and 6.44% at the threshold of 8%. Comprehensive results on each WFLW subset are shown in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year Backbone Pre-Trained 300W (NME) COFW (NME) AFLW (NME)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WFLW-Full Full</head><p>Common Challenge (NME) (FR 10% ) LAB <ref type="bibr" target="#b35">[36]</ref> 2018  Our model also benefits from the ImageNet pre-trained backbone. Without pre-training (referring to DTLD-s), NMEs increase a lot, but are still smaller than SOTA models with similar model size. The use of the parallel decoder (DTLD+) improves the detection accuracy further, leading to NME of 4.05% on WFLW and 3.02% on COFW, averagely 0.02% lower than that obtained by DTLD.</p><p>Next, we compare the model size and running speed of our models with others. As presented in <ref type="table" target="#tab_1">Table 2</ref>, DTLD has 13.3M parameters and only 2.5 GFLOPs, but achieves very competitive accuracy. The running speed is lower than PIPNet-18 and BarrelNet-18 because of the multiple refining process, but is still faster than others. DTLD+ achieves relatively higher accuracy at the sacrifice of running speed. Some landmark detection results by DTLD are visualized in <ref type="figure" target="#fig_4">Figure 5</ref>. Our model can accurately predict landmarks in the tough scenes for faces with blur, large posture changes, rich expressions, and partial occlusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies on DTLD</head><p>We conduct a series of ablation studies to analyze each part of the proposed model. The ablation experiments are performed on WFLW-Full as it includes comprehensive scenarios. Effect of Q 0 . In DTLD, we use a well-calculated Q 0 as the initial query features and calculate the initial reference points based on Q 0 . Here, we perform experiments with a randomly initialized learnt positional encodings as Q 0 , as that used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>. Experimental result in <ref type="table">Table 3</ref> shows a performance drop of 0.11%NME on WFLW. We also visualize the effect in <ref type="figure" target="#fig_5">Figure 6</ref>. As can be seen, our Q 0 will lead to image related initial reference points, instead of a fiducial landmark template.  <ref type="figure">Figure 7</ref>. Visualizations of deformable attention on pyramid backbone features. The red cross denotes the ground-truth, while others dots show the sampling points with attention weights expressed by colors. The brighter the point, the greater the weight. We combine the sampling points from all heads for each feature map. Sampling points with attention weights lower than 0.5 are omitted.  <ref type="table">Table 3</ref>. Ablation study on DTLD, with backbone, Q initial strategy and self-attention analyzed. R18 / 50 / 101 represent ResNet-18 / 50 / 101 backbone pre-trained by ImageNet . FC is our initialization compared with randomly initialized Random Init.</p><p>Effect of self-attention. Self-attention is employed in decoder layer so as to exploit the structural knowledge among landmark positions. Without self-attention, NME of DTLD reduces 0.09% (4.08% to 4.17%) as indicated in <ref type="table">Table 3</ref>. Effect of backbone. We experiment with different backbones as shown in <ref type="table">Table 3</ref>. However, the performance gain is not obvious (only 0.01%NME improve) when using deeper backbone like ResNet-50 and ResNet-101, which means DTLD is not sensitive to much deeper backbone. Effect of deformable attention. We also visualize the multi-scale deformable attention as presented in <ref type="figure">Figure 7</ref>.</p><p>The visualization shows that the deformable module can extract most related image features around the landmark point for coordinate prediction. Moreover, the first decoder layer attends more on the rear feature maps like F3 and F4 that tend to high level global information, while the last decoder layer attends more on the frontal feature maps like F1 and F2 that are apt to capture low level local features for coordinate fine tuning. Effect of model hyper-parameters. Here we conduct experiments with different model hyper-parameters on DTLD, including the feature dimension C used in decoder layers, the number of sampling points K used in deformable attention, and the head number used in all attention layers. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the higher the feature dimension, the better the performance, but 256 seems to be enough for feature encoding. In our default configuration, for each query feature, we sample 4 points from each feature level for each head. We then test other numbers such as 2, 6. However, the change has little impact on the final accuracy, indicating that our model can adaptively decouple the most critical information from redundant features. We also run models with different head number in both self and deformable attention. More heads benefit final performance. We visualize the deformable attention for each head in Supplementary. It illustrates intuitively that different heads will pay attention to different directions of image features. Effect of decoder. Encoder layers are commonly adopted to further encode the image features, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Here we also conduct experiments by adding encoder in DTLD as in deformable DETR <ref type="bibr" target="#b42">[43]</ref> and varying the number of layers in both encoder and decoder. Experimental results in <ref type="table">Table 5</ref> show that the added encoder or decoder layers indeed contribute to reduce NME furthermore, even achieving NME smaller than 4% on WFLW. Results also show that the adding of decoder layers has relatively larger effect on accuracy than that of encoder layers. However, the added layer brings more parameters (0.5M for one encoder layer and 0.6M for one decoder layer) and degrades the speed.</p><p>To improve the accuracy without increasing model size, we propose the parallel decoder, where the image features are encoded along with the decoding process. By sharing the deformable attention layers, the model size is almost the same as that without encoder layers (the little parameter increase comes from separate layer normalization), but NME further decreases. When using similar number of parameters, DTLD+ always obtains higher accuracy than DTLD counterparts. When using decoder layers ? 3, DTLD+ gets a higher accuracy compared to DTLD at similar speed. It should be noted that when we use 1 parallel decoder layer, the model exactly becomes DTLD with 1 decoder layer and 0 encoder. The lower speed is caused by image feature updating which is not used anymore. However, it may provide a chance of inferring occluded face part based on the features so as to improve model performance further. We leave it as a future work.</p><p>Moreover, we attempt to remove the backbone and compute the pyramid features simply by image dividing and patch embedding as performed in <ref type="bibr" target="#b33">[34]</ref>. The pyramid embeddings are fed into DTLD+ directly for feature encoding and landmark prediction. With 6 layers and only 6M parameters, our model achieves NME of 4.27% on WFLW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Cross-dataset Evaluation</head><p>To verify the robustness and generalization ability of our model, we conduct cross-dataset evaluation on COFW and WFLW testsets, using DTLD trained on 300W training data. To maintain distribution consistency between different datasets, we follow the practice in <ref type="bibr" target="#b15">[16]</ref>, enlarging the provided bounding boxes of 300W, COFW68, and WFLW68 by 30%, 30% and 20% respectively. Experimental results in <ref type="table">Table 6</ref> indicate the robustness of our model in cross dataset evaluation.</p><p>In addition, to analyze the model scalability, we perform an unsupervised domain adaption (UDA). More precisely, we apply the classic self-training strategy and retrain the model using COFW and WFLW training images, without landmark annotation employed. The model trained on 300W is used as a teacher model to reason the pseudolabels for unlabeled data. They are then combined with the original labeled data and re-train the model. After 3 times of re-training, we achieve NME of 4.14% on COFW68 and 6.39% on WFLW68, new SOTA accuracies on both testsets. Compared to PIPNet, the UDA improvement is more obvious, which demonstrates the good scalability of our method.</p><p>Motivated by the good scalability, we attempt to promote the model additionally by leveraging the numerous unlabeled face images from CelebA. With the same self-training paradigm, it is found that the detection accuracy can be further improved on 300W and WFLW-Full testsets. As indicated in <ref type="table">Table 7</ref>, although the unlabeled images are from a different domain compared with the test datasets, our model can still learn from them and leads to even more accurate landmark prediction. It finally achieves NME of 2.94% on 300W and 3.89% on WFLW.</p><p>Another group of cross-dataset evaluation is performed following the experimental setting in <ref type="bibr" target="#b18">[19]</ref>. To be specific, we train DTLD from scratch on the training data of 300W Split2, and evaluate it on 300W Split2 test data, Menpo frontal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref> and COFW68. There are 3837 images in 300W Split2 train set and 600 images in test set. The 6679 near-frontal training images in Menpo 2D (denoted as Menpo frontal) are adopted here for evaluation, as well as the 507 test images in COFW68. Following <ref type="bibr" target="#b18">[19]</ref>, here we adopt NME box and AUC box as the evaluation metrics. NME box uses the geometric mean of the width and height of the ground-truth bounding box ( ? w bbox ? h bbox ) as the normalization distance D. AUC (Area Under Curve) is computed as the area under the cumulative distribution curve, up to a cutoff NME value. The cumulative distribution curve is plotted by the fraction of test images whose NME is less than or equal to the specific NME value on the horizontal axis. Here we use NME box and the cutoff value of 7%. The lower the NME box , the higher the AUC box , the better the performance. Experimental results are presented in <ref type="table">Table 8</ref>. Our DTLD without any pretraining achieves the best detection accuracy on 300W Split2 test set and COFW68, even surpassing other methods pretrained on 300W-LP-2D <ref type="bibr" target="#b41">[42]</ref>. On Menpo frontal, our DTLD is still better than previous models without pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Failure Case Analysis</head><p>Although our model shows strong superiority on facial landmark detection, it is still weak for face image with severe occlusions, especially obscured by other people, as illustrated in <ref type="figure" target="#fig_0">Figure 8. Specifically, 1</ref>  <ref type="table">Table 7</ref>. Boost our model by using unlabeled images from other domain. Our model shows better scalability, which can be improved more by using unlabeled images. Note that it is the enlarged bounding boxes used in 300W that cause NME of 3.07%, larger than 2.96% presented in <ref type="table">Table 1</ref>.  <ref type="table">Table 8</ref>. Another group of cross-dataset evaluation. DTLD-s is our proposed DTLD model trained from scratch. The methods marked with * are pretrained on 300W-LP-2D. Our DTLD exceeds previous models without pretraining and is even better than some models pretrained on 300W-LP-2D. blurring, occlusion, etc.) causes a great uncertainty on face boundary inference, our model may fail. 2) If the face to be aligned is obscured by another face, our model has difficulty in distinguishing the target character, thus leading to large errors.</p><p>3) The ambiguity of landmark annotations may lead to poor performance, especially for landmarks on face boundary. For these weaknesses, a possible solution is to make better use of the connections between landmarks to infer the invisible part. We leave it as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an effective and efficient facial landmark detection network DTLD based on cascaded transformer. It directly regresses landmark coordinates and thus can be trained end-to-end. The use of self-attention and deformable attention in DTLD enables structure relationship exploring and more related image feature extracting. The simple query initialization sets up a better start point for the following refinement. Moreover, we propose a parallel decoder that refines image features and landmark positions simultaneously, improving the detection performance with few parameter increasing. Our model achieves new SOTA performance on several standard landmark detection benchmarks, surpassing the other advanced approaches. The running speed is a limitation of current work. Knowledge Distillation based methods may be exploited in the future so as to reduce the cascaded refinement steps and accelerate detection process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our entire framework. The initial query and landmark locations are generated based on the image features, and are continuously updated along with the decoding process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Detailed illustration of the basic decoder. Memory feature M and the previous Q jointly participate in updating the landmark positions on the basis of previous coordinates R. For the first decoder layer, R1 is the initial landmark location Y0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Detailed structure of the proposed parallel decoder. The feature memory M is also updated in the process, sharing the parameters and operations of cross attention and FFN with query Q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of typical landmark detection results. Red denotes the ground truth, and cyan represents our predictions. Our model is able to detect landmarks accurately in various scenarios, such as blur, makeup, expression, occlusion, or even with big pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the effect of our query initialization. Fiducial landmark positions are produced by randomly initialized Q 0 (left), while our query initialization method sets up good starting points (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Visualizations of some typical failures. Red represents the ground truth, and cyan represents our predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other methods on Parameter size, GFLOPs and FPS. Our method achieves the highest accuracy with a small amount of GFLOPs and parameters. The FPS is lower than PIPNet-18, which leaves for future improving.</figDesc><table><row><cell>Method</cell><cell>Year</cell><cell>Backbone</cell><cell>NME(%)</cell><cell>Param.(M)</cell><cell>GFLOPs</cell><cell>FPS(GPU)</cell></row><row><cell>HRNet [30]</cell><cell>2019</cell><cell>HRNet-W18</cell><cell>4.60</cell><cell>9.7</cell><cell>4.8</cell><cell>11.7</cell></row><row><cell>AWing [35]</cell><cell>2019</cell><cell>Hourglass</cell><cell>4.36</cell><cell>25.1</cell><cell>26.7</cell><cell>24.2</cell></row><row><cell>DeCaFa [7]</cell><cell>2019</cell><cell>Cascaded U-Net</cell><cell>4.62</cell><cell>10</cell><cell>?</cell><cell>32</cell></row><row><cell>LUVLi [19]</cell><cell>2020</cell><cell>DU-Net</cell><cell>4.37</cell><cell>?</cell><cell>?</cell><cell>58.8</cell></row><row><cell>PIPNet-18 [16]</cell><cell>2020</cell><cell>ResNet-18</cell><cell>4.57</cell><cell>12.0</cell><cell>2.4</cell><cell>200</cell></row><row><cell>PIPNet-101 [16]</cell><cell>2020</cell><cell>ResNet-101</cell><cell>4.31</cell><cell>45.7</cell><cell>10.5</cell><cell>56</cell></row><row><cell>BarrelNet-18 [15]</cell><cell>2021</cell><cell>ResNet-18</cell><cell>4.42</cell><cell>24</cell><cell>?</cell><cell>137</cell></row><row><cell>BarrelNet-101 [15]</cell><cell>2021</cell><cell>ResNet-101</cell><cell>4.20</cell><cell>56.4</cell><cell>?</cell><cell>55.3</cell></row><row><cell>DTLD</cell><cell>2021</cell><cell>ResNet-18</cell><cell>4.08</cell><cell>13.3</cell><cell>2.5</cell><cell>100</cell></row><row><cell>DTLD+</cell><cell>2021</cell><cell>ResNet-18</cell><cell>4.05</cell><cell>13.3</cell><cell>2.5</cell><cell>78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation on DTLD model hyper-parameters including feature dimension, sampling points, and attention heads. The effect of sampling points is tiny, while that of the others is large.</figDesc><table><row><cell># Feature Dimension</cell><cell># Sampling Points</cell><cell># Attention Head</cell><cell>NME (%)</cell></row><row><cell>256</cell><cell>4</cell><cell>8</cell><cell>4.08</cell></row><row><cell>64</cell><cell>4</cell><cell>8</cell><cell>4.47</cell></row><row><cell>128</cell><cell>4</cell><cell>8</cell><cell>4.29</cell></row><row><cell>512</cell><cell>4</cell><cell>8</cell><cell>4.07</cell></row><row><cell>256</cell><cell>2</cell><cell>8</cell><cell>4.11</cell></row><row><cell>256</cell><cell>6</cell><cell>8</cell><cell>4.09</cell></row><row><cell>256</cell><cell>4</cell><cell>4</cell><cell>4.16</cell></row><row><cell>256</cell><cell>4</cell><cell>16</cell><cell>4.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>/ 12.1 / 165 4.187 / 12.7 / 123 4.076 / 13.3 / 100 4.068 / 14.0 / 82 4.044 / 14.6 / 69 4.064 / 15.3 / 60 1 4.369 / 12.6 / 105 4.178 / 13.2 / 86 4.066 / 13.9 / 71 4.026 / 14.5 / 64 4.050 / 15.1 / 56 4.051 / 15.7 / 51 2 4.327 / 13.1 / 81 4.133 / 13.7 / 69 4.047 / 14.4 / 61 4.015 / 15.0 / 54 4.012 / 15.6 / 45 4.000 / 16.2 / 43 3 4.244 / 13.6 / 62 4.114 / 14.2 / 54 4.028 / 14.8 / 52 4.006 / 15.5 / 42 3.980 / 16.1 / 35 3.978 / 16.Experimental results on WFLW by using varying encoder and decoder layers. More encoder or decoder layer contributes to higher performance. The last line shows the effect of our proposed parallel decoder. With similar parameters, DTLD+ achieves slightly higher accuracies. The results are demonstrated by NME(%) / Model Parameter Size (M) / FPS (on V100 GPU). Cross-dataset evaluation and comparison with others. ST means supervised training only on 300W training data, but test on others. UDA means unsupervised domain adaption by utilizing COFW and WFLW training images without annotation used.</figDesc><table><row><cell>) If the challenge (i.e.,</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-driven cropping for very high resolution facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel density network for quantifying regression uncertainty in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face alignment with kernel density deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decafa: Deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6893" to="6901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The menpo benchmark for multi-pose 2d and 3d facial landmark localisation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">127</biblScope>
			<biblScope unit="page" from="599" to="624" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teacher supervises students how to learn from partially labeled images for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2385" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">When liebig&apos;s barrel meets facial landmark detection: A practical model. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pixel-in-pixel net: Towards efficient facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose recognition with cascade transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured landmark detection via topologyadapting deep graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Tung</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Fu</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic alignment: Finding semantically consistent groundtruth for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3317" to="3326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tfpose: Direct human pose estimation with transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weian</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6971" to="6981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial landmark detection: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transpose: Keypoint localization via transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="11802" to="11812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The menpo facial landmark localisation challenge: A step closer to the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via occlusionadaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meilu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face alignment across large poses A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning robust facial landmark detection via hierarchical structured ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
