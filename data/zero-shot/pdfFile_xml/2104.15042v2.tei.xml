<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divide-and-conquer based Large-Scale Spectral Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiucai</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Imakura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Sakurai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Divide-and-conquer based Large-Scale Spectral Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Spectral Clustering Landmark selection Approximate Similarity Computation Large-scale clustering Large-scale datasets</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Spectral clustering is one of the most popular clustering methods. However, how to balance the efficiency and effectiveness of the large-scale spectral clustering with limited computing resources has not been properly solved for a long time. In this paper, we propose a divide-and-conquer based largescale spectral clustering method to strike a good balance between efficiency and effectiveness. In the proposed method, a divide-and-conquer based landmark selection algorithm and a novel approximate similarity matrix approach are designed to construct a sparse similarity matrix within low computational complexities. Then clustering results can be computed quickly through a bipartite graph partition process. The proposed method achieves the lower computational complexity than most existing large-scale spectral clustering methods. Experimental results on ten large-scale datasets have demonstrated the efficiency and effectiveness of the proposed method. The MATLAB code of the proposed method and experimental datasets are available at https://github.com/Li-Hongmin/MyPaperWithCode.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is one of the most fundamental problems in data mining and machine learning, aiming to categorize data points into clusters such that the data points in the same cluster are more similar while data points in different clusters are more different from each other <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16]</ref>. Spectral clustering has attracted increasing attention due to the promising ability to deal with nonlinearly separable datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. It has been successfully applied to various problem domains such as biology <ref type="bibr" target="#b17">[18]</ref>, image segmentation <ref type="bibr" target="#b30">[31]</ref>, and recommend systems <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28]</ref>. Although spectral clustering algorithm often provides better performances than traditional clustering algorithm likes -means especially for complex datasets, it is significantly limited to be applied to large-scale datasets due to its high computational complexity and space complexity <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The conventional spectral clustering algorithm mainly consists of two high-cost steps, i.e., similarity matrix construction and eigen-decomposition. For a dataset with objects, the two steps take computational complexities of ( 2 ) and ( 3 ), respectively. The computational consumption of these two steps is the main reason that hinders the application of spectral clustering algorithms on largescale data.</p><p>In recent years, there has been an increasing amount of literature on alleviating the computational complexity of spectral clustering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Previous research <ref type="bibr" target="#b6">[7]</ref> has established that the sparse similarity matrix construed by only remaining -nearest neighbors or -nearest neighbors can efficiently reduce the space complexity. As a result, some sparse eigensolvers can solve the eigen-decomposition problems within the lower computational complexity. The matrix specification strategy can avoid storing the dense sim-ilarity matrix to reduces the space complexity, but it still needs to compute the dense similarity matrix at first, which costs ( 2 ) computational complexity. Besides the matrix specification, another commonly used strategy is based on a cross-similarity matrix construction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>. Fowlkes et al. <ref type="bibr" target="#b8">[9]</ref> apply the Nystr?m method to reduce the high complexity of spectral clustering algorithm, which first randomly selects a small subset of samples as landmarks, then construct a similarity sub-matrix between these landmarks and remaining samples. Although the random landmark selection is very efficient, it is often unstable concerning the quality of the landmark set. Moreover, it has been shown that a larger is often favorable for better approximation. To address the potential instability of random selection, Cai et al. <ref type="bibr" target="#b3">[4]</ref> extend the Nystr?m method and propose a landmark based large-scale spectral clustering (LSC) method, which uses -means to obtain cluster centers as landmark points to construct the similarity sub-matrix. With the constructed ? sub-matrix, they then convert it into sparse by preserving the -nearest landmarks of the data points and filling with zeros to others. By the -means based landmarks selection, the LSC algorithm shows better performance than Nystr?m. On this basis, some studies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12]</ref> on the landmark selection are further proposed to improve the instability of sub-matrix based large-scale spectral clustering. However, the computational complexity of the sub-matrix construction can still be a critical bottleneck when dealing with largescale clustering tasks. Huang et al. <ref type="bibr" target="#b10">[11]</ref> propose a hybrid representative (landmark) selection method that initializes candidate samples randomly from the dataset and performs -means to obtain cluster centers as the representative points then computes the approximation of -nearest representatives. It does not compute the dense similarity sub-matrix but approximates a sparse sub-matrix, further reducing similarity construction costs. However, those sub-matrix based spectral clustering algorithms are typically restricted by an ( ) or ( 1 2 ) complexity bottleneck, which is still a critical hurdle for them to deal with large-scale datasets where a larger is often desired for achieving better approximation. Although some considerable studies have been proposed in recent years, it remains a highly challenging problem, i.e., how to make spectral clustering handle large-scale datasets efficiently and effectively within limited computing resources.</p><p>In this paper, to achieve a better balance between the effectiveness and efficiency of the spectral clustering for largescale datasets, we propose the divide-and-conquer spectral clustering (DnC-SC) method. In DnC-SC, a novel divideand-conquer based landmark selection method is proposed to generate high-quality landmarks, which reduces the computational complexity of -means based selection from ( ) to ( ), where is the selection rate parameter that determines the upper bound of computational complexity. Besides, a fast approximation method for -nearest landmarks is designed to efficiently build a sparse sub-matrix with ( ) computational complexity and ( ) space complexity. A cross similarity matrix is constructed between the data points and the landmarks, which can be interpreted as the edges matrix of a bipartite graph. The bipartite graph partitioning is then conducted to solve the spectrum with ( ( + ) + 3 ), where is the number of clusters. Finally, the -means method is used to obtain the clustering result on the spectrum with ( 2 ), where is the number of iterations during -means. As it generally holds that , , ? ? , the computational and space complexity of our DnC-SC algorithm are respectively dominated by ( ) and ( ). The experimental results on ten large-scale datasets (consisting of five real-word datasets and five synthetic datasets) show the priority performance of proposed methods on both efficiency and effectiveness.</p><p>The main contributions of the proposed method are summaries as follows:</p><p>? A divide-and-conquer-based landmark selection method is proposed to efficiently find centralized subset centers as landmarks in a recursive manner.</p><p>? A fast -nearest landmarks search method is designed, which uses centers' nature of landmarks to identify the most possible -nearest landmarks candidates.</p><p>? A large-scale spectral clustering algorithm termed DnC-SC is proposed, which efficiently constructs the similarity matrix and uses bipartite graph partitioning to obtain final clustering results. Its computational and space complexity is dominated by ( ) and ( ), which achieves a lower computational complexity than most existing large-scale spectral clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>This section reviews the literature related to spectral clustering and large-scale spectral clustering extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spectral Clustering</head><p>Spectral clustering aims to partition the data points into clusters using the spectrum of the graph Laplacians <ref type="bibr" target="#b22">[23]</ref>. Given a dataset = 1 , ? , with data points, spectral clustering algorithm first constructs similarity matrix , where indicates the similarity between data points and via a similarity measure metric. Let = ? , where is called graph Laplacian and is a diagonal matrix with = ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>. The objective function of spectral clustering can be formulated based on the graph Laplacian as follow:</p><formula xml:id="formula_0">min tr , s.t. = ,<label>(1)</label></formula><p>where tr(?) denotes the trace norm of a matrix. The rows of matrix are the low dimensional embedding of the original data points. Generally, spectral clustering computes as the bottom eigenvectors of , and finally applies -means on to obtain the clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Large-scale Spectral Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Similarity Sub-matrix construction</head><p>Instead of an ? similarity matrix, many large-scale spectral clustering methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref> are using a similarity sub-matrix to represent each data points. The similarity sub-matrix consists of the cross-similarities between data points and a set of representative data points (i.e., landmarks) via some similarity measures, as</p><formula xml:id="formula_1">= ?( , ),<label>(2)</label></formula><p>where = { 1 , 2 , ? , } ( ? ) is a set of landmarks with the same dimension to , ?(?) indicate a similarity measure metric, and ? ? ? is the similarity sub-matrix to represent the ? ? ? with respect to the ? ? ? . Ideally, the landmark points 1 , 2 , ? , would roughly represent the distribution of . Some previous studies <ref type="bibr" target="#b3">[4]</ref> show the effectiveness of -means based selection. The objective function of -means based landmark selection can represented as follows:</p><formula xml:id="formula_2">= arg min 1 ,?, ? =1 ? ? ? ? ? ? ? ? ? 2 ,<label>(3)</label></formula><p>where 1 , 2 , ? , indicate the subsets that are nearest to 1 , 2 , ? , , respectively. However, directly conductingmeans on large-scale datasets faces a high time cost of ( ). Moreover, -means often needs more iterations to converges on large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Efficient Bipartite Graph Partitioning</head><p>The similarity sub-matrix reflects the relationship between and , which can be naturally treated as a bipartite graph = { , , }. The goal of bipartite graph partitioning is to partition the graph into groups. The full similarity matrix of is then designed as <ref type="bibr" target="#b28">[29]</ref> = .</p><p>(4)</p><p>The size of matrix is ( + ) ? ( + ). The conventional spectral clustering finds a low dimensional embedding via the spectrum of graph Laplacian, which solves the generalized eigen-problem <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_3">= ,<label>(5)</label></formula><p>where = ? is the graph Laplacian and is a diagonal matrix with = ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>. Note that the eigenvector can be interpreted as two parts ? ? and ? ? .</p><formula xml:id="formula_4">= ,<label>(6)</label></formula><p>where is the eigenvector on side while is the eigenvector on side. The problem is how to efficiently compute eigenvector and construct a low dimensional embedding on original data . An efficient computation method called transfer cut is often used to compute the spectrum of graph Laplacian for bipartite graph partitioning problems. Instead of directly computing by partial SVDs or dual property of SVD <ref type="bibr" target="#b3">[4]</ref>, the transfer cuts process first computes the by solving a much smaller eigen-problem as follows:</p><formula xml:id="formula_5">= ,<label>(7)</label></formula><formula xml:id="formula_6">where = ? ?1 , ? ? ? and ? ? ? are the diagonal matrices whose entries are ( , ) = ? =1 and ( , ) = ? =1</formula><p>, respectively. It has been pointed out that the eigen-problems (5) on original bipartite graph and the much smaller one <ref type="bibr" target="#b6">(7)</ref> are essential equivalence <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr">Let</ref> , =1 be the bottom eigenpairs of the eigenproblem <ref type="bibr" target="#b6">(7)</ref> and</p><formula xml:id="formula_7">0 = 1 ? ? ? &lt; 1. Then , =1</formula><p>are the bottom eigenpairs of the eigen-problem (5) and 0 = 1 ? ? ? &lt; 1. It have been proved that [14]</p><formula xml:id="formula_8">= 1 1 ? ,<label>(8)</label></formula><p>where 1 ? &lt; 1, = (2 ? ) and ?= ?1 is called the associated transition probability matrix. Therefore bottom eigenvectors 1 , ? , are calculated according to <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. Let ? ? ? be the matrix containing the vectors 1 , ? , as columns. Then is the spectral embedding of the large-scale spectral clustering algorithm. Finally,means is conducted on the embedding to obtain final clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head><p>To further reduce the complexity of spectral clustering, we propose the DnC-SC method that complies with the submatrix based formulation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4]</ref> and aims to break through the efficiency bottleneck of previous algorithms. DnC-SC consists of three phases: (1) Divide-and-conquer based landmark selection: we consider landmark selection as an optimization problem and present a divide-and-conquer based landmark selection method to find the landmarks via solving the sub-optimization problems recursively. (2) Approximate similarity matrix construction: we design a novel strategy to efficiently approximate the -nearest landmarks for each data point and construct a sparse cross-similarity matrix between the data points and the landmarks. (3) Bipartite graph partitioning: we interpret the cross-similarity as a bipartite graph and conduct the bipartite graph partitioning to obtain the clustering result. We summarize the proposed method in <ref type="figure">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Divide-and-conquer based Landmark Selection</head><p>We propose a divide-and-conquer based landmark selection method, which aims to find a set of high-quality landmarks efficiently. Instead of directly dividing data points into subsets like (3) for landmark selection, we first divide data points into subsets, then recursively divide each subset into smaller subsets ( ? ), until the total number of subsets reaches . Denote as a small number and ? ? . We define as selection rate parameter that is the upper boundary of desired subset number in each dividing process to limit the computational complexity. <ref type="figure">Figure 2</ref> gives a simple example. The data points are recursively divided into subsets until the total number of subsets is , which avoids directly applying -means to obtain too many subsets at once.</p><p>In the divide-and-conquer strategy, the number of desired subsets in each iteration is much smaller than , and the subsets are smaller and smaller during iterations than directly applying -means to datasets. Suppose each dividing process will divide subsets of the same size, the number of subsets increases exponentially and reaches subsets in ?log ? rounds. Denote (1) 1 = is the initial subset. In first round of dividing process, data points are divided into subsets <ref type="bibr" target="#b1">(2)</ref> 1 , (2) 2 , ? , <ref type="bibr" target="#b1">(2)</ref> with computational complexity of ( ), which is illustrated in the (1) of <ref type="figure">Figure  2</ref>. There are subsets currently, each current subset is then divided into new subsets in the second round with computational complexity of ( ). For example, (2) 1 is further divided into new subsets <ref type="bibr" target="#b2">(3)</ref> 1 , (3) 2 , ? , <ref type="bibr" target="#b2">(3)</ref> , which is illustrated in the (2) of <ref type="figure">Figure 2</ref>. The total computational complexity of the second round is ( ) = ( ). The dividing process of second round generates 2 new sub-</p><formula xml:id="formula_9">sets (3) 1 , (3) 2 , ? , (3) 2 totally.</formula><p>Similarly, the computational complexity of any -th ( = 1, 2, ? , ?log ?) dividing round will be ( ?1 ?1 ) = ( ). Therefore, the total computational complexity of divide-and-conquer strategy is (?log ? ). Note that log is a small value and can be treated as a constant, e.g., log = 1.77 when setting = 1000 and = 50. We further simplify the total computational cost as ( ). Compared with -means based landmark selection, the divide-and-conquer strategy can naturally reduce the computational complexity from ( ) to ( ) ( ? ). Moreover, we design an efficient dividing algorithm, named light--means, to further accelerate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Divide-and-conquer based</head><p>Landmark Selection Data <ref type="figure">Figure 1</ref>: An overview of proposed DnC-SC method. Given a dataset, the DnC-SC method first finds the landmarks via divideand-conquer based landmark selection, then approximately constructs the similarity matrix, finally conducts the bipartite graph partitioning to obtain final clustering results. Our main contributions focus on the first two phases (colored as orange), i.e., the landmark selection and similarity construction phases. </p><formula xml:id="formula_10">1 st iteration 2 nd iteration 3 rd iteration ! (!) = 3 ! ($) = 3 $ ($) = 3 % ($) = 3 (2) (1) Dividing</formula><p>Dividing Centers Samples Dividing Dividing <ref type="figure">Figure 2</ref>: An illustration of the divide-and-conquer base landmark selection: (1) The dataset is initially divided into (1) 1 = 3 initial subsets; (2) Each subset is further divided into 3 smaller subsets and the total number of subsets reaches = 9. Finally, the centers of subsets are turned as landmarks.</p><p>the whole process into ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Divide-and-conquer Selection Strategy</head><p>Before starting landmark selection, we first review the optimization problem (3). The variables in (3) are the 1 , ? , which are used to map the unique 1 , ? , . By setting the subsets 1 , ? , and landmark number as the variables, we can rewrite the optimization problem (3) into a function form as follows:</p><formula xml:id="formula_11">( , ) = arg min 1 ,?, ? =1 ? ? ? ? ? ? ? ? ? 2 ,<label>(9)</label></formula><p>where (?) indicates a centralized clustering problem that divides into subsets and is the center of subset . For any dividing problem that divides into ? subsets, the function ( , ?) can be used to describe the dividing problem, and its computational complexity is</p><formula xml:id="formula_12">(? ?? ), where ? ?</formula><p>is the total number of samples in . More importantly, function ( , ?) can be used to derive the recursive function as follows:</p><formula xml:id="formula_13">( , ?) = ? =1 ( , ),<label>(10)</label></formula><formula xml:id="formula_14">{ 1 , ? , } = ( , ),<label>(11)</label></formula><p>where is a subset of and = ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>; is the desired subset number of subset and ? = ? =1 ; &lt; ?. (10) can simply divide any optimization problem into subproblems, which builds a bridge between global problem ( , ?) and local problem ( , ). We can recursively apply (10) and <ref type="bibr" target="#b10">(11)</ref> to divide the optimization problem (9) into the sub-problems small enough and solve them locally and efficiently.</p><p>Denote as the total number of subsets during -th iteration. We will stop the recursive process in the -th iteration when reaches the desired total number of subsets . Initially, we assign all data points as one subset. As we only have one subset ( 1 = 1 &lt; ), the dividing process happens. Let ( ) be the desired number of subsets for dividing process on -th subset during -th iteration. We naturally set the desired number (1) 1 = . However, directly apply ( , ) may be time-consuming. For &gt; , we force = to obtain subsets partially. As a result, we have the initial setting as follows:</p><formula xml:id="formula_15">(1) 1 = (1) 1 = (12)</formula><p>In the first iteration, we divide (1) 1 into (1) 1 subsets in as follows:</p><formula xml:id="formula_16">{ (2) 1 , (2) 2 , ? , (2) 2 } = ( (1) 1 , (1) 1 ),<label>(13)</label></formula><p>where <ref type="bibr" target="#b0">(1)</ref> indicates the -th subset during first iteration and</p><formula xml:id="formula_17">2 = ? 1 =1 ( ) 1 = (1)</formula><p>1 is the total number of subsets. From the second iteration, there are more and more subsets being obtained. Thus, we need a subset number allocation strategy to determine the desired number of subsets and guide iteration dynamically. We define the residual sum of squares (RSS) of subset ( ) as</p><formula xml:id="formula_18">( ) = ? ? ( ) ? ? ? ? ? 2 ,<label>(14)</label></formula><p>where is the center of the subset ( ) . Consider the global problem <ref type="formula" target="#formula_2">(3)</ref>, the desired number of subsets should be proportional to their RSS. We propose a dynamical allocation strategy as follows:</p><formula xml:id="formula_19">( ) = ? ? ? ? ? ( ) ? ( ) , if ( ) ? ( ) &lt; , , otherwise,<label>(15)</label></formula><p>where ( ) is the allocated dividing number for subset ( ) . Then, all ( ) are turned as integers and fix the +1 &lt;= ,</p><formula xml:id="formula_20">where +1 = ? ( ) . After obtaining ( ) 1 , ? , ( ) ( &lt; ), we will divide each subset ( ) into ( ) smaller subsets via ( ( ) , ( ) ).</formula><p>We then collect all subsets as follows:</p><formula xml:id="formula_21">{ ( +1) 1 , ( +1)) 2 , ? , ( +1) +1 } = ? =1 ( ( ) , ( ) ). (16)</formula><p>We repeat the above process until subsets have been produced and set the subset centers as the landmarks. Take an example using <ref type="figure">Figure 2</ref>, where we set = 3 and = 9. In the first iteration, we assign all data points as one subset. Since the desired landmark number &gt; , we set (1) 1 = 3. Then we initially divide the dataset (1) 1 = into (1) 1 = 3 subsets. In the second iteration, there are three subsets <ref type="bibr" target="#b1">(2)</ref> 1 , (2) 2 and (2) 3 . According to <ref type="bibr" target="#b14">(15)</ref>, we compute the <ref type="bibr" target="#b1">(2)</ref> 1 , (2) 2 , (2) 2 . Suppose (2) 1 = (2) 2 = (2) 2 = 3, we then divide <ref type="bibr" target="#b1">(2)</ref> 1 , (2) 2 , (2) 3 into (2) 1 , (2) 2 , (2) 2 smaller subset respectively. In the third iteration, there are 9 subsets</p><formula xml:id="formula_22">(3) 1 , ? , (3) 9 .</formula><p>Since the total number of subsets 3 = 9 reaches the desired landmark number = 9, we stop the recursive process in the third iteration. Finally, compute the subset centers 1 , 2 , ? , of (3) 1 , ? , (3) 9 and set them as the landmarks.</p><p>Note that the dividing process (?) can be directly solved by the -means method. However, directly apply -means on large data is time-consuming. To further reduce the complexity, we propose a modified -means method, named light--means. When dataset size is large, we conduct the dividing process via light--means. Otherwise, we use the traditional -means method. We summary the divide-andconquer based landmark selection method in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Light--means Algorithm</head><p>We define ? as an upper bound. When the size of ( ) is larger than ? , we will use light--means to compute ( ( ) , ( ) ). The light--means is performed as the following steps:</p><p>1. Randomly select ? representatives from ( ) and denote them in a set as and the complement of is .  <ref type="bibr" target="#b15">16</ref> Collect the latest cluster centers as .</p><p>2. Conduct -means to divide into ( ) subsets; 3. Find the nearest subset centers for the remained data points in ; 4. Assign the remained data points in to their nearest subsets (with the center nearest to these points). <ref type="figure">Figure 3</ref> shows a comparison between -means and light--means method, which are the implementation examples of (1) in <ref type="figure">Figure 2</ref>. Given a subset <ref type="bibr" target="#b0">(1)</ref> 1 , the light--means first randomly select ? data points and denotes them as and the complement is</p><formula xml:id="formula_23">( (1) 1 = ? ).</formula><p>Then the -means is used to divide into (1) 1 subsets, i.e., 1 , 2 , 3 . For each data points in , find its nearest center and assign it to the subset, i.e., 1 , 2 , 3 , according to its nearest center. Finally, return the combined subsets 2 1 , 2 2 , 2 3 as the results of this dividing process.  cess ( ( ) , ( ) ) should be ( ? ( ) )+ (( ( ) ? ? ) ( ) )) = ( ( ) ( ) + ? ( ) ( ? 1)), where ( ( ) ( ) ) is the dominant term. While, -means costs ( ( ) ( ) ) for the same dividing process. Compared with -means, light--means significantly alleviates the computational complexity of iterative optimization. Empirically, the number of ? is suggested to be several times larger than , e.g., ? = 10 , to provide enough samples for the -means algorithm. Since our landmark selection focuses more on the local dividing, the light--means can effectively divide the large subsets into small ones and find more accurate subsets locally. Finally, we summarise the light--means method in Algorithm 2.</p><p>By introducing the divide-and-conquer based landmark selection, the complexity of landmark selection is reduced to ( ) from ( ) of -means based selection. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates that the proposed divide-and-conquer based landmark selection can better represent data distribution than the random selection and has similar performance -means based selection, but it has the lower complexity than -means based selection. <ref type="figure">Figure 5</ref>: An illustration of our -nearest landmarks approximation: (1) Find the ? -nearest ( ? &gt; ) landmarks of 1 , where ? and 1 is the subset center of ; (2) Find the -nearest landmarks among ? -nearest landmarks of 1 .</p><formula xml:id="formula_24">! " ! # ! " ! $ " ! % " ! &amp; " ! ' ? " ! ' " ? " ! # ! " ! # " ! $ " ! % " ! &amp; " ! ' ? " ! ' " ? (1) (2) " ! ' " " ! # = { " ! # ,?, " ! ' ,?, " ! ' " } ' ! = { " ! # ,?, " ! ' }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Approximate Similarity Matrix Construction</head><p>After landmark selection, the next object is to construct a similarity matrix between entire data points and the landmarks. Instead of dense similarity matrix, we design a similarity matrix ? ? ? according to -nearest neighbor as follows:</p><formula xml:id="formula_25">= exp( ?? ? ? 2 2 2 ), if ? ( ), 0, otherwise,<label>(17)</label></formula><p>where ( ) denotes the set of -nearest landmarks of and is the bandwidth of Gaussian kernel. Note that there are only non-zero entries in the sparse matrix . To estimate ( ), we propose a new -nearest landmarks approximation method. The main idea is to use the subset centers' nature of landmarks to estimate the possible nearest candidates, as shown in <ref type="figure">Figure 5</ref>. Formally, we denote as the subset that belongs to, and the landmark 1 as the center of . Since 1 is the subset center of , it essentially is the nearest landmark of according to <ref type="bibr" target="#b2">(3)</ref>. Take the advantage of this landmark nature, we search the -nearest landmarks of each data point ? according to the following two steps:</p><p>Step 1: Find ? possibles candidates. As (1) of <ref type="figure">Figure   5</ref> shows, we find the ? -nearest ( ? &gt; ) landmarks of 1 and denoted them as</p><formula xml:id="formula_26">? ( 1 ) = 1 , ? , ? .</formula><p>Since the exact -nearest landmarks of are highly possible closed to 1 , we treat ? ( 1 ) as possible candidates set. Empirically, the number of ? is suggested to be several times larger than , e.g., ? = 10 , to provide enough candidates to search ( ).</p><p>Step 2: Search the -nearest landmarks. As <ref type="formula" target="#formula_1">(2)</ref> of <ref type="figure">Figure 5</ref> shows, we search the -nearest landmarks of among ? ( 1 ) and denote them as ( ). After the -nearest landmarks approximation, we compute the similarity matrix according to <ref type="bibr" target="#b16">(17)</ref>. For all data points, the complexity of step 1 is ( 2 ( + )) and step 2 is ( ( + )). The computational complexity of our similarity construction is ( 2 ( + ) + ( + )). As , ? ? , the dominant term in the complexity is ( ). Compared with the exact similarity construction of ( ) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>, our method is much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bipartite Graph Partitioning</head><p>After obtaining the similarity matrix , we conduct graph partitioning on the graph Laplacian. The similarity matrix reflects the relationships between and . Therefore can be interpreted as the cross-similarity matrix of the bipartite graph:</p><formula xml:id="formula_27">= { , , },<label>(18)</label></formula><p>where ? is the node-set. As a result, the objective is changed to a bipartite graph partitioning problem. We apply transfer cuts to efficiently compute the spectrum of graph Laplacian for spectral clustering. The bottom eigenvectors 1 , 2 , ? , for side can be computed according to <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. Let ? ? ? be the matrix containing the vectors 1 , ? , as columns, then matrix will be the spectrum of graph Laplacian. In practice, we normalized by its 1-norm as? , then apply -means clustering on? to obtain the final clustering results <ref type="bibr" target="#b16">[17]</ref>. Themeans clustering is then performed on this embedding to obtain the clusters as the final clustering result with ( 2 ) computational complexity. We summarize the divided-andconquer based large-scale spectral clustering in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Computational Complexity Analysis</head><p>In this section, we summary the computational cost of the proposed method in each phase.</p><p>The divide-and-conquer based landmark selection takes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relations with Other Methods</head><p>As a large-scale spectral clustering method, the proposed method is closely related to the methods in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. We compare the proposed method with the two methods to discuss the improvements of the proposed method.  <ref type="bibr">Let</ref> be the subset that belongs to and 1 be the center of ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Obtain ? -nearest ( ? &gt; ) landmarks of 1 , denoted as ? ( 1 );</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Find the -nearest landmarks of from ? ( 1 ), denoted as ( ); 6 end 7 Construct sparse similarity sub-matrix by <ref type="formula" target="#formula_0">(17)</ref>; <ref type="bibr" target="#b7">8</ref> Calculate by solve the eigen-problem <ref type="formula" target="#formula_5">(7)</ref>; 9 Obtain spectral embedding by <ref type="formula" target="#formula_8">(8)</ref>; <ref type="bibr" target="#b9">10</ref> Conduct -means on the bottom eigenvectors of to obtain final clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Comparison of the computational complexity of several largescale spectral clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method landmark selection</head><p>Similarity construction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigendecomposition</head><formula xml:id="formula_28">Nystr?m / ( ) ( + 3 ) LSC-R / ( ) ( 2 + 3 ) LSC-K ( ) ( ) ( 2 + 3 ) U-SPEC ( 2 ) ( 1 2 ) ( ( + )+ 3 ) DnC-SC ( ) ( ) ( ( + )+ 3 ) * The final -means is ( 2 ) for each method.</formula><p>Firstly, we compare them on the landmark selection methods. Both the two methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> directly or indirectly apply -means based landmark selection. LSC-K method <ref type="bibr" target="#b3">[4]</ref> directly conduct -means algorithm to select landmarks within a high computational complexity ( ). While the U-SPEC <ref type="bibr" target="#b10">[11]</ref> indirectly conduct -means algorithm on a random set of samples to select landmarks, which finds a balance between -means and random selection within ( 2 ) time cost. Despite U-SPEC can efficiently find landmarks, it also has two limitations: 1) The quality of landmarks highly depends on how good the random set of samples is set up; 2) Since landmarks are not the centers for all data points, the center's nature of landmark can not be used to approximate the similarity matrix. The proposed method uses the divide-and-conquer based landmark selection, which can effectively produce high-quality landmarks. We design a objection function <ref type="formula" target="#formula_2">(3)</ref> is to find the landmarks that best represent all data points with minimum RSS. We then propose a divide-and-conquer strategy to divide (3) into local subproblems and use light--means to effectively solve them. Finally, we combine all sub-problems and obtain landmarks. The our landmark selection produces landmarks within ( ) computational time. Moreover, the our landmarks are essentially the centers of subsets for all data points, which can be used to approximate the similarity matrix next.</p><p>Secondly, we compare them on the similarity construction. LSC-K needs to cost ( ) to compute the dense similarity matrix at first to conduct the -nearest neighbor sparse. The U-SPEC method indirectly computes the sparse similarity sub-matrix in a coarse-to-fine mechanism to approximate the -nearest landmarks within ( 1 2 ) time cost. U-SPEC first cluster landmarks into 1 2 clusters and then compute the distances between data points and 1 2 cluster centers to find the possible range of nearest landmarks. For the proposed method, since the landmarks essentially are the cluster centers of data points, we can easily identify a highly possible range of -nearest landmarks according to the centers' nature of landmarks and find -nearest landmarks in this range. The proposed -nearest landmarks search method costs ( ) computational time. Overall, DnC-SC consists of divide-and-conquer based landmark selection, approximate similarity construction, and bipartite graph partition. It conducts spectral clustering tasks within ( ) computational complexity and ( ) space complexity, which is faster than most existing large-scale spectral clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct experiments on five real and five synthetic datasets to evaluate the performance of the proposed DnC-SC methods. The comparison experiments against several state-of-the-art spectral clustering methods show better performance on clustering quality and efficiency for DnC-SC methods. Besides that, the analysis of the parameters is performed. For each experiment, the test method is repeated 20 times, and the average performance is reported. All experiments are conducted in Matlab R2020a on a Mac Pro with 3 GHz 8-Core Intel Xeon E5 and 16 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Measures</head><p>Our experiments are conducted on ten large-scale datasets, varying from nine thousand to as large as twenty million data points. Specifically, the five real datasets are USPS <ref type="bibr">[</ref>   <ref type="figure">Figure 6</ref>: Illustration of the five synthetic datasets. Note that only a 1% or 0.1% samples of each dataset is plotted.</p><formula xml:id="formula_29">(a) TS-60K (1%) (b) TM-1M (1%) (c) TC-6M (1%) (d) CG-10M (0.1%) (e) FL-20M (0.1%)</formula><p>(FL-20M) <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b4">5</ref> . <ref type="figure">Figure 6</ref> shows the synthetic datasets. The properties of the datasets are summarized in <ref type="table" target="#tab_4">Table 2</ref>. We adopt two widely used evaluation metrics, i.e., Normalized Mutual Information (NMI) <ref type="bibr" target="#b21">[22]</ref> and Accuracy (ACC) <ref type="bibr" target="#b25">[26]</ref>, to evaluate the clustering results. Let = [ 1 , 2 , ..., ] be the data matrix. For each data point , denote and as the cluster label of ground truth and obtained cluster label from clustering methods, respectively. The ACC is defined as:</p><formula xml:id="formula_30">ACC = ? =1 ( , map( )) ,<label>(19)</label></formula><p>where is the number of data and ( , ) is a function to check and are equal or not, returning 1 if equals otherwise returning 0. The map( ) is a best mapping function that maps each predicted label to the most possibly true cluster label by permuting operations <ref type="bibr" target="#b24">[25]</ref>. Let denote a set of clusters of ground truth and obtained from clustering methods. Mutual information (MI) is defined as</p><formula xml:id="formula_31">( , ) = ? ? , ? ( , )ln ( , ) ( ) ( ) ,<label>(20)</label></formula><p>where ( ) and ( ) are marginal probabilities that a sample happens to belong to cluster or while ( , ) is the joint probabilities that a sample happens to belong to cluster both and . The NMI is the normalization of MI by the joint entropy as follow:</p><formula xml:id="formula_32">( , ) = ? ? , ? ( , )ln ( , ) ( ) ( ) ? ? ? , ? ( , )ln( ( , )) ,<label>(21)</label></formula><p>A better clustering result will provide a larger value of NMI/ACC. Both NMI and ACC are in the range of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline Methods and Experimental Settings</head><p>In this experiment, we compare the proposed method with two baseline clustering methods, which are -means clustering and spectral clustering (SC) <ref type="bibr" target="#b6">[7]</ref>, as well as six state-of-the-art large-scale spectral clustering methods. The compared spectral clustering methods are listed as follows:</p><p>1. SC <ref type="bibr" target="#b6">[7]</ref>: original spectral clustering 6 . 2. Nystr?m <ref type="bibr" target="#b8">[9]</ref>: Nystr?m spectral clustering 6 . 3. LSC-K <ref type="bibr" target="#b3">[4]</ref>: landmark based spectral clustering using -means based landmark selection 7 . 4. LSC-R <ref type="bibr" target="#b3">[4]</ref>: landmark based spectral clustering using random landmark selection 7 . 5. LSC-KH <ref type="bibr" target="#b26">[27]</ref>: Landmark-based spectral clustering using -means partition to find the hubs as the landmarks 8 . 6. LSC-RH <ref type="bibr" target="#b26">[27]</ref>: Landmark-based spectral clustering using random partition to find the hubs as the landmarks 8 . 7. U-SPEC <ref type="bibr" target="#b10">[11]</ref>: Ultra-Scalable Spectral Clustering 5 .</p><p>There are several common parameters among the methods mentioned above. We set these parameters as follow:</p><p>? We set the number of landmarks or representatives as = 1000 for DnC-SC, U-SPEC, Nystr?m, LSC-K, and LSC-R methods. The parameter analysis on will be further conducted in Section 5.3.1.</p><p>? We set the = 5 for the number of nearest neighbors for DnC-SC, U-SPEC, LSC-K, and LSC-R. The parameter analysis on will be further conducted in Section 5.3.2.</p><p>? The DnC-SC method has a unique parameter . In the experiments, = 200 is used for the datasets whose size is less than 100,000, otherwise = 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Large-scale Spectral Clustering Methods</head><p>In this section, we compare the proposed DnC-SC method with five state-of-the-art spectral clustering methods, as well as the -means clustering and original spectral clustering methods as the baseline methods.</p><p>We report the experimental results in <ref type="table" target="#tab_2">Tables 3, 4</ref> and 5, where we use N/A to denote the case when MATLAB reports the error of out of memory. Only two methods (proposed DnC-SC and U-SPEC) pass all datasets because they can approximately compute the similarity matrix within a limited memory. The proposed DnC-SC method achieves the best clustering performance of both ACC and NMI ten times on ten benchmark datasets according to <ref type="table" target="#tab_2">Table 3</ref> and 4. The proposed DnC-SC method achieves the best efficiency nine times on ten benchmark datasets according to <ref type="table" target="#tab_7">Table 5</ref>.</p><p>In addition, we report the average performance score and rank for each method in Tables 3, 4 and 5. The proposed DnC-SC method achieves the best average scores of both ACC and NMI. The DnC-SC method shows average ranks of 1.50 of ACC and 1.40 of NMI, which implies the best clustering quality in all spectral clustering methods. Moreover, the DnC-SC method costs much less average time than the other competitors and achieves a rank of 1.50, which implies the most efficient method in this experiment. Overall, the proposed DnC-SC method shows significant effectiveness and efficiency comparing with six state-of-the-art large-scale spectral clustering methods.</p><p>We conduct a series of parameters analysis experiments to demonstrate the performance of the proposed method varying different parameter settings. We select four dataset (Letters, MNIST, TS-60K, and TM-1M) as benchmark datasets to conduct the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Number of Landmarks</head><p>We first conduct parameter analysis to compare the largescale spectral clustering methods by varying the number of landmarks (also called landmarks) and report the experimental results in <ref type="table">Table 6</ref>. In general, we can see that a larger value of brings a better performance of ACC and NMI but cost more time. The proposed DnC-SC achieves the best ACC and NMI scores on all datasets except the MNIST. On MNIST dataset, the proposed DnC-SC method shows the second-best ACC and NMI scores after the LSC-K method. In terms of time cost, the proposed DnC-SC method shows the best efficiency on all datasets. Overall, the proposed DnC-SC method shows significant effectiveness and efficiency in this comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Number of Nearest Landmarks</head><p>We then conduct parameter analysis to compare the largescale spectral clustering methods by varying the number of the nearest landmark and report the experimental results in <ref type="table" target="#tab_8">Table 7</ref>. Note that the Nystr?m method does not have the parameter . Therefore, we do not show the results of the Nystr?m method in this experiment. According to Table 7, the performance of most methods varies for different values. The proposed method shows the best ACC and NMI of performance for the three of four datasets, and the second-best ACC and NMI on the MNIST dataset. Overall, the proposed DnC-SC shows superior effectiveness and the  best efficiency on this comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Number of Nearest Landmarks and selection rate</head><p>To further demonstrate the proposed method, we evaluate the performances by varying parameters and and re-port the experimental results in <ref type="table" target="#tab_9">Table 8</ref>. For proposed DnC-SC methods, the selection rate parameter directly affects the computational complexity of landmark selection, while the number of nearest landmarks affects similarity construction, respectively. As we can see, a larger or generally leads more time cost while not necessarily achieves bet-  <ref type="table">Table 6</ref> Clustering performance (ACC(%), NMI(%), and time costs(s)) for different methods by varying number of landmark .  Clustering performance (ACC(%), NMI(%), and time costs(s)) for different methods by varying number of nearest landmarks . Overall, the proposed method shows considerable robustness with various parameters on ACC and NMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Efficiency analysis</head><p>To explore the efficiency of the proposed method in each computational phase, we report the time costs of three different phrases: landmark selection, similarity construction, and graph partitioning. We choose LSC-K, LSC-R, and U-SPEC algorithms that have similar mechanisms for comparison. We list the strategies and methods used in each method in <ref type="table" target="#tab_10">Table 11</ref>. The experimental results are reported in <ref type="table" target="#tab_4">Table  12</ref>.</p><p>For landmark selection, the LSC-K and LSC-R methods apply -means and a random selection, respectively; the U-SPEC method uses a hybrid selection that conductsmeans on a small set of random candidates; DnC-SC utilizes  <ref type="table">Table 9</ref> Clustering performance (ACC(%), NMI(%), and time costs(s)) for DnC-SC using divide-and-conquer based landmark selection and -means based landmark selection. the divide-and-conquer selection. Looking at the runtime of the landmark selection, we see that the random selection of LSC-R takes a little time, while the -means selection takes much more time. The divide-and-conquer selection of DnC-SC is the second-fastest method just behind the random selection. For similarity construction, the LSC-K and LSC-R compute the exact similarity matrix without approximation, while U-SPEC and DnC-SC calculate the similarity by approximate schemes. Compared with U-SPEC, DnC-SC uses the results of landmark selection to improve the approximate <ref type="table">Table 10</ref> Clustering performance (ACC(%), NMI(%), and time costs(s)) for DnC-SC using approximate -nearest landmarks and exact -nearest landmarks. scheme. For the runtime of similarity construction, we find that DnC-SC takes significantly less time than other methods, especially for the larger-scale dataset (TM-1M). Note that the approximate similarity matrix of U-SPEC takes more time than LSC-K or LSC-R in MNIST dataset. However, the similarity of U-SPEC takes less computational complexity than LSC-K or LSC-R. This is because U-SPEC uses serial calculations in the approximation process. In MAT-LAB, it will be much faster to perform the approximation in a batch processing manner (with optimized matrix computation) than in a serial processing manner.  <ref type="table" target="#tab_4">Table 12</ref> Comparison of time costs in each phase for different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Time costs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landmark selection Similarity construction Graph Partitioning</head><p>For graph partitioning, LSC-K and LSC-R utilize SVD based method, while U-SPEC and DnC-SC apply transfer cuts. Theoretically, both two graph partitioning methods can be considered as efficient solutions for bipartite graph partitioning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>. But the transfer cuts take less computational complexity. In <ref type="table" target="#tab_4">Table 12</ref>, we can see that U-SPEC and DnC-SC take less time than LSC-K and LSC-R, which is consistent with the theoretical complexity.</p><p>Overall, DnC-SC shows the best efficiency in four methods, which is mainly due to the proposed landmark selection and approximate similarity construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Influence of Landmark Selection Strategies</head><p>Some existing works have shown that the performance of large-scale spectral clustering heavily relies on the proper strategy of landmark selection <ref type="bibr" target="#b11">[12]</ref>. In our proposed landmark selection, we propose a divide-and-conquer selection strategy and light--means to find a good balance between effectiveness and efficiency. We test the purposed method with different landmark selection methods, i.e., -means based landmark selection, divide-and-conquer selection without light--means, and divide-and-conquer selection with light--means.</p><p>In this section, we compare the performances between the divide-and-conquer based landmark selection and themeans base landmark selection. The experimental results are reported in <ref type="table">Table 9</ref>. As we mentioned, the divide-andconquer based landmark selection algorithm recursively solves the optimization problems 3, which -means methods can also solve. We have pointed out the lack of efficiency of directly applying -means on large-scale datasets in Section 4.1. Note that the number of maximum iterations of -means in landmark selection is turned as 5, which is the same setting as LSC-K and U-SPEC implementation. In <ref type="table">Table 9</ref>,means based landmark selection algorithm generally shows better ACC and NMI on most datasets except TM-1M dataset, while the difference in performance is not significant. Compared to -means based selection, our divide-and-conquer based landmark selection algorithm strikes a balance between efficiency and effectiveness. It achieves significantly better efficiency than the -means based selection and yields competitive clustering quality compared to the -means based selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Performance comparison on simulation scenarios</head><p>To further investigate the performance of divide-and-conquer selection, we conduct a simulation experiment to simulate different scenarios for landmark selection. For landmark selection, the number of landmarks is considered much larger than the desired number of clusters. If we view the landmark selection as a clustering task, then the landmark selection will be considered as a special clustering case with a large number of clusters. Therefore, we generate four synthetic datasets with 500, 1000, 1500, 2000 clusters, respectively. The synthetic datasets are 2-dimensional isotropic Gaussian blobs, which are shown in <ref type="figure" target="#fig_10">Figure 7</ref>. We treat divide-andconquer selection as a clustering algorithm to compare the clustering performance with -means. We report the clustering performance of NMI and time costs for all simulation scenarios in <ref type="table" target="#tab_2">Table 13 and Table 14</ref>.</p><p>Though divide-and-conquer selection shows slightly lower NMI than -means, its time cost is much less. As landmark increases, the performance degradation associated with divideand-conquer selection becomes progressively insignificant, while the improvement of efficiency becomes more significant. The experimental results imply that the divide-andconquer selection is suitable for a larger number of land-  marks while -means selection is suitable for a smaller number of landmarks. Usually, more landmarks will lead to a better clustering result for large-scale spectral clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Thus, the divide-and-conquer selection is more suitable than -means selection for large-scale spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Influence of Approximated -nearest Landmarks</head><p>In this section, we compare the approximated -nearest landmarks and exact -nearest landmarks. The experimental results are reported in <ref type="table">Table 10</ref>. The approximatednearest landmarks approach first finds the possible candidates according to the center's nature of landmarks and then searches the -nearest landmarks among them. The exact -nearest landmarks approach costs ( ) computational time, while the proposed approximation can reduce the time cost to ( ). As the Tables 10 shows, the exact -nearest landmarks approach achieves slightly better ACC and NMI scores than the proposed approximation. However, the performances of the two methods are not significantly different. In terms of time cost, the proposed approximation approach shows highly efficient performance compared with the exact -nearest landmarks. Note that the exact -nearest landmarks approach can not be conducted on datasets whose sizes are more than one million due to the high computational cost. Overall, the proposed approximate -nearest landmark approach shows the robustness and efficiency of this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study</head><p>To strike a good balance between efficiency and effectiveness, the proposed method applies three strategies: (a) divide-and-conquer selection, (b) light--means, and (c) approximate of -nearest landmarks. An ablation study about the influence of the combination of each part is conducted to show the contribution of each strategy. The experimental results are reported in <ref type="table" target="#tab_7">Table 15</ref>. Modules (a) and (b) are used in landmark selection. In <ref type="table" target="#tab_7">Table 15</ref>, DnC--means indicates a modified divide-and-conquer selection that utilizes -means algorithm for the dividing process, and DnC-light--mean indicates the original divide-and-conquer selection that utilizes the light--means algorithm for the dividing process. To show the effects of (a) and (b), we choose -means selection as the baseline. For -nearest landmarks, we provide the exact -nearest landmark option for each landmark selection method. There are three landmark selections and two -nearest landmark methods provided in this ablation study. Thus we have six combinations for comparison. <ref type="table" target="#tab_7">Table 15</ref> shows the performance on six combinations according to different landmark selection and -nearest landmarks methods. The bold texts represent the best ACC, NMI, and Time for each dataset. We first compare the different landmark selection methods: the -means selection archives the best ACC on two datasets and the best NMI on three datasets, but takes much more runtime on all datasets; DnC--means selection archives the best ACC on two datasets and best NMI on one dataset with much less runtime than -means selection; our DnC-light--means takes the least time on all datasets and shows a competitive performance of ACC and NMI. For -nearest landmark, the extra approach archives the best ACC on three datasets and the best NMI on two datasets, which slightly outperforms the approximate approach. We also report the average score for each combination. The combination of -means and extra -nearest landmarks show the best average scores of ACC and NMI, but the most time-consuming. Our proposed method that is the combination of DnC-light--means and approximatenearest landmarks shows the fastest speed and competitive performance of ACC and NMI.</p><p>Overall, the proposed method significantly improves the efficiency of large-scale spectral clustering while keeping the clustering quality acceptable. In detail, we can see that the use of strategy (a) provides the most important contribution to the computational efficiency, while modules (b) and (c) further reduce the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a large-scale clustering method, termed divide-and-conquer based spectral clustering (DnC-SC). In DnC-SC, a divide-and-conquer based landmark selection algorithm is designed to obtain the landmarks effectively. A new approximate similarity matrix construction approach is proposed to utilize the center's nature of the landmarks to fast construct the similarity matrix between data points and -nearest landmarks. Finally, the bipartite graph partition is conducted to obtain the final clustering results. The experimental results on synthetic and real-world datasets show that the proposed method outperforms other state-of-the-art large-scale spectral clustering methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 :1Figure 3 :</head><label>23</label><figDesc>Light--meansInput: Data ( ) , the number of cluster ( ) , number of samples ? ; Output: ( ) subsets; Randomly select ? samples from ( ) and denote them as ; 2 Denote the complement of as ;3 Apply -means to divide into ( ) subsets;<ref type="bibr" target="#b3">4</ref> Find the nearest center of samples in ;<ref type="bibr" target="#b4">5</ref> Assign the samples in to the subset according to their nearest centers.Denote ( ) as the number of samples in ( ) . The computational complexity of light--means for the dividing pro-light--means (1) Randomly select ' representatives as (2) Apply -means on (3) Find the nearest centers of &amp; (4) Assign &amp; to the nearest subset Directly apply -means on ! (!) An comparison between -means and light--means. (a) -means directly divide all samples (1) 1 into 3 subsets. (b) In light--means, -means is applied on ? representatives, which significantly reduces the complexity on large data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of the landmarks produced by (a) random selection, (b) -means based selection, and (c) Divideand-conquer based selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>( ) time. The similarity construction takes ( + 2 ( + )) time. The eigen-decomposition takes ( ( + ) + 3 ) time. The -means discretization takes ( 2 ) time. With consideration to , , ? ? , the overall computational complexity of DnC-SC is ( ( + 2 + + + 2 ) + 3 + 2 ( + )), where ( ( ) ) is the dominant term. Table 1 provides a comparison of computational complexity of our DnC-SC algorithm against several other large-scale spectral clustering algorithms. The space complexity of DnC-SC is ( ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>*</head><label></label><figDesc>LSC-KH and LSC-RH cannot be conduct on the TM-60K and TM-1M dataset due to the memory bottleneck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(k-means based seletion)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Letters</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>(a) 500 Gaussian blobs (b) 1000 Gaussian blobs (c) 1500 Gaussian blobs (d) 2000 Gaussian blobs Illustration of four datasets with 500, 1000, 1500 and 2000 isotropic Gaussian blobs. The number of samples is 100,000 for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 3 :</head><label>3</label><figDesc>Divided-and-conquer based largescale spectral clustering Input: Dataset , the number of landmarks , selection rate , the number of nearest landmarks , the number of cluster ; Output: Cluster labels; 1 Obtain landmarks by divide-and-conquer based landmark selection method;</figDesc><table /><note>2 foreach ? do 3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Properties of the real and synthetic datasets.</figDesc><table><row><cell cols="2">Dataset</cell><cell>#Object</cell><cell cols="2">#Dimension #Class</cell></row><row><cell></cell><cell>USPS</cell><cell>9298</cell><cell>256</cell><cell>10</cell></row><row><cell></cell><cell>PenDigits</cell><cell>10,992</cell><cell>16</cell><cell>10</cell></row><row><cell></cell><cell>Letters</cell><cell>20,000</cell><cell>16</cell><cell>26</cell></row><row><cell>Real</cell><cell>MNIST</cell><cell>70,000</cell><cell>784</cell><cell>10</cell></row><row><cell></cell><cell>Covertype</cell><cell>581,012</cell><cell>54</cell><cell>7</cell></row><row><cell></cell><cell>TS-60K</cell><cell>600,000</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>TM-1M</cell><cell>1,000,000</cell><cell>2</cell><cell>2</cell></row><row><cell>Synthetic</cell><cell>TC-6M</cell><cell>6,000,000</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>CG-10M</cell><cell>10,000,000</cell><cell>2</cell><cell>11</cell></row><row><cell></cell><cell>FL-20M</cell><cell>20,000,000</cell><cell>2</cell><cell>13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Clustering performance (ACC% ? std) for large-scale spectral clustering methods USPS 67.01 ?0.70 73.21 ?3.10 69.47 ?1.38 74.02 ?7.34 73.90 ?4.42 73.66 ?5.18 73.89 ?4.27 80.79 ?3.13 82.55 ?1.96 PenDigits 64.40 ?4.73 67.23 ?4.35 72.46 ?0.18 82.30 ?2.95 81.55 ?3.79 82.17 ?4.09 81.55 ?5.12 81.74 ?4.95 82.27 ?1.33 Letters 25.56 ?1.00 31.21 ?0.76 31.30 ?0.40 33.20 ?2.52 32.34 ?0.15 31.13 ?0.88 31.60 ?1.67 33.20 ?1.16 33.54 ?1.21</figDesc><table><row><cell>Dataset</cell><cell>KM</cell><cell>SC</cell><cell>Nystr?m</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>LSC-KH</cell><cell>LSC-RH</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>MINST</cell><cell>56.60 ?2.71</cell><cell>N/A</cell><cell>57.02 ?3.66</cell><cell>80.96 ?0.10</cell><cell>62.00 ?3.99</cell><cell cols="2">66.59 ?5.33 67.60 ?6.02</cell><cell>72.00 ?3.33</cell><cell>74.24 ?2.14</cell></row><row><cell>Covertype</cell><cell>24.04 ?0.22</cell><cell>N/A</cell><cell>21.65 ?1.30</cell><cell>24.71 ?1.45</cell><cell>23.62 ?1.10</cell><cell>N/A</cell><cell>N/A</cell><cell>24.40 ?2.20</cell><cell>23.48 ?1.86</cell></row><row><cell>TS-60K</cell><cell>56.96 ?0.00</cell><cell>N/A</cell><cell cols="3">55.94 ?10.17 70.37 ?4.57 62.91 ?13.74</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">65.78 ?13.63 81.00 ?9.29</cell></row><row><cell>TM-1M</cell><cell>75.21 ?0.00</cell><cell>N/A</cell><cell>64.63 ?8.40</cell><cell cols="2">51.76 ?0.54 66.41 ?26.68</cell><cell>N/A</cell><cell>N/A</cell><cell>99.96 ?0.01</cell><cell>99.96 ?0.01</cell></row><row><cell>TC-6M</cell><cell>33.34 ?0.00</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>99.86 ?0.03</cell><cell>99.87 ?0.02</cell></row><row><cell>CG-10M</cell><cell>60.47 ?2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">66.77 ?3.97 66.83 ?4.61</cell></row><row><cell>FL-20M</cell><cell>50.07 ?2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">80.17 ?3.97 81.90 ?5.61</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>70.45</cell><cell>72.59</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.80</cell><cell>5.10</cell><cell>2.80</cell><cell>3.90</cell><cell>4.70</cell><cell>4.5</cell><cell>2.30</cell><cell>1.50</cell></row></table><note>* N/A denotes the case when MATLAB reports the error of out of memory.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Clustering performance (NMI% ? std) for large-scale spectral clustering methods ?14.69 39.16 ?9.25 39.80 ?17.52</figDesc><table><row><cell>Dataset</cell><cell>KM</cell><cell>SC</cell><cell>Nystr?m</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>LSC-KH</cell><cell>LSC-RH</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>USPS</cell><cell>61.28 ?0.42</cell><cell>77.90 ?0.55</cell><cell>65.07 ?1.23</cell><cell>81.37 ?1.92</cell><cell>76.22 ?0.76</cell><cell cols="2">76.41 ?1.74 76.24 ?1.00</cell><cell>81.86 ?1.95</cell><cell>82.86 ?0.21</cell></row><row><cell>PenDigits</cell><cell>67.65 ?1.18</cell><cell>71.70 ?1.21</cell><cell>65.48 ?0.21</cell><cell>80.78 ?0.55</cell><cell>79.15 ?1.74</cell><cell cols="2">80.78 ?0.55 79.15 ?1.74</cell><cell>81.68 ?2.33</cell><cell>82.01 ?1.08</cell></row><row><cell>Letters</cell><cell>34.95 ?0.54</cell><cell>34.96 ?0.63</cell><cell>40.07 ?0.41</cell><cell>44.68 ?1.56</cell><cell>42.36 ?0.86</cell><cell cols="2">42.31 ?0.75 42.20 ?1.30</cell><cell>45.11 ?0.54</cell><cell>45.37 ?0.85</cell></row><row><cell>MINST</cell><cell>50.90 ?1.10</cell><cell>N/A</cell><cell>49.05 ?1.55</cell><cell>76.81 ?0.18</cell><cell>62.53 ?1.87</cell><cell cols="2">65.08 ?2.16 65.14 ?2.47</cell><cell>69.15 ?0.76</cell><cell>72.00 ?0.51</cell></row><row><cell>Covertype</cell><cell>7.55 ?0.00</cell><cell>N/A</cell><cell>7.98 ?0.98</cell><cell>9.21 ?0.14</cell><cell>8.06 ?0.07</cell><cell>N/A</cell><cell>N/A</cell><cell>8.19 ?0.04</cell><cell>8.30 ?0.30</cell></row><row><cell>TS-60K</cell><cell>22.22 ?0.00</cell><cell>N/A</cell><cell cols="4">21.64 N/A</cell><cell>N/A</cell><cell cols="2">62.52 ?17.01 73.84 ?5.08</cell></row><row><cell>TM-1M</cell><cell>19.21 ?0.00</cell><cell>N/A</cell><cell>8.03 ?8.58</cell><cell>0.10 ?0.05</cell><cell>28.11 ?48.63</cell><cell>N/A</cell><cell>N/A</cell><cell>99.52 ?0.08</cell><cell>99.52 ?0.05</cell></row><row><cell>TC-6M</cell><cell>34.95 ?0.54</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>99.14 ?0.19</cell><cell>99.15 ?0.08</cell></row><row><cell>CG-10M</cell><cell>64.94 ?1.61</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>79.98 ?2.10</cell><cell>80.91 ?3.59</cell></row><row><cell>FL-20M</cell><cell>65.02 ?2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">86.77 ?3.97 87.67 ?3.18</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>71.39</cell><cell>72.39</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.40</cell><cell>5.30</cell><cell>3.10</cell><cell>4.20</cell><cell>4.50</cell><cell>4.70</cell><cell>2.00</cell><cell>1.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Time costs(s) of large-scale spectral clustering methods.</figDesc><table><row><cell>Dataset</cell><cell>KM</cell><cell>SC</cell><cell>Nystr?m</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>LSC-KH</cell><cell>LSC-RH</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>USPS</cell><cell>0.37 ?0.18</cell><cell>3.15 ?0.18</cell><cell>1.44 ?0.04</cell><cell>1.35 ?0.09</cell><cell>0.64 ?0.14</cell><cell>0.71 ?0.06</cell><cell>0.88 ?0.07</cell><cell>3.36 ?0.25</cell><cell>1.25 ?0.07</cell></row><row><cell>PenDigits</cell><cell>0.05 ?0.05</cell><cell>3.15 ?0.11</cell><cell>1.61 ?0.10</cell><cell>1.20 ?0.37</cell><cell>0.77 ?0.34</cell><cell>0.71 ?0.05</cell><cell>0.68 ?0.07</cell><cell>2.07 ?0.95</cell><cell>0.64 ?0.08</cell></row><row><cell>Letters</cell><cell>0.26 ?0.05</cell><cell>13.67 ?2.35</cell><cell>4.70 ?0.17</cell><cell>3.89 ?0.28</cell><cell>2.03 ?0.34</cell><cell>2.26 ?0.17</cell><cell>2.63 ?0.28</cell><cell>1.58 ?0.06</cell><cell>0.90 ?0.10</cell></row><row><cell>MINST</cell><cell>21.40 ?1.02</cell><cell>N/A</cell><cell>6.54 ?0.11</cell><cell>17.29 ?0.82</cell><cell>5.80 ?0.31</cell><cell cols="2">18.04 ?2.35 15.38 ?2.43</cell><cell>11.96 ?0.32</cell><cell>5.11 ?0.51</cell></row><row><cell>Covertype</cell><cell>14.02 ?4.39</cell><cell>N/A</cell><cell>571.69 ?144.60</cell><cell>354.74 ?90.80</cell><cell>41.00 ?12.38</cell><cell>N/A</cell><cell>N/A</cell><cell>15.96 ?1.44</cell><cell>13.15 ?3.00</cell></row><row><cell>TS-60K</cell><cell>1.39 ?0.18</cell><cell>N/A</cell><cell>1283.33 ?248.12</cell><cell>167.29 ?39.99</cell><cell>16.35 ?1.62</cell><cell>N/A</cell><cell>N/A</cell><cell>17.36 ?20.89</cell><cell>4.01 ?1.16</cell></row><row><cell>TM-1M</cell><cell>1.12 ?0.17</cell><cell>N/A</cell><cell cols="3">3401.61 ?410.03 3997.21 ?1436.73 591.02 ?127.86</cell><cell>N/A</cell><cell>N/A</cell><cell>7.85 ?0.21</cell><cell>6.46 ?1.13</cell></row><row><cell>TC-6M</cell><cell>35.23 ?1.72</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>30.46 ?1.52</cell><cell>25.05 ?3.04</cell></row><row><cell>CG-10M</cell><cell>134.42 ?9.28</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>381.72 ?72.24</cell><cell>281.05 ?77.04</cell></row><row><cell>FL-20M</cell><cell>311.94 ?2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">1530.30 ?578.44 837.38 ?213.70</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>165.96</cell><cell>117.50</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.80</cell><cell>4.50</cell><cell>4.40</cell><cell>2.60</cell><cell>4.30</cell><cell>4.20</cell><cell>3.30</cell><cell>1.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>Clustering performance (ACC(%), NMI(%), and time costs(s)) for different methods by varying number of nearest landmark and selection rate .</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell>Letters</cell><cell>MNIST</cell><cell>TS-60K</cell><cell>TM-1M</cell></row><row><cell>ACC</cell><cell>ACC(%)</cell><cell>0 20 40</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>250 200</cell></row><row><cell></cell><cell></cell><cell cols="3"># of K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc>Comparison for three phases for different methods.</figDesc><table><row><cell>Phase</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>Landmark Selection</cell><cell>-means</cell><cell>Random</cell><cell cols="2">Hybrid representative selection Divide-and-conquer selection</cell></row><row><cell cols="2">Similarity Construction Exact</cell><cell>Exact</cell><cell>Approximate</cell><cell>Approximate</cell></row><row><cell>Graph Partitioning</cell><cell cols="3">SVD based SVD based Transfer cuts</cell><cell>Transfer cuts</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 The</head><label>13</label><figDesc></figDesc><table><row><cell cols="3">simulation performance of NMI(%) varying different land-</cell></row><row><cell>mark selection scenarios.</cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>divide-and-conquer</cell><cell>-means</cell></row><row><cell>500 Gaussian blobs</cell><cell>90.76</cell><cell>92.01</cell></row><row><cell cols="2">1000 Gaussian blobs 87.67</cell><cell>88.51</cell></row><row><cell cols="2">1500 Gaussian blobs 85.90</cell><cell>86.40</cell></row><row><cell cols="2">2000 Gaussian blobs 84.59</cell><cell>84.82</cell></row><row><cell>Table 14</cell><cell></cell><cell></cell></row><row><cell cols="3">The simulation time costs(s) varying different landmark selec-</cell></row><row><cell>tion scenarios.</cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>divide-and-conquer</cell><cell>-means</cell></row><row><cell>500 Gaussian blobs</cell><cell>0.33</cell><cell>5.95</cell></row><row><cell cols="2">1000 Gaussian blobs 0.44</cell><cell>11.43</cell></row><row><cell cols="2">1500 Gaussian blobs 0.49</cell><cell>17.02</cell></row><row><cell cols="2">2000 Gaussian blobs 0.64</cell><cell>24.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 15</head><label>15</label><figDesc>Ablation Study on the proposed divide-and-conquer selection strategy, light--means, and approximate of K-nearest landmarks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-nearest landmarks</cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>Landmark selection</cell><cell cols="2">Approximate</cell><cell></cell><cell>Exact</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">ACC(%) NMI(%) Time(s) ACC(%) NMI(%) Time(s)</cell></row><row><cell></cell><cell>-means</cell><cell>34.06</cell><cell>46.58</cell><cell>3.89</cell><cell>34.41</cell><cell>45.56</cell><cell>4.05</cell></row><row><cell>Letters</cell><cell>DnC--means</cell><cell>34.71</cell><cell>45.19</cell><cell>1.22</cell><cell>33.76</cell><cell>45.17</cell><cell>1.34</cell></row><row><cell></cell><cell cols="2">DnC-light--means 33.54</cell><cell>45.37</cell><cell>0.90</cell><cell>33.93</cell><cell>45.91</cell><cell>1.05</cell></row><row><cell></cell><cell>-means</cell><cell>75.34</cell><cell>73.07</cell><cell>15.29</cell><cell>79.28</cell><cell>74.74</cell><cell>29.02</cell></row><row><cell>MNIST</cell><cell>DnC--means</cell><cell>74.46</cell><cell>73.11</cell><cell>9.50</cell><cell>74.12</cell><cell>74.12</cell><cell>24.81</cell></row><row><cell></cell><cell cols="2">DnC-light--means 74.24</cell><cell>72.00</cell><cell>5.11</cell><cell>74.04</cell><cell>74.04</cell><cell>21.00</cell></row><row><cell></cell><cell>-means</cell><cell>83.27</cell><cell>77.18</cell><cell>165.21</cell><cell>86.41</cell><cell>76.51</cell><cell>172.72</cell></row><row><cell>TS-60K</cell><cell>DnC--means</cell><cell>81.06</cell><cell>73.92</cell><cell>8.14</cell><cell>84.30</cell><cell>70.15</cell><cell>12.75</cell></row><row><cell></cell><cell cols="2">DnC-light--means 81.00</cell><cell>73.84</cell><cell>4.01</cell><cell>80.82</cell><cell>73.12</cell><cell>9.12</cell></row><row><cell></cell><cell>-means</cell><cell>99.23</cell><cell>99.50</cell><cell cols="2">3997.12 99.95</cell><cell>99.59</cell><cell>4023.12</cell></row><row><cell>TM-1M</cell><cell>DnC--means</cell><cell>99.95</cell><cell>99.48</cell><cell>12.78</cell><cell>99.97</cell><cell>99.57</cell><cell>25.65</cell></row><row><cell></cell><cell cols="2">DnC-light--means 99.96</cell><cell>99.52</cell><cell>6.46</cell><cell>99.95</cell><cell>99.45</cell><cell>19.30</cell></row><row><cell></cell><cell>-means</cell><cell>72.98</cell><cell>74.08</cell><cell cols="2">1045.38 75.01</cell><cell>74.10</cell><cell>1057.23</cell></row><row><cell>Avg. score</cell><cell>DnC--means</cell><cell>72.55</cell><cell>72.93</cell><cell>7.91</cell><cell>73.04</cell><cell>72.25</cell><cell>16.14</cell></row><row><cell></cell><cell cols="2">DnC-light--means 72.19</cell><cell>72.68</cell><cell>4.12</cell><cell>72.19</cell><cell>73.13</cell><cell>12.62</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Divide-and-conquer based Large-Scale Spectral Clustering</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://alumni.cs.ucsb.edu/ wychen/sc.html 7 http://www.cad.zju.edu.cn/home/dengcai/Data/Clustering.html 8 https://github.com/Li-Hongmin/MyPaperWithCode</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This study was supported by in part by the New Energy and Industrial Technology Development Organization (NEDO) Grant (ID:18065620) and JST COI-NEXT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Blackard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and electronics in agriculture</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="131" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sampling with minimum sum of squared similarities for nystrom-based large scale spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Birol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering via landmarkbased sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1669" to="1680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speed up kernel discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="21" to="33" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transac</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel spectral clustering in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="568" to="586" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of kernel and spectral methods for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Camastra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Masulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="176" to="190" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Spectral grouping using the nystrom method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Letter recognition using holland-style adaptive classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Slate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="161" to="182" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ultrascalable spectral clustering and ensemble clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kwoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1212" to="1226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hubness-based sampling method for nystr?m spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Imakura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale multi-view spectral clustering via bipartite graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Superpixel segmentation using linear spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1356" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation using superpixels: A bipartite graph partitioning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="789" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding and enhancement of internal clustering validation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="982" to="994" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spectral clustering of biological sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pentney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Landmark selection for spectral clustering based on weighted pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rafailidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="465" to="472" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Clustering methods, in: Data mining and knowledge discovery handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maimon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="321" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Agglomerative information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="617" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Document clustering based on nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast approximate spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering using sparse representation based on hubness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SmartWorld, Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computing, Scalable Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People and Smart City Innovation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1731" to="1737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral clustering with adaptive similarity measure in kernel space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="751" to="765" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bipartite graph partitioning and data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved nystr?m lowrank approximation and error analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1232" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral clustering ensemble applied to sar image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sampling for nystr?m extension-based spectral clustering: incremental perspective and novel analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detection of shilling attacks in recommender systems via spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Information Fusion (FUSION), IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">He received his MS degree in computer science from the University of Tsukuba, Japan. His current research interests include clustering, machine learning</title>
		<imprint>
			<pubPlace>Japan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Hongmin Li is currently working toward a Ph.D. degree at the Department of Computer Science, University of Tsukuba</orgName>
		</respStmt>
	</monogr>
	<note>and its application fields</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Her current research interests include feature selection, clustering, bioinformatics, machine learning and its application fields</title>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Tsukuba, Japan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xiucai Ye received the Ph.D. degree in computer science from the University of Tsukuba ; Department of Computer Science, and Center for Artificial Intelligence Research (C-AIR), University of Tsukuba</orgName>
		</respStmt>
	</monogr>
	<note>She is currently an Assistant Professor with the. She is a member of IEEE</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">His current research interests include developments and analysis of highly parallel algorithms for large matrix computations. Recently, he also investigates matrix factorization-based machine learning algorithms</title>
	</analytic>
	<monogr>
		<title level="m">Japan from 2011 to 2013, and also as a JST ACTI researcher from</title>
		<meeting><address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Akira Imakura is an Associate Professor at Faculty of Engineering, Information and Systems, University of Tsukuba ; Nagoya University ; Center for Computational Sciences, University of Tsukuba</orgName>
		</respStmt>
	</monogr>
	<note>He was appointed as Japan Society for the Promotion of Science Research Fellowship for Doctor Course Student (DC2) from. He is a member of JSIAM, IPSJ and SIAM</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">He is also a visiting professor at the Open University of Japan, and a visiting researcher of Advanced Institute of Computational Science at RIKEN. He received a Ph.D. in Computer Engineering from Nagoya University in 1992. His research interests include high performance algorithms for large-scale simulations, data and image analysis, and deep neural network computations. He is a member of the</title>
	</analytic>
	<monogr>
		<title level="m">Society for Industrial and Applied Mathematics (SIAM)</title>
		<imprint>
			<publisher>MSJ), Information Processing Society of Japan</publisher>
		</imprint>
		<respStmt>
			<orgName>Tetsuya Sakurai is a Professor of Department of Computer Science, and the Director of Center for Artificial Intelligence Research (C-AIR) at the University of Tsukuba</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
