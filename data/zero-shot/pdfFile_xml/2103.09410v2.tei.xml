<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONTRASTIVE LEARNING OF MUSICAL REPRESENTATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Spijkervet</surname></persName>
							<email>janne.spijkervet@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Logic, Language, and Computation</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Ashley</forename><surname>Burgoyne</surname></persName>
							<email>j.a.burgoyne@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Logic, Language, and Computation</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONTRASTIVE LEARNING OF MUSICAL REPRESENTATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTa-gATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Supervised learning methods have been widely used in musical tasks like chord recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, key detection <ref type="bibr" target="#b2">[3]</ref>, beat tracking <ref type="bibr" target="#b4">[4]</ref>, music audio tagging <ref type="bibr" target="#b5">[5]</ref> and music recommendation <ref type="bibr" target="#b6">[6]</ref>. These methods require labeled corpora, which are difficult, expensive and time-consuming to create for music in particular <ref type="bibr" target="#b7">[7]</ref>, while raw unlabeled music data is available in vast quantities. Unsupervised alternatives to end-to-end deep learning for music are compelling, especially if they can generalise to smaller datasets.</p><p>Despite the importance of unsupervised learning for raw audio signals, unsupervised learning for musical tasks has yet to see breakthroughs comparable to those in supervised learning. There have been successes with methods like PCA, PMSC's and spherical k-means that rely on a transformation pipeline <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>, and very recently with selfsupervised methods in the time-frequency domain for general audio classifiation tasks <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref>, but learning effective representations of raw audio in an unsupervised manner has remained elusive for musical tasks.</p><p>Self-supervised representation learning is an unsupervised learning paradigm that has demonstrated advances across many tasks and research domains <ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. This includes the ability to use substantially less labeled data when fine-tuning on a specific task <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. Without ground truth, there can be no ordinary loss function for training; self-supervised learning trains by way of a proxy loss function instead. One way to preserve the amount of useful information during self-supervised learning is to define the proxy loss function with respect to a relatively simple pretext task, with the idea that a representation that is good for the pretext task will also be useful for downstream tasks. Many approaches rely on heuristics to design pretext tasks <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">22]</ref>, e.g., by witholding a pitch transformation <ref type="bibr" target="#b24">[23]</ref>. Alternatively, contrastive representation learning formulates the proxy loss directly on the learned representations and relies on contrasting multiple, slightly differing versions of any one example by often using negative sampling strategies <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b26">25]</ref> or by bootstrapping the representations <ref type="bibr" target="#b18">[18]</ref>.</p><p>In this paper, we combine the insights of a simple contrastive learning framework for images, SimCLR <ref type="bibr" target="#b17">[17]</ref>, with recent advances in representation learning for audio in the time domain <ref type="bibr" target="#b27">[26]</ref>. We also contribute a pipeline of data augmentations on musical audio, to form a simple framework for self-supervised, contrastive learning of representations of raw waveforms of music. To compare the effectiveness of this simple framework compared to a more complex self-supervised learning objective, we also evaluate representations learned by contrastive predictive coding (CPC) <ref type="bibr" target="#b15">[15]</ref>. The self-supervised models are evaluated on the downstream music tagging task, enabling us to evaluate their versatility: music tags describe many characteristics of music, e.g., genre, instrumentation and dynamics. Our key contributions are the following.</p><p>? CLMR achieves strong performance on the music classification task compared to supervised models, despite self-supervised pre-training and training a linear classifier on the downstream task with raw signals of musical audio (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>? CLMR enables efficient classification: we achieve comparable performance using as few as 1% of the labeled data.</p><p>? We show the out-of-domain transferability of representations learned from pre-training CLMR on entirely different corpora of musical audio.</p><p>? CLMR can learn from any dataset of raw music audio, requiring neither transformations nor fine-tuning on the input data; nor do the models require manually annotated labels for pre-training.</p><p>? We provide an ablation study on the effectiveness of individual audio data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The goal of representation learning is to identify features that make prediction tasks easier and more robust to the complex variations of natural data <ref type="bibr" target="#b28">[27]</ref>. In unsupervised representation learning, generative modeling and likelihood-based models typically find useful representations of the data by attempting to reconstruct the observations on the basis of their learned representations <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b30">29]</ref>. Self-supervised representation learning aims to identify the explanatory factors of the data using an objective that is formulated with respect to the learned representations directly <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">22]</ref>. Compared to vision, work on self-supervised learning in audio is still very limited, but there are a number of works that appeared very recently. Contrastive predictive coding is a universal approach to contrastive learning, and has been successful for speaker and phoneme classification using raw audio, among other tasks <ref type="bibr" target="#b15">[15]</ref>. PASE <ref type="bibr" target="#b31">[30]</ref> introduces several self-supervised workers that solve regression or binary discrimation tasks, that jointly optimise an encoder for speech recognition. To improve the representations for mismatched acoustic conditions and their transferability, they apply augmentations to the input speech signal <ref type="bibr" target="#b32">[31]</ref>. In music information retrieval, recent advances have been made in self-supervised pitch estimation <ref type="bibr" target="#b24">[23]</ref>, closely matching supervised, state-of-the-art baselines <ref type="bibr" target="#b33">[32]</ref> despite being trained without ground truth labels. L 3 -Net learns deep embeddings from audio-visual correspondence in videos by way of self-supervised learning <ref type="bibr" target="#b10">[10]</ref>. Their work uses mel-spectrograms for audio and requires more than 40 million audio-video training samples to learn optimal embeddings. Audio2Vec also operates in the timefrequency domain and learns by reconstructing spectrogram slices from past and future slices <ref type="bibr" target="#b11">[11]</ref>. With limited data, Audio2Vec outperforms supervised models in pitch and instrument classification. CLAR also uses a contrastive learning objective, and computes a loss on a concatenation of representations learned from both raw audio and mel-spectrograms <ref type="bibr" target="#b12">[12]</ref>. COLA uses a similar method with mel-spectrograms only, and uses bilinear comparisons instead of cosine similarity <ref type="bibr" target="#b13">[13]</ref>. Both works are evaluated on speech command, environmental sound classification, and on pitch and instrument classification on the NSynth dataset <ref type="bibr" target="#b34">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>This work builds on SimCLR, a simple contrastive learning framework of visual representations <ref type="bibr" target="#b17">[17]</ref>. Despite a taskagnostic, labelless discriminative pre-training approach, a linear classifier achieved performance comparable to fully supervised models in many image classification benchmarks. Its learning objective is to maximise the agreement of latent representations of augmented views of the same image using a contrastive loss. In Section 2, we will continue an overview of contrastive learning.</p><p>In CLMR, we adapt this framework to the domain of raw music audio. While most core components of CLMR have appeared in previous work, its ability to model waveforms of music cannot be explained by a single design choice, but by their composition. We will first elaborate the four core components in the following subsections:</p><p>? A stochastic composition of data augmentations that produces two correlated, augmented examples of the same audio fragment, the 'positive pair', denoted as x i and x j .</p><p>? An encoder neural network g enc (?) that maps the augmented examples to their latent representations.</p><p>? A projector neural network g proj (?) that maps the encoded representations to the latent space where the contrastive loss is formulated.</p><p>? A contrastive loss function, which aims to identify x j from the negative examples in the batch {x k =i } for a given x i . The complete framework is visualised in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Augmentations</head><p>We designed a comprehensive chain of audio augmentations for raw audio waveforms of music to make it harder for the model to identify the correct pair of examples. For details, see Appendix B 1 . Each consecutive augmentation is stochastically applied on x i and x j independently, i.e., each augmentation has an independent probability p transform of being applied to the audio. The order of augmentations applied to audio is carefully considered, e.g., applying a delay effect after reverberation empirically gives an entirely different result in music. 1. A random fragment of size s is selected from a piece of music, without trimming silence (e.g., the intro or outro of a song). The two examples x i and x j from the same audio fragment can overlap or be very disjoint, allowing the model to infer both local and global structures.</p><p>2. The polarity of the audio signal is inverted, i.e., the amplitude is multiplied by ?1.</p><p>3. Additive white Gaussian noise is added with a signalto-noise ratio of 80 decibels to the original signal.</p><p>4. The gain is reduced between [?6, 0] decibels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>A frequency filter is applied to the signal. A coin flip determines whether it is a low-pass or a high-pass filter. The cut-off frequencies are drawn from uniform distributions on [2200, 4000] or [200, 1200] Hz respectively.</p><p>6. The signal is delayed and added to the original signal with a volume factor of 0.5. The delay is randomly sampled between 200-500ms, in 50ms increments.</p><p>7. The signal is pitch shifted. The pitch transposition interval is drawn from a uniform distribution of semitones between [?5, 5], i.e., a perfect fourth compared to the original signal's scale.</p><p>8. Reverb is added to alter the signal's acoustics. The impulse response's room size, reverbation and damping factor is drawn from a uniform distribution on [0, 100].</p><p>The space of augmentations is not limited to these operations and could be extended to, e.g., randomly applying chorus, distortion and other modulations. Some of these have been shown to improve performance in selfsupervised learning for automatic speech recognition in the time-domain as well <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b35">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Batch Composition</head><p>A larger batch size N makes the contrastive learning objective harder -there are simply more negative examples the anchor sample needs to identify the positive sample from -but it can substantially improve model performance <ref type="bibr" target="#b17">[17]</ref>. We sample one song from the batch, augment it into two examples, and treat them as the positive pair. We treated the remaining 2(N ? 1) examples in the batch as negative examples, and did not sample the negative examples explicitly. Larger batch sizes introduces a practical problem for raw audio when training on a GPU, as their input dimensionality increases for higher sample rates. When training on multiple GPU's, we used global batch normalisation, i.e., we aggregate the batch statistics over all devices during parallel training, to avoid potential leakage of batch statistics because the positive examples are sampled on the same device (which improves training loss, but counteracts learning of useful representations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoder</head><p>To directly compare a state-of-the-art end-to-end supervised model used in music classification on raw waveforms against a self-supervised model, we use the Sam-pleCNN architecture as our encoder <ref type="bibr" target="#b27">[26]</ref>. Similarly, we use a fixed audio input of 59 049 samples with a sample rate of 22 050 Hz. In this configuration, the Sam-pleCNN encoder g enc consists of 9 one-dimensional convolution blocks, each with a filter size of 3, batch normalisation, ReLU activation and max pooling with pool size 3. The final output layer is removed, which yields a 512dimensional feature vector h i for every audio input. The feature vectors from the encoder can be directly used in the learning objective, but formulating the objective on encodings mapped to a different latent space by a parameterised function helps the effectiveness of the representations <ref type="bibr" target="#b17">[17]</ref>. In our experiments, we use a non-linear layer</p><formula xml:id="formula_0">z i = W (2) ReLU(W (1) h i )</formula><p>with an output dimensionality of 128 as the projection head g proj . There are 2.5 million trainable parameters in total, which is put in comparison with other state-of-the-art models in <ref type="figure" target="#fig_0">Figure 1</ref>. We used 96 examples per batch and the aforedescribed encoder configuration to directly compare our self-supervised performance with the equally expressive fully supervised method <ref type="bibr" target="#b27">[26]</ref>. We ran experiments with batch sizes of 96 on 2? NVIDIA 1080Ti, while for larger batches up to 4 ? Titan RTX's were used. With 2 1080Ti's, it takes ?5 days to train 1 000 epochs on our largest dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Contrastive Loss Function</head><p>In keeping with recent findings on several objective functions in contrastive learning <ref type="bibr" target="#b17">[17]</ref>, the contrastive loss function used in this model is normalised temperature-scaled cross-entropy loss, commonly denoted as NT-Xent loss:</p><formula xml:id="formula_1">i,j = ? log exp (sim (z i , z j ) /? ) 2N k=1 1 [k =i] exp (sim (z i , z k ) /? ) (1)</formula><p>The pairwise similarity is measured using cosine similarity and the temperature parameter ? helps the model learn from hard negatives. The indicator function 1 [k =i] evaluates to 1 iff k = i. This loss is computed for all pairs, both (z i , z j ) and (z j , z i ), for i = j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Contrastive Predictive Coding</head><p>We adjusted the original CPC encoder g enc <ref type="bibr" target="#b15">[15]</ref> to a deeper architecture for more direct comparison <ref type="bibr" target="#b27">[26]</ref>. The encoder g enc consists of 7 layers with 512 filters each, and filter sizes <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2]</ref> and strides [5, 3, 2, 2, 2, 2, 2]. Instead of relying on max-pooling, the filter sizes and strides are adjusted to parameterise and facilitate downsampling. We also increased the number of prediction steps to 20, effectively asking the network to predict 100 ms of audio into the future. The batch size is set to 64 from which 15 negative examples in the contrastive loss are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Linear Evaluation</head><p>The evaluation of representations learned by selfsupervised models is commonly done with linear evaluation <ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref>, which measures how linearly separable the relevant classes are under the learned representations. We obtain the representations for all datapoints from a frozen CLMR network after pre-training has converged, and train a linear classifier using these self-supervised representations on the downstream task of music classification. For CPC, the representations are extracted from the autoregressor, yielding a context vector of size <ref type="bibr" target="#b20">(20,</ref><ref type="bibr">256)</ref>, which is global-average pooled to obtain a single vector of 512 dimensions. For CLMR, the last 512-dimensional vector h from the encoder is used instead of z from the projection head because that yielded consistently better results for all our experiments. We compute the evaluation metrics on a held-out test set, averaged over three runs on the training set using different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Optimisers</head><p>We use the Adam optimiser <ref type="bibr" target="#b36">[35]</ref> with a learning rate of 0.0003 and ? 1 = 0.9 and ? 2 = 0.999 during pre-training and employ He initialisation for all convolutional layers. The temperature parameter ? is set to 0.5, since we observed consistent results regardless of varying batch sizes and temperature ? ? {0.1, 0.5, 1.0}. For linear evaluation, we use the Adam optimiser with a learning rate of 0.0003 and a weight decay of 10 ?6 . Backpropagation is only done in the final (linear) head for all experiments in this paper. We also employ an early stopping mechanism when the validation scores do not improve for 5 epochs.  <ref type="table">Table 1</ref>: Tag prediction performance on the MagnaTag-ATune (MTAT) dataset and Million Song Dataset (MSD), compared with fully supervised models ( ?) trained on raw audio waveforms. We omit most works that operate on (mel-) spectrograms ( ?) to make a fair comparison with our approach on raw audio. For reference, we add the Transformer model that is the current state-of-the-art in music tagging. For the self-supervised models, the scores are obtained by training a linear, logistic regression classifier using the frozen representations from self-supervised pre-training. Scores in brackets show performance when adding a hidden layer to the linear classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated the quality of our representations with music classification experiments. Predicting the top 50 semantic tags in the MagnaTagATune and Million Song datasets <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b40">39]</ref> is a popular benchmark for music classification. These semantic tags are annotated by human listeners, and have a varying degree of abstraction and describe many facets of music, including genre, instrumentation and dynamics. It is a multi-label classification task: each track can have multiple tags, of which we use the 50 most frequently occuring to compare our performance against supervised benchmarks. The MagnaTagATune dataset consists of 25k music clips from 6 622 unique songs, of which we use about 187k fragments of 2.6 seconds for training, and the same train/test split as previous work <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b27">26]</ref>. The Million Song Dataset contains a million songs, of which about 240k previews of 30 seconds are available and labeled with Last.FM tag annotations. We only use the train, validation and test split of 201 680 / 11 774 / 28 435 songs as used in previous work <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b27">26]</ref>, not all million songs during self-supervised pre-training. This results in 2.2 million music fragments of 2.6 seconds for training, i.e., almost 1 600 hours of music. The tags for the Million Song Dataset also contain overlapping genre and semantic tags, e.g., 'beautiful', 'happy' and 'sad', which are arguably harder to separate during the linear evaluation phase.</p><p>We use average tag-wise area under the receiver operating characteristic curve (ROC-AUC) and average precision (PR-AUC) scores as evaluation metrics. They are measured globally for the whole dataset, i.e., for the tag metric we measure the retrieval performance on the tag dimension (column-wise) and for the clip metric we measure the performance on the clip dimension (row-wise). PR-AUC is calculated in addition to ROC-AUC, because ROC-AUC scores can be over-optimistic for imbalanced datasets like MagnaTagATune <ref type="bibr" target="#b41">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>The most important goal set out in this paper is to evaluate the difference in performance between an otherwise identical, fully supervised network when learning representations using a self-supervised objective.</p><p>CLMR exceeds the supervised benchmark for the MagnaTagATune dataset with a PR-AUC of 35.6%, despite task-agnostic, self-supervised pre-training and a linear classifier for training, as shown in <ref type="table">Table 1</ref>. An additional 0.4% PR-AUC performance gain is added by adding an extra hidden layer to the classifier. When increasing the batch size and the number of parameters, we observe another performance gain to 37.0% PR-AUC as show in Appendix C.1. The performance on the larger Million Song Dataset is lower compared to the supervised benchmark, and especially to the current state-of-the-art model that is trained using mel-spectrograms <ref type="bibr" target="#b38">[37]</ref>, but is still remarkable given the use of a linear classifier. The tags in the Million Song Dataset are semantically more complex, e.g., 'catchy', 'sexy', 'happy', and have more similar genre tags, e.g., 'progressive rock', 'classic rock' and 'indie rock', which our proposed contrastive learning method may not distinguish.</p><p>CPC also shows competitive performance with fully supervised models in the music classification task. Despite CPC's good performance, self-supervised training does not require a memory bank or more complex loss functions, e.g., those incorporating mutual information or more explicit negative sampling strategies, to learn useful representations.</p><p>We also analyse the quality of our representations, showing they can cleanly separate audio fragments from different classes, and visualise the convolution filters of the self-supervised models in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentations</head><p>The CLMR model relies on a pipeline of strong data augmentations to facilitate the learning of representations that are more robust and allow for better generalisation in the downstream task. In <ref type="table">Table 2</ref>, we show the linear evaluation scores obtained by taking a random crop of audio and performing one additional, individual augmentation. While all datasets contain songs of variable length, we always sample a random crop of audio of the same size before applying other augmentations. This makes it harder to assess the individual contribution of each augmentation to the downstream task performance. We therefore consider an asymmetric data transformation setting: we only apply the augmentation(s) to one branch of the framework, while we settle with an identity function for the other branch (i.e., t(x j ) = x j ) <ref type="bibr" target="#b17">[17]</ref>. The model is pre-trained from scratch for 1 000 epochs after which linear evaluation is performed.  <ref type="table">Table 2</ref>: CLMR music tagging performance using a random crop together with one other audio data augmentation. When only taking a random crop of audio, we achieve a PR-AUC score of 30.5. Most individual augmentations show an increase in performance, while adding gain or delay does not impact performance as much. Adding a filter to the augmentation pipeline increases the downstream performance more significantly.</p><p>Besides evaluating the individual contribution of each augmentation with augmentation probability p t = 1, we also vary p t ? {0, 0.4, 0.8}. This is done to assess the optimal amount of augmentation to each example, i.e., the contrastive learning task should neither be too hard, nor too simple, for learning effective representations for the music classification task. The linear evaluation PR-AUC score is shown for each augmentation under a different probability p t in <ref type="figure" target="#fig_15">Figure 3</ref>. For the Polarity and Filter transformations, performing them more often with a probability of p t = 0.8 is beneficial. For the Delay, Pitch and Reverb transformations, a transformation probability of p t = 0.4 works better than performing them more aggressively. Generally, we find that strong data augmentations result in more robust representations and better downstream task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Efficient Classification Experiments</head><p>To test the efficient classification capability of the CLMR model, we train the linear classifier on consecutive, classbalanced subsets of the labels in the train dataset and report its performance. During the task-agnostic, self-supervised pre-training phase, 100% of the data is used. <ref type="figure" target="#fig_3">Figures 4  and 5</ref> show the PR-AUC scores obtained when increasing  the amount of labels available during training. For both datasets, self-supervised pre-training greatly improves performance when less labeled data is available. Using 100 times fewer labeled songs, i.e., only 259 songs, CLMR scores 33.1% PR-AUC compared to 24.8% PR-AUC obtained with an equivalent, end-to-end trained supervised model trained on about 25 000 songs. Pre-training using a self-supervised objective without labels therefore substantially improves efficient classification: only 1% of the labels are required while maintaining a similar performance. For the Million Song Dataset, a fully supervised model exceeds CLMR at 10% of the labels, which are 24 190 unique songs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Transfer Learning Experiments</head><p>To test the out-of-domain generalisability of the learned representations, we pre-trained CLMR on entirely different music datasets. After pre-training, we freeze the weights of the network, i.e., we do not fine-tune the encoder, and subsequently perform the linear evaluation procedure outlined in Section 3.6. While originally made for chord recognition, we use 461 contemporary pop songs recorded between the 1940's and 2000's from the McGill Billboard dataset <ref type="bibr" target="#b42">[41]</ref>. The Free Music Archive dataset <ref type="bibr" target="#b43">[42]</ref> consists of 22 413 songs for the 'medium' version, and the faultfiltered GTZAN dataset <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b45">44]</ref>    <ref type="table" target="#tab_3">Table 3</ref>. Both CPC and CLMR show the ability to learn effective representations from out-of-domain datasets without ground truth, and even exceed accuracy scores of previous, supervised end-to-end systems on raw audio <ref type="bibr" target="#b37">[36]</ref>. Moreover, both models even demonstrate the ability to learn useful representations on the much smaller GTZAN and Billboard datasets. The CLMR model performs better when it is pre-trained on larger datasets, which is expected as it heavily relies on the number of unique, independent examples that make the contrastive learning task harder, resulting in more robust representations. When pretraining on smaller datasets, CPC can find more useful representations, especially when adding an extra hidden layer to the fine-tune head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we presented CLMR, a self-supervised contrastive learning framework that learns useful representations of raw waveforms of musical audio. The framework requires no preprocessing of the input audio and is trained without ground truth, which enables simple and straightforward pre-training on music datasets of unprecedented scale. We tested the learned, task-agnostic representations by training a linear classifier on the music classification task on the MagnaTagATune and Million Song datasets, achieving competitive performance compared to fully supervised models. We also showed that CLMR can achieve comparable performance using 100 times fewer labeled songs, and demonstrated the out-of-domain transferability of representations learned from pre-training on entirely different datasets of music. To foster reproducibility and future research on self-supervised learning in music information retrieval, we publicly release the pre-trained models and the source code of all experiments of this paper 2 . The simplicity of training the model without any labels and without preprocessing the audio, together with encouraging results obtained with a single linear layer optimised for a challenging music task, are exciting developments towards unsupervised learning on raw musical audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>We would like to thank Jordan B.L. Smith, Wilker Aziz and Keunwoo Choi for their feedback on the draft. We would also like to extend our gratitude to the University of Amsterdam and SURFsara for giving us access to their Research Capacity Computing Services GPU cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Delay</head><p>The signal is delayed by a value chosen randomly between 200 and 500 milliseconds, in 50ms increments. Subsequently, the delayed signal is added to the original signal with a volume factor of 0.5, i.e., we multiply the signal's amplitude by 0.5. An example implementation of this digital signal processing effect is given below in Python using PyTorch: The pitch of the signal is shifted up or down, depending on the pitch interval that is drawn from a uniform distribution between -5 and 5 semitones, i.e., up to a perfect fourth higher or lower than the original signal. We assume 12-tone equal temperament tuning that divides a single octave in 12 semitones.</p><p>Pitch shifting is done using the libsox library, which is interfaced from the wavaugment Python library <ref type="bibr" target="#b35">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8 Reverb</head><p>To alter the original signal's acoustics, we apply a Schroeder reverberation effect <ref type="bibr" target="#b49">[48]</ref>. This is again done using the libsox library that is interfaced from the wavaugment Python library <ref type="bibr" target="#b35">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ADDITIONAL EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Batch Size</head><p>The complexity of our contrastive learning approach increases with larger batch sizes, which may result in better representations. We pre-train from scratch until convergence with varying batch sizes and study its effect on the linear evaluation performance in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Sample Rates</head><p>We show in <ref type="table" target="#tab_7">Table C</ref>.2 that there is a marginal penalty to the final scores for the self-supervised models when re-sampling the audio to 8 000 Hz and 16 000 Hz respectively, which is in line with previous work <ref type="bibr" target="#b27">[26]</ref>. Since re-sampling disturbs the frequency spectrum, we isolate its contribution by disregarding additional augmentations, i.e., only apply random cropping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Additional Hidden Layer and Training Duration</head><p>After pre-training with the self-supervised objective, we performed a linear evaluation to test the expressivity of the representations with a classifier of limited capacity. To further assess the representations' usability, we add a single hidden layer to our classifier and again measure the performance on the downstream task of music classification. The results of this experiment are shown in <ref type="table" target="#tab_7">Table C</ref>.3 for linear evaluation (left of the forward slashes) as well as when a hidden layer is added (right of the slashes), for different pre-training durations measured in epochs. Contrastive learning techniques also benefit from longer training compared to their supervised equivalent <ref type="bibr" target="#b17">[17]</ref>. While larger batch sizes increase the pretext task complexity as shown in Appendix C.1, training longer increases the number of natural variations of the data, which is a diserable goal in representation learning <ref type="bibr" target="#b28">[27]</ref>, due to the random augmentation scheme. We pre-train from scratch until convergence and set the batch size to 96.   <ref type="bibr" target="#b50">[49]</ref> of our best self-supervised models representations h CLMR and h CPC , for a randomly set of music tracks from the validation set. We show that both self-supervised models can cleanly seperate the classes.</p><p>To get an understanding of what the self-supervised models capture from music, we show in <ref type="figure">Figure C</ref>.2 the magnitude spectrum of the learned filters of the sample-level convolutional layers (layers 1, 4 and 6) for CLMR and CPC, pre-trained on the MagnaTagATune dataset. We perform gradient ascent on a randomly initialised waveform of length 729, i.e., a value that is close to a typical frame size and also interacts conveniently with the convolutional structure of the encoder network, and subsequently calculate the magnitude spectrum. The x-axis plots the filter number, the y-axis the magnitude spectrum for a filter number. Lastly, we sort the plot by the frequency of the peak magnitude.</p><p>In CLMR, the first layer is sensitive to a single, very small band of frequencies around 7500 Hz, while in higher layers the filters spread themselves first linearly and then non-linearly across the full range. CPC shows a similar pattern in the higher layers, but shows a strong activation of two frequencies that span an octave in the first layer. Conversely, the filters of the supervised-trained encoder have a non-linearity that is found in frame-level end-to-end learning <ref type="bibr" target="#b37">[36]</ref>, as well as in perceptual pitch scales such as mel or Bark scales <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b52">51]</ref>. <ref type="figure" target="#fig_15">Figure C.3</ref> shows the sorted tag-wise ROC-AUC scores for the top 50 tags in the MagnaTagATune dataset, reported for linear evaluation of the trained self-supervised CLMR and CPC models, and the fully end-to-end-trained supervised model. We show that no single tag loses more than 4% ROC-AUC when trained using self-supervised pre-training and fine-tuning is only performed with a linear classifier, as compared to the supervised benchmark.        Normalised magnitude spectrum of the filters of the self-supervised models in the sample-level convolution layers, sorted by the frequency of the peak magnitude. Gradient ascent is performed on a randomly initialised waveform of 729 samples (close to typical frame size) and its magnitude spectrum is calculated subsequently. Each vertical line in the graph represents the frequency spectrum of a different filter. The first three images are taken from a pre-trained, converged CLMR model, the last three from a CPC model, on the MagnaTagATune or Billboard datasets </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance and model complexity comparison of supervised models (grey) and self-supervised models (ours) in music classification of raw audio waveforms on the MagnaTagATune dataset to evaluate musical representations. Supervised models were trained end-to-end, while CLMR and CPC are pre-trained without ground truth: their scores are obtained by training a linear classifier on their learned representations but nonetheless perform competitively to the supervised models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The complete framework operating on raw audio, in which the contrastive learning objective is directly formulated in the latent space of correlated, augmented examples of pairs of raw audio waveforms of music.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>8 Figure 3 :</head><label>83</label><figDesc>PR ? AUC TAG scores for transformations under different, consecutive probabilities p ? {0.0, 0.4, 0.8}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Percentage of labels used for training vs. the achieved PR ? AUC TAG score on the MTAT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Percentage of labels used for training vs. the achieved PR ? AUC TAG score on the MSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>np ms = random.choice( np.arange(200, 500, 50) ) offset = int(ms * (sample_rate / 1000)) beginning = torch.zeros(audio.shape[0], offset) end = audio[:, :-offset] delayed_signal = torch.cat((beginning, end), dim=1) delayed_signal = delayed_signal * self.volume_factor audio = (audio + delayed_signal) / 2 B.7 Pitch Shift</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure C. 1</head><label>1</label><figDesc>shows t-SNE visualisations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure C. 1 :</head><label>1</label><figDesc>t-SNE manifolds of the hidden vectors of music audio from a subset of 10 music tracks, i.e., in this case classes, from the validation set. Each point represents a 2.67 second long music fragment belonging to a music track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure C.2: Normalised magnitude spectrum of the filters of the self-supervised models in the sample-level convolution layers, sorted by the frequency of the peak magnitude. Gradient ascent is performed on a randomly initialised waveform of 729 samples (close to typical frame size) and its magnitude spectrum is calculated subsequently. Each vertical line in the graph represents the frequency spectrum of a different filter. The first three images are taken from a pre-trained, converged CLMR model, the last three from a CPC model, on the MagnaTagATune or Billboard datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure C. 3 :</head><label>3</label><figDesc>Tag-wise ROC-AUC scores for the top-50 tags in the MagnaTagATune dataset, reported for linear, logistic regression classifiers trained on representations of self-supervised models CLMR and CPC, and compared to a fully supervised, end-to-end SampleCNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transfer</figDesc><table><row><cell>learning experiments for CLMR and</cell></row><row><cell>CPC, which are trained on a separate dataset and evalu-</cell></row><row><cell>ated on the MagnaTagATune dataset. The reported scores</cell></row><row><cell>are obtained with a frozen, pre-trained encoder and a linear</cell></row><row><cell>classifier. Scores in parenthesis are obtained when adding</cell></row><row><cell>one extra hidden layer to the classifier.</cell></row><row><cell>30 seconds, both popular for music classification.</cell></row><row><cell>The results of the transfer learning experiments are</cell></row><row><cell>shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table C.<ref type="bibr" target="#b0">1</ref>. While our smallest model already shows competitive performance compared to fully supervised models, the performance increased when using 96 examples per batch. Our largest model required more parameters to score consistently higher than our middle-sized model (?25M parameters vs. 2.5M parameters). We hypothesise that the task of inferring the positive pair of 2.6 second long raw musical audio fragments, in a pool of 254 negative examples (2 ? (128 ? 1)), may be simply too hard for a smaller encoder.</figDesc><table><row><cell></cell><cell>Tag</cell><cell></cell><cell>Clip</cell><cell></cell></row><row><cell cols="5">Batch Size ROC-AUC PR-AUC ROC-AUC PR-AUC</cell></row><row><cell>128</cell><cell>89.7</cell><cell>37.0</cell><cell>94.0</cell><cell>70.0</cell></row><row><cell>96</cell><cell>88.7</cell><cell>35.6</cell><cell>93.0</cell><cell>69.2</cell></row><row><cell>48</cell><cell>87.9</cell><cell>34.6</cell><cell>92.9</cell><cell>68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table C . 1 :</head><label>C1</label><figDesc>Effect of the batch size used during self-supervised training on the linear music classification performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table C.2: Effect of varying the input audio's sample rate on the linear music classification performance.</figDesc><table><row><cell></cell><cell>Tag</cell><cell></cell><cell>Clip</cell><cell></cell></row><row><cell cols="5">SR ROC-AUC PR-AUC ROC-AUC PR-AUC</cell></row><row><cell>8 000</cell><cell>84.8</cell><cell>29.8</cell><cell>90.6</cell><cell>62.9</cell></row><row><cell>16 000</cell><cell>85.5</cell><cell>30.4</cell><cell>91.0</cell><cell>64.1</cell></row><row><cell>22 050</cell><cell>85.8</cell><cell>30.5</cell><cell>91.3</cell><cell>64.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table C</head><label>C</label><figDesc>.3 also shows that increasing the self-supervised training duration improves downstream performance.Table C.3: Performance difference of a linear classifier and when a single hidden layer is added to the classifier on the downstream music classification performance, for different self-supervised pre-training durations.</figDesc><table><row><cell></cell><cell>Tag</cell><cell></cell><cell cols="2">Clip</cell></row><row><cell cols="2">Epochs ROC-AUC</cell><cell>PR-AUC</cell><cell>ROC-AUC</cell><cell>PR-AUC</cell></row><row><cell>10 000</cell><cell>88.7 / 89.3</cell><cell>35.6 / 36.0</cell><cell>93.2 / 93.5</cell><cell>69.3 / 70.0</cell></row><row><cell>3 000</cell><cell>88.5 / 88.9</cell><cell>35.1 / 35.5</cell><cell>93.0 / 93.3</cell><cell>69.2 / 69.7</cell></row><row><cell>1 000</cell><cell>88.3 / 88.6</cell><cell>34.4 / 34.9</cell><cell>92.3 / 93.1</cell><cell>68.6 / 69.2</cell></row><row><cell>300</cell><cell>87.1 / 87.4</cell><cell>32.7 / 32.5</cell><cell>92.0 / 92.0</cell><cell>66.6 / 66.7</cell></row><row><cell>100</cell><cell>86.4 / 86.6</cell><cell>30.9 / 31.3</cell><cell>91.3 / 91.3</cell><cell>64.1 / 64.6</cell></row></table><note>C.4 Qualitative Results</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? J. Spijkervet and J.A. Burgoyne. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: J. Spijkervet and J.A. Burgoyne, "Contrastive Learning of Musical Representations", in Proc. of the 22nd Int. Society for Music Information Retrieval Conf., Online, 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The supplementary material can be found at the accompanying webpage of this paper : https://spijkervet.github.io/CLMR</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/spijkervet/CLMR</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Contrastive Learning of Musical Representations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AUDIO PREPROCESSING</head><p>In this paper, we used raw audio waveform data for training in both the pre-training and linear evaluation phases. The default audio sample rate for all experiments is 22 050 Hz, except for the sample rate experiment in Section C.2. The MagnaTagATune dataset contains monophonic 30-second audio fragments in MP3 format, sampled at 16 000 Hz. Some of the audio fragments originate from the same song. We reconstructed the original song by concatenating the fragments into a single file, to avoid occurances of fragments of the same song in the same batch of positive-and negative pairs, thereby ensuring i.i.d. data for training.</p><p>The audio files from the Million Song Dataset were obtained from the 7digital service, which provides stereo 30-second audio fragments in MP3 format sampled at 44 100 Hz.</p><p>All files were re-sampled to 22 000 Hz, 16 000 Hz and 8 000 Hz and decoded to the PCM format with ffmpeg, using the following command:</p><p>This is the only preprocessing step that we performed before training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DATA AUGMENTATION DETAILS</head><p>The default pre-training setting, which we also used for our best models, uses 8 audio data augmentations. Not all augmentations are necessarily applied to all inputs: each independent data augmentation is applied with a probability tuned during hyperparameter gridsearch. The most effective augmentations and their probabilities are presented in Section 4.3. The implementation details for each augmentation are provided below. We used the 'torchaudio-augmentations' Python package for all our experiments <ref type="bibr" target="#b46">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Random Crop</head><p>The audio is cropped with a number of samples s ? {20 736, 43 740, 59 049} for sample rates 8 000, 16 000 and 22 050 Hz respectively. To ensure that every sample in the batch is of the same size, the fragment's window we can crop from with original length S is adjusted to S ? s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Polarity inversion</head><p>The polarity of the audio signal is inverted by multiplying the amplitude of the signal by ?1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additive White Gaussian Noise</head><p>White Gaussian noise is added to the complete signal with a signal-to-noise ratio (SNR) of 80 decibels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Gain Reduction</head><p>The gain of the audio signal is reduced at random using a value drawn uniformly between -6 and 0 decibels. In our implementation, we use the torchaudio.transforms.Vol interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Frequency Filter</head><p>A frequency filter is applied to the signal using the essentia library <ref type="bibr" target="#b47">[46]</ref>. We process the signal with either the LowPass or HighPass algorithm <ref type="bibr" target="#b48">[47]</ref>, which is determined by a coin flip.</p><p>For the low-pass filter, we draw the cut-off frequency from a uniform distributions between 2 200 and 4 000 Hz. All frequencies above the drawn cut-off frequency are filtered from the signal.</p><p>Similarly for the high-pass filter, we draw the cut-off frequency from a uniform distributions between 200 and 1200 Hz. All frequencies below the cut-off frequency are filtered from the signal.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Fully Convolutional Deep Auditory Model for Musical Chord Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612.05082" />
	</analytic>
	<monogr>
		<title level="m">IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Harmony Transformer: Incorporating Chord Segmentation Into Harmony Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-End Musical Key Estimation Using a Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th European Signal Processing Conference (EUSIPCO)</title>
		<meeting><address><addrLine>Kos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://arxiv.org/abs/1706.02921" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Beat and Downbeat Tracking with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 17th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-End Learning for Music Audio Tagging at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prockup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.02520" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<meeting>the 19th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Annotator subjectivity in harmony annotations of popular music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Koops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bransen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kent-Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Volk</surname></persName>
		</author>
		<idno type="DOI">10.1080/09298215.2019.1613436</idno>
		<ptr target="https://doi.org/10.1080/09298215.2019.1613436" />
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="252" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Society for Music Information Retrieval Conference</title>
		<meeting>the 12th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale Approaches to Music Audio Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Society for Music Information Retrieval conference</title>
		<meeting>the 14th International Society for Music Information Retrieval conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3852" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pre-Training Audio Representations With Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D C</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="600" to="604" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CLAR: Contrastive Learning of Auditory Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Al-Tahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mohsenzadeh</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v130/al-tahan21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning</title>
		<editor>Research, A. Banerjee and K. Fukumizu</editor>
		<meeting>The 24th International Conference on Artificial Intelligence and Statistics, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-04-15" />
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="2530" to="2538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive Learning of General-Purpose Audio Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="3875" to="3879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<ptr target="http://arxiv.org/abs/1807.03748" />
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<ptr target="http://arxiv.org/abs/1808.06670" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<idno>arXiv: 2002.05709</idno>
		<ptr target="http://arxiv.org/abs/2002.05709" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-Efficient Image Recognition with Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Representation Learning by Context Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<ptr target="http://ieeexplore.ieee.org/document/7410524/" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1422" to="1430" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pitch Estimation Via Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Velimirovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3527" to="3531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive Multiview Coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SampleCNN: End-to-End Deep Convolutional Neural Networks Using Very Small Filters for Music Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06434" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2605</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2605" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Task Self-Supervised Learning for Robust Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6989" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crepe: A Convolutional Representation for Pitch Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1068" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00991</idno>
		<title level="m">Data Augmenting Contrastive Learning of Speech Representations in the Time Domain</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-End Learning for Music Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6964" to="6968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised Music Tagging Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Society for Music Information Retrieval Conference</title>
		<meeting>of International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evaluation of Algorithms Using Games: The Case of Music Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Society for Music Information Retrieval Conference</title>
		<meeting>the 10th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Music Information Retrieval</title>
		<meeting>the 12th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ISMIR 2011</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Relationship between Precision-Recall and ROC Curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143874</idno>
		<ptr target="https://doi.org/10.1145/1143844.1143874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ser. ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ser. ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 12th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FMA: A Dataset for Music Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.01840" />
	</analytic>
	<monogr>
		<title level="m">18th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Musical Genre Classification of Audio Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Sturm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.1461</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Spijkervet/torchaudio-augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spijkervet</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5042440</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5042440" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ESSENTIA: An Audio Analysis Library for Music Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/10230/32252" />
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference (ISMIR&apos;13)</title>
		<meeting><address><addrLine>Curitiba, Brazil, 04</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="493" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Z?lzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Amatriain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arfib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dutilleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loscos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rocchesso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>DAFX-Digital Audio Effects. John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Natural Sounding Artificial Reverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Audio Engineering Society</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="223" />
			<date type="published" when="1962-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Scale for the Measurement of the Psychological Magnitude Pitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="185" to="190" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Subdivision of the Audible Frequency Range into Critical Bands (Frequenzgruppen)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoustical Society of America Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="1961-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
