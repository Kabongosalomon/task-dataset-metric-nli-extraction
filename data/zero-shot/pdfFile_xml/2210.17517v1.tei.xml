<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L?la: A Unified Benchmark for Mathematical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Finlayson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Rajpurohit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="department">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">L?la: A Unified Benchmark for Mathematical Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose L?la, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce Bh?skara, a general-purpose mathematical reasoning model trained on L?la. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), while the best performing model only obtains 60.40%, indicating the room for improvement in general mathematical reasoning and understanding. 1 * Equal first authors. ? Work done while at the Allen Institute for AI. ? Corresponding authors: matthewf@allenai.org, ashwinkv@allenai.org. 1 Our dataset: https://github.com/allenai/Lila. Our model: https://huggingface.co/ allenai/bhaskara.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: A data example with two Python programs in L?la. One program annotation uses function construct whereas the other one is a plain script without function. The instruction for each task and categories across four dimensions are annotated for developing L?la.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mathematical reasoning is required in all aspects of life, from buying ingredients for a recipe to controlling the world economy. Given the fundamental nature of mathematical reasoning, a number of works propose datasets to evaluate specific mathematical reasoning abilities of AI agents, e.g., <ref type="bibr">Kushman et al. (2014)</ref> (algebra word problems), Mishra et al. (2022c) (arithmetic reasoning), <ref type="bibr">Saxton et al. (2019)</ref> (templated math reasoning spanning algebra, calculus, probability, etc.) Since evaluating high-capacity models on narrowly scoped mathematical reasoning datasets risks overestimating the reasoning abilities of these AI systems, creating the need for a unified benchmark for systematic evaluation over diverse topics and problem styles.</p><p>To this end, we introduce L?la 2 , a unified mathematical reasoning benchmark that consists of 23 mathematical reasoning tasks. L?la is constructed by extending 20 existing datasets spanning a wide range of topics in mathematics, varying degrees of linguistic complexity, and diverse question formats and background knowledge requirements. Importantly, L?la extends all of these datasets to include a solution program as opposed to only an answer, and instruction annotations to enable instruction-based learning <ref type="bibr">(Sanh et al., 2021;</ref><ref type="bibr" target="#b25">Wei et al., 2021;</ref><ref type="bibr">Mishra et al., 2022b)</ref>.</p><p>In order to accurately assess the mathematical reasoning ability of models, evaluating the chain of reasoning that leads to the correct solution is equally important (if not more important) to evaluating the final answer or expression. We therefore collect Python programs that serve as reasoning chains for each question in the benchmark. We achieve this by automatically converting domainspecific language (DSL) annotations into Python programs and by manually collecting expert annotations when no DSL annotations are available. By incorporating program annotations, L?la unifies various mathematical reasoning datasets under a single problem formulation i.e., given an input problem in natural language, generate a Python program that upon execution returns the desired answer. This formulation allows neural approaches to focus on the highlevel aspects of mathematical problem solving (e.g., identifying potential solution strategies, decomposing the problem into simpler sub-problems), while leveraging external solvers (e.g., Python builtins, Sympy) to perform precise operations like adding huge numbers or simplifying expressions. <ref type="figure">Figure 1</ref> illustrates a sample from our L?la benchmark that illustrates the question, answer, program, instructions, and category tags.</p><p>In addition to evaluating high-level problem solving, we also facilitate two other key ways to make a fair assessment of models on mathematical reasoning tasks. In line with <ref type="bibr" target="#b5">Bras et al. (2020)</ref>, <ref type="bibr">Ribeiro et al. (2020)</ref> and <ref type="bibr" target="#b27">Welleck et al. (2022)</ref>, we evaluate generalization e.g., alternate formulations of a problem ("2+2=?" vs. "What is two plus two?") using an out-of-distribution evaluation set (L?la-OOD) containing datasets requiring the same underlying mathematical reasoning skills, but were collected independently of the training datasets. Further, we collect a robustness split L?la-Robust, that introduces linguistic perturbations (e.g., active vs. passive voice) via crowd-sourcing. The evaluation scheme is a combination of the performance on all three sets: L?la-Test, L?la-OOD and L?la-Robust.</p><p>Contributions 1. We present L?la, a holistic benchmark for mathematical reasoning. L?la extends 20 existing datasets with solutions in the form of Python programs and instruction annotations, and categorizes questions into 23 tasks based on their language complexity, question format and need for external knowledge. Our benchmark measures performance on out-of-distribution examples and robustness to language perturbations in addition to standard test-set. 2. We introduce Bh?skara, a multi-task model fine-tuned on our dataset. Our best-performing model achieves comparable performance to a 66? larger model pre-trained on both code and language. 3. We provide an analysis of our models' performance and find that (1) multitasking improves considerably over task-specific learning both in in-distribution and out-of-distribution evaluation (2) program synthesis substantially outperforms answer prediction, (3) few-shot prompting with codex has the strongest performance. We also identify areas for improvement for future work, e.g., data gaps in L?la categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Mathematical Reasoning Datasets. Our work builds on an existing body of mathematical reasoning literature. Early work in this areas focuses on smallscale datasets testing addition-subtraction <ref type="bibr" target="#b18">(Hosseini et al., 2014)</ref>, templated questions with equations as parameters <ref type="bibr">(Kushman et al., 2014)</ref> and other forms of arithmetic reasoning <ref type="bibr">(Koncel-Kedziorski et al., 2015;</ref><ref type="bibr">Roy and Roth, 2016;</ref><ref type="bibr" target="#b21">Upadhyay et al., 2016;</ref><ref type="bibr">Roy and</ref><ref type="bibr">Roth, 2017, 2018;</ref><ref type="bibr">Ling et al., 2017)</ref>. Later datasets increase in complexity and scale, incorporating reading comprehension <ref type="bibr" target="#b11">Dua et al. (2019b)</ref>, algebra <ref type="bibr">(Saxton et al., 2019)</ref>, and multi-modal contexts <ref type="bibr">(Lu et al., 2021a</ref><ref type="bibr">(Lu et al., , 2022</ref>. Still other numerical reasoning datasets focus on diversity <ref type="bibr">(Miao et al., 2020a)</ref> with multiple categories of numerical reasoning tasks (e.g., <ref type="bibr" target="#b2">Amini et al., 2019)</ref>. Most recently, new datasets have focused on increasing difficulty, e.g., olympiad problems <ref type="bibr">(Hendrycks et al., 2021b)</ref> and adversarial problems <ref type="bibr">(Patel et al., 2021)</ref>, as well as increasing the knowledge requirements to solve tasks, with a growing focus on commonsense reasoning <ref type="bibr" target="#b34">(Zhou et al., 2019;</ref><ref type="bibr" target="#b32">Zhang et al.;</ref><ref type="bibr">Lu et al., 2021b;</ref><ref type="bibr">Mishra et al., 2022c)</ref>. A separate line of work in mathematical reasoning includes datasets testing mathematical theorem proving (e.g., <ref type="bibr">Li et al., 2021;</ref><ref type="bibr" target="#b26">Welleck et al., 2021;</ref><ref type="bibr" target="#b33">Zheng et al., 2021;</ref>. We do not, however, consider theorem proving in our work, choosing instead to focus on numerical reasoning.</p><p>Task Hierarchy and Multi-tasking in Numerical Reasoning. We take inspiration from the success of multi-task learning in NLP <ref type="bibr" target="#b28">(Weston et al., 2015)</ref>, including benchmarks (e.g., <ref type="bibr" target="#b23">Wang et al., 2018</ref><ref type="bibr" target="#b22">Wang et al., , 2019</ref><ref type="bibr" target="#b10">Dua et al., 2019a</ref>) and multitasking models (e.g., <ref type="bibr">McCann et al., 2018;</ref><ref type="bibr">Khashabi et al., 2020;</ref><ref type="bibr">Lourie et al., 2021;</ref><ref type="bibr" target="#b1">Aghajanyan et al., 2021)</ref>. <ref type="bibr">NumGLUE (Mishra et al., 2022c)</ref> has been proposed as a multi-tasking numerical reasoning benchmark that contains 8 different tasks. L?la expands NumGLUE to provide wider coverage of mathematical abilities, along with evaluation that captures out-of-domain, robustness, and instruction-following performance. Our introduction of mathematical reasoning categories and the evaluation setup is inspired by task hierarchies in other domains such as vision <ref type="bibr" target="#b31">(Zamir et al., 2018) and</ref><ref type="bibr">NLP (Rogers et al., 2021)</ref> which appear in large scale benchmarks (e.g., <ref type="bibr">Srivastava et al., 2022;</ref><ref type="bibr" target="#b24">Wang et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">L?la</head><p>L?la is composed of 23 tasks across 4 dimensions, curated from 44 sub-datasets across 20 dataset sources. Here we discuss the construction and composition of the benchmark and provide descriptive statistics of the datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Construction</head><p>Data Sources. L?la incorporates 20 existing datasets from the mathematical reasoning literature <ref type="table" target="#tab_1">(Table 19</ref> gives a detailed list), where inputs are natural language or templated text and outputs are numerical or expressions, e.g., we exclude theorem proving <ref type="bibr" target="#b26">(Welleck et al., 2021;</ref>, where the output is not a number or expression. We leave the incorporation of formats like theorem proving to future work.</p><p>Unified format. We normalize all datasets to a unified format with the following fields: 1. The source dataset. Category tags for each of the four dimensions (math ability, language complexity, format, and external knowledge; see ?3.2). 2. The question, in English.</p><p>3. The answer to the question, as a string containing a number, expression, list, or other data format. A set of Python strings that print the answer. 4. A task-level instruction in natural language. We also retain meta-data from the original dataset.</p><p>Automatic program annotation. Most of the annotations in the source datasets do not contain output in the form of a Python program. We automatically annotate most datasets by generating Python programs using the annotations (answer, explanation, etc.) provided in the source datasets. Where possible, we generate multiple Python programs for a single question. This is to account for variation in the program space such as the choice of data structure, language construct, variable name, and programming style (e.g., declarative vs procedural). For example, <ref type="figure">Figure 1</ref> gives multiple Python programs solving the same question; in this case one program directly calculates the answer, whereas the other defines a function to solve the problem more generally.</p><p>Some datasets contain program annotations that can be captured by a domainspecifc language (DSL) in which case we write rules to convert them into Python programs, e.g., volume(sphere,3) to the Python expression 4/3*math.pi*3**3. In some cases where a DSL annotation is not provided, we use pattern matching to convert highly templated datasets like the AMPS dataset <ref type="bibr">(Hendrycks et al., 2021b)</ref> to our unified format. In other cases, instead of converting the existing dataset, we modify the data generation code to reproduce the dataset with program annotations. For the DeepMind mathematics dataset <ref type="bibr">(Saxton et al., 2019)</ref>, this allows us to create diverse, compositional math problems with program annotations using a sophisticated grammar.</p><p>Expert program annotation. For many datasets, it is not possible to obtain Python program annotations via automated methods described above; either the original dataset contains only the final answer or contains solutions expressed in free-form natural language. For such datasets, we obtain annotations from experts who are proficient in basic programming and high-school level mathematics. See Appendix B.1 for details.</p><p>Instruction annotation. Given the effectiveness of instruction learning <ref type="bibr">(Mishra et al., 2022b;</ref><ref type="bibr" target="#b25">Wei et al., 2021;</ref><ref type="bibr">Mishra et al., 2022a;</ref><ref type="bibr">Sanh et al., 2021)</ref> for effective generalization, we collect instruction annotation for each task. Each instruction contains a definition that clearly defines the task and provides guidelines, a prompt that provides a short and straight forward instruction, and examples that facilitate learning by demonstration <ref type="bibr">(Brown et al., 2020)</ref>. <ref type="figure">Figure 1</ref> shows an example instruction for the basic math task ( ?3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Categories and Tasks</head><p>We create 4 views 3 or categories of L?la along the dimensions of mathematical area, language complexity, external knowledge, and question format. Altogether, these views classify the data into 23 tasks <ref type="table" target="#tab_1">(Table 1)</ref>. By creating multiple views of the benchmark, we are able to systematically characterize the strengths and weaknesses of existing models at a granular level.</p><p>The first category, math ability, partitions the datasets into common pedagogical subjects: arithmetic, algebra, geometry, calculus, etc.</p><p>Our second category, language complexity, separates math problems by the complexity of the language used to represent them. This ranges from formal representations only (e.g., 1+1=?) to natural language (e.g., "Mariella has 3 pears. . . ").</p><p>We next partition datasets based on the type of background knowledge, required to solve the problem. For instance, commonsense questions like "How many legs to 3 people have?" or science questions like "Will water boil at 200 degrees Celsius?" require different sets of knowledge to answer.</p><p>Lastly, we categorize based on question format, putting e.g., multiple choice questions under one task and natural language inference under another. Examples of each task and the datasets included are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">L?la-OOD</head><p>In order to measure if the model has truly learned the underlying mathematical reasoning skill, we evaluate both in-distribution (IID, i.e., standard train-test splits) and out-of-distribution (OOD) performance for each task, i.e., we evaluate on examples requiring the same underlying mathematical reasoning skill but from a different dataset. To construct L?la-OOD, we follow <ref type="bibr" target="#b5">Bras et al. (2020)</ref> and <ref type="bibr" target="#b17">Hendrycks et al. (2020)</ref> by randomly assigning the datasets for each task into IID and an OOD sets, using the IID set for training and standard evaluation and the OOD set to evaluate generalization. We do not include tasks in L?la-OOD for tasks containing only one dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">L?la-Robust</head><p>In light of recent work demonstrating the brittleness of language models at solving math problems <ref type="bibr">(Patel et al., 2021)</ref>, we create a high-quality evaluation dataset, L?la-Robust, to evaluate performance on mathematical reasoning tasks when linguistic perturbations are introduced. Specifically, we define and apply a set of carefully chosen augmentation templates, summarized in <ref type="table" target="#tab_1">Table 16</ref>, on each task, yielding a set of challenging problems that are consistent answer-wise but stylistically different question-wise. Overall, we define a total of 9 templates for such question perturbations: 3 from Patel et al. (2021) and 6 of our own. From each constituent dataset, we sample 20 questions and obtain perturbed question annotations via Amazon Mechanical Turk (AMT). Refer to Appendix B.1 for additional details on the construction of L?la-Robust. <ref type="table" target="#tab_3">Table 2</ref> shows key statistics of our proposed benchmark, L?la. L?la contains ? 134K examples with significant diversity across question, answer, program and instruction length (see detailed statistics in Appendix C). <ref type="figure" target="#fig_0">Figure 2</ref> shows the diversity of questions in L?la. Note that we downsample (via random selection) some datasets like AMPS <ref type="bibr">(Hendrycks et al., 2021b)</ref> which contains numerous templated questions that can get over-representated in the distribution of examples across categories in L?la.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we introduce our modeling contributions for the L?la benchmark and discuss the overall experimental setup.</p><p>Data partition and evaluation. For the IID setup, we randomly partition the data in each task into training (70%), development (10%) and test (20%) sets. Additionally, we also evaluate on L?la-OOD and L?la-Robust settings; thus, the final evaluation scheme is a combination of the performance on all three evaluation setups  Fine-tuning. We fine-tune a series of GPT-Neo-2.7B causal language models <ref type="bibr" target="#b4">(Black et al., 2021)</ref>) on L?la. We choose GPT-Neo because it was pre-trained on both natural language and code <ref type="bibr" target="#b13">(Gao et al., 2020)</ref>, as opposed to solely on natural language. To assess the capabilities of GPT-Neo on various aspects of the dataset, we fine-tune single-task models on each of the 23 tasks in L?la. We also evaluate the benefit of transfer learning by fine-tuning a single multi-task GPT-Neo baseline on all the tasks simultaneously. We call our multitask model Bh?skara.</p><p>Prompting. We also use few-shot prompting to evaluate GPT-3 and Codex 4 <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2021)</ref>. For the IID setting, we prompt the model with a random input-output examples from the same dataset as the input. In the OOD setting, we take examples from other datasets <ref type="table" target="#tab_1">(Table 12</ref>-15) within the same task. We repeat this evaluation with increasing numbers of examples (up to the token size of models) to study the effect on performance 5 .</p><p>Evaluation. We evaluate our models under two regimes-directly outputting the answer i.e., program induction and outputting a Python program that is then executed to obtain the final answer i.e., program synthesis. In the case of our fine-tuned models, we train them to output both the final answer and the Python program conditioned on the input question. To evaluate our models under direct question answering, we use F1-score 6 to compare the model output and the gold answer. To evaluate program synthesis, we execute the model's output within a Python interpreter and compare the program output with the output of the gold program, again using F1. We evaluate based on the program output, rather than the program itself, to account for diversity in solving techniques and programming styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>A summary of all key results on our L?la benchmark are shown in <ref type="table">Table 3</ref>. In this section, we will discuss the performance of fine-tuned 2.7B GPT-Neo models ( ?5.1), performance of models along the 4 categories of tasks ( ?5.2) and finally, the few-shot performance of much larger (?175B parameters) models ( ?5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results: Fine-tuned Models</head><p>Multitasking improves IID performance, robustness, and OOD generalization. The multi-tasking model (Bh?skara) substantially improves upon the single task models (Neo). Bh?skara achieves better average in-domain performance than the 23 individual per-task models (0.480 vs. 0.394 average score), suggesting that it leverages cross-task structure not present in a single task's training set. We also find that our multi-task model is robust to the linguistic perturbations we test in L?la-Robust. We did not find any degradation in performance when testing on perturbed IID test examples. Additionally, multi-task training substantially improves out-of-domain generalization (0.448 vs. 0.238). The gap between IID and OOD performance is much smaller for Bh?skara than for the single task models <ref type="table">(Table 3)</ref>, and in one case (format) Bh?skara's OOD performance on held-out tasks is better than its IID performance <ref type="table" target="#tab_5">(Table 4</ref>). L?la's multi-task structure opens interesting future directions related to developing improved multitasking techniques, and further understanding its benefits. <ref type="table">IID  OOD  IID  OOD  IID  OOD  IID  OOD  IID  OOD  IID</ref>   <ref type="table">Table 3</ref>: Evaluations of different baselines across 23 tasks in L?la. On most tasks, Codex outperforms all baselines while Bh?skara-P outperforms all fine-tuned baselines. A model usually performs worse on the OOD data set. The bold score refers to the best score among models with the same supervision method; the underlined score refers to the best score among all models. GPT-3 and Codex performance is computed on 100 uniformly distributed examples owing to their cost and usage limit. Fine-tuned model performance is calculated on the full test set.</p><formula xml:id="formula_0">? Supervision/Size Few-shot, 175B Few-shot, 175B Fine-tuned, 2.7B Fine-tuned, 2.7B Fine-tuned, 2.7B Fine-tuned, 2.7B ? Task Category GPT-3 Codex Neo-A Neo-P Bh?skara-A Bh?skara-P</formula><p>Lastly, we do not find any benefit to fine-tuning with instructions. Our best instruction tuned model achieves 0.133 F1, whereas the worst non-instructiontuned multitask model achieves 0.290.</p><p>Program synthesis substantially outperforms answer prediction. Synthesizing the program and evaluating it to get an answer substantially outperforms directly predicting the answer. For instance, multi-task program synthesis (Bh?skara-P) has an average score of 0.480 while multi-task answer prediction (Bh?skara-A) scores 0.252. This means models are often able to generate a program that evaluates to the correct answer, even when the model cannot directly compute the answer.</p><p>Program synthesis improves over answer prediction in all math categories except Geometry, with the largest improvements in Statistics and Linear Algebra; see <ref type="table" target="#tab_6">Table 5</ref> for examples. We even see benefits of program synthesis in NLI, a classification-based task. L?la's unified problem format decouples synthesis from computation, while opening directions for further study on either aspect.   Models leverage symbolic execution and libraries. The gap between program synthesis and answer prediction suggests that the neural language model offloads computations to the symbolic Python runtime that are otherwise difficult to compute directly. We identify two common cases. First, the model leverages standard Python as a calculator. For instance, this pattern is common in the basic_math and mul_div categories, which involve evaluating arithmetic expressions; <ref type="table" target="#tab_5">Table 4</ref> shows examples. Second, the model is able to call external libraries that perform sophisticated computations. For instance, in statistics the model uses scipy.stats.entropy or np.linalg.det in linear algebra while solving problems <ref type="table" target="#tab_6">(Table 5)</ref>.</p><p>Models occasionally generate non-executable code. Roughly 10% of Bh?skara's IID programs fail to execute. 86% of these are SyntaxErrors, which often occur because decoding terminates before finishing the program or the model generates a program of the form '2+3=5', which is invalid Python. The remaining 14% of execution failures are less trivial, including NameErrors (7%) and TypeErrors (1%) (see <ref type="table">Table 6</ref>).</p><p>Bh?skara is a good starting point for further fine-tuning <ref type="table" target="#tab_6">Table 5</ref> shows that our Bh?skara model is a better starting point for downstream fine-tuning than the vanilla pre-trained GPT-Neo-2.7B. When comparing fine-tuning for direct question answering with T5-3B, we see an almost 8% absolute improvement in F1 (30.1% to 37.6%). These findings establish Bh?skara as a strong starting point for further fine-tuning on new tasks. For this reason, we release our multi-task model for public use under the name Bh?skara, with the hope that it will be useful for future research into math reasoning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results: Category-wise Analysis</head><p>In this section we discuss the trends among the tasks within each category. For brevity, we primarily consider Bh?skara, the GPT-Neo multi-task model in the program-synthesis setting.</p><p>Math ability. Among the tasks in the math category, Bh?skara excels in basic math, linear algebra, and in-domain statistics. On these tasks, it performs equal or better to Codex. On the other hand, Bh?skara struggles in advanced math and geometry, with mediocre performance in multiplicationdivision, number theory, and calculus. Codex shows analogous trends, except for performing very well on calculus (0.930) 7 .</p><p>Language complexity . Models generally show lower performance on program synthesis as language complexity increases. Bh?skara gets mean F1 over 0.5 only for datasets with the least linguistic complexity where it achieves an F1 of 0.7.</p><p>Question format. Among the format tasks in the dataset, Bh?skara does exceptionally well on multiple-choice and natural-language inference, getting performance close to 0.9 on the latter, and outperforming Codex on both. On the other hand, the model performs close to 0.25 for reading comprehension and fill-in-the-blank, though with 0.5 F1 on out-of-domain fill-in-the-blank.</p><p>Background knowledge. Bh?skara performs above 0.5 F1 only for problems requiring commonsense and math formulas and fails to do similarly on problems requiring other forms of external knowledge like physics, computer science, or real-world knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results: Few-shot Prompting</head><p>Finally, we study the few-shot performance of much larger models (?175B), to better understand the performance of the smaller trained models (?2.7B) and to provide a benchmark for evaluating other large language models. Overall, we find that few-shot prompted models generally outperform their much smaller but fine-tuned counterparts.   <ref type="table">Table 6</ref>: The IID scores for GPT-3 models with and without instruction prompting (Inst). Instruction helps slightly in zero-shot setting, but not in few-shot setting.</p><p>Instructions and more examples improve performance. We find that the number of few-shot examples greatly impacts prompt models' performance. <ref type="figure" target="#fig_2">Figure 3</ref> shows that GPT-3 answer prediction beats Codex program synthesis in zero-to one-shot settings, but Codex overtakes with more examples. Table 6 shows that prompting with instructions improves performance only in the zero-shot setting, meaning that in the limited contexts of the prompt models, examples are more important than instructions for mathematical reasoning. This is consistent with the findings of Puri et al. (2022) on instruction-example equivalence.</p><p>Few-shot GPT-3 answer prediction underperforms Bh?skara. While prompt-based models outperform our fine-tuned models in general when comparing within direct-answering and program-synthesis, when comparing Bh?skara program-synthesis to GPT-3 direct answering we find that the much smaller Bh?skara consistently outperforms GPT-3.</p><p>Few-shot Codex performance is relatively strong. Relative to the 2.7B trained models, Codex demonstrates strong few-shot IID and OOD performance. Some notable exceptions to this pattern are the statistics, linear algebra, multiplechoice question answering, and NLI tasks. Generally, OOD few-shot performs much better than OOD for the fine-tuned models.</p><p>Few-shot Codex fails on some tasks. Despite strong performance relative to Bh?skara, Codex obtains less that 0.5 F1 on several tasks, with especially poor performance on geometry, number theory, advanced math, complex language, computer science problems, science formulas, and real world knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce L?la, a unified mathematical reasoning benchmark for a holistic evaluation of AI agents. L?la consists of 23 tasks across 4 dimensions (i) mathematical abilities, (ii) language format, (iii) language complexity, (iv) external knowledge. It builds on 20 existing mathematical reasoning datasets to collect instructions and Python programs. Further, it also supports measuring out-of-distribution performance and robustness to language perturbations via L?la-OOD and L?la-Robust respectively. We also introduce Bh?skara, a 2.7B-parameter fine-tuned multi-task model. We find that multi-tasking improves over single-task performance by 21.83% F1 score on average, and that our model is a strong starting point for further fine-tuning on new math reasoning tasks. The best performing model we evaluate achieves only 60.40% F1 indicating the potential for improvement on the proposed benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations</head><p>One drawback of our unified format is the difficulty of evaluating models. In our work we use F1 for lack of a better alternative. F1 likely over-estimates performance, e.g., given the gold answer "2 apples", the predicted answers "2" and "apples" receive the same score, though the former is better. L?la contains 23 tasks which are created from 20 datasets and 44 subdatasets. There is scope to add more mathematical reasoning datasets (e.g., theorem proving.) The flexible unified format of L?la allows for future extensions. Additionally, our categorization provides a way to identify areas for extension. For instance, we only have 1 dataset for linear algebra, which happens to not use natural language, and takes the form of generative QA. Our benchmark will benefit from future linear algebra additions, perhaps with word problems formatted as fill-in-the-blank questions.   <ref type="figure" target="#fig_5">Figure 6</ref> gives an example of a non-compiling output program. <ref type="table" target="#tab_1">Tables 12-15</ref> give examples and datasets from each task for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Examples Datasets</head><p>Math <ref type="table" target="#tab_1">Table 8  Table 12  Language  Table 9  Table 13  Format  Table 10  Table 14  Knowledge Table 11</ref> Table 15 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Expert annotation</head><p>In the worker qualification process, we ask each worker to annotate 30 questions. We manually verify each annotation and qualify those whose Python annotations are satisfactory. We also provide feedback such as "write simpler programs, use representative variable names instead of just letters, add comments wherever possible" to annotators after the worker qualification process. We instruct  annotators to use a minimal set of Python libraries, and we ask them to record the Python libraries they use in a common document. We find that the annotators could get the task done just by using the sympy and the datetime libraries. We also ask annotators to report any bugs in answer annotation, which they report for a small number of questions; we subsequently fix those. We give 10 sample question annotations to annotators as illustrative examples which vary in structure, length, format, underlying reasoning skill, etc. We pay 20 dollars per hour up to 20 hours per week as compensation for the data annotation work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L?la-Robust</head><p>To create the L?la-Robust dataset, we first define a set of 9 templates, consisting of 3 variation styles defined in SVAMP <ref type="bibr">(Patel et al., 2021)</ref> as well as 6 novel templates of our own. We refer to the SVAMP templates as SVAMP-COO, SVAMP-COP, and SVAMP-IU, which correspond to changing the order of objects, changing the order of phrases, and adding irrelevant, unhelpful information to the problem statement, respectively. Our novel templates are Model: Codex Task: Statistics Problem: Simplify the expression (9x 2 + 3x + 7) + (3x 2 + 7x 5 + 2). Express your answer as a polynomial with terms arranged in decreasing order of degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Program:</head><p>from sympy import Poly p = Poly ( 9 * ( x ** 2 ) + 3 * x + 7 + 3 * ( x ** 2 ) + 7 * ( x ** 5 )</p><p>+ 2 ) answer = p . as_expr () print ( answer ) # == &gt; NameError ( x is not defined )</p><p>Gold Answer: 7x 5 +12x 2 +3x+9</p><p>Model: Bh?skara Task: Muldiv Problem: Jenny collects cans and bottles to take down to the recycling center. Each bottle weighs 6 ounces and each can weighs 2 ounces. Jenny can carry a total of 100 ounces. She collects 20 cans and as many bottles as she can carry. If she gets paid 10 cents per bottle and 3 cents per can, how much money does she make (in cents)? named ROBUST-IR, ROBUST-AP, ROBUST-ADJ, ROBUST-Q, ROBUST-RQ, and ROBUST-RM. ROBUST-IR refers to adding information that is unhelpful for solving the question but may be related to the context of the problem. ROBUST-AP refers to increasing problem verbosity by turning active speech to passive speech. ROBUST-ADJ refers to increasing problem verbosity by adding adjectives or adverbs. ROBUST-Q indicates turning a problem statement into a question, in the style of a conversation with a student. ROBUST-RQ indicates removing question words in a problem and turning it into a statement; it is roughly the inverse of ROBUST-Q. Finally, ROBUST-RM refers to the removal of mathematics terms that are implicitly defined. Examples of each template are found in <ref type="table" target="#tab_1">Table 16</ref>. For our crowdsourcing pipeline, we provide each Amazon Mechanical Turk worker with 10 questions split from 20 questions sampled from each dataset. We run a separate job for each of our 9 templates. In particular, each HIT contains the 10 split questions from the original datasets, alongside the problem solution.</p><p>Workers are asked to submit an augmentation for each question according to the style of the template assigned to each job. Thus, we run 9 separate jobs Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question category Example</head><p>Task 1</p><p>Basic math: addition, subtraction, fact based QA etc.</p><p>Original: If Jimbo is 484 feet away from a beetle and quarter of 827 feet away from a grasshopper, which insect will seem bigger to him?? "Option 1": beetle, "Option 2" :grasshopper Answer: Option 2</p><p>Task 2 Muldiv: multiplication, division along with addition, subtraction etc.</p><p>Question: Mrs. Hilt bought 2 pizzas. Each pizza had 8 slices. So, she had __ total slices of pizza. Answer: 16</p><p>Task 3 Number theory: prime, power, negation, modulus and other operators etc.</p><p>Question: How many numbers are divisible by both 2 and 3 up to 300? Answer: 50</p><p>Task 4 Algebra: equations, functions, polynomials, series etc.</p><p>Question: The sum of the three smallest of four consecutive integers is 30 more than the largest integer. What are the four consecutive integers ? Answer: 15.0</p><p>Task 5</p><p>Geometry: triangles, polygons, 3D structures etc.</p><p>Question: A hall is 6 meters long and 6 meters wide. If the sum of the areas of the floor and the ceiling is equal to the sum of the areas of four walls, what is the volume of the hall (in cubic meters)? Answer: 108</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 6</head><p>Statistics: binomial, divergence, mean, median, mode, variance etc.</p><p>Question: There are 11 boys and 10 girls in a class. If three students are selected at random, in how many ways that 3 girl and 2 boys are selected? Answer: 6600</p><p>Task 7 Calculus: differentiation, integration, gradient, series expansion etc. Task 9</p><p>Advanced math: heuristics required along with probability, statistics, or algebra, Olympiad level problems <ref type="figure">(f (1)</ref>))). Answer: 256 to obtain augmentations for all templates across all datasets. To familiarize workers with the intended style of each template, we provide 3 demonstrative augmentations within the instructions of each HIT, as summarized in <ref type="table" target="#tab_1">Table  16</ref>. We restrict our crowdsourcing pipeline to workers that had above a 98% acceptance rate with over 1000 completed HITs. We provide workers with an upper bound of 1 hour to complete each HIT but specify in the instructions that each HIT should feasible be completed in 10 minutes. Based on minimum wage policies and under the assumption that workers follow the 10-minute completion guideline, we accordingly compensate $3 per HIT. Finally, to ensure dataset quality of generations via the Amazon Mechanical Turk , we manually assess the worker augmentations produced for each template. <ref type="figure" target="#fig_8">Figure 8</ref> gives relatives sizes of tasks within each category. <ref type="figure" target="#fig_9">Figure 9</ref> illustrates the unigram frequencies in L?la, where larger words indicate higher frequency. <ref type="table" target="#tab_1">Table 17</ref> gives comprehensive statistics on each task.  <ref type="table" target="#tab_1">Table 18</ref> gives the unaggregated performance of each model on each dataset in L?la (some datasets are split across tasks).</p><formula xml:id="formula_1">Question: Let f (x) = 2 x . Find f (f (f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question category Example</head><p>Task 10 No language Compute the median of 4 ? 2, ?6, 3e, 3, ?6, ? 14 ? ? , 6. Answer: 3</p><p>Task 11 Simple language Question: Joan had 9 blue balloons, but Sally popped 5 of them. Jessica has 2 blue balloons. They have __ blue balloons now. Answer: 6</p><p>Task 12</p><p>Complex language: involving co-reference resolution etc., multi-sentence language, adversarial language: containing tricky words etc., often created adversarially  Multiple choice question answering (MCQ) Question: The fish glided with a speed of 8 m/s through the water and 5 m/s through the jello because the __ is smoother? "Option 1": jello, "Option 2": water. Answer: Option 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 16</head><p>Natural language inference (NLI) Question: "statement 1": Alyssa picked 42.0 pears from the pear tree and Nancy sold 17.0 of the pears , "statement 2" :25.0 pears were left , "options: " Entailment or contradiction? Answer: Entailment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 17</head><p>Reading comprehension (RC) Question: Passage: A late game rally by Washington led them to the Eagles' 26 yard line. A shot to the end zone by Robert Griffin III would be intercepted by Brandon Boykin, clinching an Eagles win. The Eagles would move to 6-5. This is the Eagles first win at Lincoln Financial Field since Week 4 of the 2012 season, because prior to this game, the Eagles had never won a game in their home stadium in 414 days since that same week, snapping a 10-game losing streak at home with this win. Question: How many more wins than losses did the Eagles have after this game? Answer: 1 Commonsense: temporal commonsense knowledge (e.g., people usually play basketball for a few hours and not days), numerical commonsense knowledge (e.g. birds has 2 legs) Question: Outside temple, there is a shop which charges 12 dollars for each object. Please note that one shoe is counted as an object. Same is true for socks and mobiles. Paisley went to temple with both parents. All of them kept their shoes, socks and mobiles in the shop. How much they have to pay? Answer: 180</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 20</head><p>Math formulas: algebra, geometry, probability etc.</p><p>Question: Simplify -3*(sqrt(1700) -(sqrt(1700) + (3 + sqrt(1700))*-6)) + -3. Answer: -180*sqrt(17) -57</p><p>Task <ref type="formula">21</ref> Science formulas: physics, chemistry etc. Question: Find the number of moles of H2O formed on combining 2 moles of NaOH and 2 moles of HCl. Answer: 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 22</head><p>Computer science knowledge: data structure algorithms like merge sort etc.</p><p>Question: Apply functions 'mean' and 'std' to each column in dataframe 'df' Answer: df.groupby(lambda idx: 0).agg(['mean', 'std'])</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 23</head><p>Real-world knowledge: COVID modelling, climate modelling etc.</p><p>Question: Our physics club has 20 members, among which we have 3 officers: President, Vice President, and Treasurer. However, one member, Alex, hates another member, Bob. How many ways can we fill the offices if Alex refuses to serve as an officer if Bob is also an officer? (No person is allowed to hold more than one office.) Answer: 6732  Question: A gardener is going to plant 2 red rosebushes and 2 white rosebushes. If the gardener is to select each of the bushes at random, one at a time, and plant them in a row, what is the probability that the 2 rosebushes in the middle of the row will be the red rosebushes?</p><p>Options: {A:1/12, B:1/6, C:1/5, D:1/3, E:1/2}</p><p>Answer: B</p><p>Explanation:</p><p>We are asked to find the probability of one particular pattern: wrrw. Total # of ways a gardener can plant these four bushes is the # of permutations of 4 letters wwrr, out of which 2 w' s and 2 r' s are identical, so 4 ! / 2 ! 2 ! = 6 ; so p = 1 / 6. Answer: B.</p><p>Program: import scipy n0 = 2.0 n1 = 2.0 n2 = 2.0 t0 = n0 + n0 t1 = scipy.special.comb(t0, n0) answer = 1.0 / t1 <ref type="figure">Figure 7</ref>: An example of instruction annotation.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template Name Variation</head><p>Example SVAMP-COO Change the order of objects Question: Allen bought 20 stamps at the post office in 37 cents and 20 cents denominations . If the total cost of the stamps was $ 7.06 , how many 37 cents stamps did Allen buy ? Variation: Allen bought 20 stamps at the post office in 20 cents and 37 cents denominations . If the total cost of the stamps was $ 7.06 , how many 37 cents stamps did Allen buy ?</p><p>SVAMP-COP Change the order of phrases Question: One pipe can fill a tank in 5 hours and another pipe can fill the same tank in 4 hours . A drainpipe can empty the full content of the tank in 20 hours . With all the three pipes open , how long will it take to fill the tank ? Variation: A drainpipe can empty the full content of a tank in 20 hours . One pipe can fill the tank in 4 hours and another pipe can fill the same tank in 5 hours . With all the three pipes open , how long will it take to fill the tank with all the three pipes open ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVAMP-IU</head><p>Add irrelevant, unhelpful information Question: the area of an isosceles trapezoid with sides of length 5 and bases of length 7 and 13 is ? Variation: monkeys and apes are both primates, which means they're both part of the human family tree . the area of an isosceles trapezoid with sides of length 5 and bases of length 7 and 13 is ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBUST-IR Add unhelpful, but contextually related information</head><p>Question: Tom is 15 years younger than alice . Ten years ago , Alice was 4 times as old as Tom was then . How old is each now ? Variation: Tom is 15 years younger than alice . Ten years ago , Alice was 4 times as old as Tom was then . Alice really likes pinapple pizza. How old is each now ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBUST-AP Turn active into passive speech to increase problem verbosity</head><p>Question: Hay's Linens sells hand towels in sets of 17 and bath towels in sets of 6. If the store sold the same number of each this morning, what is the smallest number of each type of towel that the store must have sold? Variation: Hand towels are sold by Hay's Linens in sets of 17 and bath towels are sold in sets of 6. If the same number of each were sold by the store this morning, what is the smallest number of each type of towel that the store must have sold?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBUST-ADJ Add adjectives and adverbs to increase problem verbosity</head><p>Question: ThereTea leaves exposed to oxygen for up to _ hours become black tea.</p><p>Variation: Black tea leaves continuously exposed to oxygen for up to _ hours become a very rich black tea.</p><p>ROBUST-Q Turn a task statement into a question Question: Product of -7 and -1469.125. Variation: What is the product of -7 and -1469.125?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBUST-RQ</head><p>Turn a question into a task statement Question: Problem: If the product of 5 and a number is increased by 4 , the result is 19. What is the number? Variation: Increasing the product of 5 and a number by 4 results is 19. Find the number.       <ref type="table" target="#tab_1">Table 18</ref>: Evaluation results of baselines across different single datasets. On most datasets, Codex performs best. Model names: GPT-3: the few-shot 175B GPT-3 model; GPT-Neo-A: the fine-tuned 2.7B GPT-3 model where the prediction output is an answer; GPT-Neo-P: the fine-tuned 2.7B GPT-3 model where the prediction output is a program; Codex: the few-shot Codex model where the prediction output is a program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBUST-RM</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Question n-gram distribution in L?la.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Average F1 scores of GPT-3 and Codex with different numbers of few-shot examples in L?la.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples with Bh?skara on Basic Math and Muldiv.A Qualitative ExamplesFigures 4 and 5 give examples of input-output behavior of Bh?skara.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>15 Figure 5 :</head><label>155</label><figDesc>Examples with Bh?skara on Statistics and Linear Algebra.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>NameErrors in Codex and Bh?skara.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Question:</head><label></label><figDesc>Let g(y) = 9*y**4 + 25*y**2 + 6. Let s(d) = 1 -d**4. Let x(t) = -g(t) + 6*s(t). What is the third derivative of x(f) wrt f? Answer: -360*f Task 8 Linear algebra: vectors, dot products, Eigen vectors, matrices etc.Question: Problem: Convert the following matrix to reduced row echelon form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Remove explicitly mathematical terms that are implicitly defined Problem: Find the arclength of the function f (x) = 2 ? x on the interval x = 2 to x = 8 Variation: Find the arclength of f (x) = 2 ? x on [2, 8]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Task diversity in L?la across math, language, format, and knowledge categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>The word cloud distribution of annotated programs in the L?la dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Categories and their associated tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Key statistics of L?la.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Multi-task models are able to generalize to unseen tasks in some categories. Program output (Neo-P) always outperforms number output (Neo-A).</figDesc><table><row><cell>Data</cell><cell cols="2">Answer (% F1)</cell><cell cols="2">Program (% F1)</cell></row><row><cell></cell><cell cols="2">Neo Multi</cell><cell cols="2">? Neo Multi</cell><cell>?</cell></row><row><cell cols="2">100% 28.4</cell><cell cols="2">32.3 +4.0 80.0</cell><cell>82.4 +2.5</cell></row><row><cell cols="2">40% 20.0</cell><cell cols="2">21.1 +1.2 75.2</cell><cell>70.3</cell><cell>-4.9</cell></row><row><cell cols="2">20% 15.8</cell><cell cols="2">18.4 +2.6 66.3</cell><cell>67.1 +0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Here we show the results of fine-tuning both GPT-Neo-2.7B (Neo) and Bh?skara (Multi) on 100%, 40%, and 20% of the held-out data from L?la-OOD. The Multi almost always outperforms Neo (the ? column shows the margin).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700. Aditya Kolachana, K Mahesh, and K Ramasubramanian. 2019. Use of calculus in hindu mathematics. In Studies in Indian Mathematics and Astronomy, pages 345-355. Springer. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</figDesc><table><row><cell>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021b. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems Track on Datasets and Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Task: Basic Math Task: Muldiv 2020. Beyond accuracy: Behavioral testing of nlp models with checklist. In Problem: Before December, cus-Problem: Tickets to the school Proceedings of the 58th Annual Meeting of the Association for Computational tomers buy 1346 ear muffs from play cost 6 for students and 8 for Linguistics, pages 4902-4912. the mall. During December, they adults. If 20 students and 12 adults</cell></row><row><cell>Benchmarks (NeurIPS 2021). Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. buy 6444, and there are none. In bought tickets, how many dollars' Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2021. Qa dataset explosion: all, how many ear muffs do the cus-worth of tickets were sold? A taxonomy of nlp resources for question answering and reading comprehension. tomers buy? arXiv preprint arXiv:2107.12708. Predicted Answer: 48</cell></row><row><cell>arXiv preprint arXiv:1806.08730. Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020a. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings print ( answer ) Processing, pages 1743-1752. answer = 1346 . 0 + 6444 . 0 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Generated Program: Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Predicted Answer: 1346.0 Generated Program:</cell></row><row><cell>of the 58th Annual Meeting of the Association for Computational Linguistics, # Result == &gt; 7790 . 0</cell></row><row><cell>pages 975-984, Online. Association for Computational Linguistics.</cell></row><row><cell>Gold Answer: 7790.0</cell></row><row><cell>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022b.</cell></row><row><cell>Cross-task generalization via natural language crowdsourcing instructions. In</cell></row><row><cell>Proceedings of the 60th Annual Meeting of the Association for Computational</cell></row><row><cell>Linguistics (Volume 1: Long Papers), pages 3470-3487.</cell></row><row><cell>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter</cell></row><row><cell>Clark, Chitta Baral, and Ashwin Kalyan. 2022c. Numglue: A suite of fun-</cell></row><row><cell>damental yet challenging mathematical reasoning tasks. In Proceedings of</cell></row><row><cell>the 60th Annual Meeting of the Association for Computational Linguistics</cell></row><row><cell>(Volume 1: Long Papers), pages 3505-3523. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019.</cell></row><row><cell>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models Analysing mathematical reasoning abilities of neural models. arXiv preprint</cell></row><row><cell>really able to solve simple math word problems? In Proceedings of the 2021 arXiv:1904.01557.</cell></row><row><cell>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021a. Inter-gps: Interpretable geometry problem solving Conference of the North American Chapter of the Association for Computa-Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, tional Linguistics: Human Language Technologies, pages 2080-2094, Online. Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Association for Computational Linguistics. Adri? Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantify-with formal language and symbolic reasoning. In The 59th Annual Meeting of Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. ing and extrapolating the capabilities of language models. arXiv preprint the Association for Computational Linguistics (ACL). How many data samples is an additional instruction worth? arXiv preprint arXiv:2206.04615.</cell></row><row><cell>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay arXiv:2203.09161. Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022. Dynamic prompt learning Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. 2019. Quarel: A dataset and models for answering questions about qualitative via policy gradient for semi-structured mathematical reasoning. arXiv preprint Equate: A benchmark evaluation framework for quantitative reasoning in relationships. In Proceedings of the AAAI Conference on Artificial Intelligence, arXiv:2209.14610. natural language inference. arXiv preprint arXiv:1901.03735. volume 33, pages 7063-7071.</cell></row></table><note>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157. Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281. Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. 2021. Isarstep: a benchmark for high-level mathematical reasoning. In International Conference on Learning Representations. Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre- trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146. Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13480-13488.Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020b. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Reframing instructional prompts to GPTk's language. In Findings of the Association for Computational Linguistics: ACL 2022, pages 589-612, Dublin, Ireland. Association for Computational Linguistics.Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413. Subhro Roy and Dan Roth. 2017. Unit dependency graph and its application to arithmetic word problem solving. In Thirty-First AAAI Conference on Artificial Intelligence. Subhro Roy and Dan Roth. 2018. Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics, 6:159-172. Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics, 3:1-13. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207. Benoy Kumar Sarkar. 1918. Hindu Achievements in Exact Science: A Study in the History of Scientific Development. Longmans, Green and Company.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Examples and datasets meta-table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Example of each task in the math ability category of the L?la benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 19</head><label>19</label><figDesc></figDesc><table><row><cell>cites each</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Example of each task in the language complexity category of the L?la benchmark.</figDesc><table><row><cell>Task</cell><cell>Question category</cell><cell>Example</cell></row><row><cell cols="2">Task 13 Fill in the blank</cell><cell>Question: Delphinium has _ florets or they are full of holes. Answer: no</cell></row><row><cell cols="2">Task 14 Generative question answering</cell><cell>Question: Calculate the remainder when 160 is divided by 125. Answer: 35</cell></row><row><cell>Task 15</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Example of each task in the question formatcategory of the L?la benchmark.</figDesc><table><row><cell>Task</cell><cell>Question category</cell><cell>Example</cell></row><row><cell>Task 18</cell><cell>No external knowledge: only mathemati-cal commonsense knowledge required</cell><cell>Question: If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how many bottle caps are in the box? Answer: 14</cell></row><row><cell>Task 19</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Example of each task in the background knowledgecategory of the L?la benchmark.</figDesc><table><row><cell>Task</cell><cell cols="2">Math category IID</cell><cell>OOD</cell></row><row><cell></cell><cell></cell><cell>addsub.json</cell><cell>MCTaco_event_duration_structured.json</cell></row><row><cell></cell><cell></cell><cell>Numersense_structured.json</cell><cell>NumGLUE_Task3.json</cell></row><row><cell></cell><cell></cell><cell>MCTaco_stationarity_structured.json</cell></row><row><cell cols="2">Task 1 Basic math</cell><cell>MCTaco_frequency_structured.json</cell></row><row><cell></cell><cell></cell><cell>MCTaco_event_typical_time_structured.json</cell></row><row><cell></cell><cell></cell><cell>MCTaco_event_ordering_structured.json</cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task7.json</cell></row><row><cell></cell><cell></cell><cell>singleop.json</cell><cell>svamp_structured.json</cell></row><row><cell></cell><cell></cell><cell>multiarith.json</cell><cell>NumGLUE_Task4.json</cell></row><row><cell></cell><cell></cell><cell>asdiv.json</cell></row><row><cell cols="2">Task 2 Muldiv</cell><cell>GSM8k_structured.json</cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task1.json</cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task2.json</cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_muldiv.json</cell></row><row><cell></cell><cell></cell><cell>mathqa_physics.json</cell><cell>mbpp_structured.json</cell></row><row><cell></cell><cell></cell><cell>APPS_structured.json</cell><cell>mathqa_other.json</cell></row><row><cell></cell><cell></cell><cell>mathqa_gain.json</cell></row><row><cell cols="2">Task 8 Number theory</cell><cell>amps_number_theory.json mathqa_general.json</cell></row><row><cell></cell><cell></cell><cell>conala_structured.json</cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task5.json</cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_numbertheory.json</cell></row><row><cell></cell><cell></cell><cell>singleq.json</cell><cell>draw_structured.json</cell></row><row><cell></cell><cell></cell><cell>simuleq.json</cell><cell>dolphin_structured.json</cell></row><row><cell cols="2">Task 4 Algebra</cell><cell>amps_algebra.json</cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task8.json</cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_algebra.json</cell></row><row><cell cols="2">Task 5 Geometry</cell><cell>amps_geometry.json</cell><cell>mathqa_geometry.json</cell></row><row><cell cols="2">Task 6 Statistics</cell><cell>amps_counting_and_stats.json</cell><cell>mathqa_probability.json</cell></row><row><cell cols="2">Task 7 Calculus</cell><cell>amps_calculus.json deepmind_mathematics_basicmath.json</cell><cell>deepmind_mathematics_calculus.json</cell></row><row><cell cols="2">Task 8 Linear algebra</cell><cell>amps_linear_algebra.json</cell></row><row><cell cols="2">Task 9 Advanced math</cell><cell>MATH_crowdsourced.json</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Raw datasets used to create different tasks in L?la across different math categories.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Raw datasets used to create different tasks in L?la across different language categories.</figDesc><table><row><cell>ID</cell><cell>Format category</cell><cell>IID</cell><cell>OOD</cell></row><row><cell cols="2">Task 13 Fill in the blank</cell><cell>NumGLUE_Task4.json</cell><cell>Numersense_structured.json</cell></row><row><cell></cell><cell></cell><cell>amps_number_theory.json</cell><cell>svamp_structured.json</cell></row><row><cell></cell><cell></cell><cell>amps_counting_and_stats.json</cell><cell>mathqa_geometry.json</cell></row><row><cell></cell><cell></cell><cell>amps_linear_algebra.json</cell><cell>amps_calculus.json</cell></row><row><cell></cell><cell></cell><cell>amps_algebra.json</cell><cell>singleq.json</cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_calculus.json</cell><cell>NumGLUE_Task2.json</cell></row><row><cell></cell><cell></cell><cell>addsub.json</cell><cell>mbpp_structured.json</cell></row><row><cell></cell><cell></cell><cell>singleop.json</cell><cell>deepmind_mathematics_numbertheory.json</cell></row><row><cell></cell><cell></cell><cell>multiarith.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>asdiv.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>GSM8k_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>APPS_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mathqa_gain.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mathqa_other.json</cell><cell></cell></row><row><cell cols="2">Task 14 Generative QA</cell><cell>simuleq.json NumGLUE_Task8.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>draw_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>dolphin_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mathqa_probability.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>MCTaco_frequency_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task1.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mathqa_general.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mathqa_physics.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>conala_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>amps_geometry.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>MATH_crowdsourced.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_calculus.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_muldiv.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_algebra.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>deepmind_mathematics_basicmath.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>NumGLUE_Task3.json</cell><cell>MCTaco_event_typical_time_structured.json</cell></row><row><cell cols="2">Task 15 MCQ</cell><cell>MCTaco_stationarity_structured.json MCTaco_event_ordering_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell>MCTaco_event_duration_structured.json</cell><cell></cell></row><row><cell cols="2">Task 16 NLI</cell><cell>NumGLUE_Task5.json</cell><cell></cell></row><row><cell cols="2">Task 17 RC</cell><cell>mathqa_physics.json</cell><cell>mbpp_structured.json</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Raw datasets used to create different tasks in L?la across different format categories.</figDesc><table><row><cell>ID</cell><cell>Knowledge</cell><cell>cate-</cell><cell>IID</cell><cell>OOD</cell></row><row><cell></cell><cell>gory</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>addsub.json</cell><cell>NumGLUE_Task4.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>singleop.json</cell><cell>GSM8k_structured.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>multiarith.json</cell><cell>svamp_structured.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>asdiv.json</cell><cell>NumGLUE_Task7.json</cell></row><row><cell cols="3">Task 18 No external knowledge</cell><cell>simuleq.json NumGLUE_Task8.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>draw_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>dolphin_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>NumGLUE_Task5.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>deepmind_mathematics_muldiv.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Numersense_structured.json</cell><cell>NumGLUE_Task1.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MCTaco_frequency_structured.json</cell><cell>MCTaco_event_ordering_structured.json</cell></row><row><cell cols="2">Task 19 Commonsense</cell><cell></cell><cell>NumGLUE_Task3.json MCTaco_stationarity_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MCTaco_event_duration_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MCTaco_event_typical_time_structured.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>amps_number_theory.json</cell><cell>amps_counting_and_stats.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>amps_linear_algebra.json</cell><cell>mathqa_general.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>amps_algebra.json</cell><cell>amps_calculus.json</cell></row><row><cell></cell><cell></cell><cell></cell><cell>deepmind_mathematics_calculus.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>mathqa_probability.json</cell><cell></cell></row><row><cell cols="2">Task 20 Math formulas</cell><cell></cell><cell>singleq.json mathqa_gain.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>mathqa_other.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>deepmind_mathematics_algebra.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>deepmind_mathematics_basicmath.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>deepmind_mathematics_calculus.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>deepmind_mathematics_numbertheory.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>amps_geometry.json</cell><cell></cell></row><row><cell cols="2">Task 21 Science formulas</cell><cell></cell><cell>NumGLUE_Task2.json</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>mathqa_physics.json</cell><cell></cell></row><row><cell>Task 22</cell><cell cols="2">Computer science knowledge</cell><cell>APPS_structured.json conala_structured.json</cell><cell>mathqa_geometry.json</cell></row><row><cell cols="3">Task 23 Real-world knowledge</cell><cell>MATH_crowdsourced.json</cell><cell>mbpp_structured.json</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Raw datasets used to create different tasks in L?la across different knowledge categories.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 16 :</head><label>16</label><figDesc>Example for each template provided to MTurk workers to produce L?la-Robust</figDesc><table><row><cell>ID</cell><cell>Category</cell><cell cols="6">Questions Unique questions Question length Programs Unique programs Program length</cell></row><row><cell>Task 1</cell><cell>Basic math</cell><cell>31,052</cell><cell>31,032</cell><cell>43.1</cell><cell>31,052</cell><cell>7,066</cell><cell>13.3</cell></row><row><cell>Task 2</cell><cell>Muldiv</cell><cell>16,021</cell><cell>15,936</cell><cell>26.9</cell><cell>16,021</cell><cell>15,279</cell><cell>8.2</cell></row><row><cell>Task 3</cell><cell>Number theory</cell><cell>44,760</cell><cell>44,183</cell><cell>41.3</cell><cell>269,232</cell><cell>261,865</cell><cell>33.2</cell></row><row><cell>Task 4</cell><cell>Algebra</cell><cell>15,882</cell><cell>15,615</cell><cell>19.3</cell><cell>16,364</cell><cell>15,986</cell><cell>12.7</cell></row><row><cell>Task 5</cell><cell>Geometry</cell><cell>3,190</cell><cell>3,149</cell><cell>36.1</cell><cell>3,190</cell><cell>3,035</cell><cell>28.7</cell></row><row><cell>Task 6</cell><cell>Counting and statistics</cell><cell>6,423</cell><cell>6,384</cell><cell>39.7</cell><cell>6,423</cell><cell>6,335</cell><cell>31.5</cell></row><row><cell>Task 7</cell><cell>Calculus</cell><cell>4,493</cell><cell>4,202</cell><cell>21.2</cell><cell>4,493</cell><cell>4,170</cell><cell>40.6</cell></row><row><cell>Task 8</cell><cell>Linear algebra</cell><cell>11,248</cell><cell>11,204</cell><cell>32.4</cell><cell>11,248</cell><cell>11,204</cell><cell>23.0</cell></row><row><cell>Task 9</cell><cell>Advanced math</cell><cell>746</cell><cell>746</cell><cell>21.2</cell><cell>746</cell><cell>745</cell><cell>27.3</cell></row><row><cell cols="2">Task 10 No language</cell><cell>41,191</cell><cell>40,551</cell><cell>21.2</cell><cell>42,466</cell><cell>41,794</cell><cell>40.6</cell></row><row><cell cols="2">Task 11 Simple language</cell><cell>66,505</cell><cell>66,172</cell><cell>26.9</cell><cell>290,184</cell><cell>258,839</cell><cell>8.2</cell></row><row><cell cols="2">Task 12 Complex language</cell><cell>26,119</cell><cell>25,728</cell><cell>36.1</cell><cell>26,119</cell><cell>25,052</cell><cell>28.7</cell></row><row><cell cols="2">Task 13 Fill in the blank</cell><cell>11,634</cell><cell>11,615</cell><cell>11.0</cell><cell>11,634</cell><cell>997</cell><cell>3.0</cell></row><row><cell cols="2">Task 14 Generative QA</cell><cell>102,493</cell><cell>101,239</cell><cell>14.7</cell><cell>327,447</cell><cell>314,652</cell><cell>16.0</cell></row><row><cell cols="2">Task 15 MCQ</cell><cell>9,989</cell><cell>9,989</cell><cell>28.3</cell><cell>9,989</cell><cell>470</cell><cell>3.0</cell></row><row><cell cols="2">Task 16 NLI</cell><cell>6,326</cell><cell>6,325</cell><cell>50.8</cell><cell>6,326</cell><cell>6,243</cell><cell>25.8</cell></row><row><cell cols="2">Task 17 RC</cell><cell>3,642</cell><cell>3,552</cell><cell>182.5</cell><cell>3,642</cell><cell>3,592</cell><cell>10.4</cell></row><row><cell cols="2">Task 18 No external knowledge</cell><cell>28,115</cell><cell>27,964</cell><cell>50.8</cell><cell>28,115</cell><cell>27,117</cell><cell>25.8</cell></row><row><cell cols="2">Task 19 Commonsense</cell><cell>24,677</cell><cell>24,658</cell><cell>30.9</cell><cell>24,677</cell><cell>823</cell><cell>3.0</cell></row><row><cell cols="2">Task 20 Math formulas</cell><cell>57,841</cell><cell>56,947</cell><cell>19.1</cell><cell>59,116</cell><cell>57,019</cell><cell>25.5</cell></row><row><cell cols="2">Task 21 Science formulas</cell><cell>10,505</cell><cell>10,319</cell><cell>36.1</cell><cell>10,505</cell><cell>9,764</cell><cell>28.7</cell></row><row><cell cols="2">Task 22 Complex knowledge</cell><cell>12,200</cell><cell>12,086</cell><cell>14.5</cell><cell>235,879</cell><cell>230,486</cell><cell>24.2</cell></row><row><cell cols="2">Task 23 Real-world knowledge</cell><cell>746</cell><cell>746</cell><cell>21.2</cell><cell>746</cell><cell>745</cell><cell>27.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>Main statistics of L?la across the total of 23 tasks.</figDesc><table><row><cell>31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Named after L?lavati, a 12 th century mathematical treatise on arithmetic that covers topics like arithmetic and geometric progressions, indeterminate equations and combinations. It is also widely known for the extensive number of math word problems. The author, Bh?skara is known for fundamental and original contributions to calculus, physics, number theory, algebra, and astronomy<ref type="bibr" target="#b9">(Colebrooke, 1817;</ref> Sarkar, 1918; Kolachana et al., 2019)   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that it is not a partition of the benchmark as each dimensions divides the constituent examples in different ways</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">text-davinci-002, code-davinci-002 5 Henceforth we refer to the max example model unless otherwise specified. 6 This is a soft version of exact match accuracy assigning partial credit when common words are present in the output and gold answer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that the training set for Codex is not known.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Dataset</head><p>References  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdsourcing for language resource development: Critical analysis of amazon mechanical turk overpowering use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kar?n</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Language and Technology Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11038</idno>
		<title level="m">Muppet: Massive multi-task representations with pre-finetuning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13319</idno>
		<title level="m">Mathqa: Towards interpretable math word problem solving with operation-based formalisms</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<title level="m">Program synthesis with large language models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04108</idno>
		<title level="m">Adversarial filters of dataset biases</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2021. Evaluating large language models trained on code</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Arithmetic and mensuration of brahmegupta and bhaskara</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henry T Colebrooke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Orb: An open reading benchmark for comprehensive evaluation of machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Gottumukkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12598</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Amazon mechanical turk: Gold mine or coal mine? Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kar?n</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Bretonnel</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proof artifact co-training for theorem proving with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Rute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<idno>abs/2102.06203</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09938</idno>
		<title level="m">Dawn Song, et al. 2021a. Measuring coding challenge competence with apps</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<title level="m">Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Dziedzic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2744" to="2751" />
		</imprint>
	</monogr>
	<note>Rishabh Krishnan, and Dawn Song</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Oren Etzioni, and Nate Kushman</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Draw: A challenging and diverse algebra word problem set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from explicit and implicit supervision jointly for algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjana</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arut</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atharva</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07705</idno>
		<title level="m">Benchmarking generalization via in-context instructions on 1,600+ language tasks</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Naturalproofs: Mathematical theorem proving in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards aicomplete question answering: A set of prerequisite toy tasks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">{INT}: An inequality benchmark for evaluating generalization in theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Baker Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to mine aligned code and natural language pairs from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196398.3196408</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mining Software Repositories, MSR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Do language embeddings capture scales?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00110</idno>
		<title level="m">Minif2f: a crosssystem benchmark for formal olympiad-level mathematics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">going on a vacation&quot; takes longer than &quot;going for a walk&quot;: A study of temporal commonsense understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3363" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
