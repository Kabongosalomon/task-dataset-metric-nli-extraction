<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lessons on Parameter Sharing across Layers in Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<email>sho.takase@nlp.c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
							<email>shun.kiyono@riken.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lessons on Parameter Sharing across Layers in Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel parameter sharing method for Transformers <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>. The proposed approach relaxes a widely used technique, which shares the parameters of one layer with all layers such as Universal Transformers <ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref>, to improve the efficiency. We propose three strategies: SEQUENCE, CYCLE, and CYCLE (REV) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in terms of the parameter size and computational time in the machine translation task. We also demonstrate that the proposed strategies are effective in the configuration where we use many training data such as the recent WMT competition. Moreover, we indicate that the proposed strategies are more efficient than the previous approach <ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref> in terms of the parameter sizes on automatic speech recognition and language modeling tasks. Our code is publicly available at https://github.com/takase/share_layer_params.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based methods have achieved notable performance in various NLP tasks <ref type="bibr" target="#b26">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>. In particular, <ref type="bibr">Brown et al. (2020)</ref> indicated that the larger parameter size we prepare, the better performance the model achieves. However, the model which is composed of many parameters occupies a large part of a GPU memory capacity. Thus, it is important to explore a parameter efficient way, which achieves better performance than a basic model with the same parameter size.</p><p>Parameter sharing is a widely used technique as a parameter efficient way <ref type="bibr" target="#b4">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b3">Dabre and Fujita, 2019;</ref>. <ref type="bibr" target="#b4">Dehghani et al. (2019)</ref> proposed Universal Transformer which consists of parameters for only one layer of a Transformer-based encoder-decoder, and uses these parameters N times for an N -layered encoder-decoder. <ref type="bibr" target="#b3">Dabre and Fujita (2019)</ref> and  also used such parameter sharing across layers for their Transformers. <ref type="bibr" target="#b4">Dehghani et al. (2019)</ref> reported that Universal Transformer achieved better performance than the vanilla Transformer in machine translation if the parameter sizes of both models are (almost) equal to each other. However, when we prepare the same number of parameters for Universal Transformer and vanilla Transformer, Universal Transformer requires much more computational time because weight matrices for each layer in Universal Transformer are much larger. For example, Universal Transformer requires twice as much training time as the vanilla Transformer in WMT English-to-German, which is a widely used machine translation dataset (see <ref type="table" target="#tab_1">Table 1</ref>).</p><p>In this paper, we propose a new parameter sharing method that is faster than using the same parameters for all layers such as Universal Transformers. Universal Transformers raise their expressiveness power by increasing the size of weight matrices for each layer. On the other hand, stacking (more) layers is another promising approach to raise expressiveness power of neural methods. Thus, the most straight-forward way to make Universal Transform-ers faster is stacking layers with smaller weight matrices for each layer. However, the approach using the same parameters for all layers limits the improvement of stacking layers <ref type="bibr" target="#b3">(Dabre and Fujita, 2019)</ref>. Therefore, instead of preparing parameters for only one layer, we prepare parameters for M layers to construct an N -layered encoder-decoder, where 1 ? M ? N . In other words, the proposed method relaxes the parameter sharing strategy in previous studies <ref type="bibr" target="#b4">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b3">Dabre and Fujita, 2019;</ref>. Because this relaxation addresses the above limitation of improvement by stacking layers, the proposed method can be fast by stacking layers with using small weight matrices for each layer. For the parameter assignment to each layer, we provide several strategies ( <ref type="figure" target="#fig_0">Figure 1</ref>) and compare them empirically.</p><p>We mainly conduct experiments on machine translation datasets. Experimental results show that the proposed method achieves comparable scores to the method assigning parameters of one layer to all layers with smaller computational time. In addition, we indicate that the proposed method slightly outperforms the previous parameter sharing method <ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref> when we spend almost the same training time. Moreover, we conduct experiments on automatic speech recognition (Section 5) and language modeling (Appendix A) tasks. Experimental results on these tasks also indicate that the proposed method are efficient in terms of the parameter size in other situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>As described in Section 1, we use parameters for M layers in the construction of an N -layered Transformer-based encoder-decoder. We provide three strategies for the parameter assignment: SE-QUENCE, CYCLE, and CYCLE (REV). We describe these strategies in this section. <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of three parameter assignment strategies for an encoder side when we set M = 3 and N = 6. Let enc i be the i-th layer of an encoder. <ref type="figure">Figure 2</ref> describes the algorithm to assign each parameter to each layer of the encoder. For the decoder side, we assign each parameter with the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SEQUENCE</head><p>The simplest strategy is to assign the same parameters to sequential N/M layers. We name this strategy SEQUENCE. For example, when we set </p><formula xml:id="formula_0">else if i ? (M ? ( N/M ? 1)) then 18: enc i ? enc ((i?1) mod M )+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>else 20:</p><formula xml:id="formula_1">enc i ? enc M ?((i?1) mod M )</formula><p>Figure 2: Proposed parameter assignment strategies for encoder construction. CreateNewLayer is a function that creates a new encoder layer. M = 3 and N = 6, two sequential layers share their parameters as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CYCLE</head><p>In CYCLE, we stack M layers whose parameters are independent from each other. Then, we repeat stacking the M layers with the identical order to the first M layers until the total number of layers reaches N . When we set M = 3 and N = 6, we stack 3 layers twice as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CYCLE (REV)</head><p>Liu et al. <ref type="bibr">(2020)</ref> reported that higher decoder layers obtain larger gradient norms when we use the post layer normalization setting, which is originally used in <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref> and widely used in machine translation. Their report implies that higher layers require more degrees of freedom than lower layers for their expressiveness. In other words, lower layers probably have redundant parameters compared to higher layers. Thus, we propose the CYCLE (REV) strategy reusing parameters of lower layers in higher layers. In this strategy, we repeat stacking M layers in the same manner as CYCLE until M * ( N/M ?1) layers. For the remaining layers, we stack M layers in the reverse order. When we set M = 3 and N = 6, we stack 3 layers and then stack the 3 layers in the reverse order as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Thus, the lowest layer and highest layer share their parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stable Training of Deep Transformers</head><p>In the proposed method, we stack many layers to raise expressiveness power of Transformers. Recent studies demonstrated that the training of a deep Transformer is often unstable <ref type="bibr" target="#b17">(Nguyen and Salazar, 2019;</ref><ref type="bibr" target="#b31">Xiong et al., 2020;</ref>. This section briefly describes layer normalizations (LNs) in Transformers because LNs are the important technique the stable training of deep Transformers.</p><p>Most of recent studies used the pre layer normalization setting (Pre-LN) when they stacked many layers <ref type="bibr" target="#b28">(Wang et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref> because Pre-LN makes the training process more stable than the post layer normalization setting (Post-LN) <ref type="bibr" target="#b17">(Nguyen and Salazar, 2019;</ref><ref type="bibr" target="#b31">Xiong et al., 2020)</ref>. However, Transformers with Post-LN achieve better performance if we succeed in training <ref type="bibr" target="#b17">(Nguyen and Salazar, 2019;</ref>. To stabilize the training process of Transformers with <ref type="bibr">Post-LN, Liu et al. (2020)</ref> proposed Admin that smooths the impact of each parameter in the early stage of training. In this study, we also use Admin to ensure the stable training of the proposed strategies in machine translation, and use Pre-LN in other experiments in accordance with baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Machine Translation</head><p>We investigate the efficiency of the proposed parameter sharing strategies. In this section, we conduct experiments on machine translation datasets. First, we focus on the English-to-German translation task because this task is widely used in the previous studies <ref type="bibr" target="#b26">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Dehghani et al., 2019;</ref><ref type="bibr" target="#b8">Kiyono et al., 2020)</ref>. We conduct comparisons based on following aspects: (i) comparison with previous methods in terms of efficiency, (ii) comparison among the proposed parameter sharing strategies, and (iii) comparison with models without restrictions on parameters to investigate the difference from the performance of the upper bound. In addition to the widely used training data, we conduct experiments on a large amount of training dataset in the English-to-German translation task. Then, we investigate if our findings can be applied to other language direction (i.e., German-to-English) and other language pair (i.e., English-to-French and French-to-English). We describe details in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We used the WMT 2016 training dataset, which is widely used in previous studies <ref type="bibr" target="#b26">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b24">Takase and Kiyono, 2021)</ref>. This dataset contains 4.5M English-German sentence pairs. Following previous studies, we constructed a vocabulary set with BPE <ref type="bibr" target="#b22">(Sennrich et al., 2016b)</ref> in the same manner. We set the number of BPE merge operations at 32K and shared the vocabulary between the source and target languages. We measured case-sensitive detokenized BLEU with SacreBLEU (Post, 2018) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Methods</head><p>For the proposed parameter assignment strategies, we fixed M = 6 and set N = 12, 18 based on the Transformer (base) setting in <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>. We compare the proposed strategies with the following baselines. Vanilla: This is the original Transformer (base) setting in <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>. Admin: We applied Admin  to the Transformer (base) setting. Universal: As the parameter sharing strategy in previous studies such as Universal Transformers <ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref>, we set M = 1 2 . In this setting, we increased the dimensions of each layer for a fair comparison in terms of the number of parameters. This configuration corresponds to the Universal Transformer base setting in Dehghani In addition, we prepare two models that consist of a large number of parameters for reference. Vanilla (big): This is the original Transformer (big) setting in <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>. Admin (deep): We stacked layers until N = 18 for the Transformer (base) setting, and applied Admin for the stable training. <ref type="table" target="#tab_1">Table 1</ref> shows BLEU scores on newstest2010-2016 for each method. We trained three models with different random seeds, and reported the averaged scores. <ref type="table" target="#tab_1">Table 1</ref> also shows the total number of parameters and computational speeds 3 . The computational speed is based on the speed of Vanilla.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>(i) Comparison with Universal in terms of efficiency In the comparison between Universal and Vanilla, Universal achieved better scores although their parameter sizes are almost the same. This result is consistent with the report in <ref type="bibr" target="#b4">Dehghani et al. (2019)</ref>. However, the training time of Universal is more than twice as much as the one of Vanilla. In addition, Universal (deep) didn't improve the performance from Universal although its negative loglikelihood on validation set slightly outperformed the one of Universal. Thus, stacking many layers have small effect on BLEU scores when the model shares parameters of one layer with all layers. <ref type="bibr">3</ref> We regard processed tokens per second during the training as the computational speed.  In contrast, the proposed strategies (SEQUENCE, CYCLE, and CYCLE (REV)) were faster and achieved slightly better scores than Universal when we set M = 6 and N = 12. Since Admin did not have a positive influence on BLEU scores as in <ref type="table" target="#tab_1">Table 1</ref> 4 , our strategies were responsible for the improvements. Thus, our proposed parameter sharing strategies are more efficient than Universal in terms of the parameter size and computational time.</p><p>In fact, when we used the same procedure as  <ref type="table">Table 2</ref>: BLEU scores on newstest2010-2016, 2018, and 2019. We add newstest2018 and 2019 to the set in the standard setting to compare the top system on WMT 2020 <ref type="bibr" target="#b8">(Kiyono et al., 2020)</ref>. <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>, the best model in <ref type="table">Table 2</ref> achieved 35.14 in the averaged BLEU score in new-stest2014. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the negative log-likelihood (NLL) values on newstest2013 for each training time. In this figure, we used M = 6 and N = 12 for our proposed strategies. This figure shows that the proposed strategies achieved better NLL values than Universal during the training. This result also indicates that the proposed strategies are more time efficient than Universal. Moreover, <ref type="figure" target="#fig_1">Figure 3</ref> shows that the proposed strategies outperformed Vanilla at the early phase of their training. Since Vanilla has converged, it would be hard for Vanilla to outperform the proposed strategies on NLL even if we spent the twice training time for Vanilla. Therefore, this figure indicates that our proposed parameter sharing strategies are efficient.</p><p>(ii) Comparison among the proposed parameter sharing strategies <ref type="table" target="#tab_1">Table 1</ref> shows that all proposed strategies achieved almost the same scores for M = 6 and N = 12. In contrast, the scores of SEQUENCE were lower than those of the other two strategies for M = 6 and N = 18. This result indicates that CYCLE and CYCLE (REV) are better strategies when we construct a deep Transformer with a small M , namely, saving parameter size. For M = 6 and N = 18, CYCLE (REV) improved by 0.41 from Universal in the averaged BLEU score even though their computational speeds were almost the same. Therefore, CYCLE and CYCLE (REV) are superior parameter efficient strategy.</p><p>(iii) Comparison with models without restrictions on parameters The lowest part of <ref type="table" target="#tab_1">Table  1</ref> indicates results when we prepared more parameters. We trained these models to investigate the performance of models without any restriction on parameters. In other words, the purpose of these settings are to investigate upper bounds of the performance. However, their scores were lower than scores of our proposed strategies. This result implies that our parameter sharing strategies are also better than the model without any restriction on parameters in terms of the BLEU score. In fact, previous studies on language modeling demonstrated that the parameter sharing achieved better performance <ref type="bibr" target="#b13">(Melis et al., 2018;</ref><ref type="bibr" target="#b14">Merity et al., 2018;</ref><ref type="bibr" target="#b25">Takase et al., 2018)</ref>. If we applied several techniques to improve the performance of a deep model <ref type="bibr" target="#b10">(Li et al., 2020)</ref> or a model consisting of many parameters <ref type="bibr" target="#b24">(Takase and Kiyono, 2021)</ref>, we might raise BLEU scores of the lowest part of Table 1. However, since our purpose is not to achieve the top score, we trained each model with the conventional training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">High Resource Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets</head><p>In the high resource setting, we constructed 44.2M translation sentence pairs as a training dataset with the procedures of <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref> which achieved the best result in the WMT 2020 news translation task. In addition, we augmented the training data by using the back-translation technique <ref type="bibr" target="#b21">(Sennrich et al., 2016a)</ref> in the same manner as <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref>. We obtained 284.3M pairs as synthetic training data. For evaluation, we add newstest2018 and 2019 to the set used in Section 4.1 to because <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref> used these two test sets. In the same as Section 4.1, we measured case-sensitive detokenized BLEU with SacreBLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Methods</head><p>We used the original Transformer (big) setting <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> as our baseline in using genuine training data. We call this setting Vanilla in this experiment. Moreover, we also prepared  Universal, which shares the parameters with all layers, namely, M = 1, N = 6. We increased the dimensions of each layer in Universal to make their parameter size almost the same as others. For the proposed strategies, we used M = 6 and N = 12.</p><p>In using both of the genuine and synthetic (backtranslated) datasets, we applied CYCLE (REV) to the BASE setting in <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref> because CYCLE (REV) achieved the best BLEU scores on most test sets in <ref type="table" target="#tab_1">Table 1</ref>. We also used M = 6 and N = 12 in this configuration. We compare the reported scores of the best model described in <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref>. Their model is composed of 9 layers (i.e., M = 9 and N = 9); thus, it contains considerably more parameters than ours. <ref type="table">Table 2</ref> shows BLEU scores of each method on each test set. Similar to the experiments in Section 4.1, we reported the averaged scores of three models trained with different random seeds. <ref type="table">Table 2</ref> also shows the total number of parameters 5 . <ref type="table">Table 2</ref> shows that the proposed strategies achieved better BLEU scores than Vanilla and Universal when we prepared almost the same number of parameters. This result indicates that the proposed strategies are also parameter efficient in the high resource setting. In addition, since we used M = 6 and N = 12 for proposed strategies, they are also more efficient than Universal in terms of computational time (see <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results</head><p>When we used additional synthetic data for training in the same manner as <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref>, CYCLE (REV) achieved comparable BLEU scores to the best system of <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref> except for newstest2019 6 even though the parameter size <ref type="bibr">5</ref> The parameter sizes of Vanilla (big) in <ref type="table" target="#tab_1">Table 1</ref> and Vanilla in <ref type="table">Table 2</ref> are different from each other due to the difference of sharing embeddings. Following <ref type="bibr" target="#b8">Kiyono et al. (2020)</ref>, we did not share embeddings in the high resource setting. <ref type="bibr">6</ref> For newstest2019, synthetic data might harm the quality of CYCLE (REV) was smaller than theirs. This result indicates that CYCLE (REV) is also efficient in the construction of models for recent competitive tasks. In addition, this result implies that our proposed strategies can be used in the configuration where we train many parameters with a tremendous amount of data such as recent pre-trained language models, e.g., GPT series <ref type="bibr">(Brown et al., 2020)</ref>. We investigate the effect of the proposed strategies on language models in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other Direction and Language Pair</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Datasets</head><p>We conduct experiments on the other direction and language pair. For the German-to-English training dataset, we used the identical data in Section 4.1.</p><p>For English-to-French and French-to-English, we used the WMT 2014 training dataset. We applied the same pre-processing as in , and used 35.8M English-French sentence pairs. Each configuration, we used newstest2013 and new-stest2014 as valid and test sets, respectively. We also measured case-sensitive detokenized BLEU with SacreBLEU in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Methods</head><p>We compare our proposed strategies with baselines used in Section 4.1. We used the Transformer (base) setting as Vanilla and prepared Universal which is M = 1, N = 6. For the proposed strategies, we used M = 6 and N = 18. In these configurations, the training time of proposed strategies are almost the same as one of Universal as described in <ref type="table" target="#tab_1">Table 1</ref>.  parameter sharing strategies (SEQUENCE, CYCLE, and CYCLE (REV)) achieved better scores than Universal in all datasets. These results are consistent with results in <ref type="table" target="#tab_1">Table 1</ref>. These results also indicate that the proposed strategies are more efficient than Universal, which shares parameters of one layer with all layers, because they achieved better performance with almost the same parameter size and computational time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results</head><p>In the comparison among the proposed strategies, CYCLE and CYCLE (REV) outperformed SE-QUENCE on German-to-English but it is difficult to conclude that CYCLE and CYCLE (REV) are superior to SEQUENCE on English-to-French and French-to-English. This result implies that the best strategy might depend on a language pair. However, it is suitable to use CYCLE or CYCLE (REV) as a first step because they were effective in construction of deep models on English-German and achieved comparable scores to SEQUENCE on English-French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Automatic Speech Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To investigate the effect of our proposed strategies on other modality, we conduct comparisons on the automatic speech recognition (ASR) task. We used the de-facto standard English ASR benchmark dataset: LibriSpeech <ref type="bibr" target="#b19">(Panayotov et al., 2015)</ref>. The dataset contains 1,000 hours of English speech from audiobooks. We used the standard splits of LibriSpeech; used all available training data for training and two configurations (clean and other) of development and test sets for evaluation. We applied the same pre-processing as in . We measured word error rate on each set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Methods</head><p>We also compare our proposed strategies with baselines in Section 4. As the base architecture, we used Transformer based speech-to-text model (T-Md) described in . In contrast to architectures in Section 4, the Transformer in T-Md consists of the pre layer normalization. We prepared 6 layers for the encoder and decoder in Vanilla and Universal. For proposed strategies, we stacked more layers for the encoder side in the same as in . We prepared N = 16 and M = 8 for the encoder side, and N = 8 and M = 4 for the decoder side. <ref type="table" target="#tab_7">Table 4</ref> shows word error rates of each method on each dataset. This table indicates that Universal outperformed Vanilla in all sets. The proposed parameter sharing strategies (SEQUENCE, CYCLE, and CYCLE (REV)) achieved better scores than Universal in all sets even though they are faster than Universal. These results are consistent with results in machine translation experiments in Section 4. Thus, the proposed strategies are also more efficient in the ASR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>In contrast to machine translation experiments, SEQUENCE outperformed CYCLE and CYCLE (REV) in the ASR task. We consider that this result is caused by the difference of layer normalization positions in the Transformer architecture. We used Post-LN based method (Admin)  in machine translation experiments, but Pre-LN based method in this ASR task.  demonstrated that the position of the layer normalization has a strong effect on the property of Transformers. The experimental results in language modeling (Appendix A) also imply that SEQUENCE is more appropriate when we use the Pre-LN based Transformer. The main focus of this study is empirical comparisons to the widely used parameter sharing strategy, Universal <ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref>, but we address theoretical analyses in the future to understand the relation between parameter sharing strategies and Transformer architectures.</p><p>In the past decade, various studies reported that a large amount of training data improve the performance in NLP tasks <ref type="bibr" target="#b23">(Suzuki and Isozaki, 2008;</ref><ref type="bibr" target="#b1">Brants et al., 2007;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013;</ref><ref type="bibr" target="#b21">Sennrich et al., 2016a;</ref>. Moreover, recent studies indicated that the larger parameter size we prepare, the better performance the model achieves when we have a large amount of training data <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>. In fact, the best system on the WMT 2020 news translation task is composed of about 10 times as many parameters as the widely used Transformer (base) setting <ref type="bibr" target="#b8">(Kiyono et al., 2020)</ref>. However, due to the limitation on a GPU memory capacity, we have to explore a parameter efficient way, which achieves better performance while saving the parameter size.</p><p>Parameter sharing is a widely used technique as a parameter efficient way <ref type="bibr" target="#b4">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b3">Dabre and Fujita, 2019;</ref><ref type="bibr" target="#b29">Xia et al., 2019;</ref>. <ref type="bibr" target="#b4">Dehghani et al. (2019)</ref> proposed Universal Transformer. Their method requires parameters for only one layer (i.e., M = 1) of a Transformerbased encoder-decoder, and shares these parameters with N layers. <ref type="bibr" target="#b3">Dabre and Fujita (2019)</ref> investigated the effectiveness of Transformer sharing parameters of one layer across all layers on various translation datasets.  used this parameter sharing strategy to construct a parameter efficient model. As reported in these studies, we can achieve better performance by the Transformer sharing parameters of one layer across all layers when we use the same parameter size as the original Transformer. However, this strategy requires much more computational time as described in <ref type="table" target="#tab_1">Table 1</ref> because weight matrices for each layer are much larger. To solve this problem, we propose a new parameter sharing strategies that prepare parameters for M layers and assign them into N layers, where 1 ? M ? N . Experimental results show that our proposed strategies are more efficient than the method sharing parameters of one layer with across layers <ref type="bibr" target="#b4">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b3">Dabre and Fujita, 2019;</ref>. <ref type="bibr" target="#b29">Xia et al. (2019)</ref> proposed an encoder-decoder which shares parameters of the encoder part and decoder part.  proposed the method to share the attention weights to make the computation of Transformers fast. These techniques are orthogonal to our proposed method. Thus, we can combine them to improve the efficiency of parame-ters and computational time.</p><p>In this study, we explore a parameter efficient method. On the other hand, recent studies proposed method to accelerate the training. <ref type="bibr" target="#b10">Li et al. (2020)</ref> proposed a training strategy for a deep Transformer. Their strategy trains a shallow model and then stacks layers to construct a deep model. They repeat this procedure until the desired deep model. They indicated that their strategy was faster than the training of whole parameters of a deep Transformer. <ref type="bibr" target="#b24">Takase and Kiyono (2021)</ref> compared regularization methods in terms of training time. Their experimental results show that the simple regularizations such as word dropout are more efficient than complex ones such as adversarial perturbations. We can use those findings to accelerate the training of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed three parameter sharing strategies: SEQUENCE, CYCLE, and CYCLE (REV), for the internal layers in Transformers. In contrast to the previous strategy, which prepares parameters for only one layer and shares them across layers such as Universal Transformers <ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref>, the proposed strategies prepare parameters for M layers to construct N layers. In the proposed strategies, we stack layers whose weight matrices are smaller than ones of Universal Transformers to raise expressiveness power with saving the increase of computational time.</p><p>Experimental results in the standard machine translation setting show that the proposed strategies achieved comparable BLEU scores to those of Universal with a small computational time when we prepared almost the same parameters for each method. In addition, the proposed strategies slightly outperformed Universal when we spent almost the same time to train them. Thus, the proposed strategies are efficient in terms of the parameter size and computational time. Through other experiments, we indicated that the proposed strategies are more efficient than Universal in the high resource setting, other language pairs, and another modality (speech-to-text).  <ref type="table">Table 5</ref>: The parameter sizes and perplexities of each method. The lower part indicates scores reported in <ref type="bibr" target="#b0">Baevski and Auli (2019)</ref> and the score of SEQUENCE with more parameters. Scores in bold denote the best results for each set. ? represents our re-run of <ref type="bibr" target="#b0">Baevski and Auli (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments on Language Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset</head><p>We focused Transformer-based encoder-decoders in experiments in previous sections. However, recent studies often employed the decoder side only as a pre-trained model. Thus, we conduct experiments on the language modeling task to investigate the efficiency of our proposed strategies when we use the decoder side only. We used Wikitext-103 <ref type="bibr" target="#b15">(Merity et al., 2017)</ref> which contains a large amount of training data. We measured perplexity of validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Methods</head><p>We used the Transformer with adaptive inputs <ref type="bibr" target="#b0">(Baevski and Auli, 2019)</ref> as the base architecture. In the same as in <ref type="bibr" target="#b0">Baevski and Auli (2019)</ref>, the Transformer in the language modeling consists of the pre layer normalization. We set N = 6 for Vanilla and Universal. For the proposed strategies, we set N = 12 and M = 6. <ref type="table">Table 5</ref> shows perplexities of each method. This table indicates that Vanilla achieved better performance than Universal. Thus, the sharing parameters of one layer with all layers might not be suitable for a large-scaled language modeling task. In contrast, the proposed strategies outperformed Vanilla. This result indicates that our proposed strategies are also more efficient than Universal in the language modeling task. Through the comparison among proposed strategies, SEQUENCE achieved the best perplexity. As described in Section 5, SEQUENCE might be more appropriate to the Transformer with the Pre-LN configuration. To explore the reason, we believe that we have to conduct the theoretical analysis of the Transformer during its training. We address this issue in the future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Results</head><p>The lower part of <ref type="table">Table 5</ref> shows the reported score of <ref type="bibr" target="#b0">Baevski and Auli (2019)</ref>, our reproduced score, and SEQUENCE with more parameters. This part indicates that SEQUENCE achieved better perplexities than others even though the parameter size of SEQUENCE is smaller. Therefore, SEQUENCE is also efficient when we prepare a large amount of parameters for a language model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of three parameter assignment strategies proposed in this study when we set M = 3 and N = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Negative log-likelihood (NLL) of each method on newstest2013. For our proposed parameter sharing strategies, we used M = 6 and N = 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm Encoder Construction Input: the total number of layers N , number of independent layers M , sharing strategy TYPE ? {SEQUENCE, CYCLE, CYCLE (REV)} Output: enc 1 , ..., enc N 1: for i in [1, ..., N ] do</figDesc><table><row><cell>2:</cell><cell>if i == 1 then</cell></row><row><cell>3:</cell><cell>enc i ? CreateNewLayer</cell></row><row><cell>4:</cell><cell>else if TYPE == SEQUENCE then</cell></row><row><cell>5:</cell><cell>if (i ? 1) mod N/M == 0 then</cell></row><row><cell>6:</cell><cell>enc i ? CreateNewLayer</cell></row><row><cell>7:</cell><cell>else</cell></row><row><cell>8:</cell><cell>enc i ? enc i?1</cell></row><row><cell>9:</cell><cell>else if TYPE == CYCLE then</cell></row><row><cell>10:</cell><cell>if i ? M then</cell></row><row><cell>11:</cell><cell>enc i ? CreateNewLayer</cell></row><row><cell>12:</cell><cell>else</cell></row><row><cell>13:</cell><cell>enc i ? enc ((i?1) mod M )+1</cell></row><row><cell>14:</cell><cell>else if TYPE == CYCLE (REV) then</cell></row><row><cell>15:</cell><cell>if i ? M then</cell></row><row><cell>16:</cell><cell>enc i ? CreateNewLayer</cell></row><row><cell>17:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>?1.00 24.16 22.01 22.33 26.13 27.13 29.83 34.41 26.57 Admin 6 6 61M ?0.97 24.14 21.93 22.25 26.14 27.05 29.59 34.23 26.48 Universal 1 6 63M ?0.48 24.37 22.33 22.70 26.40 27.65 30.24 34.60 26.90 Universal (deep) 1 12 63M ?0.25 24.42 22.30 22.61 26.52 27.76 29.75 34.01 26.77 SEQUENCE 6 12 61M ?0.63 24.65 22.32 22.83 26.98 27.88 30.27 34.99 27.13 CYCLE 6 12 61M ?0.63 24.51 22.43 22.69 26.61 27.91 30.37 34.77 The number of layers, number of parameters, computational speeds based on the original Transformer (Vanilla), BLEU scores on newstest2010-2016, and averaged scores when we trained each method on widely used WMT 2016 English-to-German training dataset. Scores in bold denote the best results for each set. The results of our proposed strategies are statistically significant (p &lt; 0.05) in comparison with Vanilla. The lowest part indicates results of methods consisting of a large number of parameters for reference. et al.(2019). Moreover, we prepared the model using twice as many layers to investigate the effect of stacking many layers in Universal Transformers.</figDesc><table><row><cell>Method</cell><cell cols="3">M N #Params Speed 2010</cell><cell>2011</cell><cell>2012</cell><cell>2013</cell><cell>2014</cell><cell>2015</cell><cell>2016 Average</cell></row><row><cell>Vanilla</cell><cell>6</cell><cell>6</cell><cell cols="6">61M 27.04</cell></row><row><cell>CYCLE (REV)</cell><cell>6</cell><cell>12</cell><cell cols="6">61M ?0.63 24.66 22.47 22.87 26.68 27.72 30.37 34.81</cell><cell>27.08</cell></row><row><cell>SEQUENCE</cell><cell>6</cell><cell>18</cell><cell cols="6">61M ?0.47 24.53 22.44 22.73 26.59 27.73 30.30 34.80</cell><cell>27.02</cell></row><row><cell>CYCLE</cell><cell>6</cell><cell>18</cell><cell cols="6">61M ?0.47 24.74 22.60 23.04 26.89 28.14 30.54 34.79</cell><cell>27.25</cell></row><row><cell>CYCLE (REV)</cell><cell>6</cell><cell>18</cell><cell cols="6">61M ?0.47 24.93 22.77 23.09 26.88 28.09 30.60 34.84</cell><cell>27.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Methods consisting of a large number of parameters for reference</cell></row><row><cell>Vanilla (big)</cell><cell>6</cell><cell>6</cell><cell cols="6">210M ?0.39 24.31 22.21 22.75 26.39 28.28 30.35 33.40</cell><cell>26.81</cell></row><row><cell>Admin (deep)</cell><cell cols="2">18 18</cell><cell cols="6">149M ?0.46 24.54 22.30 22.75 26.57 28.03 30.24 34.19</cell><cell>26.94</cell></row></table><note>We call this setting Universal (deep).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>24.09 24.51 28.51 31.40 33.52 39.08 47.11 42.80 Universal 249M 27.00 24.20 24.96 28.94 31.73 33.53 39.38 47.54 43.11 SEQUENCE 242M 27.31 24.24 24.86 29.15 31.90 33.84 39.93 48.15 43.12 CYCLE 242M 27.23 24.45 25.13 29.12 32.10 34.04 39.82 48.11 43.19 CYCLE (REV) 242M 27.37 24.46 25.14 29.16 32.06 33.98 40.28 48.34 43.43</figDesc><table><row><cell>Method</cell><cell cols="2">#Params 2010</cell><cell>2011</cell><cell>2012</cell><cell>2013</cell><cell>2014</cell><cell>2015</cell><cell>2016</cell><cell>2018</cell><cell>2019</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Genuine training data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla</cell><cell>242M</cell><cell cols="5">26.53 + Synthetic (back-translated) data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kiyono et al. (2020)</cell><cell>514M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>33.1</cell><cell>-</cell><cell>-</cell><cell>49.6</cell><cell>42.7</cell></row><row><cell>CYCLE (REV)</cell><cell>343M</cell><cell cols="9">28.29 24.99 25.98 30.01 33.54 34.93 41.37 49.55 42.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The number of layers and BLEU scores on each dataset. Each method is composed of almost the same number of parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows BLEU scores of each method on</cell></row><row><cell>each dataset. This table indicates that Universal</cell></row><row><cell>outperformed Vanilla in all datasets. The proposed</cell></row><row><cell>of a model because models trained with only genuine data</cell></row><row><cell>outperformed those trained with both data.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The parameter sizes, computational speeds based on the T-Md with 6 layers (Vanilla), and word error rates of each method. Scores in bold denote the best results for each set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The BLEU score computed by SacreBLEU is often lower than the score obtained by the procedure of<ref type="bibr" target="#b26">Vaswani et al. (2017)</ref> as reported in. In fact, when we used the same procedure as<ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>, the best model inTable 2achieved 35.14 in the averaged BLEU score in newstest2014. However, Post (2018) encouraged using SacreBLEU for the compatibility of WMT results.2  The original Universal Transformers<ref type="bibr" target="#b4">(Dehghani et al., 2019)</ref> use the sinusoidal positional encoding for each layer and adaptive computation time technique<ref type="bibr" target="#b7">(Graves, 2017)</ref> but we omitted them in this study to focus on the difference among parameter sharing strategies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"> reported that Admin improved the performance of Transformer (Vanilla inTable 1) but Admin did not have a positive effect in our configuration. In our configuration, Vanilla (and Admin) achieved better score than ones reported in. In fact, Vanilla achieved 28.78 in the averaged BLEU score in newstest2014 when we used the same procedure as<ref type="bibr" target="#b26">Vaswani et al. (2017)</ref> but reported 27.80. Thus, if we adopt hyper-parameters that make Transformer strong, Admin might not have a positive effect on BLEU scores.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33 (NeurIPS)</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent stacking of layers for compact neural machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6292" to="6299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tohoku-AIP-NTT at WMT 2020 news translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuto</forename><surname>Konno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation (WMT)</title>
		<meeting>the Fifth Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
	<note>Makoto Morishita, and</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ALBERT: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shallow-to-deep training for neural machine translation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<biblScope unit="page" from="995" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5747" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularizing and Optimizing LSTM Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Spoken Language Translation (IWSLT)</title>
		<meeting>the 16th International Conference on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Librispeech: An asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking perturbations in encoder-decoders for fast training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5767" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fairseq S2T: Fast speech-to-text modeling with fairseq</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing (AACL-IJCNLP)</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing (AACL-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tied transformers: Neural machine translation with shared encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5466" to="5473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sharing attention weights for fast transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongran</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5292" to="5298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
