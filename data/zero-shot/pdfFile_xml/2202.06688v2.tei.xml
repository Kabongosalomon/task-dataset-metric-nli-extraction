<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Transformer for Fast and Robust Point Cloud Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geometric Transformer for Fast and Robust Point Cloud Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods bypass the detection of repeatable keypoints which is difficult in low-overlap scenarios, showing great potential in registration. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it robust in low-overlap cases and invariant to rigid transformation. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to 100 times acceleration. Our method improves the inlier ratio by 17?30 percentage points and the registration recall by over 7 points on the challenging 3DLoMatch benchmark. Our code and models are available at https: //github.com/qinzheng93/GeoTransformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud registration is a fundamental task in graphics, vision and robotics. Given two partially overlapping 3D point clouds, the goal is to estimate a rigid transformation that aligns them. The problem has gained renewed interest recently thanks to the fast growing of 3D point representation learning and differentiable optimization.</p><p>The recent advances have been dominated by learningbased, correspondence-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>. A neural network is trained to extract point correspondences between two input point clouds, based on which an alignment transformation is calculated with a robust estimator, e.g., RANSAC. Most correspondence-based methods rely on keypoint detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. However, it is challenging to detect repeatable keypoints across two point clouds, especially when they have small overlapping area. This usually results in low inlier ratio in the putative correspondences.</p><p>Inspired by the recent advances in image matching <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref>, keypoint-free methods <ref type="bibr" target="#b40">[41]</ref> downsample the input point clouds into superpoints and then match them through examining whether their local neighborhood (patch) overlaps. Such superpoint (patch) matching is then propagated to individual points, yielding dense point correspondences. Consequently, the accuracy of dense point correspondences highly depends on that of superpoint matches. Superpoint matching is sparse and loose. The upside is that it reduces strict point matching into loose patch overlapping, thus relaxing the repeatability requirement. Meanwhile, patch overlapping is a more reliable and informative constraint than distance-based point matching for learning correspondence; consider that two spatially close points could be geodesically distant. On the other hand, superpoint matching calls for features capturing more global context.</p><p>To this end, Transformer <ref type="bibr" target="#b31">[32]</ref> has been adopted <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref> to encode contextual information in point cloud registration. However, vanilla transformer overlooks the geometric structure of the point clouds, which makes the learned features geometrically less discriminative and induces numerous outlier matches ( <ref type="figure" target="#fig_0">Fig. 1(top)</ref>). Although one can inject positional embeddings <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43]</ref>, the coordinate-based encod-arXiv:2202.06688v2 [cs.CV] 12 Mar 2022 ing is transformation-variant, which is problematic when registering point clouds given in arbitrary poses. We advocate that a point transformer for registration task should be learned with the geometric structure of the point clouds so as to extract transformation-invariant geometric features. We propose Geometric Transformer, or GeoTransformer for short, for 3D point clouds which encodes only distances of point pairs and angles in point triplets.</p><p>Given a superpoint, we learn a non-local representation through geometrically "pinpointing" it w.r.t. all other superpoints based on pair-wise distances and triplet-wise angles. Self-attention mechanism is utilized to weigh the importance of those anchoring superpoints. Since distances and angles are invariant to rigid transformation, GeoTransformer learns geometric structure of point clouds efficiently, leading to highly robust superpoint matching even in lowoverlap scenarios. <ref type="figure" target="#fig_0">Fig. 1</ref>(left) demonstrates that GeoTransformer significantly improves the inlier ratio of superpoint (patch) correspondences. For better convergence, we devise an overlap-aware circle loss to make GeoTransformer focus on superpoint pairs with higher patch overlap.</p><p>Benefitting from the high-quality superpoint matches, our method attains high-inlier-ratio dense point correspondences ( <ref type="figure" target="#fig_0">Fig. 1(right)</ref>) using an optimal transport layer <ref type="bibr" target="#b26">[27]</ref>, as well as highly robust and accurate registration without relying on RANSAC. Therefore, the registration part of our method runs extremely fast, e.g., 0.01s for two point clouds with 5K correspondences, 100 times faster than RANSAC. Extensive experiments on both indoor and outdoor benchmarks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref> demonstrate the efficacy of GeoTransformer. Our method improves the inlier ratio by 17?30 percentage points and the registration recall by over 7 points on the 3DLoMatch benchmark <ref type="bibr" target="#b15">[16]</ref>. Our main contributions are:</p><p>? A fast and accurate point cloud registration method which is both keypoint-free and RANSAC-free. ? A geometric transformer which learns transformationinvariant geometric representation of point clouds for robust superpoint matching. ? An overlap-aware circle loss which reweights the loss of each superpoint match according to the patch overlap ratio for better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Correspondence-based Methods. Our work follows the line of the correspondence-based methods <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b14">15]</ref>. They first extract correspondences between two point clouds and then recover the transformation with robust pose estimators, e.g., RANSAC. Thanks to the robust estimators, they achieve state-of-the-art performance in indoor and outdoor scene registration. These methods can be further categorized into two classes according to how they extract correspondences. The first class aims to detect more repeatable keypoints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> and learn more powerful descriptors for the keypoints <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33]</ref>. While the second class <ref type="bibr" target="#b40">[41]</ref> retrieves correspondences without keypoint detection by considering all possible matches. Our method follows the detection-free methods and improves the accuracy of correspondences by leveraging the geometric information. Direct Registration Methods. Recently, direct registration methods have emerged. They estimate the transformation with a neural network in an end-to-end manner. These methods can be further classified into two classes. The first class <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref> follows the idea of ICP <ref type="bibr" target="#b4">[5]</ref>, which iteratively establishes soft correspondences and computes the transformation with differentiable weighted SVD. The second class <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref> first extracts a global feature vector for each point cloud and regresses the transformation with the global feature vectors. Although direct registration methods have achieved promising results on single synthetic shapes, they could fail in large-scale scenes as stated in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Deep Robust Estimators. As traiditional robust estimators such as RANSAC suffer from slow convergence and instability in case of high outlier ratio, deep robust estimators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref> have been proposed as the alternatives for them. They usually contain a classification network to reject outliers and an estimation network to compute the transformation. Compared with traditional robust estimators, they achieve improvements in both accuracy and speed. However, they require training a specific network. In comparison, our method achieves fast and accurate registration with a parameter-free local-to-global registration scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given two point clouds P = {p i ? R 3 | i = 1, ..., N } and Q = {q i ? R 3 | i = 1, ..., M }, our goal is to estimate a rigid transformation T = {R, t} which aligns the two point clouds, with a 3D rotation R ? SO(3) and a 3D translation t ? R 3 . The transformation can be solved by:</p><formula xml:id="formula_0">min R,t (p * x i ,q * y i )?C * R ? p * xi + t ? q * yi 2 2 .<label>(1)</label></formula><p>Here C * is the set of ground-truth correspondences between P and Q. Since C * is unknown in reality, we need to first establish point correspondences between two point clouds and then estimate the alignment transformation. Our method adopts the hierarchical correspondence paradigm which finds correspondences in a coarse-to-fine manner. We adopt KPConv-FPN to simultaneously downsample the input point clouds and extract point-wise features (Sec. 3.1). The first and the last (coarsest) level downsampled points correspond to the dense points and the superpoints to be matched. A Superpoint Matching Module is used to extract superpoint correspondences whose neighboring local patches overlap with each other (Sec. 3.2). Based on that, a Point Matching Module then refines the superpoint correspondences to dense points (Sec. 3.3). At last, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Superpoint Sampling and Feature Extraction</head><p>We utilize the KPConv-FPN backbone <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> to extract multi-level features for the point clouds. A byproduct of the point feature learning is point downsampling. We work on downsampled points since point cloud registration can actually be pinned down by the correspondences of a much coarser subset of points. The original point clouds are usually too dense so that point-wise correspondences are redundant and sometimes too clustered to be useful.</p><p>The points correspond to the coarsest resolution, denoted byP andQ, are treated as superpoints to be matched. The associated learned features are denoted asF P ? R |P|?d and F Q ? R |Q|?d . The dense point correspondences are computed at 1/2 of the original resolution, i.e., the first level downsampled points denoted byP andQ. Their learned features are represented byF P ? R |P|?d andF Q ? R |Q|?d .</p><p>For each superpoint, we construct a local patch of points around it using the point-to-node grouping strategy <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>. In particular, each point inP and its features fromF P are assigned to its nearest superpoint in the geometric space:</p><formula xml:id="formula_1">G P i = {p ?P | i = argmin j ( p ?p j 2 ),p j ?P}. (2)</formula><p>This essentially leads to a Voronoi decomposition of the input point cloud seeded by superpoints. The feature matrix associated with the points in G P i is denoted as F P i ?F P . The superpoints with an empty patch are removed. The patches {G Q i } and the feature matrices {F Q i } for Q are computed and denoted in a similar way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Superpoint Matching Module</head><p>Geometric Transformer. Global context has proven critical in many computer vision tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>. For this reason, transformer has been adopted to leverage global contextual information for point cloud registration. However, existing methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref> usually feed transformer with only high-level point cloud features and does not explicitly encode the geometric structure. This makes the learned features geometrically less discriminative, which causes severe matching ambiguity and numerous outlier matches, especially in low-overlap cases. A straightforward recipe is to explicitly inject positional embeddings <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43]</ref> of 3D point coordinates. However, the resultant coordinate-based transformers are naturally transformation-variant, while registration requires transformation invariance since the input point clouds can be in arbitrary poses.</p><p>To this end, we propose Geometric Transformer which not only encodes high-level point features but also explicitly captures intra-point-cloud geometric structures and inter-point-cloud geometric consistency. GeoTransformer is composed of a geometric self-attention module for learning intra-point-cloud features and a feature-based crossattention module for modeling inter-point-cloud consistency. The two modules are interleaved for N t times to extract hybrid features? P and? Q for reliable superpoint matching (see <ref type="figure" target="#fig_1">Fig. 2</ref> (bottom left)). Geometric self-attention. We design a geometric selfattention to learn the global correlations in both feature and geometric spaces among the superpoints for each point cloud. In the following, we describe the computation forP and the same goes forQ. Given the input feature matrix X ? R |P|?dt , the output feature matrix Z ? R |P|?dt is the weighted sum of all projected input features:</p><formula xml:id="formula_2">z i = |P| j=1 a i,j (x j W V ),<label>(3)</label></formula><p>where the weight coefficient a i,j is computed by a row-wise softmax on the attention score e i,j , and e i,j is computed as:</p><formula xml:id="formula_3">e i,j = (x i W Q )(x j W K + r i,j W R ) T ? d t .<label>(4)</label></formula><p>Here, r i,j ? R dt is a geometric structure embedding to be described in the next. W Q , W K , W V , W R ? R dt?dt are the respective projection matrices for queries, keys, values and geometric structure embeddings. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the structure and the computation of geometric self-attention. We design a novel geometric structure embedding to encode the transformation-invariant geometric structure of the superpoints. The core idea is to leverage the distances and angles computed with the superpoints which are consistent across different point clouds of the same scene. Given two superpointsp i ,p j ?P, their geometric structure embedding consists of a pair-wise distance embedding and a tripletwise angular embedding, which will be described below.</p><p>(1) Pair-wise Distance Embedding. Given the distance ? i,j = p i ?p j 2 betweenp i andp j , the distance embedding r D i,j between them is computed by applying a sinusoidal function <ref type="bibr" target="#b31">[32]</ref> on ? i,j /? d . Here, ? d is a hyper-parameter used to tune the sensitivity on distance variations. Please refer to the Appx. A.1 for detailed computation.</p><p>(2) Triplet-wise Angular Embedding. We compute angular embedding with triplets of superpoints. We first select the k nearest neighbors K i ofp i . For eachp x ? K i , we compute the angle ? x i,j =?(? x,i , ? j,i ), where ? i,j :=p i ?p j . The triplet-wise angular embedding r A i,j,x is then computed with a sinusoidal function on ? x i,j /? a , with ? a controlling the sensitivity on angular variations.</p><p>Finally, the geometric structure embedding r i,j is computed by aggregating the pair-wise distance embedding and the triplet-wise angular embedding:</p><formula xml:id="formula_4">r i,j = r D i,j W D + max x r A i,j,x W A ,<label>(5)</label></formula><p>Sinusoidal Function <ref type="figure">Figure 4</ref>. An illustration of the distance-and-angle-based geometric structure encoding and its computation.</p><p>where W D , W A ? R dt?dt are the respective projection matrices for the two types of embeddings. We use max pooling here to improve the robustness to the varying nearest neighbors of a superpoint due to self-occlusion. <ref type="figure">Fig. 4</ref> illustrates the computation of geometric structure embedding. Feature-based cross-attention. Cross-attention is a typical module for point cloud registration task <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>, used to perform feature exchange between two input point clouds. Given the self-attention feature matrices X P , X Q forP,Q respectively, the cross-attention feature matrix Z P ofP is computed with the features ofQ:</p><formula xml:id="formula_5">z P i = |Q| j=1 a i,j (x Q j W V ).<label>(6)</label></formula><p>Similarly, a i,j is computed by a row-wise softmax on the cross-attention score e i,j , and e i,j is computed as the feature correlation between the X P and X Q :</p><formula xml:id="formula_6">e i,j = (x P i W Q )(x Q j W K ) T ? d t .<label>(7)</label></formula><p>The cross-attention features for Q are computed in the same way. While the geometric self-attention module encodes the transformation-invariant geometric structure for each individual point cloud, the feature-based cross-attention module can model the geometric consistency across the two point clouds. The resultant hybrid features are both invariant to transformation and robust for reasoning correspondence. Superpoint matching. To find the superpoint correspondences, we propose a matching scheme based on global feature correlation. We first normalize? P and? Q onto a unit hypersphere and compute a Gaussian correlation matrix</p><formula xml:id="formula_7">S?R |P|?|Q| with s i,j = exp(? ? P i ?? Q j 2 2</formula><p>). In practice, some patches of a point cloud are less geometrically discriminative and have numerous similar patches in the other point cloud. Besides our powerful hybrid features, we also perform a dual-normalization operation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> on S to further suppress ambiguous matches, leading toS with</p><formula xml:id="formula_8">s i,j = s i,j |Q| k=1 s i,k ? s i,j |P| k=1 s k,j .<label>(8)</label></formula><p>We found that this suppression can effectively eliminate wrong matches. Finally, we select the largest N c entries inS as the superpoint correspondences:</p><formula xml:id="formula_9">C = {(p xi ,q yi ) | (x i , y i ) ? topk x,y (s x,y )}.<label>(9)</label></formula><p>Due to the powerful geometric structure encoding of Geo-Transformer, our method is able to achieve accurate registration in low-overlap cases and with less point correspondences, and most notably, in a RANSAC-free manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Point Matching Module</head><p>Having obtained the superpoint correspondences, we extract point correspondences using a simple yet effective Point Matching Module. At point level, we use only local point features learned by the backbone. The rationale is that point level matching is mainly determined by the vicinities of the two points being matched, once the global ambiguity has been resolved by superpoint matching. This design choice improves the robustness.</p><p>For each superpoint correspondence? i = (p xi ,q yi ), an optimal transport layer <ref type="bibr" target="#b26">[27]</ref> is used to extract the local dense point correspondences between G P xi and G Q yi . Specifically, we first compute a cost matrix C i ? R ni?mi :</p><formula xml:id="formula_10">C i = F P xi (F Q yi ) T / d ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_11">n i = |G P xi |, m i = |G Q yi |.</formula><p>The cost matrix C i is then augmented intoC i by appending a new row and a new column as in <ref type="bibr" target="#b26">[27]</ref>, filled with a learnable dustbin parameter ?. We then utilize the Sinkhorn algorithm <ref type="bibr" target="#b27">[28]</ref> onC i to compute a soft assignment matrixZ i which is then recovered to Z i by dropping the last row and the last column. We use Z i as the confidence matrix of the candidate matches and extract point correspondences via mutual top-k selection, where a point match is selected if it is among the k largest entries of both the row and the column that it resides in:</p><formula xml:id="formula_12">C i ={(G P xi (x j ), G Q yi (y j ))| (x j , y j )?mutual topk x,y (z i x,y )}.<label>(11)</label></formula><p>The point correspondences computed from each superpoint match are then collected together to form the final global dense point correspondences:</p><formula xml:id="formula_13">C = Nc i=1 C i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">RANSAC-free Local-to-Global Registration</head><p>Previous methods generally rely on robust pose estimators to estimate the transformation since the putative correspondences are often predominated by outliers. Most robust estimators such as RANSAC suffer from slow convergence. Given the high inlier ratio of GeoTransformer, we are able to achieve robust registration without relying on robust estimators, which also greatly reduces computation cost.</p><p>We design a local-to-global registration (LGR) scheme. As a hypothesize-and-verify approach, LGR is comprised of a local phase of transformation candidates generation and a global phase for transformation selection. In the local phase, we solve for a transformation T i ={R i , t i } for each superpoint match using its local point correspondences:</p><formula xml:id="formula_14">R i , t i = min R,t (p x j ,q y j )?Ci w i j R ?p xj + t ?q yj 2 2 . (12)</formula><p>This can be solved in closed form using weighted SVD <ref type="bibr" target="#b4">[5]</ref>. The corresponding confidence score for each correspondence in Z i is used as the weight w i j . Benefitting from the high-quality correspondences, the transformations obtained in this phase are already very accurate. In the global phase, we select the transformation which admits the most inlier matches over the entire global point correspondences:</p><formula xml:id="formula_15">R, t = max Ri,ti (p x j ,q y j )?C R i ?p xj + t i ?q yj 2 2 &lt; ? a ,<label>(13)</label></formula><p>where ? is the Iverson bracket. ? a is the acceptance radius. We then iteratively re-estimate the transformation with the surviving inlier matches for N r times by solving Eq. <ref type="bibr" target="#b11">(12)</ref>. As shown in Sec. 4.1, our approach achieves comparable registration accuracy with RANSAC but reduces the computation time by more than 100 times. Moreover, unlike deep robust estimators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>, our method is parameterfree and no network training is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>The loss function L = L oc + L p is composed of an overlap-aware circle loss L oc for superpoint matching and a point matching loss L p for point matching.</p><p>Overlap-aware circle loss. Existing methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref> usually formulate superpoint matching as a multi-label classification problem and adopt a cross-entropy loss with dualsoftmax <ref type="bibr" target="#b28">[29]</ref> or optimal transport <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41]</ref>. Each superpoint is assigned (classified) to one or many of the other superpoints, where the ground truth is computed based on patch overlap and it is very likely that one patch could overlap with multiple patches. By analyzing the gradients from the cross-entropy loss, we find that the positive classes with high confidence scores are suppressed by positive gradients in the multi-label classification <ref type="bibr" target="#b0">1</ref> . This hinders the model from extracting reliable superpoint correspondences.</p><p>To address this issue, we opt to extract superpoint descriptors in a metric learning fashion. A straightforward solution is to adopt a circle loss <ref type="bibr" target="#b29">[30]</ref> similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. However, the circle loss overlooks the differences between the positive samples and weights them equally. As a result, it struggles in matching patches with relatively low overlap. For this reason, we design an overlap-aware circle loss to focus the model on those matches with high overlap. We select the patches in P which have at least one positive patch in Q to form a set of anchor patches, A. A pair of patches are positive if they share at least 10% overlap, and negative if they do not overlap. All other pairs are omitted. For each anchor patch G P i ? A, we denote the set of its positive patches in Q as ? i p , and the set of its negative patches as ? i n . The overlap-aware circle loss on P is then defined as:</p><formula xml:id="formula_16">L P oc = 1 |A| G P i ?A log[1+ G Q j ?? i p e ? j i ? i,j p (d j i ??p) ? G Q k ?? i n e ? i,k n (?n?d k i ) ],<label>(14)</label></formula><formula xml:id="formula_17">where d j i = ? P i ?? Q j 2 is the distance in the feature space, ? j i = (o j i ) 1 2</formula><p>and o j i represents the overlap ratio between G P i and G Q j . The positive and negative weights are computed for each sample individually with ? i,j p = ?(d j i ? ? p ) and</p><formula xml:id="formula_18">? i,k n = ?(? n ? d k i ).</formula><p>The margin hyper-parameters are set to ? p = 0.1 and ? n = 1.4. The overlap-aware circle loss reweights the loss values on ? i p based on the overlap ratio so that the patch pairs with higher overlap are given more importance. The same goes for the loss L Q oc on Q. And the overall loss is L oc = (L P oc + L Q oc )/2. Point matching loss. The ground-truth point correspondences are relatively sparse because they are available only for downsampled point clouds. We simply use a negative log-likelihood loss <ref type="bibr" target="#b26">[27]</ref> on the assignment matrixZ i of each superpoint correspondence. During training, we randomly sample N g ground-truth superpoint correspondences {? * i } instead of using the predicted ones. For each? * i , a set of ground-truth point correspondences M i is extracted with a matching radius ? . The sets of unmatched points in the two patches are denoted as I i and J i . The individual point matching loss for? * i is computed as:</p><formula xml:id="formula_19">L p,i = ? (x,y)?Mi logz i x,y ? x?Ii logz i x,mi+1 ? y?Ji logz i ni+1,y ,<label>(15)</label></formula><p>The final loss is computed by averaging the individual loss over all sampled superpoint matches:</p><formula xml:id="formula_20">L p = 1 Ng Ng i=1 L p,i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate GeoTransformer on indoor 3DMatch <ref type="bibr" target="#b41">[42]</ref> and 3DLoMatch <ref type="bibr" target="#b15">[16]</ref> benchmarks (Sec. 4.1) and outdoor KITTI odometry <ref type="bibr" target="#b13">[14]</ref> benchmark (Sec. 4.2). We introduce the implementation details in Appx. A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Indoor Benchmarks: 3DMatch &amp; 3DLoMatch</head><p>Dataset. 3DMatch <ref type="bibr" target="#b41">[42]</ref> contains 62 scenes among which 46 are used for training, 8 for validation and 8 for testing. We use the training data preprocessed by <ref type="bibr" target="#b15">[16]</ref> and evaluate on both 3DMatch and 3DLoMatch <ref type="bibr" target="#b15">[16]</ref> protocols. The point cloud pairs in 3DMatch have &gt; 30% overlap, while those in 3DLoMatch have low overlap of 10% ? 30%. Metrics. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, we evaluate the performance with three metrics: (1) Inlier Ratio (IR), the fraction of putative correspondences whose residuals are below a certain threshold (i.e., 0.1m) under the ground-truth transformation, (2) Feature Matching Recall (FMR), the fraction of point cloud pairs whose inlier ratio is above a certain threshold (i.e., 5%), and (3) Registration Recall (RR), the fraction of point cloud pairs whose transformation error is smaller than a certain threshold (i.e., RMSE &lt; 0.2m). Correspondence results. We first compare the correspondence results of our method with the recent state of the arts: PerfectMatch <ref type="bibr" target="#b14">[15]</ref>, FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b3">[4]</ref>, SpinNet <ref type="bibr" target="#b0">[1]</ref>, Predator <ref type="bibr" target="#b15">[16]</ref>, YOHO <ref type="bibr" target="#b32">[33]</ref> and CoFiNet <ref type="bibr" target="#b40">[41]</ref> in Tab. 1(top and middle). Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, we report the results with different numbers of correspondences. The details of the correspondence sampling schemes are given in Appx. A.3. For Feature Matching Recall, our method achieves improvements of at least 5 percentage points (pp) on 3DLo-Match, demonstrating its effectiveness in low-overlap cases. For Inlier Ratio, the improvements are even more prominent. It surpasses the baselines consistently by 7?33 pp on 3DMatch and 17?31 pp on 3DLoMatch. The gain is larger with less correspondences. It implies that our method extracts more reliable correspondences. Registration results.  <ref type="bibr" target="#b3">[4]</ref> RANSAC-50k 5000 81.6 37.2 0.024 3.088 3.112 SpinNet <ref type="bibr" target="#b0">[1]</ref> RANSAC-50k 5000 88.6 59.8 60.248 0.388 60.636 Predator <ref type="bibr" target="#b15">[16]</ref> RANSAC-50k 5000 89.0 59.8 0.032 5.120 5.152 CoFiNet <ref type="bibr" target="#b40">[41]</ref> RANSAC-50k 5000 89.3 67.5 0.115 1.807 1.922 GeoTransformer (ours) RANSAC-50k 5000 92.0 75.0 0.075 1.558 1.633 FCGF <ref type="bibr" target="#b7">[8]</ref> weighted SVD 250 42.1 3.9 0.052 0.008 0.056 D3Feat <ref type="bibr" target="#b3">[4]</ref> weighted SVD 250 37.4 2.8 0.024 0.008 0.032 SpinNet <ref type="bibr" target="#b0">[1]</ref> weighted SVD 250 34.0 2.5 60.248 0.006 60.254 Predator <ref type="bibr" target="#b15">[16]</ref> weighted SVD 250 50.0 6.4 0.032 0.009 0.041 CoFiNet <ref type="bibr" target="#b40">[41]</ref> weighted SVD 250 64.6 21.6 0.115 0.003 0.118 GeoTransformer (ours) weighted SVD 250 86.5 59.9 0.075 0.003 0.078</p><p>CoFiNet <ref type="bibr" target="#b40">[41]</ref> LGR all 87.6 64.8 0.115 0.028 0.143 GeoTransformer (ours)</p><p>LGR all 91.5 74.0 0.075 0.013 0.088 its efficacy in both high-and low-overlap scenarios. More importantly, our method is quite stable under different numbers of samples, so it does not require sampling a large number of correspondences to boost the performance as previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>We then compare the registration results without using RANSAC in Tab. 2. We start with weighted SVD over correspondences in solving for alignment transformation. The baselines either fail to achieve reasonable results or suffer from severe performance degradation. In contrast, Geo-Transformer (with weighted SVD) achieves the registration recall of 86.5% on 3DMatch and 59.9% on 3DLoMatch, close to Predator with RANSAC. Without outlier filtering by RANSAC, high inlier ratio is necessary for successful registration. However, high inlier ratio does not necessarily lead to high registration recall since the correspondences could cluster together as noted in <ref type="bibr" target="#b15">[16]</ref>. Nevertheless, our method without RANSAC performs well by extracting reliable and well-distributed superpoint correspondences.</p><p>When using our local-to-global registration (LGR) for computing transformation, our method brings the registration recall to 91.5% on 3DMatch and 74.0% on 3DLo-Match, surpassing all RANSAC-based baselines by a large margin. The results are also very close to those of ours with RANSAC, but LGR gains over 100 times acceleration over RANSAC in the pose time. These results demonstrate the superiority of our method in both accuracy and speed. Ablation studies.</p><p>We conduct extensive ablation studies for a better understanding of the various modules in our method 2 . To evaluate superpoint (patch) matching, we introduce another metric Patch Inlier Ratio (PIR) which is the fraction of patch matches with actual overlap. The FMR and IR are reported with all dense point correspondences, with LGR being used for registration.</p><p>To study the effectiveness of the geometric self-attention, <ref type="bibr" target="#b1">2</ref> Due to space limit, we present some ablation studies in Appx. D.3.  we compare seven methods for intra-point-cloud feature learning in Tab. 3: (a) graph neural network <ref type="bibr" target="#b15">[16]</ref>, (b) self-attention with no positional embedding <ref type="bibr" target="#b40">[41]</ref>, (c) absolute coordinate embedding <ref type="bibr" target="#b26">[27]</ref>, (d) relative coordinate embedding <ref type="bibr" target="#b42">[43]</ref>, (e) point pair features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> embedding, (f) pair-wise distance embedding, (g) geometric structure embedding. Generally, injecting geometric information boosts the performance. But the gains of coordinate-based embeddings are limited due to their transformation variance. Surprisingly, GNN performs well on RR thanks to the transformation invariance of kNN graphs. However, it suffers from limited receptive fields which harms the IR performance. Although PPF embedding is theoretically invariant to transformation, it is hard to estimate accurate normals for the superpoints in practice, which leads to inferior performance. Our method outperforms the alternatives by a large margin on all the metrics, especially in the low-overlap scenarios, even with only the pair-wise distance embedding, demontrating the strong robustness of our method. Next, we ablate the overlap-aware circle loss in Tab. 4. We compare four loss functions for supervising the superpoint matching: (a) cross-entropy loss <ref type="bibr" target="#b26">[27]</ref>, (b) weighted cross-entropy loss <ref type="bibr" target="#b40">[41]</ref>, (c) circle loss <ref type="bibr" target="#b29">[30]</ref>, and (d) overlapaware circle loss. For the first two models, an optimal transport layer is used to compute the matching matrix as in <ref type="bibr" target="#b40">[41]</ref>. Circle loss works much better than the two variants of cross-entropy loss, verifying the effectiveness of supervising superpoint matching in a metric learning fashion. Our overlap-aware circle loss beats the vanilla circle loss by a large margin on all the metrics. Qualitative results. <ref type="figure">Fig. 5</ref> provides a gallery of the registration results of the models with vanilla self-attention and our geometric self-attention. Geometric self-attention helps infer patch matches in structure-less regions from their geometric relationships to more salient regions (1 st row) and reject outlier matches which are similar in the feature space but different in positions (2 nd and 3 rd rows). <ref type="figure" target="#fig_4">Fig. 6</ref> visualizes the attention scores learned by our   <ref type="figure">Figure 5</ref>. Registration results of the models with vanilla self-attention and geometric self-attention. In the columns (e) and (f), we visualize the features of the patches with t-SNE. In the first row, the geometric self-attention helps find the inlier matches on the structure-less wall based on their geometric relationships to the more salient regions (e.g., the chairs). In the following rows, the geometric self-attention helps reject the outlier matches between the similar flat or corner patches based on their geometric relationships to the bed or the sofa. geometric self-attention, which exhibits significant consistency between the anchor patch matches. It shows that our method is able to learn inter-point-cloud geometric consistency which is important to accurate correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Outdoor Benchmark: KITTI odometry</head><p>Dataset. KITTI odometry <ref type="bibr" target="#b13">[14]</ref> consists of 11 sequences of outdoor driving scenarios scanned by LiDAR. We follow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref> and use sequences 0-5 for training, 6-7 for validation and 8-10 for testing. As in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, the ground-truth poses are refined with ICP and we only use point cloud pairs that are at least 10m away for evaluation. Metrics. We follow <ref type="bibr" target="#b15">[16]</ref> to evaluate our GeoTransformer with three metrics: (1) Relative Rotation Error (RRE), the geodesic distance between estimated and ground-truth rotation matrices, (2) Relative Translation Error (RTE), the Euclidean distance between estimated and ground-truth translation vectors, and (3) Registration Recall (RR), the fraction of point cloud pairs whose RRE and RTE are both below certain thresholds (i.e., RRE&lt;5 ? and RTE&lt;2m).  Registration results. In Tab. 5(top), we compare to the state-of-the-art RANSAC-based methods: 3DFeat-Net <ref type="bibr" target="#b38">[39]</ref>, FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b3">[4]</ref>, SpinNet <ref type="bibr" target="#b0">[1]</ref>, Predator <ref type="bibr" target="#b15">[16]</ref> and CoFi-Net <ref type="bibr" target="#b40">[41]</ref>. Our method performs on par with these methods, showing good generality on outdoor scenes. We further compare to three RANSAC-free methods in Tab. 5(bottom): FMR <ref type="bibr" target="#b16">[17]</ref>, DGR <ref type="bibr" target="#b6">[7]</ref> and HRegNet <ref type="bibr" target="#b21">[22]</ref>. Our method outperforms all the baselines by large margin. In addition, our method with LGR beats all the RANSAC-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented Geometric Transformer to learn robust coarse-to-fine correspondence for point cloud registration. Through encoding pair-wise distances and triplet-wise angles among superpoints, our method captures the geometric consistency across point clouds with transformation invariance. Thanks to the reliable correspondences, it attains fast and accurate registration in a RANSAC-free manner. We discuss the limitations of our method in Appx. E. In the future, we would like to extend our method to crossmodality (e.g., 2D-3D) registration with richer applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Geometric Structure Embedding</head><p>First, we provide the detailed computation for our geometric structure embedding. The geometric structure embedding encodes distances in superpoint pairs and angles in superpoint triplets. Due to the continuity of the sinusoidal embedding function <ref type="bibr" target="#b31">[32]</ref>, we use it instead of learned embedding vectors to compute the pair-wise distance embedding and the triplet-wise angular embedding.</p><p>Given the distance ? i,j = p i ?p j 2 betweenp i andp j , the pair-wise distance embedding r D i,j is computed as:</p><formula xml:id="formula_21">? ? ? ? ? r D i,j,2k = sin( d i,j /? d 10000 2k/dt ) r D i,j,2k+1 = cos( d i,j /? d 10000 2k/dt ) ,<label>(16)</label></formula><p>where d t is the feature dimension, and ? d is a temperature which controls the sensitivity to distance variations. The triplet-wise angular embedding can be computed in the same way. Given the angle ? k i,j , the triplet-wise angular embedding r A i,j,k is computed as:</p><formula xml:id="formula_22">? ? ? ? ? ? ? r A i,j,k,2x = sin( ? k i,j /? a 10000 2x/dt ) r A i,j,k,2x+1 = cos( ? k i,j /? a 10000 2x/dt ) ,<label>(17)</label></formula><p>where ? a is another temperature to control the sensitivity to angular variantions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Point Matching Module</head><p>For completeness, we then provide the details of the optimal transport layer <ref type="bibr" target="#b26">[27]</ref> in the point matching module. For each superpoint correspondence (p xi ,q yi ), its local point correspondences are extracted from their local patches G P xi and G Q yi . We first compute a cost matrix C i ?R ni?mi using the feature matrices of the two patches:</p><formula xml:id="formula_23">C i = F P xi (F Q yi ) T / d ,<label>(18)</label></formula><p>where n i = |G P xi |, m i = |G Q yi |. The cost matrix C i is then augmented toC i ? R (ni+1)?(mi+1) by appending a new row and a new column filled with a learnable dustbin parameter ? as in <ref type="bibr" target="#b26">[27]</ref>. The point matching problem can then be formulated as an optimal transport problem which maximizes j,kC i ?Z i , whereZ i ? R (ni+1)?(mi+1) is the soft assignment matrix satisfying:</p><formula xml:id="formula_24">mi+1 k=1z i j,k = 1, 1 ? j ? n i m i , j = n i + 1 ,<label>(19)</label></formula><formula xml:id="formula_25">ni+1 j=1z i j,k = 1, 1 ? k ? m i n i , k = m i + 1 .<label>(20)</label></formula><p>HereZ i can be solved by the differentiable Sinkhorn algorithm <ref type="bibr" target="#b27">[28]</ref> with doubly-normalization iterations:</p><formula xml:id="formula_26">(t) u i j = log ? i j ? log mi+1 k=1 exp(c i j,k + (t?1) v i k ),<label>(21)</label></formula><formula xml:id="formula_27">(t) v i k = log ? i k ? log ni+1 j=1 exp(c i j,k + (t) u i j ),<label>(22)</label></formula><p>where</p><formula xml:id="formula_28">? i j = 1 ni+mi , 1 ? j ? n i mi ni+mi , j = n i + 1 ,<label>(23)</label></formula><formula xml:id="formula_29">? i k = 1 ni+mi , 1 ? k ? m i ni ni+mi , k = m i + 1 .<label>(24)</label></formula><p>The algorithm starts with (0) u = 0 ni+1 and (0) v = 0 mi+1 . The assignment matrixZ i is then computed as:</p><formula xml:id="formula_30">z i j,k = exp(c i j,k + u i j + v i k ) ? (n i +m i ).<label>(25)</label></formula><p>We run t 0 = 100 Sinkhorn iterations following <ref type="bibr" target="#b26">[27]</ref>.Z i is then recovered to Z i ? R ni?mi by dropping the last row and the last column, which is used as the confidence matrix of the candidate matches. The local point correspondences are extracted by mutual top-k selection on Z i . We ignore the matches whose confidence scores are too small (i.e., z i xj ,yj &lt;0.05). The hyper-parameter k controls the number of point correspondences as described in Appx. A.3. At last, the final global dense point correspondences are generated by combining the local point correspondences from all superpoint correspondences together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Network Configurations</head><p>Backbone. We use a KPConv-FPN backbone for feature extraction. The grid subsampling scheme <ref type="bibr" target="#b30">[31]</ref> is used to downsample the point clouds. Before being fed into the backbone, the input point clouds are first downsampled with a voxel size of 2.5cm on 3DMatch and 30cm on KITTI. The voxel size is then doubled in each downsampling operation. We use a 4-stage backbone for 3DMatch and a 5-stage backbone for KITTI because the point clouds in KITTI are much larger than those in 3DMatch. The configurations of KPConv are the same as in <ref type="bibr" target="#b15">[16]</ref>. And we use group normalization <ref type="bibr" target="#b35">[36]</ref> with 8 groups after the KPConv layers. The detailed network configurations are shown in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superpoint Matching Module.</head><p>At the beginning of the superpoint matching module, a linear projection is used to compress the feature dimension. For 3DMatch, the feature dimension is 256. For KITTI, we halve the feature dimension to 128 to reduce memory footprint. We then interleave <ref type="bibr" target="#b5">6</ref> -NearestUpsampling UnaryConv(3072 ? 1024)  the geometric self-attention module and the feature-based cross-attention module for N t = 3 times:</p><formula xml:id="formula_31">(1)FP self = GeometricSelfAtt(P,F P W in ),<label>(26)</label></formula><formula xml:id="formula_32">(1)FQ self = GeometricSelfAtt(Q,F Q W in ),<label>(27)</label></formula><formula xml:id="formula_33">(t)FP self = GeometricSelfAtt(P, (t?1)FP cross ),<label>(28)</label></formula><formula xml:id="formula_34">(t)FQ self = GeometricSelfAtt(Q, (t?1)FQ cross ),<label>(29)</label></formula><formula xml:id="formula_35">(t)FP cross = FeatureCrossAtt( (t)FP self , (t)FQ self ),<label>(30)</label></formula><formula xml:id="formula_36">(t)FQ cross = FeatureCrossAtt( (t)FQ self , (t)FP cross ).<label>(31)</label></formula><p>All attention modules have 4 attention heads. In the geometric structure embedding, we use ? d = 0.2m on 3DMatch and ? d = 4.8m on KITTI (i.e., the voxel size in the coarsest resolution level), while ? a = 15 ? on both datasets. The computation of the feature-based cross-attention forP is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Afterwards, we use another linear projection to project the features to 256-d, i.e., the final? P and? Q : Local-to-Global Registration. In the local-to-global registration, we only use the superpoint correspondences with at least 3 local point correspondences to compute the transformation candidates. To select the best transformation, the acceptance radius is ? a = 10cm on 3DMatch and ? a = 60cm on KITTI. At last, we iteratively recompute the transformation with the surviving inlier matches for N r = 5 times, which is similar with the post-refinement process in <ref type="bibr" target="#b2">[3]</ref>. However, we do not change the weights of the correspondences during the refinement. The impact of the number of iterations in the refinement is studied in Appx. D.3.</p><formula xml:id="formula_37">H P = (Nt)FP cross W out ,<label>(32)</label></formula><formula xml:id="formula_38">H Q = (Nt)FQ cross W out .<label>(33)</label></formula><p>Implementation details. We implement and evaluate our GeoTransformer with PyTorch [24] on a Xeon Glod 5218 CPU and an NVIDIA RTX 3090 GPU. The network is trained with Adam optimizer <ref type="bibr" target="#b17">[18]</ref> for 40 epochs on 3DMatch and 80 epochs on KITTI. The batch size is 1 and the weight decay is 10 ?6 . The learning rate starts from 10 ?4 and decays exponentially by 0.05 every epoch on 3DMatch and every 4 epochs on KITTI. We use the matching radius of ? = 5cm for 3DMatch and ? = 60cm for KITTI (i.e., the voxel size in the resolution level ofP and Q) to determine overlapping during the generation of both superpoint-level and point-level ground-truth matches. The same data augmentation as in <ref type="bibr" target="#b15">[16]</ref> is adopted. We randomly sample N g = 128 ground-truth superpoint matches during training, and use N c = 256 putative ones during testing.</p><p>Correspondences sampling strategy. For 3DMatch, we vary the hyper-parameter k in the mutual top-k selection of the point matching module to control the number of the point correspondences for GeoTransformer, i.e., k = 1 for 250/500/1000 matches, k = 2 for 2500 matches, and k = 3 for 5000 matches. And we use top-k selection to sample a certain number of the correspondences instead of random sampling as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>, which makes our correspondences deterministic. For the registration with LGR (Tab. 2(bottom) of our main paper), we use k = 3 to generate around 6000 correspondences for each point cloud pair. For the baselines, we report the results from their original papers or official models in Tab. 1 of our main paper. For the registration with weighted SVD (Tab. 2(middle) of our main paper), the correspondences of the baselines are extracted in the following manner: we first sample 5000 keypoints and generate the correspondences with mutual nearest neighbor selection in the feature space, and then the top 250 correspondences with the smallest feature distances are used to compute the transformation. The weights of the correspondences are computed as</p><formula xml:id="formula_39">w i = exp(? f P xi ?f Q yi 2</formula><p>2 ), where f P xi and f Q yi are the respective descriptors of the correspondences. In the sampling strategies that we have tried, this scheme achieves the best registration results.</p><p>For KITTI, we use k = 2 and select the top 5000 point correspondences following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. All other hyperparameters are the same as those in 3DMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics</head><p>Following common practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, we use different metrics for 3DMatch and KITTI. On 3DMatch, we report Inlier Ratio, Feature Matching Recall and Registration Recall. We also report Patch Inlier Ratio to evaluate the superpoint (patch) correspondences. On KITTI, we report Relative Rotation Error, Relative Translation Error and Registration Recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. 3DMatch/3DLoMatch</head><p>Inlier Ratio (IR) is the fraction of inlier matches among all putative point matches. A match is considered as an inlier if the distance between the two points is smaller than ? 1 = 10cm under the ground-truth transformationT P?Q :</p><formula xml:id="formula_40">IR = 1 |C| (p x i ,q y i )?C T P?Q (p xi ) ? q yi 2 &lt; ? 1 ,<label>(34)</label></formula><p>where ? is the Iversion bracket. Feature Matching Recall (FMR) is the fraction of point cloud pairs whose IR is above ? 2 = 0.05. FMR measures the potential success during the registration:</p><formula xml:id="formula_41">FMR = 1 M M i=1 IR i &gt; ? 2 ,<label>(35)</label></formula><p>where M is the number of all point cloud pairs. Registration Recall (RR) is the fraction of correctly registered point cloud pairs. Two point clouds are correctly registered if their transformation error is smaller than 0.2m. The transformation error is computed as the root mean square error of the ground-truth correspondences C * after applying the estimated transformation T P?Q :</p><formula xml:id="formula_42">RMSE = 1 |C * | (p * x i ,q * y i )?C * T P?Q (p * xi ) ? q * yi 2 2 ,<label>(36)</label></formula><formula xml:id="formula_43">RR = 1 M M i=1 RMSE i &lt; 0.2m .<label>(37)</label></formula><p>Patch Inlier Ratio (PIR) is the fraction of superpoint (patch) matches with actual overlap under the ground-truth transformation. It reflects the quality of the putative superpoint (patch) correspondences:</p><formula xml:id="formula_44">PIR = 1 |?| (p x i ,q y i )?? ?p ? G P xi ,q ? G Q yi s.t. p?q 2 &lt; ? ,<label>(38)</label></formula><p>where the matching radius is ? = 5cm as stated in A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. KITTI</head><p>Relative Rotation Error (RRE) is the geodesic distance in degrees between estimated and ground-truth rotation matrices. It measures the differences between the predicted and the ground-truth rotation matrices.</p><formula xml:id="formula_45">RRE = arccos trace(R T ?R ? 1) 2 .<label>(39)</label></formula><p>Relative Translation Error (RTE) is the Euclidean distance between estimated and ground-truth translation vectors. It measures the differences between the predicted and the ground-truth translation vectors.</p><formula xml:id="formula_46">RTE = t ?t 2 .<label>(40)</label></formula><p>Registration Recall (RR) on KITTI is defined as the fraction of the point cloud pairs whose RRE and RTE are both below certain thresholds (i.e., RRE &lt;5 ? and RTE&lt;2m).</p><formula xml:id="formula_47">RR = 1 M M i=1 RRE i &lt; 5 ? ? RTE i &lt; 2m .<label>(41)</label></formula><p>Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41]</ref>, we compute the mean RRE and the mean RTE only for the correctly registered point cloud pairs in KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Cross-Entropy Loss</head><p>In this section, we first give an analysis that adopting the cross-entropy loss in multi-label classification problem could suppress the classes with high confidence scores. Given the input vector y ? R n and the label vector g ? {0, 1} n , the confidence vector z is computed by adopting a softmax on y:</p><formula xml:id="formula_48">z i = exp(y i ) n j=1 exp(y j ) .<label>(42)</label></formula><p>The cross-entropy loss is computed as:</p><formula xml:id="formula_49">L = ? n i=1 g i log(z i ).<label>(43)</label></formula><p>And the gradient vector d of y is computed as:</p><formula xml:id="formula_50">d i = ?L ?y i = ( n j=1 g j )z i ? g i .<label>(44)</label></formula><p>The zero point of d i w.r.t. z i is c i = g i / n j=1 g j . If there are multiple positive classes, we have 0 &lt; c i &lt; 1 for each positive class as n j=1 g j &gt; 1. Hence y i will be increased if z i &lt; c i (d i &lt; 0), and be reduced if z i &gt; c i (d i &gt; 0). This indicates that the cross-entropy loss will suppress the positive classes with higher confidence scores during training. Now we go back to context of superpoint matching. To supervise the superpoint matching, CoFiNet <ref type="bibr" target="#b40">[41]</ref> adopts a cross-entropy loss with an optimal transport layer, which formulates the superpoint matching as a multi-class classification problem for each superpoint. The ground-truth superpoint correspondences are determined by whether their neighboring point patches overlap. In practice, one patch usually overlaps with multiple patches in the other point cloud, so superpoint matching is a multi-class classification problem. According to the analysis above, the positive matches with higher confidence scores will be suppressed by the cross-entropy loss, which hinders the model from extracting reliable superpoint correspondences. CoFi-Net <ref type="bibr" target="#b40">[41]</ref> further designs a reweighting method which gives better zero points for the gradients, but the problem cannot be solved completely. On the contrary, our overlap-aware circle loss supervises the superpoint matching in a metric learning manner, which avoids this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experiments</head><p>In this section, we conduct more experiments to evaluate our method. In Appx. D.1, we provide more detailed comparison on 3DMatch and 3DLoMatch. In Appx. D.2, we compare our method with recent deep robust estimators. In Appx. D.3, we conduct more ablation studies to better understand our design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Detailed Results on 3DMatch</head><p>Registration results with different overlaps. We first compare the performance of the models with vanilla selfattention and our geometric self-attention under different overlap ratios on 3DMatch and 3DLoMatch. As shown in Tab. 7, our method consistently outperforms the vanilla selfattention counterpart on all the metrics in all levels of overlap ratio. The gains are greater when the overlap ratio is below 30%, demonstrating our method is more robust in low-overlap scenarios.</p><p>Scene-wise registration results. We present the scenewise registration results on 3DMatch and 3DLoMatch in Tab. 9. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, we report mean median RRE and RTE for the successfully registered point cloud pairs. For the registration recall, our method outperforms the baselines in most scenes on 3DMatch, especially the hard scenes such as Home 2 and Lab. And it surpasses the baselines by a large margin in all scenes on 3DLoMatch. Moreover, our GeoTransformer also achieves consistently superior results  on the rotation and translation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Comparison with Deep Robust Estimators</head><p>We further compare with recent deep robust estimators: 3DRegNet <ref type="bibr" target="#b22">[23]</ref>, DGR <ref type="bibr" target="#b6">[7]</ref>, PointDSC <ref type="bibr" target="#b2">[3]</ref>, DHVR <ref type="bibr" target="#b18">[19]</ref> and PCAM <ref type="bibr" target="#b5">[6]</ref> on 3DMatch and KITTI. Following common practice, we report RTE, RRE and RR on both benchmarks. Here RR is defined as in Appx. B.2. The RTE threshold is 30cm on 3DMatch and 60cm on KITTI, while the RRE threshold is 15 ? on 3DMatch and 5 ? on KITTI. As shown in Tab. 8, our method outperforms all the baselines on both benchmarks. Although different correspondence extractors are used, these results can already demonstrate the superiority of GeoTransformer. It is noteworthy that our LGR is parameter-free and does not require training a specific network, which contributes to faster registration speed (0.013s vs. 0.08s <ref type="bibr" target="#b2">[3]</ref> in our experiments).   <ref type="table" target="#tab_0">Table 10</ref>. Ablation experiments with rotated superpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Additional Ablation Studies</head><p>Transformation invariance. We first evaluate the transformation invariance of different positional embeddings in Tab. 10. For each model, we randomly apply arbitrary rotations to the superpoints when computing the superpoint embeddings. Among all the variants, enlarged rotations severely degrade the performance of (a) self-attention with absolute coordinate embedding, which indicates the lack of transformation variance in it. Surprisingly, the performance of (b) self-attention with relative coordinate embedding is quite stable. However, after masking the relative coorinate embedding out, we find that the results of this model still remain the same, which means the relative coordinate embedding contributes little to the final performance during testing. In constrast, our (c) geometric self-attention shows strong invariance to rigid transformation.</p><p>Geometric structure embedding. Next, we study the design of geometric structure embedding. We first vary the number of nearest neighbors for computing the triplet-wise angular embedding. As shown in Tab. 11(top), increasing the neighbors slightly improves the registration recall, but also requires more computation. To better balance accuracy and speed, we select k = 3 in our experiments unless  <ref type="table" target="#tab_0">Table 11</ref>. Additional ablation experiments. otherwise noted. We then replace max pooling with average pooling when aggregating the triplet-wise angular embedding in Eq. (5) of our main paper. From Tab. 11(middle), the results of two pooling methods are very close and max pooling performs slightly better than average pooling. and speed, we choose 5 iterations in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Limitations</head><p>GeoTransformer relies on uniformly downsampled superpoints to hierarchically extract correspondences. However, there could be numerous superpoints if the input point clouds cover a large area, which will cause huge memory usage and computational cost. In this case, we might need to carefully select the downsampling rate to balance performance and efficiency.</p><p>Besides, it is inflexible to uniformly sample superpoints (patches). In practice, it is common that a single object is decomposed into multiple patches, which could be easily registered as a whole. So we think that it is a very promising topic to integrate point cloud registration with semantic scene understanding tasks (e.g., semantic segmentation and object detection), which converts scene registration into object registration. We will leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>We provide more qualitative results on 3DLoMatch in <ref type="figure">Fig. 9</ref>. The registration results from Predator <ref type="bibr" target="#b15">[16]</ref> and CoFi-Net <ref type="bibr" target="#b40">[41]</ref> are also shown for comparison. Here Predator and CoFiNet use RANSAC-50k for registration, while LGR is used in GeoTransformer. Our method performs quite well in these low-overlap cases. It is noteworthy that our method can distinguish similar objects at different positions (see the comparison of CoFiNet and GeoTransformer in the 4 th and 6 th rows) thanks to the transformation invariance obtained from the geometric self-attention. <ref type="figure" target="#fig_0">Fig. 10</ref> visualizes the registration results of GeoTransformer in the bird's-eye view on KITTI. It can be observed that our method attains very accurate registration even without using RANSAC.</p><p>We also provide some failure cases of our method on 3DLoMatch in <ref type="figure" target="#fig_0">Fig. 11</ref>. Generally, if the overlaping region between two point clouds is small and geometrically indiscriminative (e.g., wall, ceiling and floor) or the nonoverlapping region is relatively complicated, the registration could fail. A commonality of these cases is that they cannot provide adequate geometric cues to detect overlapping area and extract reliable correspondences. A possible solution could combine the information from multiple point clouds. We will leave this for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Given two low-overlap point clouds, GeoTransformer improves inlier ratio over vanilla transformer significantly, both for superpoint (patch) level (left) and for dense point level (right). A few representative patch correspondences are visualized with distinct colors. Notice how GeoTransformer preserves the spatial consistency of the matching patches across two point clouds. It corrects the wrongly matched patches around the symmetric corners of the chair back (see the yellow point cloud).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The backbone downsamples the input point clouds and learns features in multiple resolution levels. The Superpoint Matching Module extracts high-quality superpoint correspondences betweenP andQ using the Geometric Transformer which iteratively encodes intra-point-cloud geometric structures and inter-point-cloud geometric consistency. The superpoint correspondences are then propagated to dense pointsP andQ by the Point Matching Module. Finally, the transformation is computed with a local-to-global registration method.the alignment transformation is recovered from the dense correspondences without relying on RANSAC (Sec. 3.4). The pipeline is illustrated inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Left: The structure of geometric self-attention module. Right: The computation graph of geometric self-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>vanilla -patch correspondences (f) geometric -patch correspondences (g) vanilla -point correspondences (h) geometric -point correspondences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualizing geometric self-attention scores on four pairs of point clouds. The overlap areas are delineated with purple lines. The anchor patches (in correspondence) are highlighted in red and the attention scores to other patches are color-coded (deeper is larger). Note how the attention patterns of the two matching anchors are consistent even across disjoint overlap areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Left: The structure of feature-based cross-attention module. Right: The computation graph of cross-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>w/ dual-normalization 86.1 97.7 70.3 91.5 54.9 88.1 43.3 74.0 (i) w/o dual-normalization 86.2 97.7 70.3 91.0 53.5 87.9 42.8 73.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Ablation experiments on the pose refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>PerfectMatch<ref type="bibr" target="#b14">[15]</ref> 95.0 94.<ref type="bibr" target="#b2">3</ref> 92.9 90.1 82.9 63.6 61.7 53.6 45.2 34.2 FCGF [8] 97.4 97.3 97.0 96.7 96.6 76.6 75.4 74.2 71.7 67.3 D3Feat [4] 95.6 95.4 94.5 94.1 93.1 67.3 66.7 67.0 66.7 66.5 98.3 98.1 98.2 98.3 83.1 83.5 83.3 83.1 82.6 GeoTransformer (ours) 97.9 97.9 97.9 97.9 97.6 88.3 88.6 88.8 88.6 88.3 GeoTransformer (ours) 92.0 91.8 91.8 91.4 91.2 75.0 74.8 74.2 74.1 73.5 Evaluation results on 3DMatch and 3DLoMatch. The comparison with deep robust estimators is present in Appx. D.2.</figDesc><table><row><cell></cell><cell>3DMatch</cell><cell>3DLoMatch</cell></row><row><cell># Samples</cell><cell cols="2">5000 2500 1000 500 250 5000 2500 1000 500 250</cell></row><row><cell></cell><cell>Feature Matching Recall (%) ?</cell></row><row><cell>SpinNet [1]</cell><cell cols="2">97.6 97.2 96.8 95.5 94.3 75.3 74.9 72.5 70.0 63.6</cell></row><row><cell>Predator [16]</cell><cell cols="2">96.6 96.6 96.5 96.3 96.5 78.6 77.4 76.3 75.7 75.3</cell></row><row><cell>YOHO [33]</cell><cell cols="2">98.2 97.6 97.5 97.7 96.0 79.4 78.1 76.3 73.8 69.1</cell></row><row><cell>CoFiNet [41]</cell><cell>98.1 Inlier Ratio (%) ?</cell></row><row><cell>PerfectMatch [15]</cell><cell cols="2">36.0 32.5 26.4 21.5 16.4 11.4 10.1 8.0 6.4 4.8</cell></row><row><cell>FCGF [8]</cell><cell cols="2">56.8 54.1 48.7 42.5 34.1 21.4 20.0 17.2 14.8 11.6</cell></row><row><cell>D3Feat [4]</cell><cell cols="2">39.0 38.8 40.4 41.5 41.8 13.2 13.1 14.0 14.6 15.0</cell></row><row><cell>SpinNet [1]</cell><cell cols="2">47.5 44.7 39.4 33.9 27.6 20.5 19.0 16.3 13.8 11.1</cell></row><row><cell>Predator [16]</cell><cell cols="2">58.0 58.4 57.1 54.1 49.3 26.7 28.1 28.3 27.5 25.8</cell></row><row><cell>YOHO [33]</cell><cell cols="2">64.4 60.7 55.7 46.4 41.2 25.9 23.3 22.6 18.2 15.0</cell></row><row><cell>CoFiNet [41]</cell><cell cols="2">49.8 51.2 51.9 52.2 52.2 24.4 25.9 26.7 26.8 26.9</cell></row><row><cell cols="3">GeoTransformer (ours) 71.9 75.2 76.0 82.2 85.1 43.5 45.3 46.2 52.9 57.7</cell></row><row><cell></cell><cell>Registration Recall (%) ?</cell></row><row><cell>PerfectMatch [15]</cell><cell cols="2">78.4 76.2 71.4 67.6 50.8 33.0 29.0 23.3 17.0 11.0</cell></row><row><cell>FCGF [8]</cell><cell cols="2">85.1 84.7 83.3 81.6 71.4 40.1 41.7 38.2 35.4 26.8</cell></row><row><cell>D3Feat [4]</cell><cell cols="2">81.6 84.5 83.4 82.4 77.9 37.2 42.7 46.9 43.8 39.1</cell></row><row><cell>SpinNet [1]</cell><cell cols="2">88.6 86.6 85.5 83.5 70.2 59.8 54.9 48.3 39.8 26.8</cell></row><row><cell>Predator [16]</cell><cell cols="2">89.0 89.9 90.6 88.5 86.6 59.8 61.2 62.4 60.8 58.1</cell></row><row><cell>YOHO [33]</cell><cell cols="2">90.8 90.3 89.1 88.6 84.5 65.2 65.5 63.2 56.5 48.0</cell></row><row><cell>CoFiNet [41]</cell><cell cols="2">89.3 88.9 88.4 87.4 87.0 67.5 66.2 64.2 63.1 61.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>To evaluate the registration performance, we first compare the Registration Recall obtained by RANSAC in Tab. 1(bottom). Following [4, 16], we run 50K RANSAC iterations to estimate the transformation. GeoTransformer attains new state-of-the-art results on both 3DMatch and 3DLoMatch. It outperforms the previous best by 1.2 pp on 3DMatch and 7.5 pp on 3DLoMatch, showing</figDesc><table><row><cell>Model</cell><cell>Estimator #Samples</cell><cell>RR(%) 3DM 3DLM Model Pose Total Time(s)</cell></row><row><cell>FCGF [8]</cell><cell cols="2">RANSAC-50k 5000 85.1 40.1 0.052 3.326 3.378</cell></row><row><cell>D3Feat</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Registration results w/o RANSAC on 3DMatch (3DM) and 3DLoMatch (3DLM). The model time is the time for feature extraction, while the pose time is for transformation estimation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Model 3DMatch 3DLoMatch PIR FMR IR RR PIR FMR IR RR (a) graph neural network 73.3 97.9 56.5 89.5 39.4 84.9 29.2 69.8 (b) vanilla self-attention 79.6 97.9 60.1 89.0 45.2 85.6 32.6 68.4 (c) self-attention w/ ACE 83.2 98.1 68.5 89.3 48.2 84.3 38.9 69.3 (d) self-attention w/ RCE 80.0 97.9 66.1 88.5 46.1 84.6 37.9 68.7 (e) self-attention w/ PPF 83.5 97.5 68.5 88.6 49.8 83.8 39.9 69.5 (f) self-attention w/ RDE 84.9 98.0 69.1 90.7 50.6 85.8 40.3 72.1 (g) geometric self-attention 86.1 97.7 70.3 91.5 54.9 88.1 43.3 74.0 Ablation experiments of the geometric self-attention.</figDesc><table><row><cell>Model</cell><cell>3DMatch PIR FMR IR RR PIR FMR IR RR 3DLoMatch</cell></row><row><cell>(a) cross-entropy loss</cell><cell>80.0 97.7 65.7 90.0 45.9 85.1 37.4 68.4</cell></row><row><cell cols="2">(b) weighted cross-entropy loss 83.2 98.0 67.4 90.0 49.0 86.2 38.6 70.7</cell></row><row><cell>(c) circle loss</cell><cell>85.1 97.8 69.5 90.4 51.5 86.1 41.3 71.5</cell></row><row><cell>(d) overlap-aware circle loss</cell><cell>86.1 97.7 70.3 91.5 54.9 88.1 43.3 74.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation experiments of the overlap-aware circle loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Registration results on KITTI odometry. The comparison with deep robust estimators is present in Appx. D.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Network architecture for 3DMatch and KITTI.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Comparison of the models with the vanilla self-attention and the geometric self-attention under different overlap ratios. The results are reported on the union of 3DMatch and 3DLoMatch.</figDesc><table><row><cell>Overlap</cell><cell cols="3">Vanilla Self-attention PIR(%) IR(%) RR(%)</cell><cell cols="3">Geometric Self-attention PIR(%) IR(%) RR(%)</cell></row><row><cell>90%?100%</cell><cell>0.974</cell><cell>0.829</cell><cell>1.000</cell><cell>0.989</cell><cell>0.894</cell><cell>1.000</cell></row><row><cell>80%?90%</cell><cell>0.948</cell><cell>0.787</cell><cell>1.000</cell><cell>0.969</cell><cell>0.859</cell><cell>1.000</cell></row><row><cell>70%?80%</cell><cell>0.902</cell><cell>0.731</cell><cell>0.931</cell><cell>0.935</cell><cell>0.815</cell><cell>0.931</cell></row><row><cell>60%?70%</cell><cell>0.884</cell><cell>0.686</cell><cell>0.933</cell><cell>0.939</cell><cell>0.783</cell><cell>0.946</cell></row><row><cell>50%?60%</cell><cell>0.843</cell><cell>0.644</cell><cell>0.957</cell><cell>0.913</cell><cell>0.750</cell><cell>0.970</cell></row><row><cell>40%?50%</cell><cell>0.787</cell><cell>0.579</cell><cell>0.935</cell><cell>0.867</cell><cell>0.689</cell><cell>0.944</cell></row><row><cell>30%?40%</cell><cell>0.716</cell><cell>0.523</cell><cell>0.917</cell><cell>0.818</cell><cell>0.644</cell><cell>0.940</cell></row><row><cell>20%?30%</cell><cell>0.560</cell><cell>0.406</cell><cell>0.781</cell><cell>0.666</cell><cell>0.518</cell><cell>0.839</cell></row><row><cell>10%?20%</cell><cell>0.377</cell><cell>0.274</cell><cell>0.639</cell><cell>0.466</cell><cell>0.372</cell><cell>0.705</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell>RTE(cm)</cell><cell>RRE( ? )</cell><cell>RR(%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">3DMatch</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FCGF+3DRegNet [23]</cell><cell>8.13</cell><cell>2.74</cell><cell>77.8</cell><cell></cell></row><row><cell cols="2">FCGF+DGR [7]</cell><cell></cell><cell>7.36</cell><cell>2.33</cell><cell>86.5</cell><cell></cell></row><row><cell cols="2">FCGF+PointDSC [3]</cell><cell></cell><cell>6.55</cell><cell>2.06</cell><cell>93.3</cell><cell></cell></row><row><cell cols="2">FCGF+DHVR [19]</cell><cell></cell><cell>6.61</cell><cell>2.08</cell><cell>91.4</cell><cell></cell></row><row><cell cols="2">PCAM [6]</cell><cell></cell><cell>?7</cell><cell>2.16</cell><cell>92.4</cell><cell></cell></row><row><cell cols="3">GeoTransformer (ours, LGR)</cell><cell>5.69</cell><cell>1.98</cell><cell>95.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">3DLoMatch</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FCGF+PointDSC [3]</cell><cell></cell><cell>10.50</cell><cell>3.82</cell><cell>56.2</cell><cell></cell></row><row><cell cols="2">FCGF+DHVR [19]</cell><cell></cell><cell>11.76</cell><cell>3.88</cell><cell>55.6</cell><cell></cell></row><row><cell cols="3">GeoTransformer (ours, LGR)</cell><cell>8.55</cell><cell>2.98</cell><cell>77.5</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>KITTI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FCGF+DGR [7]</cell><cell></cell><cell>21.7</cell><cell>0.34</cell><cell>96.9</cell><cell></cell></row><row><cell cols="2">FCGF+PointDSC [3]</cell><cell></cell><cell>20.9</cell><cell>0.33</cell><cell>98.2</cell><cell></cell></row><row><cell cols="2">FCGF+DHVR [19]</cell><cell></cell><cell>19.8</cell><cell>0.29</cell><cell>99.1</cell><cell></cell></row><row><cell cols="2">PCAM [6]</cell><cell></cell><cell>?8</cell><cell>0.33</cell><cell>97.2</cell><cell></cell></row><row><cell cols="3">GeoTransformer (ours, LGR)</cell><cell>6.5</cell><cell>0.24</cell><cell>99.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Comparison with deep robust estimators on 3DMatch and KITTI. The RTE of PCAM is rounded to centimeter in the original paper<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Home 2 Hotel 1 Hotel 2 Hotel 3 Study Lab Mean Kitchen Home 1 Home 2 Hotel 1 Hotel 2 Hotel 3 Study Lab Mean</figDesc><table><row><cell>Model</cell><cell cols="8">3DMatch Kitchen Home 1 Registration Recall (%) ?</cell><cell></cell><cell></cell><cell cols="2">3DLoMatch</cell><cell></cell><cell></cell></row><row><cell>3DSN [15]</cell><cell>90.6</cell><cell>90.6</cell><cell>65.4</cell><cell>89.6</cell><cell>82.1</cell><cell>80.8</cell><cell>68.4 60.0 78.4</cell><cell>51.4</cell><cell>25.9</cell><cell>44.1</cell><cell>41.1</cell><cell>30.7</cell><cell>36.6</cell><cell>14.0 20.3 33.0</cell></row><row><cell>FCGF [8]</cell><cell>98.0</cell><cell>94.3</cell><cell>68.6</cell><cell>96.7</cell><cell>91.0</cell><cell>84.6</cell><cell>76.1 71.1 85.1</cell><cell>60.8</cell><cell>42.2</cell><cell>53.6</cell><cell>53.1</cell><cell>38.0</cell><cell>26.8</cell><cell>16.1 30.4 40.1</cell></row><row><cell>D3Feat [4]</cell><cell>96.0</cell><cell>86.8</cell><cell>67.3</cell><cell>90.7</cell><cell>88.5</cell><cell>80.8</cell><cell>78.2 64.4 81.6</cell><cell>49.7</cell><cell>37.2</cell><cell>47.3</cell><cell>47.8</cell><cell>36.5</cell><cell>31.7</cell><cell>15.7 31.9 37.2</cell></row><row><cell>Predator [16]</cell><cell>97.6</cell><cell>97.2</cell><cell>74.8</cell><cell>98.9</cell><cell>96.2</cell><cell>88.5</cell><cell>85.9 73.3 89.0</cell><cell>71.5</cell><cell>58.2</cell><cell>60.8</cell><cell>77.5</cell><cell>64.2</cell><cell>61.0</cell><cell>45.8 39.1 59.8</cell></row><row><cell>CoFiNet [41]</cell><cell>96.4</cell><cell>99.1</cell><cell>73.6</cell><cell>95.6</cell><cell>91.0</cell><cell>84.6</cell><cell>89.7 84.4 89.3</cell><cell>76.7</cell><cell>66.7</cell><cell>64.0</cell><cell>81.3</cell><cell>65.0</cell><cell>63.4</cell><cell>53.4 69.6 67.5</cell></row><row><cell cols="2">P2PNet (ours) 98.9</cell><cell>97.2</cell><cell>81.1</cell><cell>98.9</cell><cell>89.7</cell><cell>88.5</cell><cell>88.9 88.9 91.5</cell><cell>85.9</cell><cell>73.5</cell><cell>72.5</cell><cell>89.5</cell><cell>73.2</cell><cell>66.7</cell><cell>55.3 75.7 74.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relative Rotation Error ( ? ) ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN [15]</cell><cell>1.926</cell><cell>1.843</cell><cell>2.324</cell><cell>2.041</cell><cell>1.952</cell><cell cols="3">2.908 2.296 2.301 2.199 3.020</cell><cell>3.898</cell><cell>3.427</cell><cell>3.196</cell><cell>3.217</cell><cell cols="2">3.328 4.325 3.814 3.528</cell></row><row><cell>FCGF [8]</cell><cell>1.767</cell><cell>1.849</cell><cell>2.210</cell><cell>1.867</cell><cell>1.667</cell><cell cols="3">2.417 2.024 1.792 1.949 2.904</cell><cell>3.229</cell><cell>3.277</cell><cell>2.768</cell><cell>2.801</cell><cell cols="2">2.822 3.372 4.006 3.147</cell></row><row><cell>D3Feat [4]</cell><cell>2.016</cell><cell>2.029</cell><cell>2.425</cell><cell>1.990</cell><cell>1.967</cell><cell cols="3">2.400 2.346 2.115 2.161 3.226</cell><cell>3.492</cell><cell>3.373</cell><cell>3.330</cell><cell>3.165</cell><cell cols="2">2.972 3.708 3.619 3.361</cell></row><row><cell>Predator [16]</cell><cell>1.861</cell><cell>1.806</cell><cell>2.473</cell><cell>2.045</cell><cell>1.600</cell><cell cols="3">2.458 2.067 1.926 2.029 3.079</cell><cell>2.637</cell><cell>3.220</cell><cell>2.694</cell><cell>2.907</cell><cell cols="2">3.390 3.046 3.412 3.048</cell></row><row><cell>CoFiNet [41]</cell><cell>1.910</cell><cell>1.835</cell><cell>2.316</cell><cell>1.767</cell><cell>1.753</cell><cell cols="3">1.639 2.527 2.345 2.011 3.213</cell><cell>3.119</cell><cell>3.711</cell><cell>2.842</cell><cell>2.897</cell><cell cols="2">3.194 4.126 3.138 3.280</cell></row><row><cell cols="2">P2PNet (ours) 1.797</cell><cell>1.353</cell><cell>1.797</cell><cell>1.528</cell><cell>1.328</cell><cell cols="3">1.571 1.952 1.678 1.625 2.356</cell><cell>2.305</cell><cell>2.541</cell><cell>2.455</cell><cell>2.490</cell><cell cols="2">2.504 3.010 2.716 2.547</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relative Translation Error (m) ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DSN [15]</cell><cell>0.059</cell><cell>0.070</cell><cell>0.079</cell><cell>0.065</cell><cell>0.074</cell><cell cols="3">0.062 0.093 0.065 0.071 0.082</cell><cell>0.098</cell><cell>0.096</cell><cell>0.101</cell><cell>0.080</cell><cell cols="2">0.089 0.158 0.120 0.103</cell></row><row><cell>FCGF [8]</cell><cell>0.053</cell><cell>0.056</cell><cell>0.071</cell><cell>0.062</cell><cell>0.061</cell><cell cols="3">0.055 0.082 0.090 0.066 0.084</cell><cell>0.097</cell><cell>0.076</cell><cell>0.101</cell><cell>0.084</cell><cell cols="2">0.077 0.144 0.140 0.100</cell></row><row><cell>D3Feat [4]</cell><cell>0.055</cell><cell>0.065</cell><cell>0.080</cell><cell>0.064</cell><cell>0.078</cell><cell cols="3">0.049 0.083 0.064 0.067 0.088</cell><cell>0.101</cell><cell>0.086</cell><cell>0.099</cell><cell>0.092</cell><cell cols="2">0.075 0.146 0.135 0.103</cell></row><row><cell>Predator [16]</cell><cell>0.048</cell><cell>0.055</cell><cell>0.070</cell><cell>0.073</cell><cell>0.060</cell><cell cols="3">0.065 0.080 0.063 0.064 0.081</cell><cell>0.080</cell><cell>0.084</cell><cell>0.099</cell><cell>0.096</cell><cell cols="2">0.077 0.101 0.130 0.093</cell></row><row><cell>CoFiNet [41]</cell><cell>0.047</cell><cell>0.059</cell><cell>0.063</cell><cell>0.063</cell><cell>0.058</cell><cell cols="3">0.044 0.087 0.075 0.062 0.080</cell><cell>0.078</cell><cell>0.078</cell><cell>0.099</cell><cell>0.086</cell><cell cols="2">0.077 0.131 0.123 0.094</cell></row><row><cell cols="2">P2PNet (ours) 0.042</cell><cell>0.046</cell><cell>0.059</cell><cell>0.055</cell><cell>0.046</cell><cell cols="3">0.050 0.073 0.053 0.053 0.062</cell><cell>0.070</cell><cell>0.071</cell><cell>0.080</cell><cell>0.075</cell><cell cols="2">0.049 0.107 0.083 0.074</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 .</head><label>9</label><figDesc>Scene-wise registration results on 3DMatch and 3DLoMatch.</figDesc><table><row><cell>Model</cell><cell cols="2">3DMatch original rotated</cell><cell cols="2">3DLoMatch original rotated</cell></row><row><cell>(a) self-attention w/ ACE</cell><cell>89.3</cell><cell>87.2 -2.1</cell><cell>69.3</cell><cell>67.4 -1.9</cell></row><row><cell>(b) self-attention w/ RCE</cell><cell>88.5</cell><cell>88.5 same</cell><cell>68.7</cell><cell>68.7 same</cell></row><row><cell>(c) geometric self-attention</cell><cell>91.5</cell><cell>91.4 -0.1</cell><cell>74.0</cell><cell>73.8 -0.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The detailed analysis is presented in Appx. C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Dual-normalization. We then investigate the effectiveness of the dual-normalization operation in the superpoint matching module. As shown in Tab. 11(bottom), it slightly improves the accuracy of the superpoint correspondences in low-overlap scenarios. As there is less overlapping context when the overlapping area is small, it is much easier to extract outlier matches between the less geometrically discriminative patches. The dual-normalization operation can mitigate this issue and slightly improves the performance.Pose refinement. At last, we evaluate the impact of the pose refinement in LGR. As shown inFig. 8, the registration recall consistently improves with more iterations and gets saturated after about 5 iterations. To better balance accuracy</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported in part by the NSFC (62132021, 62102435) and the National Key R&amp;D Program of China (2018AAA0102200).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spinnet: Learning a general surface descriptor for 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11753" to="11762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnetlk: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7163" to="7172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointdsc: Robust point cloud registration using deep spatial consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">D3feat: Joint learning of dense detection and description of 3d local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pcam: Product of cross-attention matrices for rigid registration of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Quan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="602" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>scale. 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust point cloud registration framework based on deep graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexue</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8893" to="8902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predator: Registration of 3d point clouds with low overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Featuremetric registration: A fast semi-supervised approach for robust point cloud registration without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11366" to="11374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep hough voting for robust global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hregnet: A hierarchical network for large-scale outdoor lidar point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqing</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3dregnet: A deep neural network for 3d point registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Dias Pais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacinto</forename><forename type="middle">C</forename><surname>Venu Madhav Govindu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using 2 point+ normal sets for fast registration of point clouds with small overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5652" to="5658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Concerning nonnegative matrices and doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Knopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Loftr: Detector-free local feature matching with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="8922" to="8931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">You only hypothesize once: Point cloud registration with rotation-equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bisheng</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00182</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prnet: self-supervised learning for partial-to-partial registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8814" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Omnet: Learning overlapping mask for partialto-partial point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangfu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3132" to="3141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3dfeat-net: Weakly supervised local 3d features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="607" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11824" to="11833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cofinet: Reliable coarse-to-fine correspondences for robust point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14076</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3dmatch: Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Epipolar-guided pixel-level correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4669" to="4678" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
