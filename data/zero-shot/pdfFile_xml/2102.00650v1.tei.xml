<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 RETHINKING SOFT LABELS FOR KNOWLEDGE DISTIL- LATION: A BIAS-VARIANCE TRADEOFF PERSPECTIVE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helong</forename><surname>Zhou</surname></persName>
							<email>helong.zhou@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
							<email>lsong8@buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">University at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Chen</surname></persName>
							<email>jiajie.chen@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhou</surname></persName>
							<email>ye.zhou@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
							<email>guoli.wang@horizon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">University at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
							<email>qian01.zhang@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 RETHINKING SOFT LABELS FOR KNOWLEDGE DISTIL- LATION: A BIAS-VARIANCE TRADEOFF PERSPECTIVE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network. The outputs from the teacher network are used as soft labels for supervising the training of a new network. Recent studies <ref type="bibr" target="#b20">(M?ller et al., 2019;</ref><ref type="bibr" target="#b43">Yuan et al., 2020)</ref> revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network. From the perspective of statistical learning, regularization aims to reduce the variance, however how bias and variance change is not clear for training with soft labels. In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels. Specifically, we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing. Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance. Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise biasvariance tradeoff. Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available at https://github.com/ bellymonster/Weighted-Soft-Label-Distillation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Considering that soft labels are targets for distillation, the evidence of the regularization brought by soft labels drives us to rethink soft labels for KD: Soft labels are both supervisory signals and regularizers. Meanwhile, it is known that there is a tradeoff between fitting the data and imposing regularizations, i.e., the bias-variance dilemma <ref type="bibr" target="#b15">(Kohavi &amp; Wolpert, 1996;</ref><ref type="bibr" target="#b4">Bishop, 2006)</ref>, but it is unclear how bias and variance change for distillation with soft labels. Since the bias-variance tradeoff is an important issue in statistical learning, we investigate whether the bias-variance tradeoff exists for soft labels and how the tradeoff affects distillation performance.</p><p>We first compare the bias and variance decomposition of direct training with that of distillation with soft labels, noticing that distillation results in a larger bias error and a smaller variance. Then, we rewrite distillation loss into the form of a regularization loss adding the direct training loss. Through inspecting the gradients of the two terms during training, we notice that for soft labels, the biasvariance tradeoff varies sample-wisely. Moreover, by looking into a conclusion from <ref type="bibr" target="#b20">(M?ller et al., 2019)</ref>, we observe that under the same temperature setting, the distillation performance is negatively associated with the number of some certain samples. These samples lead to bias increase and variance decrease and we name them as regularization samples. To investigate how regularization samples affect distillation, we first examine if we can design ad hoc filters for soft labels to avoid training with regularization samples. But completely filtering out regularization samples also deteriorates distillation performance, leading us to speculate that regularization samples are not well handled by standard KD. In the light of these findings, we propose weighted soft labels for distillation to handle the sample-wise bias-variance tradeoff, by adaptively assigning a lower weight to regularization samples and a larger weight to the others. To sum up, our contributions are:</p><p>? For knowledge distillation, we analyze how the soft labels work from a perspective of biasvariance tradeoff. ? We discover that the bias-variance tradeoff varies sample-wisely. Also, we discover that if we fix the distillation temperature, the number of regularization samples is negatively associated with the distillation performance. ? We design straightforward schemes to alleviate negative impacts from regularization samples and then propose the novel weighted soft labels for distillation. Experiments on large scale datasets validate the effectiveness of the proposed weighted soft labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Knowledge distillation. <ref type="bibr" target="#b12">Hinton et al. (2015)</ref> proposed to distill outputs from large and cumbersome models into smaller and faster models, which is named as knowledge distillation. The outputs for large networks are averaged and formulated as soft labels. Also, other kinds of soft labels have been widely used for training deep neural networks <ref type="bibr" target="#b34">(Szegedy et al., 2016;</ref><ref type="bibr" target="#b28">Pereyra et al., 2017)</ref>. Treating soft labels as regularizers were pointed out in <ref type="bibr" target="#b12">(Hinton et al., 2015)</ref> since a lot of helpful information can be carried in soft labels. More recently, <ref type="bibr" target="#b20">M?ller et al. (2019)</ref> showed the adverse effect of label smoothing upon distillation. It is a thought-provoking discovery for the reason that both label smoothing and distillation are exploiting the regularization property behind soft labels. <ref type="bibr" target="#b43">Yuan et al. (2020)</ref> further investigated the regularization property of soft labels and then proposed a teacher free distillation scheme.</p><p>Distillation loss. One of our main contributions is that we improve the distillation loss. For adaptively adjusting the distillation loss, <ref type="bibr" target="#b35">Tang et al. (2019)</ref> pays attention to hard-to-learn and hard-tomimic samples, and the latter is weighted based on the prediction gap between teacher and student. However, it does not consider that the teacher may give an incorrect guide to the student, under which the prediction gap is still large and such a method may lead to the performance being hurt. Saputra et al. (2019) transfers teacher's guidance only on the samples where the performance of the teacher surpasses the student, while <ref type="bibr" target="#b39">Wen et al. (2019)</ref> deals with the incorrect guidance by probability shifting strategy. Our approach is different from the above methods, in terms of motivations as well as the proposed solutions.</p><p>Bias-variance tradeoff. Bias-variance tradeoff is a well-studied topic in machine learning <ref type="bibr" target="#b15">(Kohavi &amp; Wolpert, 1996;</ref><ref type="bibr" target="#b6">Domingos, 2000;</ref><ref type="bibr" target="#b38">Valentini &amp; Dietterich, 2004;</ref><ref type="bibr" target="#b4">Bishop, 2006)</ref> and for neural networks <ref type="bibr" target="#b7">(Geman et al., 1992;</ref><ref type="bibr" target="#b22">Neal et al., 2018;</ref><ref type="bibr" target="#b3">Belkin et al., 2019;</ref><ref type="bibr" target="#b41">Yang et al., 2020)</ref>. Existing methods are mainly concerned with the variance brought by the choice of network models. Our perspective is different from the previous methods since we focus on the behavior of samples during training. In our work, based on the results from <ref type="bibr" target="#b11">Heskes (1998)</ref>, we present the decomposition of distillation loss, which is defined by Kullback-Leibler divergence. Besides, our main contribution is not to study how to theoretically analyze the tradeoff, but how to adaptively tune the sample-wise tradeoff during training. Soft labels play the role of supervisory signals and regularizations at the same time, which inspires us to rethink soft labels from the perspective of the bias-variance tradeoff. We begin our analysis with some mathematical descriptions. For a sample x labeled as i-th class, let the ground-truth label be a one-hot vector y where y i = 1 and other entries are 0. Then for x and softmax output temperature ? , the soft label predicted by the teacher network is denoted as? t ? and the output from the student is denoted as? s ? . The soft label y t ? is then used for training the student by the distillation loss, i.e. L kd = ?? 2 k? t k,? log? s k,? , where? s k,? ,? t k,? means the k-th element of the student's output? s ? and the teacher's output? t ? , respectively. With the above notations, the cross-entropy loss for training with one-hot labels is L ce = ?y k log? s k,1 . We now present the bias-variance decomposition for L ce and L kd , based on the definition and notations from <ref type="bibr" target="#b11">Heskes (1998)</ref>. First, we denote the train dataset as D and the output distribution on a sample x of the network trained without distillation as? ce = f ce (x; D). For the network trained with distillation, the model also depends on the teacher network, so we define the output on x a? y kd = f kd (x; D, T ), where T is the selected teacher network. Then, let the averaged output of? kd and? ce be? kd and? ce , that is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BIAS-VARIANCE TRADEOFF FOR SOFT LABELS</head><formula xml:id="formula_0">y ce = 1 Z ce exp(E D [log? ce ]),? kd = 1 Z kd exp(E D,T [log? kd ]),<label>(1)</label></formula><p>where Z ce , Z kd are two normalization constant. Then according to <ref type="bibr" target="#b11">Heskes (1998)</ref>, we have the following decomposition for the expected error on the sample x and y = t(x) is the ground truth label:</p><formula xml:id="formula_1">error ce = E x,D [?y log? ce ] = E x,D ?y log y + y log ? y ce + y log? c? y ce = E x [?y log y] + E x y log ? y ce + E D E x y log? c? y ce = E x [?y log y] + D KL (y,? ce ) + E D [D KL (? ce ,? ce )] = intrinsic noise + bias + variance,<label>(2)</label></formula><p>where D KL is the Kullback-Leibler divergence. The derivation of the variance term is based on the facts that log?ce</p><formula xml:id="formula_2">E D [log?ce] is a constant and E x [y] = E x [? ce ] = 1.</formula><p>Detailed derivations can be found from Eq. (4) in <ref type="bibr" target="#b11">Heskes (1998)</ref>. Next, we analyze the bias-variance decomposition of L kd . As mentioned above, when training with soft labels, extra randomness is introduced for the selection of a teacher network. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we illustrate the corresponding bias and variance for the selection process of a set of soft labels, which are generated by a teacher network. In this case, a high variance model indicates the model (grey point) is closer to the one-hot trained model (black point), while a low variance model indicates that the model is closer to other possible models trained with soft labels (red points). Although for KD there are more sources introducing randomness, the overall variance brought by L kd is not necessarily higher than L ce . In fact, existing empirical results strongly suggest that the overall variance is smaller with KD. For example, students trained with soft labels are better calibrated than one-hot baselines <ref type="bibr" target="#b20">(M?ller et al., 2019)</ref> and KD makes the predictions of students more consistent when facing adversarial noise <ref type="bibr" target="#b23">(Papernot et al., 2016)</ref>. Here, we present these empirical evidence as an assumption:</p><p>Assumption 1 The variance brought by KD is smaller than direct training, that is,</p><formula xml:id="formula_3">E D,T [D KL (? kd ,? kd )] E D [D KL (? ce ,? ce )].</formula><p>Similar to Eq. (2), we write the decomposition for L kd as</p><formula xml:id="formula_4">error kd = E x [?y log y] + D KL (y,? ce ) + E x y log ? c? y kd + E D,T [D KL (? kd ,? kd )]. (3)</formula><p>An observation here is that? ce converges to one-hot labels while? kd converges to soft labels, s? y ce is closer to the one-hot ground-truth distribution y than? kd , i.e., E x y log ? c? y kd 0. If we rewrite L kd as L kd = L kd ? L ce + L ce , then L kd ? L ce causes that the bias increases by E x y log ? c? y kd and the variance decreases by</p><formula xml:id="formula_5">E D [D KL (? ce ,? ce )] ? E D,T [D KL (? kd ,? kd )].</formula><p>From the above analysis, we separate L kd into two terms, and L kd ? L ce leads to variance reduction, and L ce leads to bias reduction. In the following sections, we first analyze how L kd ? L ce links to the bias-variance tradeoff during training. Then we analyze the changes in the relative importance between bias reduction and variance reduction during training with soft labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE BIAS-VARIANCE TRADEOFF DURING TRAINING</head><p>It is known that bias reduction and variance reduction are often in conflict and we cannot minimize bias and variance together. However, if we consider the change of bias and variance during the training process, the importance of tuning the tradeoff also changes during training. Specifically, shortly after the training of the network starts, the bias error dominates the total error and the variance is less important. As training goes on, gradients of reducing the bias error (induced by L ce ) and reducing the variance (induced by L kd ? L ce ) can be of the same scale for some samples, then we need to balance the tradeoff because reducing one term is likely to increase another one. Therefore for soft labels, we need to handle the bias-variance tradeoff in a sample-wise manner and take the training process into consideration.</p><p>To study the bias-variance tradeoff during training, we consider the gradients of bias and variance reduction. Let z be the logits output of the student on input x and z i is i-th element of it, then we are interested in ?(L kd ?Lce) ?zi . For simplifying analysis, we are concerned with the gradients on the ground-truth related logit, that is, the sample x is labeled as i-th class. Mathematically, for the gradients of variance reduction, we have</p><formula xml:id="formula_6">?(L kd ? L ce ) ?z i = ? (? s i,? ?? t i,? ) ? (? s i,1 ? y i ) = ? e zi/? k e z k /? ?? t i,? ? e zi k e z k ? y i ,<label>(4)</label></formula><p>where? t i,? denotes the i-th element of the teacher's prediction, i.e.,? t ? . The term L kd ?L ce is easy to understand when ? = 1 since the gradient now becomes y i ?? t i,1 . Meanwhile, for the bias reduction, we have ?Lce ?zi =? s i,1 ? y i , so ?Lce ?zi and ?(L kd ?Lce) ?zi always have different signs, leading to a tradeoff.</p><p>If ?Lce ?zi is much higher than ?(L kd ?Lce) ?zi , the bias reduction dominates the overall optimization direc-</p><formula xml:id="formula_7">tion. Instead, if ?(L kd ?Lce)</formula><p>?zi becomes higher, the sample is used for variance reduction. Interestingly, we discover that under a fixed distillation temperature, the final performance is worse when more training samples are used for variance reduction, which will be introduced in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">REGULARIZATION SAMPLES</head><p>Our analysis starts with a conclusion from <ref type="bibr" target="#b20">M?ller et al. (2019)</ref>: if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. Inspired by the phenomenon, we gather the impact of bias and variance during training with different distillation settings. Let a = ?Lce ?zi and b = ?(L kd ?Lce) ?zi , then as introduced before, we use a and b to represent the impact of bias and variance, respectively. If we have b &gt; a for a sample, we name the sample as a regularization sample since the variance dominates the optimization direction. From the collected data, we find that the number of regularization samples is closely related to distillation performance.  In Tab. 1, we present the count of regularization samples for a student network trained by knowledge distillation. For distillation with a temperature higher than 1, which is the common setting, we observe that if the teacher network is trained with label smoothing, more samples will be involved in variance reduction. Also, distillation from a teacher trained with label smoothing performs worse, which is consistent with <ref type="bibr" target="#b20">M?ller et al. (2019)</ref>. Therefore, we conclude that for distillation with soft labels, the regularization samples during training affect the final distillation performance.</p><p>Moreover, we plot the number of regularization samples with respect to different training epochs in <ref type="figure" target="#fig_1">Fig. 2</ref>. As demonstrated in the plots, the number of such samples increases much faster when using the teacher trained with label smoothing for distillation. For regularization samples, the gap of their number between with and without label smoothing becomes larger for more training epochs. These observations verify our motivation that the bias-variance tradeoff varies sample-wisely and evolves during the training process.</p><p>From the above results, we conclude that bias-variance tradeoff for soft labels varies sample-wisely, therefore the strategy for tuning the tradeoff should also be sample-wise. In the next section, we set up ad hoc filters for soft labels and further investigate how regularization samples affect distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HOW REGULARIZATION SAMPLES AFFECT DISTILLATION</head><p>The results presented in the last section suggest that we should avoid training with regularization samples. Hence, we design two straightforward solutions and then find that totally filtering out regularization samples deteriorates the distillation performance.  The first experiment we conduct is to manually resolve the conflicting gradient on the label related logit, as defined in section 3.2. Specifically, we apply a mask to the distillation loss L kd such that ?L kd ?zi = 0 where i is the label. Consequently, the loss for this sample now becomes L * kd = k =i? t k,? log? s k,? . The motivation behind the masked distillation loss is that we only transfer the knowledge of resemblances among the labels. Another experiment is to figure out what role in distillation those regularization samples will play. To investigate this, we carry out knowledge distillation on two subsets of samples: 1) L kd is not valid on regularization samples, and 2) L kd is valid only on regularization samples.</p><p>The results of the two experiments are presented in Tab. 2. We can observe that all of the three approaches are not as good as the baseline knowledge distillation performance, but are better than the direct training baseline. First, since masking L kd loss on the label related logit results in worse performance compared to standard KD, we cannot resolve the tradeoff by applying a mask on the ground truth related logit. Then, from the second experiment, we can see that filtering out regularization samples deteriorates the distillation performance. Moreover, the result of the third experiment is higher than the direct training baseline, indicating that regularization samples are still valuable for distillation. The above results motivate us to think that regularization samples are not fully exploited by standard KD and we can tune the tradeoff to fulfill the potential of regularization samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WEIGHTED SOFT LABELS</head><p>From the last section, we realize that the bias-variance tradeoff varies sample-wisely during training and under fixed distillation settings, the number of regularization samples is negatively associated with the final distillation performance. Yet, discarding regularization samples deteriorates distillation performance and distilling knowledge from these samples is better than the direct training baseline. The above evidence inspires us to lower the weight of regularization samples.</p><p>Recall that regularization samples are defined by the relative value of a and b, we propose to assign importance weight to a sample according to a and b. However, since L kd is computed with the hyperparameter temperature, a and b are correlated with the temperature and thus bring difficulty to tuning the hyperparameter. To make the weighting scheme independent of the temperature hyperparameter, we compare a and b with temperature ? = 1. Note that when ? = 1, a =? s i,1 ? y i and b = y i ?? t i,1 , so we compare? s i,1 and? t i,1 instead. Finally, in the light of previous works that assign sample-wise weights <ref type="bibr" target="#b18">(Lin et al., 2017;</ref><ref type="bibr" target="#b35">Tang et al., 2019)</ref>, we propose weighted soft labels for knowledge distillation, which is formally defined as</p><formula xml:id="formula_8">L wsl = 1 ? exp ? log? s i,1 log? t i,1 L kd = 1 ? exp ? L s ce L t ce L kd ,<label>(5)</label></formula><p>where i is the ground truth class of the sample. The above equation means that a weighting factor is assigned to each sample's L kd according to the predictions of the teacher and the student. In this way, if compared to the teacher, a student network is relatively better trained on a sample, we hav? y s i,1 &gt;? t i,1 , then a smaller weight is assigned to this sample. In <ref type="figure" target="#fig_2">Fig. 3</ref>, the whole computational graph of knowledge distillation with the proposed weighted soft labels is demonstrated. Finally, we add L wsl and L ce together to supervise the network, i.e., L total = L ce + ?L wsl , where ? is a balancing hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To evaluate our weighted soft labels comprehensively, we first conduct experiments with various teacher-student pair settings on CIFAR-100 <ref type="bibr" target="#b16">(Krizhevsky et al., 2009</ref>). Next, we compare our method with current state-of-the-art distillation methods on ImageNet <ref type="bibr" target="#b5">(Deng et al., 2009)</ref>. To validate the effectiveness of our method in terms of handling the bias-variance tradeoff, we conduct ablation experiments by applying weighted soft labels on different subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATASET AND HYPERPARAMETER SETTINGS</head><p>The datasets used in our experiments are CIFAR-100 <ref type="bibr" target="#b16">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b5">(Deng et al., 2009)</ref>. CIFAR-100 contains 50K training and 10K test images of size 32 ? 32. ImageNet contains 1.2 million training and 50K validation images. Except the loss function, training settings like learning rate or training epochs are the same with <ref type="bibr" target="#b36">Tian et al. (2020)</ref> for CIFAR-100 and <ref type="bibr" target="#b10">Heo et al. (2019)</ref> for ImageNet. For distillation, we set the temperature ? = 4 for CIFAR and ? = 2 for ImageNet. For loss function, we set ? = 2.25 for distillation on CIFAR and ? = 2.5 for ImageNet via grid search. The teacher network is well-trained previously and fixed during training.</p><p>For comparison, the following recent state-of-the-art methods are chosen: FitNet <ref type="bibr" target="#b31">(Romero et al., 2015)</ref>, <ref type="bibr">AT (Zagoruyko &amp; Komodakis, 2017)</ref>, SP <ref type="bibr" target="#b37">(Tung &amp; Mori, 2019)</ref>, CC <ref type="bibr" target="#b27">(Peng et al., 2019)</ref>, VID <ref type="bibr" target="#b0">(Ahn et al., 2019)</ref>, RKD , PKT <ref type="bibr" target="#b25">(Passalis &amp; Tefas, 2018)</ref>, AB <ref type="bibr" target="#b10">(Heo et al., 2019)</ref>, FT <ref type="bibr" target="#b14">(Kim et al., 2018)</ref>, FSP <ref type="bibr" target="#b42">(Yim et al., 2017)</ref>, NST <ref type="bibr" target="#b13">(Huang &amp; Wang, 2017)</ref>, Overhaul <ref type="bibr" target="#b10">(Heo et al., 2019)</ref> and CRD <ref type="bibr" target="#b36">(Tian et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MODEL COMPRESSION</head><p>Results on CIFAR-100 In Tab. 3, we present the Top-1 classification accuracy of our method and comparison methods. The results of comparison methods are quoted from <ref type="bibr" target="#b36">Tian et al. (2020)</ref>. Teacher-student pairs of the same and different architecture styles are considered. For pairs of same architecture style, we use wide residual networks (Zagoruyko &amp; Komodakis, 2017) and residual networks <ref type="bibr" target="#b9">(He et al., 2016)</ref>. For pairs of different architecture style, residual networks and ShuffleNet <ref type="bibr">(Zhang et al., 2018)</ref> pairs are chosen for experiments. As shown in the table, for distillation with both same and different architecture style, our method reached new state-of-the-art results. Specifically, our method outperforms standard KD by a large margin, which verifies the effectiveness of our method.</p><p>Results on ImageNet In Tab. 4, we compare our method with current SOTA methods on Ima-geNet. Note that for the ResNet34 ? ResNet-18 distillation setting, the result of CRD is trained 10 more extra epochs while ours is the same as other methods. For ResNet-50 ? MobileNet-v1 distillation setting, NST and FSP are not chosen for comparison as the two methods require too large GPU memories, so we include the accuracy of FT and AB reported in <ref type="bibr" target="#b10">Heo et al. (2019)</ref> for comparison. Our results outperform all the existing methods, verifying the practical value of our method. Weighted soft labels on different subsets. Recall that we propose weighted soft labels for tuning samplewise bias-variance tradeoff, it is still unclear whether the improvements come from a well-handled samplewise bias-variance tradeoff. To investigate this issue, we compare the performance gain of weighted soft labels on different training subsets. Similar to the settings used in Tab. 2, we apply weighted soft labels on two different subsets: only the regularization samples and excluding regularization samples. In Tab. 5, we show the results on subsets of only regularization samples and excluding regularization samples. From the significant improvements, we can see that our method can not only improve performance on the RS subset, the improvements on excluding RS subset is also significant. We conclude that weighted soft labels can tune sample-wise bias-variance tradeoff globally and lead to an improved distillation performance. Distillation with label smoothing trained teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ABLATION STUDIES</head><p>Our exploration of bias-variance tradeoff starts with the conclusion made in <ref type="bibr" target="#b20">M?ller et al. (2019)</ref>: a teacher network trained with the label smoothing trick is less effective for distillation. It is worthwhile to study whether the conclusion remains true for distillation with our weighted soft labels. As discussed before, we hold the opinion that too many regularization samples make the distillation less effective. Since our weighted soft label is proposed to mitigate the negative effects of the regularization samples, with the same settings from Tab. 1, we conduct comparison experiments in Tab. 6 to see if the negative effects still exist. It is evident that weighted soft labels significantly improve the distillation performance, especially for distillation from the teacher trained with label smoothing. Besides, using the teacher trained with label smoothing still performs worse than that without label smoothing, which again verifies the conclusion drawn by <ref type="bibr" target="#b20">M?ller et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Recent studies <ref type="bibr" target="#b20">(M?ller et al., 2019;</ref><ref type="bibr" target="#b43">Yuan et al., 2020)</ref> point out that one important reason behind the effectiveness of distillation is the regularization effect brought by being soft. In this paper, we rethink the soft labels for distillation from a bias-variance tradeoff perspective. The tradeoff varies sample-wisely and we propose weighted soft labels to handle the tradeoff, of which the effectiveness is verified with experiments on standard evaluation benchmarks.  <ref type="bibr" target="#b26">(Pedersen et al., 2004)</ref>, (d) WUP similarity <ref type="bibr" target="#b26">(Pedersen et al., 2004)</ref>. Darker areas denote larger values. ?zi equals y i ?? t i,1 . As y t i,? is the output from the teacher network and computed by a linear mapping of the activations in the teacher's penultimate layer, the regularization indicates that the student should follow the learned the resemblances between classes <ref type="bibr" target="#b12">(Hinton et al., 2015;</ref><ref type="bibr" target="#b20">M?ller et al., 2019)</ref>. Still, two questions are unclear: 1) what the resemblances are and 2) whether the regularization still indicates resemblances if ? is set to 4, a widely adopted hyperparameter <ref type="bibr" target="#b36">(Tian et al., 2020)</ref>.</p><p>Towards answering the questions, we visualize the value of gradient vector ?(L kd ?Lce) ?z concerning each class. Specifically, on ImageNet <ref type="bibr" target="#b5">(Deng et al., 2009</ref>) training set and ? = 4, we calculate the average value of ?(L kd ?Lce) ?z for each class. Let M be the matrix of values with the ij-th entry M ij means averaged ?(L kd ?Lce) ?zi for class j. Since i M ij = 0, diagonal elements are ignored for visualization. The results are visualized in <ref type="figure" target="#fig_3">Fig. 4</ref>. We find that plotting the common correlation matrix heatmap is ambiguous, because the matrix to be visualized is of large size (1000 ? 1000) with a large variance. By treating each entry M ij as a vertex and then constructing a mesh for the matrix, we apply subdivision <ref type="bibr" target="#b19">(Loop, 1987)</ref> to the mesh for smoothing the extreme points and finally rendering the mesh by ray-tracing package PlotOptiX. We can observe the several facts from the figures: 1) Comparing the sub-figure (a) and (b), we can see that for distillation resemblances implied by regularizers are similar across different teacher-student pairs. 2) Comparing (ab) with (cd), we can see that the resemblances are consistent with the semantic similarity of image class names.</p><p>In a word, for ? = 4, the variance reduction brought by soft labels still implies resemblances among labels, which are consistent with the semantic distance of class names. In the next section, we will analyze how bias-variance tradeoff changes when training with soft labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 INTERMEDIATE STATES BETWEEN EXCLUDING AND ONLY ON REGULARIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAMPLES</head><p>To further investigate the phenomenon about regularization samples, we conduct experiments to show the intermediate states between excluding and only on regularization samples. Two settings are considered here: First, we gradually exclude regularization samples during training, from excluding all regularization samples to excluding 25% regularization samples; Second, we keep all regularization samples and then gradually add non-regularization samples. Since we judge a sample is regularization or not according to the training loss, we cannot pre-define a sample set such that a certain percentage samples are kept or dropped. Therefore, we propose to conduct these experiments by assigning a probability to whether backward the loss computed with regularization samples. For example, if during training, a sample is marked as regularization sample according to the value of a and b, we backward the loss of this sample by a probability p = 0.5. In this way, we can get the performance of excluding 75% regularization samples. In Tab. 7, we first present result with KD in (a) and then present result with weighted soft labels applied in (b). We can observe that weighted soft labels are indeed balancing the sample-wise, not on dataset scale, bias and variance.</p><p>A.3 COMBINING WITH RKD (PARK ET AL., 2019).</p><p>To investigate how the weighted soft labels can be applied to the variants of KD, we conduct an experiment of combining RKD  with our weighted soft labels. Relational knowledge distillation measures the L2 distance of features between two samples or the angle formed by three samples as knowledge to transfer. In other words, the knowledge in RKD is measured by  the relations between sample pairs. It is no longer sample-independent, which is different from the weighted soft labels applied to KD which can assign the weights sample-wisely. We currently take the averaged weighting factors of the involved sample pairs when calculating the distance/angle matrix. The results on CIFAR-100 are presented in Tab. 8 (averaged over 5 runs). As can be observed from the table, the weighted soft label applied to RKD still brings improvements, though not that big compared with WSL applied to KD. Also, we believe that it is an important future direction to explore the applications to more variants of KD.</p><p>A.4 OTHER VARIANTS OF WEIGHTING.</p><p>In the work, the weighting scheme is defined as 1 ? exp ? ? 1. In Tab. 9, we present the comparison between adopted weighting form and the Sigmoid baseline. We can see that as long as we can adaptively tune the sample-wise bias-variance tradeoff, the performance is better than KD, i.e., without weighted soft labels.Therefore, although the proposed weighting form is not mathematically optimal, the not-too-big or not-too-small weights for these regularization examples are not hard to tune. These results verify our main contribution that there is sample-wise biasvariance tradeoff and we need to assign weights to the regularization examples.</p><p>A.5 ABLATION ON ?.</p><p>In Tab. 10 We first tune the value of ? on CIFAR100, with four values {1, 2, 3, 4} tested. Then we test with three values in [2, 3] in (b). Finally, we tune ? on ImageNet in (c). As a conclusion, the results are not very sensitive to ? and the cost of searching ? in our work is not expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 RESULTS ON MULTINLI</head><p>To further validate our method, we conduct experiments on an NLP dataset MultiNLI <ref type="bibr" target="#b40">(Williams et al., 2018)</ref>. In this setting, the teacher is BERT-base-cased with 12 layers, 768 Hidden and 108M params. The student is T3 with 3 layers, 768 Hidden and 44M params. Besides, we follow the training setting in <ref type="bibr" target="#b33">Sun et al. (2019)</ref>. In Tab. 11, we present the result comparisons of standard KD and our weighted soft labels.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Bias and variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The number of regularization samples with respect to training epochs. The distillation settings are the same as the settings in Tab. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Computational graph of knowledge distillation with our proposed weighted soft labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the resemblances introduced by soft label regularizers: (a) VGG-19 (Teacher) ? VGG-16 (Student), (b) ResNet-50 (Teacher) ? ResNet-18 (Student). And semantic similarity between label names: (c) LCH similarity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>in [0, 1], so that the weights of regularization samples are lower than those non-regularization samples. A straightforward baseline is that we can use the Sigmoid function to convert</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>We count the number of regularization samples with different distillation settings on CIFAR-100. The teacher-student network pair is WRN-40-2 (Zagoruyko &amp; Komodakis, 2017) and WRN-16-2. Results are averaged over 5 repeated runs. The temperature column means the temperature for distillation and the label smoothing column means whether the teacher network is trained with label smoothing trick.</figDesc><table><row><cell cols="4">Baseline Top-1 Acc</cell><cell cols="2">Teacher: 76.55 w/ label smoothing, 75.61 w/o label smoothing Student: 73.26</cell></row><row><cell cols="6">Temperature Label smoothing? Student Top-1 Acc Number of regularization samples</cell></row><row><cell>? = 2</cell><cell></cell><cell></cell><cell></cell><cell>74.79 74.62</cell><cell>15379 25235</cell></row><row><cell>? = 4</cell><cell></cell><cell></cell><cell></cell><cell>74.92 74.59</cell><cell>17709 24775</cell></row><row><cell>? = 6</cell><cell></cell><cell></cell><cell></cell><cell>75.10 74.46</cell><cell>17408 24538</cell></row><row><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Study of the impact on distillation for regularization samples. Loss function presented here is for the loss on a specific sample. Results are classification Top-1 accuracy. We follow the settings used in Tab. 1 and set ? = 4. Results are averaged over 5 runs.</figDesc><table><row><cell cols="4">Teacher: 75.61; Student with direct training: 73.26; Student with standard KD: 74.92</cell><cell></cell></row><row><cell>Setting</cell><cell>Loss function</cell><cell>Student performance</cell><cell cols="2">Performance gap</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">to direct training to KD</cell></row><row><cell>Mask KD loss on the label related logit</cell><cell>L ce + L  *  kd</cell><cell>73.51</cell><cell>+0.25</cell><cell>-1.41</cell></row><row><cell>Excluding regularization samples</cell><cell>L ce , if a &lt; b L ce + L kd , if a b</cell><cell>74.59</cell><cell>+1.33</cell><cell>-0.33</cell></row><row><cell>Only on regularization samples</cell><cell>L &lt; b</cell><cell>73.86</cell><cell>+0.60</cell><cell>-1.06</cell></row></table><note>ce , if a b L ce + L kd , if a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-1 classification accuracy results on CIFAR-100. Comparison results are quoted from<ref type="bibr" target="#b36">Tian et al. (2020)</ref>. We report our results over 5 repeated runs.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Same architecture style</cell><cell></cell><cell cols="3">Different architecture style</cell></row><row><cell cols="5">Teacher WRN-40-2 resnet56 resnet110 resnet110 resnet32x4</cell><cell>resnet32x4</cell><cell>resnet32x4</cell><cell>WRN-40-2</cell></row><row><cell cols="3">Student WRN-40-1 resnet20 resnet20</cell><cell>resnet32</cell><cell>resnet8x4</cell><cell>ShuffleNetV1</cell><cell>ShuffleNetV2</cell><cell>ShuffleNetV1</cell></row><row><cell>Teacher</cell><cell>75.61</cell><cell>72.34 74.31</cell><cell>74.31</cell><cell>79.42</cell><cell>79.42</cell><cell>79.42</cell><cell>75.61</cell></row><row><cell>Student</cell><cell>71.98</cell><cell>69.06 69.06</cell><cell>71.14</cell><cell>72.50</cell><cell>70.5</cell><cell>71.82</cell><cell>70.5</cell></row><row><cell>FitNet</cell><cell>72.24</cell><cell>69.21 68.99</cell><cell>71.06</cell><cell>73.50</cell><cell>73.59</cell><cell>73.54</cell><cell>73.73</cell></row><row><cell>AT</cell><cell>72.77</cell><cell>70.55 70.22</cell><cell>72.31</cell><cell>73.44</cell><cell>71.73</cell><cell>72.73</cell><cell>73.32</cell></row><row><cell>SP</cell><cell>72.43</cell><cell>69.67 70.04</cell><cell>72.69</cell><cell>72.94</cell><cell>73.48</cell><cell>74.56</cell><cell>74.52</cell></row><row><cell>CC</cell><cell>72.21</cell><cell>69.63 69.48</cell><cell>71.48</cell><cell>72.97</cell><cell>71.14</cell><cell>71.29</cell><cell>71.38</cell></row><row><cell>VID</cell><cell>73.30</cell><cell>70.38 70.16</cell><cell>72.61</cell><cell>73.09</cell><cell>73.38</cell><cell>73.40</cell><cell>73.61</cell></row><row><cell>RKD</cell><cell>72.22</cell><cell>69.61 69.25</cell><cell>71.82</cell><cell>71.90</cell><cell>72.28</cell><cell>73.21</cell><cell>72.21</cell></row><row><cell>PKT</cell><cell>73.45</cell><cell>70.34 70.25</cell><cell>72.61</cell><cell>73.64</cell><cell>74.10</cell><cell>74.69</cell><cell>73.89</cell></row><row><cell>AB</cell><cell>72.38</cell><cell>69.47 69.53</cell><cell>70.98</cell><cell>73.17</cell><cell>73.55</cell><cell>74.31</cell><cell>73.34</cell></row><row><cell>FT</cell><cell>71.59</cell><cell>69.84 70.22</cell><cell>72.37</cell><cell>72.86</cell><cell>71.75</cell><cell>72.50</cell><cell>72.03</cell></row><row><cell>FSP</cell><cell>n/a</cell><cell>69.95 70.11</cell><cell>71.89</cell><cell>72.62</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>NST</cell><cell>72.24</cell><cell>69.60 69.53</cell><cell>71.96</cell><cell>73.30</cell><cell>74.12</cell><cell>74.68</cell><cell>74.89</cell></row><row><cell>KD</cell><cell>73.54</cell><cell>70.66 70.67</cell><cell>73.08</cell><cell>73.33</cell><cell>74.07</cell><cell>74.45</cell><cell>74.83</cell></row><row><cell>CRD</cell><cell>74.14</cell><cell>71.16 71.46</cell><cell>73.48</cell><cell>75.51</cell><cell>75.11</cell><cell>75.65</cell><cell>76.05</cell></row><row><cell>Ours</cell><cell>74.48</cell><cell>72.15 72.19</cell><cell>74.12</cell><cell>76.05</cell><cell>75.46</cell><cell>75.93</cell><cell>76.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Top-1 and Top-5 classification accuracy results on ImageNet validation set. All training hyperparameter like learning rate and training epochs are in accordance with<ref type="bibr" target="#b10">(Heo et al., 2019)</ref>.</figDesc><table><row><cell cols="3">Teacher: ResNet-34 ? Student: ResNet-18 Method Top-1 Acc Top-5 Acc</cell><cell cols="3">Teacher: ResNet-50 ? Student: MobileNet-v1 Method Top-1 Acc Top-5 Acc</cell></row><row><cell>Teacher</cell><cell>73.31</cell><cell>91.42</cell><cell>Teacher</cell><cell>76.16</cell><cell>92.87</cell></row><row><cell>Student</cell><cell>69.75</cell><cell>89.07</cell><cell>Student</cell><cell>68.87</cell><cell>88.76</cell></row><row><cell>KD</cell><cell>70.67</cell><cell>90.04</cell><cell>KD</cell><cell>70.49</cell><cell>89.92</cell></row><row><cell>AT</cell><cell>71.03</cell><cell>90.04</cell><cell>AT</cell><cell>70.18</cell><cell>89.68</cell></row><row><cell>NST</cell><cell>70.29</cell><cell>89.53</cell><cell>FT</cell><cell>69.88</cell><cell>89.5</cell></row><row><cell>FSP</cell><cell>70.58</cell><cell>89.61</cell><cell>AB</cell><cell>68.89</cell><cell>88.71</cell></row><row><cell>RKD</cell><cell>70.40</cell><cell>89.78</cell><cell>RKD</cell><cell>68.50</cell><cell>88.32</cell></row><row><cell>Overhaul</cell><cell>71.03</cell><cell>90.15</cell><cell>Overhaul</cell><cell>71.33</cell><cell>90.33</cell></row><row><cell>CRD</cell><cell>71.17</cell><cell>90.13</cell><cell>CRD</cell><cell>69.07</cell><cell>88.94</cell></row><row><cell>Ours</cell><cell>72.04</cell><cell>90.70</cell><cell>Ours</cell><cell>71.52</cell><cell>90.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">: Performance on different subsets</cell></row><row><cell cols="3">with soft labels and our weighted soft la-</cell></row><row><cell cols="3">bels. RS means regularization samples.</cell></row><row><cell cols="2">Results are averaged over 5 runs.</cell><cell></cell></row><row><cell>Subsets</cell><cell cols="2">Standard KD Weighted</cell></row><row><cell>Only on RS</cell><cell>73.86</cell><cell>74.46</cell></row><row><cell>Excluding RS</cell><cell>74.59</cell><cell>75.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Distillation using weighted soft labels and teacher trained with label smoothing (denoted as LS?). Results are averaged over 5 runs.</figDesc><table><row><cell cols="3">? LS? Standard KD Weighted</cell></row><row><cell>4</cell><cell>74.92</cell><cell>75.78</cell></row><row><cell>4</cell><cell>74.59</cell><cell>75.60</cell></row><row><cell>6</cell><cell>75.10</cell><cell>75.74</cell></row><row><cell>6</cell><cell>74.46</cell><cell>75.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Intermediate states between excluding and only on regularization samples</figDesc><table><row><cell cols="4">(a) CIFAR100 (WRN-40-2?WRN-40-1 with KD)</cell></row><row><cell cols="4">Percentage of excluded regularization samples.</cell></row><row><cell cols="2">100% 75%</cell><cell>50%</cell><cell>25%</cell></row><row><cell cols="4">74.59 74.63 74.72 74.87</cell></row><row><cell cols="4">Percentage of adding non-regularization samples</cell></row><row><cell>0%</cell><cell>25%</cell><cell>50%</cell><cell>75%</cell></row><row><cell cols="4">73.86 74.12 74.47 74.71</cell></row><row><cell cols="4">(b) CIFAR100 (WRN-40-2?WRN-40-1 with weighted soft labels)</cell></row><row><cell cols="4">Percentage of excluded regularization samples</cell></row><row><cell cols="2">100% 75%</cell><cell>50%</cell><cell>25%</cell></row><row><cell cols="4">75.35 75.48 75.61 75.72</cell></row><row><cell cols="4">Percentage of adding non-regularization samples</cell></row><row><cell>0%</cell><cell>25%</cell><cell>50%</cell><cell>75%</cell></row><row><cell cols="4">74.46 74.79 75.18 75.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Combining weighted soft labels with RKD.</figDesc><table><row><cell cols="4">Distillation settings WRN-40-2 ? WRN-16-2 WRN-40-2 ? WRN-40-1 resnet56 ? resnet20 Teacher 75.61 75.61 72.34</cell></row><row><cell>Student</cell><cell>73.26</cell><cell>71.98</cell><cell>69.06</cell></row><row><cell>RDK</cell><cell>74.12</cell><cell>73.34</cell><cell>70.25</cell></row><row><cell>WSL + RKD</cell><cell>74.65</cell><cell>73.89</cell><cell>70.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">: Comparison to other weighting forms. (Setting: CIFAR100, WRN-40-2?WRN-40-1)</cell></row><row><cell>?</cell><cell>2.0</cell><cell>3.0</cell><cell>4.0</cell></row><row><cell cols="4">Sigmoid baseline 74.13 73.97 73.29</cell></row><row><cell>Ours</cell><cell cols="3">74.38 74.12 73.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation on ?.</figDesc><table><row><cell cols="5">(a) CIFAR100 (WRN-40-2?WRN-40-1)</cell></row><row><cell>?</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="5">Top1 73.67 74.38 74.12 73.46</cell></row><row><cell cols="5">(b) CIFAR100 (WRN-40-2?WRN-40-1)</cell></row><row><cell>?</cell><cell>2.25</cell><cell>2.5</cell><cell>2.75</cell><cell></cell></row><row><cell cols="4">Top1 74.48 74.34 74.21</cell><cell></cell></row><row><cell cols="5">(c) ImageNet (ResNet-34?ResNet-18)</cell></row><row><cell>?</cell><cell>2</cell><cell>2.25</cell><cell>2.5</cell><cell></cell></row><row><cell cols="4">Top1 71.91 71.96 72.04</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Results on MultiNLI.</figDesc><table><row><cell></cell><cell cols="4">Teacher (BERT-12) Student (BERT-3) KD (BERT-3) Ours (BERT-3)</cell></row><row><cell>Results reported by Sun et al. (2019)</cell><cell>83.7</cell><cell>74.8</cell><cell>75.4</cell><cell>-</cell></row><row><cell>Our replications</cell><cell>83.57</cell><cell>75.06</cell><cell>75.50</cell><cell>76.28</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work is supported in part by a gift grant from Horizon Robotics and National Science Foundation Grant CNS-1951952. We thank Yichen Gong, Chuan Tian, Jiemin Fang and Yuzhu Sun for the discussion and assistance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Anoop Korattikara Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconciling modern machinelearning practice and the classical bias-variance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified bias-variance decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural networks and the bias/variance dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bias/variance decompositions for likelihood-based estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1425" to="1433" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias plus variance decomposition for zero-one loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="275" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge consistency between neural networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Smooth subdivision surfaces based on triangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Loop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>University of Utah, Department of Mathematics</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uniform convergence may be unable to explain generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Zico</forename><surname>Vaishnavh Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11615" to="11626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Tantia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scicluna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08591</idno>
		<title level="m">Simon Lacoste-Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wordnet:: Similarity-measuring the relatedness of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5007" to="5016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards understanding knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5142" to="5151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plotoptix</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/plotoptix/.(Accessedon09/08/2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distilling knowledge from a deep pose regressor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhamad</forename><surname>Risqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Pb</forename><surname>De Gusmao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Almalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4314" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning efficient detector with semi-supervised adaptive distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00366</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bias-variance analysis of support vector machines for the development of svm-based ensemble methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="725" to="775" />
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Preparing lessons: Improve knowledge distillation with better supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07471</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rethinking bias-variance trade-off for generalization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11328</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
