<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneous Similarity-based Self-Distillation for Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
						</author>
						<title level="a" type="main">Simultaneous Similarity-based Self-Distillation for Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code available at https://github. com/MLforHealth/S2SD.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero-shot applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives. However, generalization capacity is known to scale with the embedding space dimensionality. Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications. To remedy this, we propose Simultaneous Similarity-based Self-distillation (S2SD). S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining testtime cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while also setting a new state-ofthe-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Metric Learning (DML) aims to learn embedding space models in which a predefined distance metric reflects not only the semantic similarities between training samples, but also transfers to unseen classes. The generalization capabilities of these models are important for applications in image retrieval <ref type="bibr" target="#b40">(Wu et al., 2017)</ref>, face recognition <ref type="bibr" target="#b32">(Schroff et al., 2015)</ref>, clustering <ref type="bibr" target="#b3">(Bouchacourt et al., 2018)</ref> and representation learning <ref type="bibr">(He et al., 2020)</ref>. Still, transfer learning into unknown test distributions remains an open problem, with <ref type="bibr" target="#b30">Roth et al. (2020b)</ref> and <ref type="bibr" target="#b18">Musgrave et al. (2020)</ref> revealing strong performance saturation across DML training objectives. However, <ref type="bibr" target="#b30">Roth et al. (2020b)</ref> also show that embedding space dimensionality can be a driver for generalization across objectives due to higher representation capacity. Indeed, this insight can be linked to recent work targeting other objective-independent improvements to DML via artificial samples <ref type="bibr" target="#b51">(Zheng et al., 2019)</ref>, higher feature distribution moments <ref type="bibr">(Jacob et al., 2019)</ref> or orthogonal features , which have shown promising relative improvements over selected DML objectives. Unfortunately, these methods come at a cost; be it longer training times or limited applicability. Similarly, drawbacks can be found when naively increasing the operating (base) dimensionality, incurring increased cost for data retrieval at test time, which is especially problematic on larger datasets. This limits realistically usable embedding dimensionalities and leads to benchmarks being evaluated against fixed, predefined dimensionalities. In this work, we propose Simultaneous Similarity-based Self-Distillation (S2SD) to show that complex higherdimensional information can actually be effectively leveraged in DML without changing the base dimensionality and test time cost, which we motivate from two key elements. Firstly, in DML, an additional embedding space can be spanned by a multilayer perceptron (MLP) operating over the feature representation shared with the base embedding space (see e.g. ). With larger dimensionalities, we can thus cheaply learn a secondary high-dimensional representation space simultaneously, also denoted as target embedding space. Relative to the large feature backbone, and with the batchsize capping the number of additional high dimensional operations, only little additional training cost is introduced. While we can not utilize the high-dimensional target embedding space at test time for aforementioned reasons, we may utilize it to boost the performance of the base embeddings. Unfortunately, a simple connection of base and additional target embedding spaces through the shared feature backbone is insufficient for the base representation space to benefit from the auxiliary, high-dimensional information. Thus, secondly, to efficiently leverage the high-dimensional context, we use insights from knowledge distillation <ref type="bibr">(Hinton et al., 2015)</ref>, where a small "student" model is trained to approximate a larger "teacher" model. However, while knowledge distillation can be found in DML <ref type="bibr">(Chen et al., 2018)</ref>, few-shot learning <ref type="bibr" target="#b36">(Tian et al., 2020)</ref> and self-supervised extensions thereof <ref type="bibr" target="#b26">(Rajasegaran et al., 2020)</ref>, the reliance arXiv:2009.08348v3 [cs.CV] 4 Jun 2021 on additional, commonly larger teacher networks or multiple training runs <ref type="bibr">(Furlanello et al., 2018)</ref>, introduces much higher training cost. Fortunately, we find that the target embedding space learned simultaneously at higher dimension can sufficiently serve as a "teacher" during trainingthrough knowledge distillation of its sample similarities, the performance of the base embedding space can be improved notably. Such distillation intuitively encourages the lowerdimensional base embedding space to embed semantic similarities similar to the more expressive target embedding space and thus incorporate dimensionality-related generalization benefits. Furthermore, S2SD makes use of the low cost to span additional spaces to introduce multiple teacher spaces. Operating each of them at higher, but varying dimensionality, joint distillation can then be used to enforce reusability in the distilled content akin to feature reusability in meta-learning <ref type="bibr" target="#b25">(Raghu et al., 2020)</ref> for additional generalization boosts. Finally, in DML, the base embedding space is spanned over a penultimate feature space of much higher dimensionality, which introduces a dimensionality-based bottleneck . By applying the distillation objective between feature and base embedding space in S2SD, we further encourage better feature usage in base embedding space. This facilitates the approximation of high-dimensional context through the base embedding space for additional improvements in generalization. The benefits to generalization are highlighted in performance boosts across three standard benchmarks, CUB200-2011 <ref type="bibr" target="#b37">(Wah et al., 2011)</ref>, <ref type="bibr">CARS196 (Krause et al., 2013)</ref> and Stanford Online Products <ref type="bibr" target="#b19">(Oh Song et al., 2016)</ref>, where S2SD improves test-set recall@1 of already strong DML objectives by up to 7%, while also setting a new state-ofthe-art. Improvements are even more significant in very low dimensional base embedding spaces, making S2SD attractive for large-scale retrieval problems which can benefit from reduced embedding dimensionalities. Importantly, as S2SD is applied during the same DML training process on the same network backbone, no large teacher networks or additional training runs are required. Simple experiments even show that S2SD can outperform comparable 2-stage distillation at much lower cost. In summary, our contributions can be described as: 1) We propose Simultaneous Similarity-based Self-Distillation (S2SD) for DML, using knowledge distillation of high-dimensional context without large additional teacher networks or training runs. 2) We motivate and evaluate this approach through detailed ablations and experiments, showing that the method is agnostic to choices in objectives, backbones, and datasets.</p><p>3) Across benchmarks, we achieve significant improvements over strong baseline objectives and state-of-the-art performance, with especially large boosts for very lowdimensional embedding spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Metric Learning (DML) has proven useful for zeroshot image/video retrieval &amp; clustering <ref type="bibr" target="#b32">(Schroff et al., 2015;</ref><ref type="bibr" target="#b40">Wu et al., 2017;</ref><ref type="bibr" target="#b4">Brattoli et al., 2020)</ref>, face verification <ref type="bibr" target="#b11">(Liu et al., 2017;</ref><ref type="bibr">Deng et al., 2019)</ref> and contrastive (selfsupervised) representation learning (e.g. He et al. (2020); <ref type="bibr">Chen et al. (2020)</ref>; <ref type="bibr" target="#b16">Misra &amp; van der Maaten (2020)</ref>). Approaches can be divided into 1) improved ranking losses, 2) tuple sampling methods and 3) extensions to the standard DML training approach. 1) Ranking losses place constraints on the relations available in image tuples, ranging from pairs (s.a. <ref type="bibr">Hadsell et al. (2006)</ref>) to triplets <ref type="bibr" target="#b32">(Schroff et al., 2015)</ref> and more complex orderings <ref type="bibr">(Chen et al., 2017;</ref><ref type="bibr" target="#b19">Oh Song et al., 2016;</ref><ref type="bibr" target="#b33">Sohn, 2016;</ref><ref type="bibr" target="#b39">Wang et al., 2019)</ref>. 2) As the number of possible tuples scales exponentially with dataset size, tuple sampling approaches have been introduced to tackle tuple redudancy and to ensure that meaningful tuples are presented during training. These tuple sampling methods can follow heuristics <ref type="bibr" target="#b32">(Schroff et al., 2015;</ref><ref type="bibr" target="#b40">Wu et al., 2017)</ref>, be of hierarchical nature <ref type="bibr">(Ge, 2018)</ref> or learned <ref type="bibr" target="#b29">(Roth et al., 2020a)</ref>. Similarly, learnable proxies to replace tuple members <ref type="bibr" target="#b17">(Movshovitz-Attias et al., 2017;</ref><ref type="bibr" target="#b22">Kim et al., 2020;</ref><ref type="bibr" target="#b24">Qian et al., 2019)</ref> can also remedy the sampling issue, which can be extended to tackle DML from a classification viewpoint <ref type="bibr" target="#b47">(Zhai &amp; Wu, 2018;</ref><ref type="bibr">Deng et al., 2019)</ref>. 3) Finally, extensions to the basic training scheme can involve synthetic data <ref type="bibr" target="#b9">(Lin et al., 2018;</ref><ref type="bibr" target="#b51">Zheng et al., 2019;</ref><ref type="bibr" target="#b9">Duan et al., 2018)</ref>, complementary features <ref type="bibr" target="#b28">(Roth et al., 2019;</ref>, a division into subspaces <ref type="bibr" target="#b31">(Sanakoyeu et al., 2019;</ref><ref type="bibr" target="#b41">Xuan et al., 2018;</ref><ref type="bibr">Kim et al., 2018;</ref><ref type="bibr" target="#b20">Opitz et al., 2018)</ref>, training of multiple networks  using mutual learning <ref type="bibr" target="#b50">(Zhang et al., 2018)</ref> or higher-order moments <ref type="bibr">(Jacob et al., 2019)</ref>. S2SD can similarly be seen as an extension to DML, though we specifically focus on capturing and distilling complex high-dimensional sample relations within lower dimensional embedding spaces to improve generalization. Knowledge Distillation involves knowledge transfer from teacher to (usually smaller) student models, e.g. by matching network softmax outputs/logits <ref type="bibr" target="#b5">(Bucilu? et al., 2006;</ref><ref type="bibr">Hinton et al., 2015)</ref>, (attention-weighted) feature maps <ref type="bibr" target="#b27">(Romero et al., 2015;</ref><ref type="bibr" target="#b46">Zagoruyko &amp; Komodakis, 2016)</ref>, or latent representations <ref type="bibr" target="#b1">(Ahn et al., 2019;</ref><ref type="bibr" target="#b21">Park et al., 2019;</ref><ref type="bibr" target="#b35">Tian et al., 2019;</ref><ref type="bibr" target="#b8">Laskar &amp; Kannala, 2020)</ref>. Importantly, <ref type="bibr" target="#b35">Tian et al. (2019)</ref> show that under fair comparison, basic matching via Kullback-Leibler (KL) Divergences as used in <ref type="bibr">Hinton et al. (2015)</ref> performs very well, which we also find to be the case for S2SD. This is further supported in recent few-shot learning literature <ref type="bibr" target="#b36">(Tian et al., 2020)</ref>, wherein KLdistillation alongside self-distillation (by iteratively reusing the same network as a teacher for beneficial generalization and regulatory properties <ref type="bibr">(Furlanello et al., 2018;</ref><ref type="bibr" target="#b7">Lan et al., 2018;</ref><ref type="bibr" target="#b49">Zhang et al., 2019b;</ref><ref type="bibr" target="#b0">Abnar et al., 2020;</ref><ref type="bibr" target="#b45">Yun et al., 2020)</ref>) in additional meta-training stages improves feature Figure 1. S2SD. We use a standard encoder ?, embedding f , and multiple auxiliary embedding networks gi (used only during training) depending on the S2SD approach used. During training, for each batch of embeddings produced by the respective embedding network gi, we compute DML losses while applying embedding distillation on the respective batch-similarity matrices (DSD/MSD). We further distill from the feature representation space for additional information gain (MSDF).</p><p>representation strength important for generalization <ref type="bibr" target="#b25">(Raghu et al., 2020)</ref>.</p><p>Our work is closest to <ref type="bibr" target="#b48">Zhang et al. (2019a)</ref> and <ref type="bibr" target="#b10">Liu et al. (2020)</ref>, which propose to break down a network into a cascading set of subnetworks, wherein each subsequent subnetwork builds on its predecessors. In doing so, each subnetwork is trained independently on a classification task at hand. Knowledge distillation is then applied either from the full network <ref type="bibr" target="#b48">(Zhang et al., 2019a)</ref> acting as a teacher or via soft targets generated from a meta-learned label generator <ref type="bibr" target="#b10">(Liu et al., 2020)</ref>, to each smaller student subnetwork during the same training run to improve overall performance. In a related manner, S2SD utilizes similar concurrent, but relational self-distillation to instead encode high-dimensional sample relation context from multiple, higher-dimensional teacher embedding spaces; this is crucial to improve the generalization capabilities of a single student embedding space for zero-shot, out-of-distribution image retrieval tasks. As such, it operates orthogonally to proposals made by <ref type="bibr" target="#b48">Zhang et al. (2019a)</ref> and <ref type="bibr" target="#b10">Liu et al. (2020)</ref>. The concurrency of the self-distillation in turn is a consequence of the novel insight that solely the dimensionality of embedding spaces can serve as meaningful teachers, as these can be spanned cheaply over a large, shared feature backbone. The novel dimensionality-based concurrent distillation also sets S2SD apart from existing knowledge distillation applications to DML, which are done in a generic manner with separate, larger teacher networks or additional training stages <ref type="bibr">(Chen et al., 2018;</ref><ref type="bibr" target="#b43">Yu et al., 2019;</ref><ref type="bibr" target="#b39">Han et al., 2019;</ref><ref type="bibr" target="#b8">Laskar &amp; Kannala, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We now introduce key elements for Simultaneous Similaritybased Self-Distillation (S2SD) to improve generalization of embedding spaces by utilizing higher dimensional context. We begin with the preliminary notation and fundamentals to Deep Metric Learning ( ?3.1), before defining the three key elements to S2SD: Firstly, the Dual Self-Distillation (DSD) objective, which uses KL-Distillation on a concurrently learned embedding space of higher dimensionality ( ?3.2) to introduce important high-dimensional context into a low-dimensional embedding space during training. We then extend this to Multiscale Self-Distillation (MSD) with distillation from several different high-dimensional embedding spaces to encourage reusability in the distilled context ( ?3.3). Finally, we shift to self-distillation from normalized feature representations (MSDF) to counter dimensionality bottlenecks commonly encountered in DML ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>DML builds on generic Metric Learning which aims to find a (parametrized) distance metric</p><formula xml:id="formula_0">d ? : ? ? ? ? R on the fea- ture space ? ? R d *</formula><p>over images X that best satisfy ranking constraints usually defined over class labels Y. This holds also for DML. However, while Metric Learning relies on a fixed feature extraction method to obtain ?, DML introduces deep neural networks to concurrently learn a feature representation. Most such DML approaches aim to learn Mahalanobis distance metrics, which cover the parametrized family of inner product metrics <ref type="bibr" target="#b34">(Su?rez et al., 2018;</ref><ref type="bibr" target="#b6">Chen et al., 2019)</ref>. These metrics, with some restrictions <ref type="bibr" target="#b34">(Su?rez et al., 2018)</ref>, can be reformulated as</p><formula xml:id="formula_1">d(? 1 , ? 2 ) = (L(? 1 ? ? 2 ) T L(? 1 ? ? 2 ) = L? 1 ? L? 2 2 = ? 1 ? ? 2 2 (1) with learned linear projection L ? R d?d * from d * -dim. fea- tures ? i ? ? to d-dim. embeddings ? i := (f ??)(x i ) ? ? f</formula><p>with embedding function f : ? i ? L? i . Importantly, this redefines the motivation behind DML as learning ddimensional image embeddings ? s.t. their euclidean distance d(?, ?) = ? ? ? 2 is connected to semantic similarities in X . This embedding-based formulation offers the significant advantage of being compatible with fast approximate similarity search methods (e.g. <ref type="bibr">(Johnson et al., 2017)</ref>), allowing for large-scale applications at test time. In this work, we assume ? f to be normalized to the unit hypersphere S ? f , which is commonly done <ref type="bibr" target="#b40">(Wu et al., 2017;</ref><ref type="bibr" target="#b31">Sanakoyeu et al., 2019;</ref><ref type="bibr" target="#b11">Liu et al., 2017;</ref> for beneficial regularizing purposes <ref type="bibr" target="#b40">(Wu et al., 2017;</ref>. For the remainder we hence set ? to refer to S ? . Common approaches to learn such a representation space involve training surrogates on ranking constraints defined by class labels. Such approaches start from pair or tripletbased ranking objectives <ref type="bibr">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b32">Schroff et al., 2015)</ref>, where the latter is defined as</p><formula xml:id="formula_2">L triplet = 1 |T B | (xi,xj ,x k )?T B [d(? i , ? j ) ? d(? i , ? k ) + m] +<label>(2)</label></formula><p>with margin m and the set of available triplets (x i , x j , x k ) ? T B in a mini-batch B ? X , with y i = y j = y k . This can be extended with more complex ranking constraints or tuple sampling methods. We refer to <ref type="bibr">Supp. B and Roth et al. (2020b)</ref> for further insights and detailed studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Embedding Space Self-Distillation</head><p>For the aforementioned standard DML setting, generalization performance of a learned embedding space can be linked to the utilized embedding dimensionality. However, high dimensionality results in notably higher retrieval cost on downstream applications, which limits realistically usable dimensions. In S2SD, we show that high-dimensional context can be used as a teacher during the training run of the low-dimensional base or reference embedding space. As the base embedding model is also the one that is evaluated, test time retrieval costs are left unchanged.</p><p>To achieve this, we simultaneously train an additional highdimensional auxiliary/target embedding space ? g := (g ? ?)(X ) spanned by a secondary embedding branch g. g is parametrized by a MLP or a linear projection, similar to the base embedding space ? f spanned by f , see ?3.1. Both f and g operate on the same large, shared feature backbone ?. For simplicity, we train ? f and ? g using the same DML objective L DML .</p><p>Unfortunately, higher expressivity and improved generalization of high-dimensional embeddings in ? g hardly benefit the base embedding space, even with a shared feature backbone. To explicitly leverage high-dimensional context for our base embedding space, we utilize knowledge distillation from target to base space. However, while common knowledge distillation approaches match single embeddings or features between student and teacher, the different dimensionalities in ? f and ? g inhibit naive matching.</p><p>Instead, S2SD matches sample relations (see e.g. <ref type="bibr" target="#b35">(Tian et al., 2019)</ref>) defined over batch-similarity matrices D ? R B?B in base and target space, D f and D g , with batchsize B. We thus encourage the base embedding space to relate different samples in a similar manner to the target space. To compute D, we use a cosine similarity by default, given as D i,j = ? T i ? j , since ? i is normalized to the unit hypersphere. Defining ? max as the softmax operation and D KL (p, q) = log(p) log(p) /log(q) as the Kullback-Leiblerdivergence, we thus define the simultaneous self-distillation objective as</p><formula xml:id="formula_3">L dist (D f , D g ) = |B| i D KL ? max D f i,:/T , ? ? max ( D g i,:/T )<label>(3)</label></formula><p>with temperature T , as visualized in <ref type="figure">Figure 1</ref>. ( ? ) denotes no gradient flow to target branches g as we only want the base space to learn from the target space. By default, we match rows or columns of D, D i,: , effectively distilling the relation of an anchor embedding ? i to all other batch samples. Embedding all batch samples in base dimension,</p><formula xml:id="formula_4">? B f : B ? ? f (B), and higher dimension, ? B g : B ? ? g (B)</formula><p>, the (simultaneous) Dual Self-Distillation (DSD) training objective then becomes</p><formula xml:id="formula_5">L DSD (? B f , ? B g ) = 1 /2 ? L DML (? B f ) + L DML (? B g ) + ? ? L dist (D f , D g )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reusable Relations by Multiscale Distillation</head><p>While DSD encourages the reference embedding space to recover complex sample relations by distilling from a higherdimensional target space spanned by g, it is not known a priori which distillable sample relations actually benefit generalization of the reference space.</p><p>To encourage the usage of sample relations that more likely aid generalization, we follow insights made in <ref type="bibr" target="#b25">Raghu et al. (2020)</ref> on the connection between reusability of features across multiple tasks and better generalization thereof. We motivate reusability in S2SD by extending DSD to Multiscale Self-Distillation (MSD) with distillation instead from m multiple different target spaces spanned by G = {g k } k?{1,m} . Importantly, each of these highdimensional target spaces operate on different dimensionalities, i.e. dim f &lt; dim g 1 &lt; ... &lt; dim g m?1 &lt; dim g m . As this results in each target embedding space encoding sample relations differently, application of distillation across all spaces spanned by G pushes the base branch towards learning from sample relations that are reusable across all higher dimensional embedding spaces and thereby more likely to generalize (see also <ref type="figure">Fig. 1</ref>).</p><p>Specifically, given the set of target similarity matrices {D k } k?{f,g1,...,gm} and target batch embeddings ? m := {? B k } k?{f,g1,...,gm} , we then define the MSD training objective as</p><formula xml:id="formula_6">L MSD (? m ) = 1 2 ? L DML (? B f ) + 1 m m i=1 L DML (? B gi ) + ? m m i=1 L dist (D f , D gi )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Tackling the Dimensionality Bottleneck by Feature Space Self-Distillation</head><p>As noted in ?3.1, the base embedding ? utilizes linear projections f from the (penultimate) feature space ? where dim ? is commonly much larger than dim ?. While compressed semantic spaces encourage stronger representations <ref type="bibr" target="#b2">(Alemi et al., 2016;</ref><ref type="bibr">Dai &amp; Wipf, 2019)</ref> to be learned,  show that the actual test performance of the lower-dimensional embedding space ? is inferior to that of the non-adapted, but higher-dimensional feature space ?.</p><p>This supports a dimensionality-based loss of information beneficial to generalization, which can hinder the base embedding space to optimally approximate the highdimensional context introduced in ?3.2 and 3.3. To rectify this, we apply self-distillation following eq. 3 on the normalized feature representations ? n generated by normalizing the backbone output ?. With the batch of normalized feature representations ? B ? n we get multiscale self-distillation with feature distillation (MSDF) (see also <ref type="figure">Fig. 1</ref>)</p><formula xml:id="formula_7">L MSDF (? m , ? B ? n ) = L MSD (? m ) + ?L dist (D f , D ? n ) (6)</formula><p>In the same manner, one can also address other architectural information bottlenecks such as through the generation of feature representations from a single global pooling operation. While not noted in the original publication, <ref type="bibr" target="#b22">Kim et al. (2020)</ref> address this in the official code release by using both global max-and average pooling to create their base embedding space. While this naive usage changes the architecture at test time, in S2SD we can fairly leverage potential benefits by only spanning the auxiliary spaces (and distilling) from such feature representations (denoted as DSDA/MSDA/MSDFA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We study S2SD in four experiments to establish 1) method ablation performance &amp; relative improvements, 2) state-ofthe-art, 3) comparisons to standard 2-stage distillation, benefits to low-dimensional embedding spaces &amp; generalization properties and 4) motivation for architectural choices.</p><p>Method Notation. We abbreviate ablations of S2SD (see ?3) in our experiments as:</p><formula xml:id="formula_8">DSD &amp; MSD for Dual (3.2) &amp; Multiscale Self -Distillation (3.3)</formula><p>, MSDF the addition of Feature distillation (3.4) and DSDA/MSD(F)A the inclusion of multiple pooling operations in the auxiliary branches (also ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments</head><p>Fair Evaluation of S2SD. ?5.1 specifically applies S2SD and its ablations to three DML baselines. To show realistic benefit, S2SD is applied to best-performing objectives evaluated in <ref type="bibr" target="#b30">Roth et al. (2020b)</ref>, namely (i) Margin loss with Distance-based Sampling <ref type="bibr" target="#b40">(Wu et al., 2017)</ref>, (ii) their proposed Regularized Margin loss and (iii) Multisimilarity loss <ref type="bibr" target="#b39">(Wang et al., 2019)</ref>, following their experimental training pipeline. This setup utilizes no learning rate scheduling and fixes common implementational factors of variation in DML pipelines such as batchsize, base embedding dimension, weight decay or feature backbone architectures to ensure comparability in DML (more details in Supp. A.2). As such, our results are directly comparable to their large set of examined methods and guaranteed that relative improvements solely stem from the application of S2SD.</p><p>Comparison to literature. ?5.2 further highlights the benefits of S2SD by comparing S2SD's boosting properties across literature standards, with different backbone architectures and base embedding dimensions: (1) ResNet50 with d = 128 <ref type="bibr" target="#b40">(Wu et al., 2017;</ref><ref type="bibr" target="#b28">Roth et al., 2019)</ref> and (2) d = 512 <ref type="bibr" target="#b47">(Zhai &amp; Wu, 2018)</ref> as well as <ref type="formula" target="#formula_3">(3)</ref> variants to Inception-V1 with Batch-Normalization at d = 512 <ref type="bibr" target="#b39">(Wang et al., 2019;</ref><ref type="bibr" target="#b24">Qian et al., 2019;</ref>. Only here do we conservatively apply learning rate scheduling, since all references noted in <ref type="table" target="#tab_2">Table 2</ref> employ scheduling as well. We categorize published work based on backbone architecture and embedding dimension for fairer comparison. Note that this is a less robust comparison than done in ?5.1, due to potential implementation differences between our pipeline and reported literature results.</p><p>Comparison to 2-Stage Distillation. ?5.3 compares S2SD to 2-stage distillation, investigates benefits to very low dimensional reference spaces and examines the connection between improvements and changes in embedding space density and spectral decay (see Supp. D), which have been linked to improved generalization.</p><p>Ablation Study. ?5.4 ablates and motivates architectural  <ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">1536,</ref><ref type="bibr">2048]</ref>.</p><p>We found it beneficial to activate the feature distillation after n = 1000 iterations to ensure that meaningful features are learned first before feature distillation is applied. Additional embedding spaces are spanned by two layer MLPs with row-wise KL-distillation of high-dimensional similarities (eq. 3), applied as in L multi (eq. 5). By default, we use Multisimilarity Loss as stand-in for L DML . Hyperpa-   <ref type="table" target="#tab_2">Table 2</ref> shows that S2SD can boost baseline objectives to reach and even surpass other state-of-the- art methods, in parts with a notable margin. This holds even when compared to much more complex methods with feature mining or RL-policies such as MIC <ref type="bibr" target="#b28">(Roth et al., 2019)</ref>, DiVA  or PADS <ref type="bibr" target="#b29">(Roth et al., 2020a)</ref>, to which S2SD operates orthogonally. Finally, we note that these insights are true even with our results reported with confidence intervals, which is commonly neglected in DML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Benefits of S2SD</head><p>Comparison to standard distillation. With a student S using the same objective and embedding dimensionality as the reference branch in DSD) and a teacher T at the highest optimal dimensionality d = 2048, we find that separating DSD into a standard 2-stage distillation setup actually degenerates performance (see <ref type="figure" target="#fig_2">Fig. 3A</ref>, compare to Dist.).</p><p>In addition, S2SD allows for easy integration of teacher ensembles, realized by MSD(F,A), to even outperform the teacher by a notable margin. This is specifically interesting as S2SD retains the operating embedding dimensionality of the student.</p><p>Benefits to lower base dimensions. We now show that our module is able to vastly boost networks limited to very low embedding dimensions, which we visualize in <ref type="figure" target="#fig_2">figure 3B</ref>). Embedding space metrics. We now look at relative changes in embedding space density and spectral decay (see supplementary of <ref type="bibr" target="#b30">Roth et al. (2020b)</ref>) when applying S2SD. Our study, visualized in figure 2, shows that the application of S2SD increases embedding space density and lowers the spectral decay (thus providing a more featurediverse embedding space) across criteria, which is aligned with properties of improved generalization in DML as noted in <ref type="bibr" target="#b30">Roth et al. (2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Motivating S2SD Architecture Choices</head><p>Distillation improves generalization through S2SD. <ref type="figure" target="#fig_2">Fig.  3A</ref> (Joint) and <ref type="figure" target="#fig_2">Fig. 3F (? = 0)</ref> highlight how crucial self-distillation is, as incorporating a secondary embedding space without any distillation link hardly improves performance. <ref type="figure" target="#fig_2">Fig. 3A</ref> (Concur.) further shows that joint training of a detached reference embedding f , while otherwise training in high dimension, similarly doesn't offer notable improvement. Finally, <ref type="figure" target="#fig_2">Figure 3F</ref> shows robustness to changes in ?, with peaks around ? = 50 and ? = 5 for CUB200/CARS196 and SOP. We also found best performance for temperatures T ? [0.2, 2] and hence set T = 1 by default.</p><p>Best way to enforce reusability. To motivate our many-toone self-distillation L MSD (eq. 5, here also dubbed L Multi ), we evaluate against other distillation setups that could support reusability of distilled sample relations: (1) Nested distillation, where instead of distilling all target spaces only to the reference space, we distill from a target space to all lower-dimensional embedding spaces:</p><formula xml:id="formula_9">L Nested (? m ) = 1 2 L DML (? B f ) + 1 m m i=1 L DML (? B gi ) + ? m m?1 m i=0,j=1,j =i dim gj ?dim gi L dist (? B gi , ? B gj )<label>(7)</label></formula><p>In the second term, g 0 denotes the base embedding f .</p><p>(2) Chained distillation, which distills target spaces only to the immediate lower-dimensional embedding space: <ref type="figure" target="#fig_2">Figure 3E</ref> shows that a many-to-one distillation performs notably better, supporting the reusability aspect and L multi as our default method.</p><formula xml:id="formula_10">L Chained (? m ) = 1 2 L DML (? B f ) + 1 m m i=1 L DML (? B gi ) + ? m m?1 i=1 L dist (? B gi , ? B gi?1 )<label>(8)</label></formula><p>Choice of distillation method &amp; branch structures. <ref type="figure" target="#fig_2">Fig.  3C</ref> evaluates various distillation objectives, finding KLdivergence between vectors of similarities to perform better than KL-divergence applied over full similarity matrices or row-wise means thereof, as well as cosine/euclidean distance-based distillation (see e.g. <ref type="bibr" target="#b43">Yu et al. (2019)</ref>). <ref type="figure" target="#fig_2">Figure  3D</ref> shows insights into optimal auxiliary branch structures, with two-layer MLPs giving the largest benefit, although even a linear target mapping reliably boosts performance. This coincides with insights made by <ref type="bibr">Chen et al. (2020)</ref>. Further network depth only deteriorates performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel DML training paradigm based on dimensionality-based knowledge distillation, Simultaneous Similarity-based Self-Distillation (S2SD). S2SD allows for the inclusion of reusable, context-rich, highdimensional relational information for improved generalization. This is achieved by solving the standard DML objective simultaneously in higher-dimensional embedding spaces while applying knowledge distillation concurrently between these high-dimensional teacher spaces and a lowerdimensional reference space. In doing so, S2SD introduces little additional computational overhead, with no extra cost at test time. Thorough ablations and experiments show S2SD significantly improving the generalization performance of existing DML objectives regardless of embedding dimensionality, thereby also setting a new state-of-the-art. funded in part by a CIFAR AI Chair at the Vector Institute, Microsoft Research, and an NSERC Discovery Grant. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute www.vectorinstitute.ai/#partners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reviewer Comments</head><p>Re: More advanced baseline methods. We do believe that our reported results are representative of the current state of Deep Metric Learning -under fair comparison <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref>, Multisimilarity and Margin loss achieve best or competitive results. In addition, S2SD is applied to regularized Deep Metric Learning (R-Margin loss <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref>) and shows high improvements throughout. Prox-yAnchor (Kim et al., 2020) was not included in the literature comparison due to different architecture settings (lines 268-271), thus not allowing for a fair comparison. DiVA in turn offers a separate extension to DML and should be seen as orthogonal to S2SD, which however we consistently outperform. However, for completeness, we have applied S2SD to ProxyAnchor using the fair comparison setup in <ref type="bibr" target="#b30">Roth et al. (2020b)</ref> (see section 5.1) on all benchmarks to highlight the general applicability of S2SD. We perform no hyperparameter tuning and use those already mentioned in the paper and find notable and consistent performance improvements. Results are available in the supplementary <ref type="table">(Table 6</ref>).</p><p>Re: Runtime Analysis. We offer a dimensionality versus runtime analysis in the supplementary <ref type="table" target="#tab_4">(Table 3)</ref>, which shows significant runtime reduction by up to a magnitude: With performance of d = 64 roughly matching that of d = 2048, retrieval runtime on N = 250000 synthetic evaluation samples can be reduced from 27.21 ? 0.17 to 1.98 ? 0.00 seconds. The high performance boost also offers the benefit of reduced memory storage needed to retain embedding vectors for comparable retrieval performance.</p><p>Re: Necessity of loss terms. There are only three effective loss terms. Firstly, the default DML objective L DML applied to each branch. Secondly, the actual distillation objective over all branches ? m m i=1 L Dist (D f , D gi ) (see eq. 5) and thirdly, the feature space self-distillation to counter the dimensionality bottleneck (see eq. 6). These last two correspond to the S2SD variants MSD and MSDF, which we evaluate against different variants of L DML in section 5.1. As can be seen, both loss-terms operate complementary. Regarding general convergence behaviour and training dynamics, we find only minor changes, with maximum performance achieved at similar training stages.</p><p>Re: Validation Information. All benchmarks only offer a train/test split. As such, we use a 80-20 train/validation split of the original training split to determine hyperparameters (e.g. <ref type="bibr" target="#b30">Roth et al. (2020b)</ref> and <ref type="bibr" target="#b22">Kim et al. (2020)</ref>), and use those for training on the full training dataset and evaluation on the test set used throughout literature (see <ref type="bibr">Sec. 4)</ref>.</p><p>Re: Theoretical Grounding. While we do not offer detailed theoretical formalism in this paper as to why S2SD is so effective, we do evaluate changes in the embedding space structure (see <ref type="figure" target="#fig_0">figure 2)</ref>, which highlight that S2SD encourages noticeably higher feature diversity and embedding space uniformity linked to improved generalization <ref type="bibr" target="#b30">(Roth et al., 2020b;</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8675-curvilinear</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation</head><p>We now provide further details regarding the training and testing setup utilized. For any study except the comparison against the state-of-the-art <ref type="table" target="#tab_2">(Table 2)</ref>  Training runs on CUB200-2011 and CARS196 are done over 150 epochs and 100 epochs for SOP for all experiments without any learning rate scheduling, except for the state-of-the-art experiments (see again 2). For the latter, we made use of slightly longer training to account for conservative learning rate scheduling, which is similarly done across reference methods noted in tab. 2. Schedule and decay values are determined over validation subset performances. All baseline DML objectives we apply our self-distillation module S2SD on use the default parameters noted in <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> with the single exception of Margin Loss on SOP, where we found class margins ? = 0.9 to be more beneficial for distillation than the default ? = 1.2. This was done as changing from ? = 1.2 to ? = 0.9 had no notable impact on the baseline performance. Finally, similar to (Kim et al., 2020), we found a warmup epoch of all MLPs to improve convergence on SOP. Spectral decay computations in ?5.3 follow the setting described in Supp. D.</p><p>We implement everything in PyTorch <ref type="bibr" target="#b23">(Paszke et al., 2017)</ref>. Experiments are done on GPU servers containing Nvidia Titan X, P100 and T4s, however memory usage never exceeds 12GB. Each result is averaged over five seeds, and for the sake of reproducibilty and result validity, we report mean and standard deviation, even though this is commonly neglected in DML literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Methods</head><p>This section provides a more detailed explanation of the DML baseline objectives we used alongside our self-distillation module S2SD in the experimental section 4. For additional details, we refer to the supplementary material in <ref type="bibr" target="#b30">(Roth et al., 2020b</ref>). For the mathematical notation, we refer to Section 3.1. We use ? = f ? ? to denote the feature network ? with embedding f , and ? i the embedding of a sample x i . Finally, alongside the method descriptions we provide the used hyperparameters.</p><p>Margin Loss <ref type="bibr" target="#b40">(Wu et al., 2017)</ref> builds on triplet/pair-based losses, but introduces both class-specific, learnable boundaries {? y k } k=1...C (with number of classes C) between positive and negative pairs, as well as distance-based sampling for negatives:</p><formula xml:id="formula_11">L margin = xi,xj ?P B [m + (?1) Iy i =y j (? yi ? d(? i , ? j ))] + (9) p(x j |x i , y i = y j ) = min ?, d(? i , ? j ) n?2 (1 ? 1 4 d(? i , ? j ) 2 ) n?3 2 ?1<label>(10)</label></formula><p>where P B denotes the available pairs in minibatch B, and n the embedding dimension. Throughout this work, we use ? = 1.2 except for S2SD on SOP, where we found ? = 0.9 to work better without changing the baseline performance. We set the learning rate for the class boundaries as 5 ? 10 ?4 , and margin m = 0.2.</p><p>Regularized Margin Loss <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> proposes a simple regularization scheme on the margin loss that increases the number of directions of significant variance in the embedding space by randomly exchanging a negative sample with a positive one with probability p switch . For ResNet-backbones, we use p switch = 0.4 for CUB200, p switch = 0.35 for CARS196 and p switch = 0.15 for SOP as done in <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref>. For Inception-based backbones, we set p switch = 0.15 for CUB200 and CARS196 and p switch = 0.3 for SOP.</p><p>Multisimilarity Loss <ref type="bibr" target="#b39">(Wang et al., 2019)</ref> incorporates more similarities into training by operating directly on all positive and negative samples for an anchor x i , while also incorporating a sampling operation that encourages the usage of harder training samples:</p><formula xml:id="formula_12">d * c (i, j) = ? ? ? ? ? d c (? i , ? j ) d c (? i , ? j ) &gt; min j?Pi d c (? i , ? j ) ? d c (? i , ? j ) d c (? i , ? j ) &lt; max k?Ni d c (? i , ? k ) + 0 otherwise (11) L multisim = 1 b i?B ? ? 1 ? log[1 + j?Pi exp(??(d * c (? i , ? j ) ? ?))] ? ? + i?B 1 ? log[1 + k?Ni exp(?(d * c (? i , ? k ) ? ?))]<label>(12)</label></formula><p>where d c denotes the cosine similarity instead of the euclidean distance, and P i /N i the set of positives and negatives for x i in the minibatch, respectively. We use the default values ? = 2, ? = 40, ? = 0.5 and = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>The evaluation metrics used throughout this work are recall @ 1 (R@1), recall @ 2 (R@2) and Normalized Mutual Information (NMI), capturing two distinct embedding space properties.</p><p>Recall@K, see e.g. in <ref type="bibr">(Jegou et al., 2011)</ref>, especially Recall@1 and Recall@2, is the primary metric used to compare the performance of DML methods and approaches, as it offers strong insights into retrieval performances of the learned embedding spaces. Given the set of embedded samples ? i ? ? with ? i = ?(x i ) and x i ? X , and the sorted set of k nearest neighbours for any sample ? a , F k a = min sort</p><formula xml:id="formula_13">d(?a,?) arg min F ?X ,|F |=k x f ?F d(? a , ? f )<label>(13)</label></formula><p>Recall@K is measured as</p><formula xml:id="formula_14">Recall@K = 1 |X | xi?X 1 ?x k ? F k i s.t.y k = y i 0 otherwise (14)</formula><p>which evaluates how likely semantically corresponding pairs (as determined here by the labelling y i ? Y) will occur in a neighbourhood of size k.</p><p>Normalized Mutual Information (NMI), see <ref type="bibr" target="#b13">(Manning et al., 2010)</ref>, evaluates the clustering quality of the embedded samples ? (taken from X ). It is computed by first clustering with K cluster centers, usually corresponding to the number of classes available, using a cluster method of choice s.a. K-Means <ref type="bibr" target="#b12">(Lloyd, 1982)</ref>. This assigns each sample x i a cluster label/id ? i based on the nearest cluster centroid. With ? k = {i|? i = ? k } the set of samples with cluster label, ? = {? k } K k the set of cluster sets, ? k = {i|y i = y k } the set of samples with true label y k and ? = {? k } K k the set of class label sets, the Normalized Mutual Information is given as</p><formula xml:id="formula_15">NMI(?, ?) = I(?, ?) 2 ? (H(?) + H(?))<label>(15)</label></formula><p>with mutual information I(?, ?) and entropy H(?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalization Metrics</head><p>Embedding Space Density. Given sets of embeddings ?, we first define the average inter-class distance as</p><formula xml:id="formula_16">? inter (?) = 1 Z inter y l ,y k ,l =k d(?(? y l ), ?(? y k ))<label>(16)</label></formula><p>which measures the average distances between groups of embeddings with respective classes y l and y k , estimated by the respective class centers ?(?). Z inter denotes a normalization constant based on the number of available classes. We also introduce the average intra-class distance as the mean distance between samples within their respective class</p><formula xml:id="formula_17">? intra (?) = 1 Z intra y l ?Y ?i,?j ??y l ,i =j d(? i , ? j )<label>(17)</label></formula><p>again with normalization constant Z intra and set of embeddings with class y l , ? y l . Given these two quantities, the embedding space density is then defined as</p><formula xml:id="formula_18">? ratio (?) = ? intra (?) ? inter (?)<label>(18)</label></formula><p>and effectively measured how densely samples and classes are grouped together. <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> show that optimizing the DML problem while keeping the embedding space density high, i.e. without aggressive clustering, encourages better generalization to unseen test classes.</p><p>Spectral Decay. The spectral decay metric ?(?) defines the KL-divergence between the (sorted) spectrum of D singular values S singular ? (obtained via Singular Value Decomposition (SVD)) and a D-dimensional uniform distribution U D , and is inversly related to the entropy of the embedding space:</p><formula xml:id="formula_19">?(?) = D KL U D , S singular ?<label>(19)</label></formula><p>It does not account for class distributions. <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> show that doing DML while encouraging a high-entropy feature space notably benefits the generalization performance. In our experiments, we disregard the first 10 singular vectors (out of 128) to highlight the feature diversity. This is important, as we evaluate the spectral decay within the same objectives, which results in the first few singular values to be highly similar. Increasing target dimensions offers notable improvements. We opt for a target dimension of 2048 due to slightly higher mean improvements. For multiple embedding branches (#B), there seems to be an optimum at four branches. (B) Furthermore, feature distillation gives another notable boost. However, this only holds for the globally averaged penultimate feature representation. When distilling more fine-grained feature representations, performance degenerates (where #P denotes smaller pooling windows applied to the penultimate feature representation). (C) We show that detached auxiliary branches for distillation are crucial to higher improvements, as we want the reference embedding space to approximate the higher-dimensional one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Experiments</head><p>This part extends the set of ablations experiments performed in section 5.4 in the main paper. a. Detaching target spaces for distillation. We examine whether it is preferable to detach the target embeddings from the distillation loss (see eq. 3), as we want the reference embedding space to approximate the higher-dimensional relations. Similarly, we do not want the target embedding networks g i to reduce high-dimensional to lower-dimensional relations to optimizer for the distillation constraint. As can be seen in <ref type="figure" target="#fig_4">fig 4C,</ref> it is indeed the case that detaching the target embedding spaces is notably beneficial for a stronger reference embedding, supporting the previous motivation. b. Influence of varying target dimensions. As noted at the beginning of section 4, we set the target dimension for dual self-distillation (DSD) to d = 2048, which we motivate through a small ablation study in <ref type="figure" target="#fig_4">fig. 4A</ref>, with TD denoting the target dimension of choice. As can be seen, benefits plateau when the target dimension reaches more than four times the reference dimension. However, to be directly comparable to high-dimensional reference settings, we set d = 2048 as default. c. Ablating multiple distillation scales. Going further, we extend the module with additional embedding branches to the multiscale self-distillation approach (MSD), all operating in different, but higher-than-reference dimension. As already shown in <ref type="figure" target="#fig_2">Figure 3B</ref> in the main paper, there is a benefit of multiscale distillations by encouraging reusable sample relations. In this part, we motivate the choice of four target branches (as noted in sec. 4). Looking at <ref type="figure" target="#fig_4">figure 4A,</ref> where B denotes the number of additional target spaces, we can see a benefit in multiple additional target spaces of ascending dimension. As the improvements saturate after B = 4, we simply set this as the default value. However, the additional benefits of going to multiscale from dual distillation are not as high as going from no to dual target space distillation, showcasing the general benefit of high-dimensional concurrent self-distillation. Finally, we highlight that a multiscale approach slightly outperforms a multibranch distillation setup <ref type="figure" target="#fig_4">(Fig. 4A, Multi-B)</ref> where each target branch has the same target dimension of 2048 while introducing less additional parameters. d. Finer-grained feature distillation. As already shown in section 4 and again in <ref type="figure" target="#fig_4">figure 4B</ref>, we see benefits of feature distillation, using the (globally averaged) normalized penultimate feature space. It therefore makes sense to investigate the benefits of distilling even more fine-grained feature representation. Defining P = [(3, 3), (1, 1), (2, 2), (4, 1)] as the pooling window size applied to the non-average penultimate feature representation, we investigate less compressed feature representation space. As can be seen in <ref type="figure" target="#fig_4">fig. 4B</ref>, where P denotes the index to P, there appears to be no benefits in distilling feature representations higher up the network. e. Runtime comparison of base dimensionalities. We highlight relative retrieval times at different base dimensionalities in Tab. 3 using faiss (Johnson et al., 2017) on a NVIDIA 1080Ti and a synthetic set of N = 250000 embeddings of dimensionality d ? <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Detailed Evaluation Results</head><p>This table contains all method ablations for a fair evaluation as used in Section 5.2 and <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Evaluation Results using mAP@R</head><p>This table measures performance of methods investigated in <ref type="table" target="#tab_0">Table 1</ref> using the mAP@R(@1000) metric used in <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref>. The results here coincide with those measured using Recall@1. This comes at no surprise, as both metrics are strongly correlated when measuring the performance of Deep Metric Learning methods <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Detailed Ablation Results</head><p>Detailed values to the ablation experiments done in section 5.4 and E.  <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> and <ref type="bibr" target="#b18">(Musgrave et al., 2020)</ref> and based on the formulation used in <ref type="bibr" target="#b30">(Roth et al., 2020b)</ref>) against well performing DML objectives examined in section 5.2.. All results are computed over 5-run averages.</p><p>( * ) For Margin Loss and SOP, we found ? = 0.9 to give better distillation results without notably influencing baseline performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Generalization metrics. S2SD increases embedding space density and lowers spectral decay. rameters were determined previous to the result runs using a 80-20 training and validation split, similar to<ref type="bibr" target="#b30">Roth et al. (2020b)</ref> and<ref type="bibr" target="#b22">Kim et al. (2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For example, networks operating on d = 32 &amp; 64 trained with S2SD can match the performance of networks trained and evaluated on embedding dimensions four or eight times the size. For d = 128, S2SD even outperforms the highest dimensional baseline at d = 2048 by a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>S2SD study and ablations. (A) DSD outperforms comparable two-stage distillation on student S (Dist.) using teacher (T), with MSD(FA) even outperforming T. We further see that distillation is essential -training multiple spaces in parallel (Joint.) or a detached lower-dimensional base embedding (Concur.) gives little benefit. (B) We see benefits across base dimensionalities, especially in the low-dimensional regime.(C) We find KL-distillation between similarity vectors (R-KL) to work best. (D) An additional non-linearity in aux. branches g gives a boost, but going deeper hurts generalization. (E) Distilling each aux. embed. space (Multi) separately compares favourable against other distillation setups s.a. Nested and Chained distillation. (F) Performance is robust to changes in weight values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>which uses different backbones and embedding dimensions, we follow the setup used by<ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> 1 : This includes a ResNet50 (He et al., 2016) with frozen Batch-Normalization (Ioffe &amp; Szegedy, 2015), normalization of the output embeddings with dimensionality 128 and optimization with Adam (Kingma &amp; Ba, 2015) using a learning rate of 10 ?5 and weight decay of 3 ? 10?4. The input images are randomly resized and cropped from the original image size to 224 ? 224 for training. Further augmentation by random horizontal flipping with p = 0.5 is applied. During testing, center crops of size 224 ? 224 are used. The batchsize is set to 112.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Additional ablations. (A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>S2SD comparison against strong baseline objectives. Bold denotes best results per objective, bluebold marks best overall results. mAP@R results as proposed in<ref type="bibr" target="#b30">(Roth et al., 2020b)</ref> and<ref type="bibr" target="#b18">(Musgrave et al., 2020)</ref> as well as ProxyAnchor evaluations<ref type="bibr" target="#b22">(Kim et al. (2020)</ref>, using a different setup) can be found in the Supplementary(Table 5and 6), further showing the notable benefits of S2SD.<ref type="bibr" target="#b40">(Wu et al., 2017)</ref> 63.09 ? 0.46 68.21 ? 0.33 79.86 ? 0.33 67.36 ? 0.34 78.43 ? 0.07 90.40 ? 0.03 + DSD 65.11 ? 0.18 69.65 ? 0.44 83.19 ? 0.18 69.28 ? 0.56 79.05 ? 0.12 90.52 ? 0.18 + MSD 66.13 ? 0.34 70.83 ? 0.27 83.63 ? 0.31 69.80 ? 0.36 79.26 ? 0.15 90.60 ? 0.10 + MSDF 67.58 ? 0.32 71.47 ? 0.19 85.55 ? 0.23 71.68 ? 0.54 79.63 ? 0.15 90.70 ? 0.09 + MSDFA 67.21 ? 0.23 71.43 ? 0.25 86.45 ? 0.35 71.46 ? 0.24 78.82 ? 0.09 90.49 ? 0.06 R-Margin, ? = 0.6, (Roth et al., 2020b) 64.93 ? 0.42 68.36 ? 0.32 82.37 ? 0.13 68.66 ? 0.47 77.58 ? 0.11 90.42 ? 0.03 + DSD 66.58 ? 0.08 70.03 ? 0.41 84.64 ? 0.16 70.87 ? 0.18 77.86 ? 0.10 90.50 ? 0.03 + MSD 66.81 ? 0.27 70.47 ? 0.16 85.01 ? 0.10 71.67 ? 0.40 78.00 ? 0.06 90.47 ? 0.04 + MSDF 68.12 ? 0.30 71.80 ? 0.33 85.78 ? 0.22 72.24 ? 0.31 78.57 ? 0.09 90.58 ? 0.02 + MSDFA 68.58 ? 0.26 71.64 ? 0.40 86.81 ? 0.35 71.48 ? 0.29 78.00 ? 0.11 90.41 ? 0.02 Multisimilarity (Wang et al., 2019) 62.80 ? 0.70 68.55 ? 0.38 81.68 ? 0.19 69.43 ? 0.38 77.99 ? 0.09 90.00 ? 0.02 + DSD 65.57 ? 0.26 70.08 ? 0.33 83.51 ? 0.20 70.30 ? 0.05 78.23 ? 0.04 90.08 ? 0.04 + MSD 65.80 ? 0.16 70.66 ? 0.01 83.98 ? 0.10 71.34 ? 0.09 78.42 ? 0.09 90.09 ? 0.03 + MSDF 67.04 ? 0.29 71.87 ? 0.19 85.69 ? 0.19 72.77 ? 0.13 78.59 ? 0.08 90.09 ? 0.06 + MSDFA 67.68 ? 0.29 71.40 ? 0.21 85.89 ? 0.15 71.45 ? 0.26 78.07 ? 0.06 89.88 ? 0.10 choices in S2SD used throughout ?4. Pseudo code and detailed results are available in Supp. F, G, and I. In all experiments, we evaluate on standard DML benchmarks: CUB200-2011 (Wah et al., 2011), CARS196 (Krause et al., 2013) and Stanford Online Products (SOP) (Oh Song et al., 2016). Performance is measured in recall at 1 (R@1) and at 2 (R@2) (Jegou et al., 2011) as well as Normalized Mutual Information (NMI) (Manning et al., 2010). Results measured on mean Average Precision evaluated on Recall (mAP@R) are available in the Supplementary along with additional dataset details. Hertz, 1992) of 4 ? 10 ?5 for regularization. Additional details are available in Supp. (A). For ?5.1-5.4, we only adjust the respective pipeline elements in questions. For S2SD, unless noted otherwise (s.a. in ?5.4), we set ? = 50, T = 1 for all objectives on CUB200 and CARS196, and ? = 5, T = 1 on SOP. DSD uses target-dim. d = 2048 and MSD uses target-dims. d ? [</figDesc><table><row><cell>BENCHMARKS?</cell><cell cols="2">CUB200-2011</cell><cell></cell><cell>CARS196</cell><cell>SOP</cell><cell></cell></row><row><cell>APPROACHES ?</cell><cell>R@1</cell><cell>NMI</cell><cell>R@1</cell><cell>NMI</cell><cell>R@1</cell><cell>NMI</cell></row><row><cell>Margin, ? = 1.2,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In Tab. 1 (full table in Supp. Tab. 4), we show that under fair experimental protocol, utilizing S2SD and its ablations gives an objective and benchmark independent, significant boost in performance by up to 7% opposing the existing DML objective performance plateau. This holds even for previous state-of-the-art regularized objectives s.a. R-Margin loss as well as proxy-based objectives such as ProxyAnchor<ref type="bibr" target="#b22">(Kim et al. (2020)</ref>, see Supplementary), highlighting the effectiveness of S2SD for DML. Across objectives, S2SDbased changes in wall-time do not exceed negligible 5% with only minor convergence impacts.</figDesc><table><row><cell>5. Results</cell></row><row><cell>5.1. Fair performance study</cell></row></table><note>5.2. Setting a new State-of-the-Art Motivated by Tab. 1, we use MSDFA for CUB200/CARS196 and MSDF for SOP.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>State-of-the-art comparison. We show that S2SD, represented by its variants MSDF(A), boosts baseline objectives to state-of-theart across literature. ( * ) stands for Inception-V1 with frozen Batch-Norm. Bold: best results per literature setup. Bluebold: best results per overall benchmark.</figDesc><table><row><cell>BENCHMARKS ?</cell><cell></cell><cell cols="3">CUB200 (Wah et al., 2011)</cell><cell cols="3">CARS196 (Krause et al., 2013)</cell><cell cols="3">SOP (Oh Song et al., 2016)</cell></row><row><cell>METHODS ?</cell><cell></cell><cell>R@1</cell><cell>R@2</cell><cell>NMI</cell><cell>R@1</cell><cell>R@2</cell><cell>NMI</cell><cell>R@1</cell><cell>R@10</cell><cell>NMI</cell></row><row><cell>ResNet50-128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Div&amp;Conq (Sanakoyeu et al., 2019)</cell><cell>65.9</cell><cell>76.6</cell><cell>69.6</cell><cell>84.6</cell><cell>90.7</cell><cell>70.3</cell><cell>75.9</cell><cell>88.4</cell><cell>90.2</cell></row><row><cell cols="2">MIC (Roth et al., 2019)</cell><cell>66.1</cell><cell>76.8</cell><cell>69.7</cell><cell>82.6</cell><cell>89.1</cell><cell>68.4</cell><cell>77.2</cell><cell>89.4</cell><cell>90.0</cell></row><row><cell cols="2">PADS (Roth et al., 2020a)</cell><cell>67.3</cell><cell>78.0</cell><cell>69.9</cell><cell>83.5</cell><cell>89.7</cell><cell>68.8</cell><cell>76.5</cell><cell>89.0</cell><cell>89.9</cell></row><row><cell>Multisimilarity+S2SD</cell><cell></cell><cell cols="9">68.0 ? 0.2 78.7 ? 0.1 71.7 ? 0.4 86.3 ? 0.1 91.8 ? 0.3 72.0 ? 0.3 79.0 ? 0.2 90.2 ? 0.1 90.6 ? 0.1</cell></row><row><cell>Margin+S2SD</cell><cell></cell><cell cols="9">67.6 ? 0.3 78.2 ? 0.2 70.8 ? 0.3 86.0 ? 0.2 91.8 ? 0.2 72.2 ? 0.2 80.2 ? 0.2 91.5 ? 0.1 90.9 ? 0.1</cell></row><row><cell>R-Margin+S2SD</cell><cell></cell><cell cols="9">68.9 ? 0.3 79.0 ? 0.3 72.1 ? 0.4 87.6 ? 0.2 92.7 ? 0.2 72.3 ? 0.2 79.2 ? 0.2 90.3 ? 0.1 90.8 ? 0.1</cell></row><row><cell>ResNet50-512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">EPSHN (Xuan et al., 2020)</cell><cell>64.9</cell><cell>75.3</cell><cell>-</cell><cell>82.7</cell><cell>89.3</cell><cell>-</cell><cell>78.3</cell><cell>90.7</cell><cell>-</cell></row><row><cell cols="2">NormSoft (Zhai &amp; Wu, 2018)</cell><cell>61.3</cell><cell>73.9</cell><cell>-</cell><cell>84.2</cell><cell>90.4</cell><cell>-</cell><cell>78.2</cell><cell>90.6</cell><cell>-</cell></row><row><cell cols="2">DiVA (Milbich et al., 2020)</cell><cell>69.2</cell><cell>79.3</cell><cell>71.4</cell><cell>87.6</cell><cell>92.9</cell><cell>72.2</cell><cell>79.6</cell><cell>91.2</cell><cell>90.6</cell></row><row><cell>Multisimilarity+S2SD</cell><cell></cell><cell cols="9">69.2 ? 0.1 79.1 ? 0.2 71.4 ? 0.2 89.2 ? 0.2 93.8 ? 0.2 74.0 ? 0.2 80.8 ? 0.2 92.2 ? 0.2 90.5 ? 0.3</cell></row><row><cell>Margin+S2SD</cell><cell></cell><cell cols="9">68.8 ? 0.2 78.5 ? 0.2 72.3 ? 0.1 89.3 ? 0.2 93.8 ? 0.2 73.7 ? 0.3 81.0 ? 0.2 92.1 ? 0.2 91.1 ? 0.3</cell></row><row><cell>R-Margin+S2SD</cell><cell></cell><cell cols="9">70.1 ? 0.2 79.7 ? 0.2 71.6 ? 0.2 89.5 ? 0.2 93.9 ? 0.3 72.9 ? 0.3 80.0 ? 0.2 91.4 ? 0.2 90.8 ? 0.1</cell></row><row><cell>Inception-BN-512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DiVA (Milbich et al., 2020)</cell><cell>66.8</cell><cell>77.7</cell><cell>70.0</cell><cell>84.1</cell><cell>90.7</cell><cell>68.7</cell><cell>78.1</cell><cell>90.6</cell><cell>90.4</cell></row><row><cell>Multisimilarity+S2SD</cell><cell cols="2">66., 2019) 65.4</cell><cell>76.4</cell><cell>69.3</cell><cell>84.5</cell><cell>90.7</cell><cell>70.1</cell><cell>78.3</cell><cell>90.3</cell><cell>92.0</cell></row><row><cell cols="2">Multisimilarity  *  (Wang et al., 2019)</cell><cell>65.7</cell><cell>77.0</cell><cell>-</cell><cell>84.1</cell><cell>90.4</cell><cell>-</cell><cell>78.2</cell><cell>90.5</cell><cell>-</cell></row><row><cell>Multisimilarity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>7 ? 0.3 77.5 ? 0.3 70.5 ? 0.2 83.8 ? 0.3 90.3 ? 0.2 69.8 ? 0.3 78.5 ? 0.2 90.6 ? 0.2 90.6 ? 0.1 Margin+S2SD 66.8 ? 0.2 77.9 ? 0.2 69.9 ? 0.3 84.3 ? 0.2 90.7 ? 0.2 69.8 ? 0.2 78.4 ? 0.2 90.5 ? 0.2 90.4 ? 0.1 R-Margin+S2SD 67.4 ? 0.3 78.0 ? 0.4 70.3 ? 0.2 83.9 ? 0.3 90.3 ? 0.2 69.4 ? 0.2 78.1 ? 0.2 90.4 ? 0.3 90.3 ? 0.2 Softtriple* (Qian et al.* +S2SD 68.2 ? 0.3 79.1 ? 0.2 71.6 ? 0.2 86.3 ? 0.2 92.2 ? 0.2 72.0 ? 0.3 78.9 ? 0.2 90.8 ? 0.2 90.6 ? 0.1 Margin* +S2SD 68.3 ? 0.2 78.8 ? 0.2 71.2 ? 0.2 87.1 ? 0.2 92.4 ? 0.1 72.2 ? 0.2 79.1 ? 0.2 91.0 ? 0.3 90.4 ? 0.1 R-Margin* +S2SD 69.6 ? 0.3 79.6 ? 0.3 71.2 ? 0.1 86.6 ? 0.3 92.1 ? 0.3 70.9 ? 0.2 78.5 ? 0.1 90.5 ? 0.2 90.0 ? 0.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>-distance-metric-learning. pdf. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. 2020. URL https://arxiv.org/ abs/2002.05709. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference onIn this part, we report all relevant benchmark details omitted in the main document as well as further implementation details.</figDesc><table><row><cell cols="2">Supplementary: Simultaneous Similarity-based Self-Distillation for Deep</cell></row><row><cell cols="2">Metric Learning</cell></row><row><cell></cell><cell>Computer Vision and Pattern Recognition (CVPR), June</cell></row><row><cell>Chen, W., Chen, X., Zhang, J., and Huang, K. Beyond</cell><cell>2020.</cell></row><row><cell>triplet loss: a deep quadruplet network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. A. More Benchmark &amp; Implementation Details</cell><cell>arXiv:1503.02531, 2015. Hinton, G., Vinyals, O., and Dean, J. the knowledge in a neural network. arXiv preprint Distilling</cell></row><row><cell cols="2">Chen, Y., Wang, N., and Zhang, Z. rank: Accelerating deep metric learning via Dark-cross sample similarities transfer, 2018. A.1. Benchmarks URL https://www.aaai.org/ocs/index.php/ CUB200-2011 (Wah et al., 2011) contains 200 bird classes over 11,788 images, where the first and last 100 classes with Ioffe, S. and Szegedy, C. Batch normalization: Accelerat-ing deep network training by reducing internal covariate shift. pp. 448-456, 2015. URL http://jmlr.org/ proceedings/papers/v37/ioffe15.pdf. AAAI/AAAI18/paper/view/17147. 5864/5924 images are used for training and testing, respectively.</cell></row><row><cell cols="2">Jacob, P., Picard, D., Histace, A., and Klein, E. Metric CARS196 (Krause et al., 2013) contains 196 car classes and 16,185 images, where again the first and last 98 classes with Dai, B. and Wipf, D. P. Diagnosing and enhancing VAE learning with horde: High-order regularizer for deep em-8054/8131 images are used to create the training/testing split. models. CoRR, abs/1903.05789, 2019. URL http: beddings. In The IEEE Conference on Computer Vision Stanford Online Products (SOP) (Oh Song et al., 2016) is build around 22,634 product classes over 120,053 images and //arxiv.org/abs/1903.05789. and Pattern Recognition (CVPR), 2019. contains a provided split: 11318 selected classes with 59551 images are used for training, and 11316 classes with 60502</cell></row><row><cell>Deng, J., Guo, J., Xue, N., and Zafeiriou, S. Arcface: images for testing.</cell><cell></cell></row><row><cell>Additive angular margin loss for deep face recognition.</cell><cell>Jegou, H., Douze, M., and Schmid, C. Product quantization</cell></row><row><cell>In 2019 IEEE/CVF Conference on Computer Vision and</cell><cell>for nearest neighbor search. IEEE transactions on pattern</cell></row><row><cell>Pattern Recognition (CVPR), pp. 4685-4694, 2019. doi:</cell><cell>analysis and machine intelligence, 33(1):117-128, 2011.</cell></row><row><cell>10.1109/CVPR.2019.00482.</cell><cell></cell></row><row><cell></cell><cell>Johnson, J., Douze, M., and J?gou, H. Billion-scale similar-</cell></row><row><cell>Duan, Y., Zheng, W., Lin, X., Lu, J., and Zhou, J. Deep</cell><cell>ity search with gpus. arXiv preprint arXiv:1702.08734,</cell></row><row><cell>adversarial metric learning. In The IEEE Conference on</cell><cell>2017.</cell></row><row><cell>Computer Vision and Pattern Recognition (CVPR), June</cell><cell></cell></row><row><cell>2018.</cell><cell>Kim, S., Kim, D., Cho, M., and Kwak, S. Proxy anchor loss</cell></row><row><cell></cell><cell>for deep metric learning. In Proceedings of the IEEE/CVF</cell></row><row><cell>Furlanello, T., Lipton, Z. C., Tschannen, M., Itti, L.,</cell><cell>Conference on Computer Vision and Pattern Recognition</cell></row><row><cell>and Anandkumar, A. Born-again neural networks. In Dy, J. G. and Krause, A. (eds.), Proceedings of</cell><cell>(CVPR), June 2020.</cell></row><row><cell>the 35th International Conference on Machine Learn-</cell><cell>Kim, W., Goyal, B., Chawla, K., Lee, J., and Kwon, K.</cell></row><row><cell>ing, ICML 2018, Stockholmsm?ssan, Stockholm, Swe-</cell><cell>Attention-based ensemble for deep metric learning. In</cell></row><row><cell>den, July 10-15, 2018, volume 80 of Proceedings of</cell><cell>Proceedings of the European Conference on Computer</cell></row><row><cell>Machine Learning Research, pp. 1602-1611. PMLR,</cell><cell>Vision (ECCV), 2018.</cell></row><row><cell>2018. URL http://proceedings.mlr.press/</cell><cell></cell></row><row><cell>v80/furlanello18a.html.</cell><cell>Kingma, D. P. and Ba, J. Adam: A method for stochas-</cell></row><row><cell>Ge, W. Deep metric learning with hierarchical triplet loss. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 269-285, 2018.</cell><cell>tic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Represen-tations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:</cell></row><row><cell>Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality re-</cell><cell>//arxiv.org/abs/1412.6980.</cell></row><row><cell>duction by learning an invariant mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2006.</cell><cell>Krause, J., Stark, M., Deng, J., and Fei-Fei, L. 3d ob-ject representations for fine-grained categorization. In Proceedings of the IEEE International Conference on</cell></row><row><cell></cell><cell>Computer Vision Workshops, pp. 554-561, 2013.</cell></row><row><cell></cell><cell>Krogh, A. and Hertz, J. A. A simple weight decay can im-</cell></row><row><cell></cell><cell>prove generalization. In Advances in Neural Information</cell></row><row><cell></cell><cell>Processing Systems. 1992.</cell></row></table><note>Han, J., Zhao, T., and Zhang, C. Deep distillation metric learning. Proceedings of the ACM Multimedia Asia, 2019. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.Table 3 .</head><label>3</label><figDesc>With S2SD matching d = 64/128 to base dimensionalities d = 512/2048 (see ?5.3), runtime can be reduced by up to a magnitude. 98?0.00 2.71?0.00 4.35?0.00 7.38?0.01 13.83?0.02 27.21?0.17 Sample retrieval times for 250000 embeddings with varying base dimensionalities. ### Compute ref. sample relations and loss on ref. embedding space</figDesc><table><row><cell>Dimensionality d Runtime (s) base_smat = batch.mm(batch.T) 32 64 base_loss = self.base_criterion(batch, labels, ** kwargs) 128 256 ### Do global average pooling (and max. pool if wanted) avg_pre_batch = nn.AdaptiveAvgPool2d(1)(pre_batch).view(bs,-1) 512 avg_pre_batch += nn.AdaptiveMaxPool2d(1)(pre_batch).view(bs,-1) ### Computing MSDA loss (Targets &amp; Distillations) dist_losses, trgt_losses = [], [] for trgt_crit,trgt_net in zip(self.trgt_criteria,self.trgt_nets): trgt_batch = norm(trgt_net(avg_pre_batch),dim=-1) trgt_loss = trgt_crit(trgt_batch, labels, ** kwargs) trgt_smat = trgt_batch.mm(trgt_batch.T) base_trgt_dist = self.kl_div(base_smat, trgt_smat.detach()) trgt_losses.append(trgt_loss) dist_losses.append(base_trgt_dist) ### MSDA loss multi_dist_loss = (base_loss+torch.stack(trgt_losses).mean())/2. multi_dist_loss += self.dist_gamma * torch.stack(dist_losses).mean() ### Distillation of penultimate features -&gt; MSDFA src_feat_dist = 0 if self.it_count&gt;=self.it_before_feat_distill: n_avg_pre_batch = norm(avg_pre_batch, dim=-1).detach() avg_feat_smat = n_avg_pre_batch.mm(n_avg_pre_batch.T) src_feat_dist = self.kl_div(base_smat, avg_feat_smat.detach()) 1024 ### Total S2SD training objective total_loss = multi_distill_loss + self.dist_gamma * src_feat_dist self.it_count+=1 return total_loss 56 def kl_div(self, A, B, T=1): 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 48 49 50 51 52 53 54 55 57 log_p_A = F.log_softmax(A/self.T, dim=-1) 58 p_B = F.softmax(B/self.T, dim=-1) 1.54?0.00 1.23 59 kl_d = F.kl_div(log_p_A, p_B,reduction='sum') * T ** 2/A.size(0)</cell><cell>2048</cell></row></table><note>60 return kl_d Listing 1. PyTorch Implementation for S2SD.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Detailed Comparison of Recall@1 and NMI performances against well performing DML objectives examined in section 5.2. This is the complete version to table 1. All results are computed over 5-run averages. ( * ) For Margin Loss and SOP, we found ? = 0.9 to give better distillation results without notably influencing baseline performance. )63.09 ? 0.46 68.21 ? 0.33 79.86 ? 0.33 67.36 ? 0.34 78.43 ? 0.07 90.40 ? 0.03 + DSD 65.11 ? 0.18 69.65 ? 0.44 83.19 ? 0.18 69.28 ? 0.56 79.05 ? 0.12 90.52 ? 0.18 + DSDA 65.77 ? 0.55 69.85 ? 0.25 83.92 ? 0.08 69.95 ? 0.21 77.78 ? 0.15 90.29 ? 0.08 + MSD 66.13 ? 0.34 70.83 ? 0.27 83.63 ? 0.31 69.80 ? 0.36 79.26 ? 0.15 90.60 ? 0.10 + MSDA 66.14 ? 0.32 70.82 ? 0.18 84.31 ? 0.12 70.17 ? 0.30 78.04 ? 0.11 90.45 ? 0.05 + MSDF 67.58 ? 0.32 71, 47 ? 0.19 85.55 ? 0.23 71.68 ? 0.54 79.63 ? 0.14 90.70 ? 0.09 + MSDFA 67.21 ? 0.23 71.43 ? 0.25 86.45 ? 0.35 71.46 ? 0.24 78.82 ? 0.09 90.49 ? 0.06 R-Margin 64.93 ? 0.42 68.36 ? 0.32 82.37 ? 0.13 68.66 ? 0.47 77.58 ? 0.11 90.42 ? 0.03 + DSD 66.58 ? 0.08 70.03 ? 0.41 84.64 ? 0.16 70.87 ? 0.18 77.86 ? 0.10 90.50 ? 0.03 + DSDA 67.11 ? 0.43 70.39 ? 0.48 84.32 ? 0.36 70.85 ? 0.16 77.79 ? 0.11 90.37 ? 0.04 + MSD 66.81 ? 0.27 70.47 ? 0.16 85.01 ? 0.10 71.67 ? 0.40 78.00 ? 0.06 90.47 ? 0.04 + MSDA 67.31 ? 0.41 71.01 ? 0.24 85.34 ? 0.17 71.85 ? 0.20 77.93 ? 0.06 90.29 ? 0.08 + MSDF 68.12 ? 0.30 71.80 ? 0.33 85.78 ? 0.22 72.24 ? 0.31 78.57 ? 0.09 90.58 ? 0.02 + MSDFA 68.58 ? 0.26 71.64 ? 0.40 86.81 ? 0.35 71.48 ? 0.29 78.00 ? 0.11 90.41 ? 0.02 Multisimilarity 62.80 ? 0.70 68.55 ? 0.38 81.68 ? 0.19 69.43 ? 0.38 77.99 ? 0.09 90.00 ? 0.02 + DSD 65.57 ? 0.26 70.08 ? 0.33 83.51 ? 0.20 70.30 ? 0.05 78.23 ? 0.04 90.08 ? 0.04 + DSDA 66.60 ? 0.43 70.74 ? 0.40 84.42 ? 0.28 70.36 ? 0.34 77.92 ? 0.12 89.99 ? 0.04 + MSD 65.80 ? 0.16 70.53 ? 0.01 83.98 ? 0.10 71.34 ? 0.09 78.42 ? 0.09 90.09 ? 0.03 + MSDA 66.96 ? 0.36 70.77 ? 0.05 85.04 ? 0.14 71.09 ? 0.23 77.98 ? 0.05 90.02 ? 0.04 + MSDF 67.04 ? 0.29 71.87 ? 0.19 85.69 ? 0.19 72.77 ? 0.13 78.59 ? 0.08 90.09 ? 0.06 + MSDFA 67.68 ? 0.29 71.40 ? 0.21 85.89 ? 0.15 71.45 ? 0.26 78.07 ? 0.06 89.88 ? 0.10</figDesc><table><row><cell>Benchmarks?</cell><cell cols="2">CUB200-2011</cell><cell cols="2">CARS196</cell><cell>SOP</cell><cell></cell></row><row><cell>Approaches ?</cell><cell>R@1</cell><cell>NMI</cell><cell>R@1</cell><cell>NMI</cell><cell>R@1</cell><cell>NMI</cell></row><row><cell>Margin(</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Detailed Comparison of mAP@R (as used in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Bold denotes best results per objective and dataset. Bluebold denotes best performance per dataset. ) 32.63 ? 0.40 32.50 ? 0.28 46.90 ? 0.16 + DSD 33.85 ? 0.38 34.01 ? 0.39 47.39 ? 0.18 + MSD 34.79 ? 0.35 34.64 ? 0.31 48.17 ? 0.07 + MSDF 35.68 ? 0.29 35.26 ? 0.41 48.24 ? 0.10 + MSDFA 35.98 ? 0.23 35.98 ? 0.40 47.04 ? 0.26 R-Margin 33.38 ? 0.27 34.57 ? 0.30 46.02 ? 0.14 + DSD 34.46 ? 0.30 35.12 ? 0.22 46.20 ? 0.19 + MSD 35.11 ? 0.41 35.78 ? 0.40 46.59 ? 0.16 + MSDF 35.99 ? 0.36 37.32 ? 0.40 47.08 ? 0.17 + MSDFA 36.25 ? 0.37 37.67 ? 0.35 46.71 ? 0.16 Multisimilarity 30.92 ? 0.49 31.92 ? 0.44 46.23 ? 0.08 + DSD 33.20 ? 0.34 33.67 ? 0.27 46.21 ? 0.15 + MSD 34.00 ? 0.35 34.67 ? 0.26 46.45 ? 0.11 + MSDF 35.16 ? 0.32 35.52 ? 0.51 46.52 ? 0.17 + MSDFA 35.35 ? 0.24 35.13 ? 0.35 45.39 ? 0.28 Table 6. Additional ProxyAnchor (Kim et al., 2020) results with and without S2SD variants using the proposed, but different, default architecture in (Kim et al., 2020) to highlight that S2SD works equally well on already strong proxy-based objectives objectives with different architectural settings as well. ProxyAnchor 64.58 ? 0.23 68.95 ? 0.24 82.55 ? 0.41 69.49 ? 0.30 78.33 ? 0.08 90.24 ? 0.06 + DSD 65.50 ? 0.47 69.97 ? 0.55 83.52 ? 0.11 70.76 ? 0.17 78.33 ? 0.08 90.24 ? 0.06 + MSD 65.92 ? 0.28 69.88 ? 0.21 83.99 ? 0.33 70.95 ? 0.19 78.47 ? 0.03 90.29 ? 0.06 + MSDF 66.71 ? 0.12 70.60 ? 0.24 85.20 ? 0.09 71.19 ? 0.18 78.50 ? 0.04 90.31 ? 0.03 Table 7. Experiment: Comparison of concurrent self-distillation against standard 2-stage distillation. This table also shows that training without distillation (Joint) or training in high dimension while learning a detached low-dimensional embedding layer (Concur.) does not benefit performance notably. See fig. 3A. All results are computed over 5-run averages. Experiment: Methods of distillation between reference and target embedding spaces. See fig. 3C. Used Method: DSDA. All results are computed over 5-run averages. Mean 64.00 ? 0.55 Basic 62.70 ? 0.53 Table 10. Experiment: Structure of the secondary branch. More specifically, this table contains specific values used in fig. 3D. Used Method: DSDA. All results are computed over 5-run averages. Layers 66.51 ? 0.18 3 Layers 66.03 ? 0.29 4 Layers 65.76 ? 0.65 Linear 65.28 ? 0.48 Basic 62.70 ? 0.53 Table 11. Experiment: Different distillation hierarchies. See fig. 3E. Used Method: MSDA. All results are computed over 5-run averages. Straight 66.96 ? 0.36 Fully 65.58 ? 0.46 Stacked 65.21 ? 0.25</figDesc><table><row><cell cols="3">Benchmarks? CUB200-2011 CARS196</cell><cell>SOP</cell></row><row><cell>Approaches ?</cell><cell>mAP</cell><cell>mAP</cell><cell>mAP</cell></row><row><cell>Margin(</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Repository: github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Samarth Sinha (University of Toronto, Vector), Matthew McDermott (MIT) and Mengye Ren (University of Toronto, Vector) for insightful discussions and feedback on the paper draft. This work was</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transferring inductive biases through knowledge distillation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.00555" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00938</idno>
		<idno>doi: 10. 1109/cvpr.2019.00938</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2019.00938" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1612.00410</idno>
		<ptr target="http://arxiv.org/abs/1612.00410" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-level variational autoencoder: Learning disentangled representations from grouped observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curvilinear distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4223" to="4232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-referenced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno>abs/1811.07598</idno>
		<ptr target="http://arxiv.org/abs/1811.07598" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Data-efficient ranking distillation for image retrieval. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Laskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.05299" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Metadistiller: Network self-boosting via meta-learned top-down distillation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.12094" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sphereface</surname></persName>
		</author>
		<title level="m">Deep hypersphere embedding for face recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="103" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Diverse visual feature aggregation for deep metric learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bharadhwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diva</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.13458" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sharing matters for generalization in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3009620</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00674</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00674" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A metric learning reality check. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.08505" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00409</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2019.00409" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Diversified mutual learning for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rapid learning or feature reuse? towards understanding the effectiveness of MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgMkCEtPB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised knowledge distillation for few-shot learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.09785" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6550" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining interclass characteristics for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8000" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pads: Policy-adapted sampling for visual similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting training strategies and generalization performance in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/roth20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>III, H. D. and Singh, A.</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Divide and conquer the embedding space for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A tutorial on distance metric learning: Mathematical foundations, algorithms and software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno>abs/1812.05944</idno>
		<ptr target="http://arxiv.org/abs/1812.05944" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contrastive representation distillation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.10699" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="723" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning metrics from teachers: Compact networks for image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Yazici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<idno type="DOI">10.1109/CVPR.2019.00302</idno>
		<idno>doi: 10.1109/ cvpr.2019.00302</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2019.00302" />
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Regularizing classwise predictions via self-knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1612.03928</idno>
		<ptr target="http://arxiv.org/abs/1612.03928" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Making classification competitive for deep metric learning. CoRR, abs/1811.12649</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.12649" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/1905.08094</idno>
		<ptr target="http://arxiv.org/abs/1905.08094" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
