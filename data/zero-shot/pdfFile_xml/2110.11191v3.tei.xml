<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Degardin</surname></persName>
							<email>bruno.degardin@ubi.pt</email>
							<affiliation key="aff0">
								<orgName type="department">IT -Instituto de Telecomunica??es</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">DeepNeuronic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Neves</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NOVA LINCS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasco</forename><surname>Lopes</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NOVA LINCS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">DeepNeuronic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Brito</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">DeepNeuronic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Yaghoubi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">C4-Cloud Computing Competence Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Proen?a</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IT -Instituto de Telecomunica??es</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Synthesising the spatial and temporal dynamics of the human body skeleton remains a challenging task, not only in terms of the quality of the generated shapes, but also of their diversity, particularly to synthesise realistic body movements of a specific action (action conditioning). In this paper, we propose Kinetic-GAN, a novel architecture that leverages the benefits of Generative Adversarial Networks and Graph Convolutional Networks to synthesise the kinetics of the human body. The proposed adversarial architecture can condition up to 120 different actions over local and global body movements while improving sample quality and diversity through latent space disentanglement and stochastic variations. Our experiments were carried out in three well-known datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in terms of distribution quality metrics while having the ability to synthesise more than one order of magnitude regarding the number of different actions. Our code and models are publicly available at https://github.com/ DegardinBruno/Kinetic-GAN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human behaviour analysis through skeleton-based data has been widely investigated for decades. The advent of deep learning-based architectures increased its popularity even more, mainly due to the robustness of skeleton data in handling dynamic circumstances, appearance variations, and cluttered backgrounds. Over the last decade, the rise of data-driven approaches highly correlates performance with the scale of the learning set. Hence, generating high-quality synthetic human actions can address the problem of limited data. However, existing methods are still severely limited, particularly in conditioning desirable actions and considering the generation at the global movement level.</p><p>The existing skeleton-based human action synthesis algorithms are classified into two categories: autoregres- <ref type="figure">Figure 1</ref>: Synthetic set of actions generated by our graph convolutional generator trained on NTU RGB+D <ref type="bibr" target="#b31">[32]</ref> (first two rows) and NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref> (last row). Kinetic-GAN is able to generate up to 120 different actions even under global movement settings. See accompanying video. sive and generative approaches. Autoregressive approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref> are usually based on Recurrent Neural Networks (RNNs) and consider the skeleton data as a vector sequence to model an action from several seen frames. Despite the decent quality of the individual samples, autoregressive approaches have two disadvantages: (1) Extracting inherent structural body information using vectorized skeleton sequences is suboptimal. <ref type="bibr" target="#b1">(2)</ref> The use of LSTMs <ref type="bibr" target="#b15">[16]</ref>, GRU <ref type="bibr" target="#b0">[1]</ref>, and Seq2Seq <ref type="bibr" target="#b36">[37]</ref> potentially limits the scalability regarding the bidirectional temporal dependency, i.e., a future frame modifying the past ones, and the increased difficulty of a past frame conditioning a distant future frame. To solve the latter drawback, generative approaches such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> use the concept of Generative Adversarial Networks <ref type="bibr" target="#b11">[12]</ref> (GANs) to produce an entire body skeleton sequence from a latent space. However, this category also has The blue and red components belong to the generator and discriminator, respectively. First, both the Gaussian random noise z and the embedded action class representation y are concatenated and mapped to an intermediate latent space W, which is fed to the generator and upsampled spatially and temporally through each resolution level l. The discriminator receives a skeleton graph sequence G l together with its embedded class (channelwise concatenation) and learns to discriminate by downsampling spatially (coarsening) and temporally from level l to 0. some shortcomings: (1) most approaches still employ manually structured vector sequences to model skeleton data, and (2) they rely on autoregressive techniques and Gaussian processes to solve long-term relationships over the latent space, which greatly limits their scalability in action conditioning. This paper proposes a Generative Adversarial Graph Convolutional Network (Kinetic-GAN) to address the above-mentioned limitations. Our architecture leverages the benefits of GANs and Graph Convolutional Networks (GCNs), such that we generate conditioned human action sequences directly from the latent space while maintaining the long-term relationships between frames.</p><p>Inspired by the generalization of convolutions from images to graphs, we use spatiotemporal graph convolutions in the generator and discriminator to model skeleton data and exploit the skeleton's inherent graph structure, rather than manually structuring them as coordinate vector sequences. Also, the proposed approach improves the stateof-the-art in action conditioning, controlling up to 120 different actions (examples in <ref type="figure">Fig. 1</ref>), whereas previous methods could only control around 10 actions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>. Additionally, the generation of human actions directly from the latent space may inspire some future directions in human behaviour analysis, such as interpretable latent space directions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref> or style transfer between samples <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the overview of the proposed framework.</p><p>In summary, our main contributions are three-fold: 1) A new scalable Generative Adversarial Graph Convolutional Network architecture to synthesise human actions. 2) An architecture that can be extended to a conditional model, generating desirable actions up to 120 different classes. 3) We perform extensive experiments on three datasets, NTU RGB+D <ref type="bibr" target="#b31">[32]</ref>, NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref> and Human3.6M <ref type="bibr" target="#b17">[18]</ref>, where Kinetic-GAN exceeds the state-of-the-art performance by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human action synthesis regards the generation of understandable spatial and temporal kinematics of the human body skeleton. Current methods extract structural and dynamic patterns from either manually structured sequences or graph-based structures. Those representations are then used to synthesise actions via either autoregressive (to learn temporal dependencies) or generative models (to learn a probability distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Skeleton-based Behaviour Analysis</head><p>Body pose estimation is one of the auspicious cues in human behaviour analysis. This semantically rich and very descriptive representation of human dynamics attenuates appearance noises that RGB and depth data contain, driving the learning process solely over human behaviour.</p><p>Over the last decade, skeleton-based behaviour analysis has evolved from pseudo-images with CNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref> and sequence coordinate vectors with RNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>, to the solid improvements of GCNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47]</ref> which models skeleton data as a spatiotemporal graph, which better represents the embedded structural information. Still, most current methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref> employ manually structured sequence coordinate vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Autoregressive Models</head><p>Inspired by action prediction models, some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref> employ autoregressive algorithms to generate human actions from several seen frames. Fragkiadaki et al. <ref type="bibr" target="#b9">[10]</ref> proposed to incorporate an encoder-decoder network preand post-LSTM-units, capturing the temporal dependencies directly from the low-dimensional representation of the input skeleton. Zhou et al. <ref type="bibr" target="#b48">[49]</ref> presented a conditioned LSTM network, in which the generated data were conditioned at regular intervals of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Generative Models</head><p>Generative adversarial networks <ref type="bibr" target="#b11">[12]</ref> inspired Generative-based human action synthesis methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, whereas some approaches are yet attached to autoregressive techniques <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> or Gaussian processes <ref type="bibr" target="#b42">[43]</ref>, which limits their temporal flexibility, stochastic variation, and action conditioning. Kundu et al. <ref type="bibr" target="#b23">[24]</ref> proposed a hierarchical feature fusion based on an RNN auto-encoder architecture with a manually structured tree of limb connections. Yu et al. <ref type="bibr" target="#b44">[45]</ref> suggested using a graph convolutional network on top of an RNN to analyze the latent temporal dependencies. However, this architecture has a limitation regarding temporal length (50 frames) and action conditioning (10 actions). Yan et al. <ref type="bibr" target="#b42">[43]</ref> proposed a Gaussian process prior to the generator to analyze the dimensions of the latent space one by one (1024 dimensions). Despite reporting temporal long-term latent relations, its heavy computation on latent points correlation limits their diversity and action conditioning. This paper proposes a novel approach that generates long-term human actions without handcrafted procedures (sequence coordinate vectors or Gaussian process priors), enabling us to synthesise far more different actions with significantly more quality even with higher temporal lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Kinetic-GAN aims to improve the generation quality of controllable action samples while increasing intra-action diversity through stochastic variation and latent space disentanglement. This section presents the proposed approach by initially introducing the GCN adopted, followed by network description, and the discussion of the improvements caried out in the generator, discriminator, and adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Convolutional Networks Preliminaries</head><p>Similar to image modelling with GANs, the proposed solution employs a generator and discriminator to perform upsampling and downsampling on samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-frame Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Adjacency Matrix</head><p>Inter-frame Convolution Temporal Graph Convolution Spatial Graph Convolution <ref type="figure">Figure 3</ref>: Spatiotemporal graph convolution used in Kinetic-GAN. The spatial graph convolution takes as input a skeleton graph G l where its corresponding adjacency matrix A l handles the intra-frame (spatial) convolution (red dotted line) through the respective root node (red joint) neighbourhood. The temporal (inter-frame) convolution consists of a 1-dimensional convolution performed on the same positional joints across consecutive frames.</p><p>However, since we are modelling skeleton human actions, those operations can not be treated as conventional images with CNNs, which would lead to spatial and temporal distortions. Specifically, the conventional convolutional kernels will lose some structural information embedded in the skeleton data since adjacent joints in the pseudo-image are considered as connected joints. Hence, in each skeleton graph's resolution level l, the Kinetic-GAN employs graph convolutions to circumvent this issue due to their ability to exploit the skeleton's inherent graph structure. In graph convolutional networks (GCNs), a spatiotemporal graph G l = (V l , E l ) represents the skeleton data with N l joints and T l frames, where l = {1, ..., L}, and L is the number of levels of the skeleton graph resolution. Therefore, the feature map of the skeleton sequence is represented as X l ? R N l ?T l ?C , where C is the number of channels, representing the joints coordinates at resolution level L. A GCN consists of both spatial and temporal graph convolutions. Typically, in the spatial dimension, an adjacency matrix A l ? {0, 1} N l ?N l and the corresponding identity matrix I l define the intra-body joints connections, which are used to regulate the receptive fields of the convolution. Due to its high-level formulation, a partitioning strategy is defined to represent the neighbours set of each joint for constructing convolution operations, whereas A l and I l are dismantled into three partitions p (spatial configuration proposed by <ref type="bibr" target="#b43">[44]</ref>), so A l + I l = p A lp . For a single frame at resolution level l, the graph convolution can be visualized in <ref type="figure">Fig. 3</ref> (left), which is computed as:</p><formula xml:id="formula_0">S(X l ) = p i=1 ? ? 1 2 li A li ? ? 1 2 li X l W li ,<label>(1)</label></formula><p>NTU RGB+D Human3.6M <ref type="figure">Figure 4</ref>: Graph upsampling and downsampling paths. Graph pyramids applied by Kinetic-GAN, where the left side refers to NTU RGB+D <ref type="bibr" target="#b31">[32]</ref> and NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref>, and the right side shows the Human3.6M <ref type="bibr" target="#b17">[18]</ref>. The red nodes represents the root node of the respective dataset.</p><p>where the degree matrix ? ii lp = j (A ij lp ) normalizes the adjacency matrix A lp through the number of edges attached to each joint node. W lp denotes the stacked weight vectors for each partition group p from resolution level l.</p><p>Since multiple graph convolutional layers are used, different layers may contain multilevel semantic information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>, and simply using A l in Eq. 1 results in the same pre-defined spatial weight configuration to every layer. Hence, we also resort to a learnable weight matrix M l ? R N l ?N l (initialized as an all-one matrix) on each layer of both generator and discriminator. Thus, we can adaptively learn to optimize the spatial weight configuration of A l , and Eq. 1 becomes:</p><formula xml:id="formula_1">S(X l ) = p i=1 ? ? 1 2 li (A li M l )? ? 1 2 li X l W li ,<label>(2)</label></formula><p>Over the temporal axis, considering that consecutive frames define consecutive skeletons, one-dimensional kernels are used as the temporal graph convolution, which is applied after the spatial graph convolution ( <ref type="figure">Fig. 3</ref>). Finally, our spatiotemporal graph convolution at resolution l is given by convolving the positional features joint-wise as:</p><formula xml:id="formula_2">T S(X l ) = S(X l ) * w l ,<label>(3)</label></formula><p>where w l ? R 1?t?C is the temporal kernel at resolution l with t as the number of frames to be convolved in the kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Adversarial Graph Convolutional Network</head><p>Kinetic-GAN consists of a generative adversarial network composed of a graph convolutional-based generator and discriminator. As previously stated, we perform upsampling and downsampling on samples, respectively, in the generator and discriminator (see <ref type="figure">Fig. 4</ref>). Since each spatial graph resolution G l has its corresponding adjacency matrix A l , we compute Eq. 3 at any resolution level l of both upsampling and downsampling streams.</p><p>Typically, generative-based action synthesis methods feed a latent vector to the generator and apply autoregressive techniques <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref>, or Gaussian processes <ref type="bibr" target="#b42">[43]</ref> to capture the temporal relationships in a latent sequence. From a generative perspective, the multiplicative interactions across the latent sequence provoke the entangling of factors of variation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. We depart from this constraint and propose to synthesise human actions from a single latent point z as conventional image generators. However, considering the numerous variation factors in human actions, our generator starts with a non-linear mapping network to transform the latent code z ? Z to produce an intermediate latent space W. The rationale is to allow less entangled latent factors, as previously confirmed on image modelling <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>, and consequently increasing the linearity of factors of variation, where the generation of realistic actions becomes easier for the generator. This phenomenon is verified in the ablation study.</p><p>The proposed generator will gradually increase the resolution of a single intermediate latent point w over the spatial and temporal dimensions. First, spatial graph upsampling is computed by introducing new vertices and assigning the corresponding connected joints' average values. One-dimensional interpolation is performed over the temporal axis to increase the number of frames in the skeleton sequence. The discriminator will distinguish between the synthesised actions from the real ones by gradually coarsening the input skeleton's graph, where the vertices are removed, and corresponding neighbours are reconnected. Following the upsampling and downsampling streams from <ref type="figure">Fig. 4</ref>, the resolution is increased and decreased as:</p><formula xml:id="formula_3">X l+1 = T S U p(X l ) , X l?1 = Down T S(X l )</formula><p>(4) Additionally, we propose to use residual blocks performing a skip connection with solely a temporal graph convolution to learn the temporal mappings more efficiently, which improves training stability and reduces the appearance of artefacts (we verify this phenomenon in the ablation study). Hence, we define our generator and discriminator residual block as:</p><formula xml:id="formula_4">X l+1 = T S U p(X l ) + T U p(X l ) X l?1 = Down T S(X l ) + T (X l )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conditional Adversarial Training</head><p>The proposed architecture can be extended to a conditional model by feeding additional information about factors that we aim to condition. The generation of desired actions is imperative in human action synthesis; thus, we provide the embedded class information of the action y to both generator and discriminator. Specifically, in the generator, the embedded class representation y is concatenated to the prior input noise z before being mapped to the intermediate latent space W. The discriminator is fed with the channelwise concatenation of the skeleton with the embedded class representation y. In this paper, we rely on the WGAN-GP objective formulation <ref type="bibr" target="#b12">[13]</ref>, which is conditioned as:</p><formula xml:id="formula_5">min G max D Discriminator loss E x?Pr [D(x|y)] ? Ex ?Pg [D(x|y)] +? Gradient penalty Ex ?Px [( ?xD (x|y) 2 ? 1) 2 ],<label>(6)</label></formula><p>where P r is the data distribution and P g is the model distribution implicitly defined byx = G(z, y), z ? p(z) (the input z is sampled from a noise distribution p, which is then concatenated with the embedded action class representation y). Px is sampled uniformly along straight lines between pairs of points sampled from the data distribution P r and generator distribution P g . The loss weight ? for gradient penalty is set to 10 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Improving Quality and Diversity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Stochastic Variation</head><p>As previously stated, the current state-of-the-art methods are still attached to autoregressive techniques and Gaussian processes. Such procedures performed over the latent space reduce the ability of variation between samples. Aside from the non-linear mapping network to attain less entangled latent factors, we propose an individual stochastic variation to circumvent this issue without affecting the skeleton structure in the action sequence itself. Specifically, we add random noise to each joint independently in the generator after each spatiotemporal graph convolution, which can be learned by assigning weights to every channel. Our noise injection operation is introduced in the generator's residual block as:</p><formula xml:id="formula_6">X l+1 = T S U p(X l ) + T U p(X l ) + r l+1 w l+1 ,<label>(7)</label></formula><p>where r l+1 denotes the Gaussian random noise for each joint to be added at resolution level l + 1, and w l+1 is the respective weight vector for each channel. The rationale is to provide a second input to the generator, which is handled to produce variation between samples without forcing the generator to use earlier activations from the latent space to generate random noise. Moreover, each layer has a corresponding per-channel weight w l and receives a new random noise r l , giving the flexibility needed to adaptively learn to produce stochastic variation without compromising the skeleton structure of the action (this phenomenon is verified in the ablation study). Two human skeletons at the same frame from two action sequences (walking) generated by the same latent point. Zoom-in areas correspond to the respective coloured skeleton. c) Standard deviation over 100 different realizations from the respective frame, highlighting the skeleton parts affected by the noise. d) Cumulative standard deviation of each joint w.r.t. c). A cohesive behaviour is reproduced, where naturally the edges of limbs (hands, fingers, heels, feet) have a higher deviation than their parent joints. <ref type="figure" target="#fig_1">Fig. 5</ref> illustrates the effect of stochastic variation on a walking sequence generated from the same latent point with different noise realizations. 5 a) and b) show how noise affects different parts of the skeletons at the same frame, and c) highlights the standard deviation from 100 different realizations at the same frame, where most affected areas are the legs and arms (w.r.t. the walking action). The lower plot d) comprises a quantitative view of the standard deviation of each joint in c). It can be seen that the noise injector correctly learned the intrinsic natural variation of limbs since it affects more the edges, such as hands and feet than their corresponding parent joints (knees and elbows). Moreover, left limbs (blue) are slightly higher than their corresponding right ones (red) since the respective frame of the skeleton is moving the left leg forward, so naturally, its deviation will be higher than the right leg. For instance, in a throwing action, the arm that throws the object will always have more variation than the other arm. See accompanying video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Reducing Spatial Artefacts</head><p>Typically, the increased quality of generated samples leads to unpleasant artefacts, which is a phenomenon already well-known in image modelling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41]</ref>. Those are often the result of normalization methods over the generator, which improves training stability by eliminating covariate shift. Even so, the feature map's normalization omits any information concerning the individual feature's magnitude. It is assumed that the generator magnifies those magnitudes, and through the normalization process, it becomes unnoticed by the discriminator <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>This hypothesis is supported by the fact that removing these normalizations from our generator eradicated the appearance of artefacts. Still, it disabled our ability to manipulate human actions. Therefore, we found that reducing the number of layers that employ batch normalization led to better stability in action conditioning while completely removing artefacts. <ref type="figure">Fig. 6</ref> illustrates an artefact example in action synthesis.  <ref type="figure">Figure 6</ref>: Spatial artefacts in action synthesis. First row shows an occurrence of spatial artefacts on a jumping action sequence generated by configuration (A) in <ref type="table">Table 1</ref>. Similar to image modelling, the remaining information is unaffected as we can identify a human skeleton jumping. Second row shows a jumping action sequence with corrected artefacts by configuration (C) in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Truncation Trick on W</head><p>As described, the Kinetic-GAN samples z from N (0, 1) and maps it an intermediate representation W of that distribution, which is then fed to the generator. So naturally, ranges of low density (in the training data) are not well represented, becoming difficult to learn for the generator, resulting in an important open problem in generative algorithms.</p><p>As previously confirmed, sample quality can be improved from truncated <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>, or shrunken <ref type="bibr" target="#b22">[23]</ref> sampling spaces. Thus, despite some variation losses, we follow the same approach to balance sample quality and diversity. During inference time, we scale the deviation of a given intermediate latent point w from the centre mass of W as: </p><formula xml:id="formula_7">w = E z?Pz [f (z)] + ?(w ? E z?Pz [f (z)]),<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-View No truncation Cross-Subject</head><p>No truncation <ref type="figure">Figure 7</ref>: Improvements through the "truncation trick". The horizontal lines correspond to the respective FID obtained in <ref type="table">Table 1</ref> without truncations. All action sequences illustrated in this paper uses ? = 0.95.</p><p>where ? ? 1, f (?) denote our mapping network, and P z is the latent space distribution from 1000 points. Despite Brock et al. <ref type="bibr" target="#b2">[3]</ref> reporting that only a subset of networks is manageable to such truncations even when orthogonal regularization is used, truncation in our intermediate space W successfully works even without adjustments to the loss function. As shown in <ref type="figure">Fig. 7</ref>, we can increase the generation quality in both benchmarks of NTU RGB+D <ref type="bibr" target="#b31">[32]</ref>. However, for ? ? 0.9, the variation starts to decrease and, consequently, the FID starts to increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussion</head><p>In this section, the experiments are split into local and global movement settings concerning if the 3D locations were positionally normalized into 2D space or not. The ablation studies verify the efficacy of the proposed model's properties over global movement. Then, our best performing model is compared to the state-of-the-art approaches over four datasets: NTU RGB+D <ref type="bibr" target="#b31">[32]</ref> and NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref> (global movement) and Human3.6M <ref type="bibr" target="#b17">[18]</ref> and NTU-2D RGB+D <ref type="bibr" target="#b31">[32]</ref> (local movement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metrics</head><p>NTU RGB+D <ref type="bibr" target="#b31">[32]</ref>. This dataset is composed of 56,880 video samples with 60 action classes. 3D skeleton data from 40 volunteers are provided for each action sample, with 25 joints for each skeleton. Authors recommend two benchmarks: 1) cross-subject, where models are trained with 20 subjects and tested with the remaining ones; and 2) crossview, where models are trained with camera views 2 and 3 and tested on camera view 1. A curated version of this dataset is also used (2D joints locations, global to local movement normalization and selected samples) for a fair comparison in <ref type="table">Table 4</ref>, denoted by NTU-2D RGB+D.</p><p>NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref>. This dataset is an extended version of its predecessor, comprising 114,480 video samples with 120 action classes. Samples were captured with three camera views in 32 different setups and 106 volunteers with 25 body joints for each skeleton. Authors recommend two benchmarks: 1) cross-subject, where models are trained with 53 subjects and tested with the remaining ones, and 2) cross-setup, where models are trained from samples with even setup IDs and tested on odd setup IDs.</p><p>Human3.6M <ref type="bibr" target="#b17">[18]</ref>. This dataset is a more simplistic set with 2D human motions and 15 body joints for each skeleton. For a fair comparison between the state-of-the-art, the same pre-processing and settings as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref> were followed with corresponding 10 action classes.</p><p>Evaluation metrics. Two evaluation metrics are used for estimating the quality of synthetic samples. Similar to image modelling, we use the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b14">[15]</ref>, measuring the distance between the real data distribution and the synthesised one, considering the output activations of a specific layer from an InceptionV3 network <ref type="bibr" target="#b37">[38]</ref>. Additionally, we measure the Maximum Mean Discrepancy (MMD) between real and synthetic samples based on a two-sample test to measure the discrepancy of both distributions. The MMD over motion dynamics corresponds to the average MMD across each frame, denoted by MMD a , and the MMD over whole sequences are indicated as MMD s . For joints over 2D space, only the MMD is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Global Movement Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ablation study</head><p>Before diving into state-of-the-art comparisons, we first demonstrate experimentally that Kinetic-GAN properties improve sample quality considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FID</head><p>MMDa MMDs FID MMDa MMDs  <ref type="table">Table 1</ref>: Evaluating different generator designs. The FID and MMD scores (lower is better) between real and synthetic samples generated under global body movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU RGB+D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Subject Cross-View</head><p>Different generator designs. In <ref type="table">Table 1</ref>, we compare the FID and MMD for various generator architectures in both benchmarks of NTU RGB+D, evaluating each distribution under the respective settings of each method. While the baseline (CSGN <ref type="bibr" target="#b42">[43]</ref>) generates local movement in 3D space, we generate global movement and still exceed their performance by a significant margin. In CSGN <ref type="bibr" target="#b42">[43]</ref>, the application of Gaussian processes limits their generation diversity due to the multiplicative interactions across each di-mension of the latent space, which emulate the entangling of factors of variation. Additionally, the application of such processes has a high computational cost.</p><p>We start with our proposed configuration (A) with conditional sampling applying temporal skip connections and batch normalization over each generator layer. We then confirm the importance of residual blocks in our method by removing them from the generator (B), which clearly reduces sample quality, mainly due to training instability and, consequently, the appearance of artefacts. Since configuration (A) also generates occasional artefacts and eradicating batch normalization sacrificed our ability to generate desirable actions, we propose to regularize the use of batch normalization (C) by removing it when spatial upsampling is performed. This allows us to eliminate artefacts and have complete control to produce desirable actions while still improving sample quality compared to (A). We also introduce noise injection (D) that further improves the results by adding even more diversity. Finally, we also include our mapping network (E), where we distinctly overcome previous baselines and configurations due to the better disentangled latent space.  <ref type="table">Table 2</ref>: Importance of the mapping network. The number in method indicates the depth of the mapping network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mapping network effectiveness. The importance of the mapping network is presented in <ref type="table">Table 2</ref>, where we compare configuration (D) (not using a mapping network) with the increased number of layers in the mapping network. The cross-view benchmark required a deeper network than the cross-subject to attain the optimal performance. We justify this phenomenon due to the increasing number of different subjects in the data, which results in a more complex latent space representation. This indeed confirms the importance of the mapping network since naturally, if there is a greater number of different people, the variations will be higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Synthesising 120 different actions</head><p>Apart from our ablation studies in Section 4.2.1 which were produced considering global movement settings, <ref type="table">Table 3</ref> also presents the performance obtained by Kinetic-GAN over the NTU-120 RGB+D with 120 different actions, being currently the most extensive and challenging set with 3D joints annotations. Despite achieving a significantly better quality than previous methods, the ability to generate 120 different actions under global movement settings is a substantial improvement in comparison with previous state-ofthe-art where they could only generate under local movement settings, and just 10 different actions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>.  <ref type="table">Table 3</ref>: Global movement generation results on NTU-120 RGB+D with 120 different actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Local Movement Settings</head><p>Since most state-of-the-art methods were still limited to local movement settings, we also present the results under the same conditions for a fair comparison, where 3D locations were projected into 2D space and normalized positions. We follow the same settings as previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> and report the results on <ref type="table">Table 4</ref>. On both datasets, we verify the superiority of our model, where we achieve state-of-the-art performance with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMDa MMDs</head><p>Human3.6M  <ref type="table">Table 4</ref>: Local movement generation results. The MMD scores (lower is better) between real and synthetic samples generated over Human3.6M and NTU-2D RGB+D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Increasing temporal action length</head><p>In addition to generation quality, action conditioning (desirable actions) and local/global movement, modelling longterm human actions is also a significant concept in action synthesis. However, NTU RGB+D could not be employed for such a study since the action execution average is 64 frames, where the remaining frames (maximum 300) are all set to 0 for normalization purposes. For this reason, such normalization can disturb the learning process of an action synthesis algorithm. So, we performed long-term experiments over Human3.6M, <ref type="figure" target="#fig_4">Fig. 8</ref>. Considering that our model can perform bidirectional temporal dependency, mainly due to generating a whole human action sequence altogether, we can generate up to 1024 frames (34 seconds). Some autoregressive models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref> gradually freeze poses during the sequence as a result of losing temporal dependencies for such sequence lengths. It can also be observed that its generation quality will stabilize over lengths &gt; 256 frames, which can also be positively regarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Further Work</head><p>This paper introduced a novel Generative Adversarial Graph Convolutional Network for human action synthesis. By generating a human action in a holistic way directly from the latent space, we are able to better disentangle the variation factors through a mapping network, obtaining better representations in the latent space. Furthermore, the introduction of learnable noise injection modules facilitates the generation of variety without compromising the skeleton structure. As a result, we can generate up to 120 different complex actions, which, to the best of our knowledge, were particularly challenging for previous approaches under global movement settings. The proposed method was evaluated on three well known datasets (NTU RGB+D, NTU-120 RGB+D and Human3.6M), advancing the stateof-the-art performance metrics by a significant margin. Acknowledgements: This work was partially supported by the FCT/MEC through National Funds and by the FEDER-PT2020 Partnership Agreement under the Projects UIDB/50008/2020, POCI-01-0247-FEDER-033395, CENTRO-01-0247-FEDER-113023 -DeepNeuronic, operation Centro-01-0145-FEDER-000019 -C4 -Centro de Compet?ncias em Cloud Computing, co-funded by the European Regional Development Fund (ERDF) through the Programa Operacional Regional do Centro (Centro 2020), in the scope of the Sistema de Apoio a Investiga??o Cient?fica e Tecnol?gica -Programas Integrados de IC&amp;DT and NOVA LINCS under grant 'UIDB/04516/2020'. This research was also supported by 'FCT -Funda??o para a Ci?ncia e Tecnologia' through the research grant 'UI/BD/150765/2020' and '2020.04588.BD'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameters and training configurations</head><p>Datasets settings. For global movement experiments in Section 4.2 , which included NTU RGB+D <ref type="bibr" target="#b31">[32]</ref> and NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref> datasets, the temporal length of the skeleton sequences was normalized to t = 64 frames. The reason behind the chosen temporal length resides in the action execution average of the dataset (64 frames). Despite both datasets containing some annotation errors (some inaccurate 3D joints position), no sample filtering was applied. We confirm the superiority of our method in action conditioning by using every action class in both datasets (60 for NTU RGB+D and 120 for NTU-120 RGB+D). For local movement experiments in Section 4.3 , which included Human3.6M <ref type="bibr" target="#b17">[18]</ref> and NTU-2D RGB+D <ref type="bibr" target="#b31">[32]</ref> datasets, the same settings were applied as previous approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>. Specifically, the temporal length was normalized to t = 50 frames, the number of action classes used are 10, and both datasets were normalized from real/global movement to local movement, which facilitates the generation process. In Human3.6M <ref type="bibr" target="#b17">[18]</ref> dataset, the following action classes are used: sitting, sitting down, discussion, walking, greeting, direction, phoning, eating, smoking and posing. In NTU-2D RGB+D <ref type="bibr" target="#b31">[32]</ref> dataset, the following action classes are used: drinking water, jump up, make phone call, hand waving, standing up, wear jacket, sitting down, throw, cross hand in front and kicking something. Also, for a fair comparison, training samples from NTU-2D RGB+D <ref type="bibr" target="#b31">[32]</ref> were carefully selected from each class on NTU RGB+D <ref type="bibr" target="#b31">[32]</ref> similar to previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Training configurations. We train the networks using Adam <ref type="bibr" target="#b21">[22]</ref> optimizer with ? = 2 ? 10 ?4 , ? 1 = 0.5, ? 2 = 0.999 and = 10 ?8 for all datasets with a minibatch size of 32. Since we rely on the WGAN-GP loss <ref type="bibr" target="#b12">[13]</ref>, we set n critic = 5, which sets the number of iterations of the discriminator per generator iteration.</p><p>Upsampling and downsampling details. As illustrated in <ref type="figure">Fig. 3 (paper)</ref>, the spatial resolution of the skeleton is increased from the intermediate latent point as 1 ? 5 ? 11 ? 25 joints for the NTU RGB+D <ref type="bibr" target="#b31">[32]</ref>, NTU-2D RGB+D <ref type="bibr" target="#b31">[32]</ref> and NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref> datasets. For the Human3.6M <ref type="bibr" target="#b17">[18]</ref> dataset the spatial resolution is increased as 1 ? 2 ? 7 ? 15 joints. In all datasets, the temporal resolution is increased by doubling t/16 until reaching the dataset's temporal length t. The same resolutions reversed are applied for the downsampling paths in the discriminator.</p><p>Mapping network structure. Our non-linear mapping network comprises fully connected layers with 512 as the dimensionality of the input and output activations. As demonstrated in <ref type="table">Table 2</ref> , the increasing number of different subjects in the training data results in a more complex latent representation requiring a deeper mapping network. For this reason, we set 6 layers for the Human3.6M <ref type="bibr" target="#b17">[18]</ref>, and 8 layers for the NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref> dataset. NTU RGB+D and NTU-2D RGB+D <ref type="bibr" target="#b31">[32]</ref> datasets follow the same settings as studied in <ref type="table">Table 2</ref> .</p><p>Noise injection details. The noise injector described in Section 3.4.1 samples a random noise r l using N (0, 1). Each joint at resolution level l has a respective weight to each channel and receives a different noise added channelwise. This operation is applied to every generator's layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action complexity</head><p>We include several action samples synthesised by our graph convolutional generator that demonstrate various aspects related to action complexity (see also accompanying video). Apart from the ability to generate up to 120 different action classes, we are able to generate global (real) body movement in 3D space, which, to the best of our knowledge, such complex actions under global movement settings had proven to be uncharted territory for previous methods. <ref type="figure">Figure 10</ref> shows different action examples illustrating the detail and expressiveness achievable using our method in NTU RGB+D <ref type="bibr" target="#b31">[32]</ref>. In <ref type="figure">Figure 9</ref>, we demonstrate the ability to generate desired actions among 120 different classes from NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref>. <ref type="figure">Figure 9</ref>: Synthetic set of actions generated by our graph convolutional generator trained on NTU-120 RGB+D <ref type="bibr" target="#b25">[26]</ref>. <ref type="figure">Figure 10</ref>: Synthetic set of actions generated by our graph convolutional generator trained on NTU RGB+D <ref type="bibr" target="#b31">[32]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Cohesive view of the proposed Kinetic-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Examples of stochastic variation. a) and b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Increasing action length on Human3.6M.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04942</idno>
		<title level="m">Isolating sources of disentanglement in variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-scale spatial temporal graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation via generative entangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5474</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep generative adversarial compression artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4826" to="4835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Generative adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A recurrent variational autoencoder for human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Yearsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning of human actions as trajectories in pose embedding manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Phani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1459" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A largescale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marchesi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00082</idno>
		<title level="m">Megapixel size image creation using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9243" to="9252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Twostream adaptive graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3215</idno>
		<title level="m">Sequence to sequence learning with neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6038" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of interpretable directions in the gan latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9786" to="9796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning diverse stochastic human-action generators by learning smooth latent transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12281" to="12288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional sequence generation for skeletonbased action synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4394" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structure-aware human-action generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context aware graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14333" to="14342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Auto-conditioned recurrent networks for extended complex human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjiu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
