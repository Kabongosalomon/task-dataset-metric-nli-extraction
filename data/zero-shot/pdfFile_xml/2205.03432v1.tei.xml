<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANSFORMER-BASED MULTI-ASPECT MULTI-GRANULARITY NON-NATIVE ENGLISH SPEAKER PRONUNCIATION ASSESSMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
							<email>yuangong@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">MIT CSAIL</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Chen</surname></persName>
							<email>chenziyi253@pingan.com.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>94306</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iek-Heng</forename><surname>Chu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>94306</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chang</surname></persName>
							<email>changpeng805@pingan.com.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>94306</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">MIT CSAIL</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRANSFORMER-BASED MULTI-ASPECT MULTI-GRANULARITY NON-NATIVE ENGLISH SPEAKER PRONUNCIATION ASSESSMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Pronunciation assessment, Transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic pronunciation assessment is an important technology to help self-directed language learners. While pronunciation quality has multiple aspects including accuracy, fluency, completeness, and prosody, previous efforts typically only model one aspect (e.g., accuracy) at one granularity (e.g., at the phoneme-level). In this work, we explore modeling multi-aspect pronunciation assessment at multiple granularities. Specifically, we train a Goodness Of Pronunciation feature-based Transformer (GOPT) with multi-task learning. Experiments show that GOPT achieves the best results on speechocean762 with a public automatic speech recognition (ASR) acoustic model trained on Librispeech. Code at https://github.com/YuanGongND/gopt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Computer assisted pronunciation training (CAPT) is an important technology for self-directed language learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, which facilitates non-native (L2) speakers to learn foreign spoken (L1) languages. Compared with conventional classes, CAPT is more economical and convenient, and also allows language learners to receive immediate feedback on their pronunciation. Due to its usefulness, CAPT has been extensively studied, with the majority of these efforts focusing on scoring phoneme-level pronunciation quality (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>). Overall pronunciation quality includes many other aspects such as word-and utterancelevel fluency, prosody, stress, etc., which have been typically modeled separately (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>). However, phoneme-, word-, and utterance-level scores of accuracy, fluency, prosody, and stress are potentially correlated, therefore modeling them jointly instead of separately may allow a machine learning model to learn a more comprehensive representation and in turn improve its performance. In reality, it is also desirable to have a single model that can assess multiple aspects of pronunciation simultaneously.</p><p>As a step in this direction, in this paper we propose a new pronunciation assessment model, named GOPT, based on Goodness of Pronunciation (GOP) features and a Transformer self-attention architecture <ref type="bibr" target="#b16">[17]</ref>. We use the open-source spee-chocean762 dataset <ref type="bibr" target="#b17">[18]</ref> that contains one phoneme-level, three word-level, and five utterance-level labels including accuracy, prosody, and fluency and apply multi-aspect multigrained supervision for GOPT training. This not only enables GOPT to measure multiple aspects of pronunciation quality, but also boosts its performance for each assessment task. In addition, the Transformer architecture captures the contextual information between phonemes and words of an utterance. As a consequence, GOPT noticeably outperforms previous methods on the speechocean762 benchmark for both phoneme-and utterance-level assessment tasks (there is no previous work reporting word-level scores). To our knowledge, this is the first work studying multi-aspect L2 speaker pronunciation assessment in a multi-granularity fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>As mentioned, CAPT has been extensively studied with a long history. One major focus of this area is automatic mispronunciation detection, where GOP <ref type="bibr" target="#b3">[4]</ref> and its variants (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>) are dominant methods. To capture the correlation between phonemes and words of an utterance, self-attention based models such as Transformer <ref type="bibr" target="#b16">[17]</ref> have been added on top of GOP features for score modeling to improve performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. There are also some non-GOP based methods such as a wav2vec2-based method <ref type="bibr" target="#b19">[20]</ref> and a deep feature based method <ref type="bibr" target="#b20">[21]</ref> where transfer learning is usually needed due to the limited L2 training material.</p><p>Conversely, automatic assessment of other aspects of pronunciation quality are usually modeled independently, e.g., fluency <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, prosody <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, intonation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. There are only a few previous efforts on multi-granularity pronunciation assessment <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. In these works, however, only a single score is considered for each granularity. In addition, the hierarchical architecture in <ref type="bibr" target="#b18">[19]</ref> requires a relatively sophisticated training scheme to optimize.</p><p>To the best of our knowledge, this paper is the first to simultaneously consider multiple pronunciation quality aspects (accuracy, fluency, prosody, etc) along with multiple granularities (phoneme, word, utterance). In addition, we show that a BERT-style <ref type="bibr" target="#b22">[23]</ref> non-hierarchical standard Transformer architecture can perform well on most assessment tasks. Unlike many previous efforts using non-public datasets or acoustic models, in this work, we intentionally use a public acoustic  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GOODNESS OF PRONUNCIATION TRANSFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Speechocean762 Dataset</head><p>Speechocean762 <ref type="bibr" target="#b17">[18]</ref> is a free open-source dataset designed for pronunciation assessment, consisting of a total of 5,000 English utterances collected from 250 non-native speakers. One major advantage of speechocean762 is that it provides rich label information. Specifically, for each utterance, it provides five utterance-level aspect scores: accuracy, fluency, completeness, prosody, and total score (ranging from 0-10). For each word, it provides three word-level aspect scores: accuracy, stress, and total score (ranging from 0-10). It also provides an accuracy score for each phoneme (ranging from 0-2). Each score is annotated by five experts. Thus, it provides a total of 8 labels for different granularities and pronunciation quality aspects. However, the rich annotation has not been fully utilized by previous work. We re-scale utterance and word-level scores to 0-2, making them on the same scale as the phoneme scores. The training set consists of 2,500 utterances, 15,849 words, and 47076 phones; the test set consists of 2,500 utterances, 15,967 words, and 47,369 phones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GOPT Architecture Overview</head><p>An overview of the GOPT architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For the pronunciation assessment task, the canonical transcription is known. We first input the audio and corresponding canonical transcription to the acoustic module to get a sequence of frame-level phonetic posterior-probabilities, which are then force-aligned at the phoneme-level and converted to 84-dimensional goodness of pronunciation (GOP) features (discussed in Section 3.3). The GOP feature is then projected to 24-dimensions with a dense layer. In parallel, we generate a sequence of canonical phoneme embeddings (also at the phoneme-level) by first converting each canonical phoneme to a one-hot encoding and then projecting it to the same 24dimensions as the projected GOP feature. The reason for using a canonical phoneme embedding is because different phonemes have different characteristics and thus the canonical phoneme provides useful information to the Transformer model <ref type="bibr" target="#b20">[21]</ref>. We then add the projected GOP feature, canonical phoneme embedding, and a 24-dimensional trainable positional embedding together and input it to the Transformer encoder. For simplicity, we intentionally follow the original Transformer encoder architecture <ref type="bibr" target="#b16">[17]</ref> as close as possible but scale it down to 3 layers and an embedding dimension of 24.</p><p>Unlike previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref> that use a hierarchical architecture to get utterance level representations, we prepend a set of five trainable [cls] tokens to the phoneme-level input sequence in a similar way as BERT <ref type="bibr" target="#b22">[23]</ref>, each corresponding to one utterance aspect label, and use the output of the Transformer encoder of these [cls] aspect tokens as the corresponding utterance-level representations. The reason why this regime works is that the Transformer can learn the correlation between the utterance-level tokens and phoneme-level tokens through the attention mechanism.</p><p>During training we apply multi-task learning to the model. Specifically, we use one regression head for each phoneme, word, and utterance label (eight in total). Each regression head is a 24 ? 1 dense layer with layer normalization. Utterance-level regression heads are added on top of the output of the Transformer of the corresponding utterance [cls] tokens. Phoneme-and word-level regression heads are added on top of the Transformer output of each corresponding phoneme. We propagate the word score to each of its phonemes during training and average the output of phonemes that belong to the word in inference. We use mean squared error (MSE) loss for each assessment task. Since we normalize the scores to the same scale, for simplicity, we first average the losses of each granularity and then sum them up with the same weight, i.e.,  <ref type="table">Table 1</ref>. Comparing the performance of various pronunciation assessment tasks between GOPT and baseline models. GOPT (PAII-A) depends on a different acoustic model so its results (shown in grey) cannot be directly compared with other models. and L word are averaged utterance and word level losses of five utterance-level labels and three word-level labels, respectively; L phoneme is the phoneme loss. The entire network (except the acoustic model) is trained end-to-end.</p><formula xml:id="formula_0">L = L utterance + L word + L phoneme , where L utterance Phoneme Score Word Score (PCC) Utterance Score (PCC) Model MSE ? PCC ? Accuracy ? Stress ? Total ? Accuracy ? Completeness ? Fluency ? Prosodic ? Total ? RF [18] 0.130 0.440 - - - - - - - - SVR [18] 0.160 0.450 - - - - - - - Lin et.al [21] - - - - - - - - - 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Acoustic Model and GOP Feature</head><p>For our main experiment we use a public ASR acoustic model 1 trained with Librispeech <ref type="bibr" target="#b23">[24]</ref> 960-hour data. The model is based on the factorized time-delay neural network (TDNN-F) and trained with the Kaldi Librispeech S5 recipe.</p><p>Acoustic model trained on both L1 and L2 speech generates better alignment for L2 speech and may output better GOP features <ref type="bibr" target="#b24">[25]</ref>. To explore if GOPT works with different acoustic models, we also test with two PAII internal acoustic models PAII-A and PAII-B, both are also TDNN-F models. PAII-A is trained with 452 hours L1 TED-LIUM 3 <ref type="bibr" target="#b25">[26]</ref> data and 1,696 hours of L2 data collected from 5,994 nonnative speakers; PAII-B is trained with 995 hours of L1 data (from WSJ <ref type="bibr" target="#b26">[27]</ref>, TED-LIUM 3 <ref type="bibr" target="#b25">[26]</ref>, and Librispeech <ref type="bibr" target="#b23">[24]</ref>) and 6,591 hours of L2 data from 672k non-native speakers.</p><p>In this work, we use the log phone posterior (LPP) and log posterior ratio (LPR) defined in <ref type="bibr" target="#b7">[8]</ref> as GOP features. Specifically, the LPP of a phone p is defined as follows:</p><formula xml:id="formula_1">LP P (p) ? 1 t e ? t s + 1 te t=ts log p(p|o t ) (1) p(p|o t ) = s?p p(s|o t )<label>(2)</label></formula><p>where t s and t e are the start and end frame indexes; o t is the input observation of the frame t, s is the state belonging to the phone p. LPR of a phone p j versus p i is defined as:</p><formula xml:id="formula_2">LP R(p j |p i ) = log p(p j |o; t s , t e ) ? log p(p i |o; t s , t e ) (3)</formula><p>The Librispeech acoustic model we use has a total of 42 pure phones, thus the GOP feature of phone p can be defined as a 84-dimensional vector as follows:</p><p>[LP P (p 1 )..., LP P (p 42 ), LP R(p 1 |p)..., LP R(p 42 |p)] (4) 1 https://kaldi-asr.org/models/m13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>For all experiments, we train the model with an Adam optimizer, an initial learning rate of 1e-3, a batch size of 25, and MSE loss for 100 epochs using the official speechocean762 training set, and evaluate on the official test set. The learning rate is cut in half every five epochs after the 20th epoch, and the result of the last epoch is reported. We repeat each experiment five times with different random seeds and report the mean and standard deviation of the results. Since the speechocean762 labels are imbalanced (biased towards high scores), we use the Pearson correlation coefficient (PCC) as the main evaluation metric but also report MSE of the phoneme accuracy score to make a comparison with previous work. Note that while we re-scale the utterance and word level scores, PCCs and phoneme-level MSE are not impacted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>We compare the following six models: 1) Random forest regression (RF) model implemented in the code repository of <ref type="bibr" target="#b17">[18]</ref>; 2) Support vector regressor (SVR) based model in <ref type="bibr" target="#b17">[18]</ref>; 3) Deep feature and transfer learning-based model presented in <ref type="bibr" target="#b20">[21]</ref>; 4) An LSTM based model implemented by us. To make a fair comparison, the LSTM model has the same depth and embedding dimension as the GOPT model and is trained with the same setting. The output of the last token is used as the utterance representation and, as with GOPT, the word score is propagated to its phones; 5) The proposed GOPT model with the Librispeech acoustic model. 6) The proposed GOPT model with the PAII-A acoustic model. It is worth mentioning that models 1-5 are all based on acoustic models trained with the same Librispeech data, and models 1,2,4,5, and 6 use the same GOP features (model 3 does not use GOP features but deep transfer learning). Therefore, we make a fair comparison and the performance difference is not due to the acoustic model and GOP features. We show the results in <ref type="table">Table 1</ref>. The key findings are as follows: First, the proposed GOPT model can perform well on most assessment tasks except word stress score and sentence completeness score assessment, demonstrating that it  <ref type="table">Table 2</ref>. The ablation results, we only show the PCC of phoneme, word, and utterance total scores due to space limitation. * denotes the setting used in the base GOPT model. is possible to have a single model for multi-aspect and multigranularity pronunciation assessment. Specifically, the GOPT achieves 0.085 MSE and 0.612 PCC for the phoneme accuracy score assessment, noticeably outperforming the models in <ref type="bibr" target="#b17">[18]</ref>; GOPT achieves 0.742 PCC for the utterance-level score assessment, noticeably outperforming the model in <ref type="bibr" target="#b20">[21]</ref> which uses more sophisticated features than GOP. We hypothesize that the poor utterance completeness assessment performance is due to the highly imbalanced distribution of the completeness score in the training data. Second, the multitask learning scheme can be also applied to an LSTM, which achieves similar results for utterance assessment with GOPT. However, the performance of the LSTM for phoneme-level and word-level assessment are worse than the GOPT, demonstrating that the Transformer architecture is better at modeling fine-grained pronunciation units. Third, using the PAII-A acoustic model trained on both L1 and L2 speech can further boost the phoneme and word assessment performance by around 10%, but the utterance-level performance is worse than just using the Librispeech acoustic model. We also evaluate GOPT with PAII-B acoustic model, it leads to similar results with GOPT with PAII-A acoustic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablations</head><p>We conduct a set of ablation studies to show the performance impact of various factors. We set the GOPT model mentioned in Section 3 with three Transformer layers, embedding dimension of 24, canonical phoneme embedding, and trained with  all phoneme, word, and utterance assessment tasks as the base GOPT model, and then change one factor at a time to observe the performance change.</p><p>We show the results in <ref type="table">Table 2</ref>. First, we see that the GOPT trained with multi-task learning achieves better results than any single-task learning model, demonstrating that multi-task learning not only allows the model to conduct multi-aspect and multi-granularity pronunciation assessment simultaneously, but also improves the performance of each individual task. Second, we see that the canonical phoneme embedding is crucial to the performance as the model trained without it performs much worse for all tasks. However, it is worth mentioning that canonical phoneme embedding is not the reason why GOPT outperforms previous methods since canonical phoneme embedding is also used in <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, each phoneme has a separate classifier, which serves a similar function as a canonical phoneme embedding. Third, we explore the performance impact of the size of GOPT model, and see that increasing either the width or depth of the network cannot further improve the performance, indicating that a small model is preferred with the relatively small dataset. Further, although the GOP feature is 84-dimensional, we show that an embedding size of 24 is sufficient to represent pronunciation quality with a Transformer.</p><p>Finally, in <ref type="table" target="#tab_4">Table 3</ref>, we compare the phoneme assessment performance between the SVR <ref type="bibr" target="#b17">[18]</ref> model and the proposed GOPT model with various acoustic models. We show that the proposed GOPT consistently leads to a significant performance improvement regardless of the acoustic model, demonstrating that the GOPT is model agnostic and can be used with different acoustic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we present the Transformer-based multi-aspect multi-granularity pronunciation assessment model GOPT. We show that with the multi-task learning scheme, a single GOPT model can conduct multiple pronunciation tasks simultaneously, and its performance is better than the same model trained with a single task. Experiments show the GOPT can noticeably outperform previous methods on speechocean762.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the proposed GOPT architecture with a sample utterance "Its Name", actual utterances used are longer. model and dataset for our main experiments (which achieves state-of-the-art results) for easy reproduction and comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Total Score Classification Tokens Positional Embedding Canonical Phoneme Embedding GoP Features Input Phoneme Scores Word Scores</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">Word [Its] Accuracy/Stress/Word [Name] Accuracy/Stress/Total Score</cell></row><row><cell></cell><cell cols="5">Utterance Scores</cell><cell></cell><cell cols="2">Phone</cell><cell cols="2">Phone</cell><cell cols="2">Phone</cell><cell cols="2">Phone</cell><cell cols="2">Phone</cell><cell>Phone</cell></row><row><cell>Accuracy Score</cell><cell>Fluency Score</cell><cell cols="3">Score Completeness</cell><cell>Score Prosodic</cell><cell>Score Total</cell><cell cols="2">[IH] Score</cell><cell cols="2">[T] Score</cell><cell cols="2">[S] Score</cell><cell cols="2">[N] Score</cell><cell cols="2">[EY] Score</cell><cell>[M] Score</cell></row><row><cell>Utterance Accuracy</cell><cell cols="2">Utterance Fluency</cell><cell>Utterance Complete</cell><cell cols="2">Utterance Prosodic</cell><cell>Utterance Total</cell><cell>Phn Head</cell><cell>Word Head</cell><cell>Phn Head</cell><cell>Word Head</cell><cell>Phn Head</cell><cell>Word Head</cell><cell>Phn Head</cell><cell>Word Head</cell><cell>Phn Head</cell><cell>Word Head</cell><cell>Phn Head</cell><cell>Word Head</cell></row><row><cell>Head</cell><cell>Head</cell><cell></cell><cell>Head</cell><cell></cell><cell>Head</cell><cell>Head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Transformer Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pos[0]</cell><cell>Pos[1]</cell><cell></cell><cell>Pos[2]</cell><cell></cell><cell>Pos[3]</cell><cell>Pos[4]</cell><cell cols="2">Pos[5]</cell><cell cols="2">Pos[6]</cell><cell cols="2">Pos[7]</cell><cell cols="2">Pos[8]</cell><cell cols="3">Pos[9] Pos[10]</cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell cols="2">+</cell><cell cols="2">+</cell><cell cols="2">+</cell><cell>+</cell><cell>(24-dim)</cell></row><row><cell>CLS</cell><cell>CLS</cell><cell></cell><cell>CLS</cell><cell></cell><cell>CLS</cell><cell>CLS</cell><cell cols="2">Phn[IH]</cell><cell cols="2">Phn[T]</cell><cell cols="2">Phn[S]</cell><cell cols="2">Phn[N]</cell><cell cols="2">Phn[EY]</cell><cell>Phn[M]</cell><cell>Phone Projection Layer</cell></row><row><cell>[utt-acc]</cell><cell>[utt-flu]</cell><cell></cell><cell>[utt-comp]</cell><cell></cell><cell>[utt-pros]</cell><cell>[utt-total]</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell cols="4">+ GoP Projection Layer +</cell><cell cols="2">+</cell><cell>+</cell><cell>One-hot Encoding</cell></row><row><cell>arXiv:2205.03432v1 [cs.SD] 6 May 2022</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GoP[IH]</cell><cell cols="2">GoP[T]</cell><cell cols="7">Acoustic Model GoP[S] GoP[N] GoP[EY] GoP[M]</cell><cell>(84-dim)</cell><cell>Canonical Transcription IH T S | N EY M (Its Name)</cell><cell>Audio</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PCC ? MSE ? PCC ? MSE ? PCC ?</figDesc><table><row><cell>Scoring</cell><cell></cell><cell></cell><cell cols="2">Acoustic Model</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Librispeech</cell><cell cols="2">PAII-A</cell><cell cols="2">PAII-B</cell></row><row><cell cols="2">MSE ? SVR 0.160</cell><cell>0.450</cell><cell>0.118</cell><cell>0.538</cell><cell>0.115</cell><cell>0.561</cell></row><row><cell>GOPT</cell><cell>0.085 ?0.001</cell><cell>0.612 ?0.003</cell><cell>0.069 ?0.000</cell><cell>0.679 ?0.001</cell><cell>0.071 ?0.001</cell><cell>0.662 ?0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparing the phoneme assessment performance between the SVR based<ref type="bibr" target="#b17">[18]</ref> model and proposed GOPT model with various acoustic models.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An overview of spoken language technology for education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic scoring of non-native spontaneous speech in tests of spoken english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic error detection in pronunciation training: Where we are and where we need to go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Silke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Automatic Detection on Errors in Pronunciation Training</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Phone-level pronunciation scoring and assessment for interactive language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic mispronunciation detection for mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Analysis and utilization of mllr speaker adaptation technique for learners&apos; pronunciation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved approaches of modeling and detecting error patterns with empirical analysis for computer-aided pronunciation training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yow-Bang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved mispronunciation detection with deep neural network trained acoustic models and transfer learning based logistic regression classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Context-aware goodness of pronunciation for computer-assisted pronunciation training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Using non-native error patterns to improve pronunciation verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Joost Van Doremalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cucchiarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantitative assessment of second language learners&apos; fluency: an automatic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catia</forename><surname>Cucchiarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmer</forename><surname>Strik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lwj</forename><surname>Boves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICSLP</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quantitative assessment of second language learners&apos; fluency by means of automatic speech recognition technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catia</forename><surname>Cucchiarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmer</forename><surname>Strik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic prosodic analysis for computer aided pronunciation teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul Christopher</forename><surname>Bagshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic syllable stress detection using prosodic features for pronunciation evaluation of language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tepperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic intonation assessment for computer aided language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nestor</forename><surname>Becerra Yoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiram</forename><surname>Vivanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intonation classification for l2 english speech using multi-distribution deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Speech and Language</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">speechocean762: An open-source non-native english speech corpus for pronunciation assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic scoring at multi-granularity for l2 pronunciation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghuai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Explore wav2vec 2.0 for mispronunciation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueteng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghuai</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep feature transfer learning for automatic pronunciation assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghuai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic pronunciation scoring of words and sentences independent from the non-native&apos;s first language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Cincarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Investigating the role of l1 in automatic pronunciation evaluation of l2 speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Grabek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPECOM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
