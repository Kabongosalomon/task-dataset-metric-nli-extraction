<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pedestrian Detection by Exemplar-Guided Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zebin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanglin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Life Fellow, IEEE</roleName><forename type="first">David</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Lu</surname></persName>
						</author>
						<title level="a" type="main">Pedestrian Detection by Exemplar-Guided Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-pedestrian detection, contrastive learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typical methods for pedestrian detection focus on either tackling mutual occlusions between crowded pedestrians, or dealing with the various scales of pedestrians. Detecting pedestrians with substantial appearance diversities such as different pedestrian silhouettes, different viewpoints or different dressing, remains a crucial challenge. Instead of learning each of these diverse pedestrian appearance features individually as most existing methods do, we propose to perform contrastive learning to guide the feature learning in such a way that the semantic distance between pedestrians with different appearances in the learned feature space is minimized to eliminate the appearance diversities, whilst the distance between pedestrians and background is maximized. To facilitate the efficiency and effectiveness of contrastive learning, we construct an exemplar dictionary with representative pedestrian appearances as prior knowledge to construct effective contrastive training pairs and thus guide contrastive learning. Besides, the constructed exemplar dictionary is further leveraged to evaluate the quality of pedestrian proposals during inference by measuring the semantic distance between the proposal and the exemplar dictionary. Extensive experiments on both daytime and nighttime pedestrian detection validate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Pedestrian detection is a challenging task in Computer Vision with many important applications, such as video surveillance <ref type="bibr" target="#b0">[1]</ref>, driving assistance <ref type="bibr" target="#b1">[2]</ref> and intelligent robotics <ref type="bibr" target="#b2">[3]</ref>. Deep pedestrian detectors <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, which benefit from excellent feature learning for images by deep neural networks, have achieved great progress in recent years.</p><p>Most existing deep pedestrian detectors view pedestrian detection as a particular case of object detection, and are thus designed following the routine object detection methods. A prominent example is Adapted Faster R-CNN <ref type="bibr" target="#b8">[9]</ref>, which directly adapts Faster R-CNN <ref type="bibr" target="#b15">[16]</ref> to pedestrian detection. Based on such pedestrian detection framework of Adapted Faster R-CNN, many methods are proposed to deal with various challenges for pedestrian detection that are not shared in general object detection. To deal with mutual occlusions between adjacent pedestrians in crowded pedestrian detection scenarios, Repulsion loss <ref type="bibr" target="#b16">[17]</ref> is designed to maximize the separation between two adjacent pedestrians. To tackle the same challenge of crowded pedestrian detection, OR-CNN <ref type="bibr" target="#b17">[18]</ref> performs body division for pedestrian proposals to highlight the visible body parts while suppressing the occluded parts. Another interesting method for addressing the same problem is Case <ref type="bibr" target="#b12">[13]</ref>, which proposes a count-and-similarity-aware branch to predict the pedestrian number of a proposal. Another typical challenge of pedestrian detection is how to deal with various scale (size) of pedestrians, which motivates many research works to address it. Typical methods include ALFNet <ref type="bibr" target="#b18">[19]</ref> which is designed based on SSD <ref type="bibr" target="#b19">[20]</ref> and adopts similar multi-stage predictors as Cascade R-CNN <ref type="bibr" target="#b20">[21]</ref>, and SAF R-CNN <ref type="bibr" target="#b5">[6]</ref> which designs multi-scale detectors to deal with different scales of pedestrians correspondingly.</p><p>A crucial challenge of pedestrian detection, which has not been addressed well, is to detect pedestrians with a large amount of appearance diversities, especially in large-scale pedestrian detection scenarios. These appearance diversities are potentially resulted from different body silhouettes, different viewpoints, different dressing, different illumination, etc. A robust pedestrian detector should be insensitive to these appearance diversities but focus on the distinction between pedestrians and background. However, most existing methods perform pedestrian detection following the routine way of object detection, and do not explicitly learn to adapt to (ignore) these intra-class appearance differences. As a result, these methods have to allocate much model capacity for learning to recognize each of these diverse appearances (appeared in training data) as individual positive features for pedestrians, which is hardly generalized to large-scale pedestrian detection.</p><p>To address above potential limitation, we propose to perform contrastive learning to guide the feature learning in such a way that two objectives are satisfied: 1) the appearance variations between different pedestrians are ignored in the learned feature space, and 2) the semantic distance in the feature space between pedestrians and background is maximized. To this end, we learn a contrastive feature transformation module by contrastive learning, and embed it into typical pedestrian detection frameworks to project the initial feature space into a new feature space in which above two objectives are achieved. Consequently, the pedestrian diversities are eliminated in the feature space and our method can focus on distinguishing between pedestrians and background, namely a binary-classification task, which is the essential of pedestrian detection.</p><p>Typically a robust contrastive learning system demands a large amount of training data to be able to generalize to various positive and negative cases. <ref type="bibr">To</ref>   <ref type="figure">Fig. 1</ref>. Visualization of the feature maps learned by the Baseline model (Adapted Faster R-CNN) and by our EGCL model for randomly selected samples from CityPersons <ref type="bibr" target="#b8">[9]</ref> validation dataset. Left: the feature maps of whole images (C5 block of the features learning head before RPN module) are visualized for both the baseline model and our EGCL. Right: the feature maps (resized to 7 ? 7) for cropped region proposals (RoIs by RPN module) are visualized for both models. Our EGCL is able to detect pedestrians more precisely than the baseline model in both cases since our EGCL is designed to maximize the semantic distance between pedestrians and background in the feature space while minimizing the intra-class semantic distance between pedestrians. Note that the Adapted Faster R-CNN is adopted as the baseline model.</p><p>ing efficiency and effectiveness, we construct an exemplar dictionary for pedestrians to be a representative set covering various appearance variations of pedestrians. The obtained exemplar dictionary is not only used to construct effective training set for contrastive learning, but also leveraged to evaluate the quality of the pedestrian proposals during inference by measuring the semantic similarity between proposals and the exemplar dictionary. Such exemplar-contrastive inference is performed jointly with the typical proposal confidence measurement indicated by classification score to achieve more reliable prediction. <ref type="figure">Figure 1</ref> presents several real-world examples for pedestrian detection, in which the learned feature maps are visualized for both the baseline model (Adapted Faster R-CNN) and our model. Benefiting from the proposed Exemplar-Guided Contrastive Learning framework (EGCL), our model is able to distinguish pedestrians from the background more clearly than the baseline model. To conclude, we make following contributions.</p><p>? We construct an exemplar dictionary for pedestrians, which comprises representative pedestrian appearances, to facilitate the efficiency and effectiveness of contrastive learning. Furthermore, the constructed exemplar dictionary is also used to refine the confidence score of proposals during inference, based on the our designed metric for semantic distance between proposals and the exemplar dictionary. ? Based on the exemplar dictionary, we propose an Exemplar-Guided Contrastive Learning framework (EGCL) to guide feature learning such that the intraclass semantic distance between pedestrians in the learned feature space is minimized to ignore appearance diversities while the semantic distance between pedestrians and background is maximized. ? Extensive experiments are conducted on three typical datasets involving both daytime and nighttime pedestrian detection to validate the effectiveness of our method, both in quantitative and qualitative manners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we firstly make a brief review of pedestrian detection. Then we review the related work on pedestrian detection that focuses on tackling occlusions and dealing with different scales of pedestrians, respectively. Finally, we summarize the methods for contrastive learning briefly.</p><p>A. Pedestrian Detection 1) Typical Pedestrian Detection: Following the routine object detection methods, pedestrian detectors generally consists of three parts, i.e, proposals generation, feature extraction, and classification and regression. Traditional pedestrian detectors utilize the handcrafted features together with downstream classifiers to detect pedestrians. Thus a lot of handcrafted features are proposed to facilitate the separation between pedestrians and background. Dalal et al. <ref type="bibr" target="#b21">[22]</ref> proposes the Histogram of Oriented Gradients (HOG) feature to reflect the pedestrian's shape and edge information by calculating and integrating the direction and magnitude of the gradient of each pixel. The extracted HOG features are then sent into the downstream classifiers such SVM <ref type="bibr" target="#b21">[22]</ref> or AdaBoost <ref type="bibr" target="#b22">[23]</ref> to detect pedestrians. Owing to the excellent representation of objects, the HOG feature makes a great improvement in pedestrian detection and its variants such as Aggregated Channel Features (ACF) <ref type="bibr" target="#b23">[24]</ref>, Integratal Channel Features (ICF) <ref type="bibr" target="#b24">[25]</ref> and Checkerboards <ref type="bibr" target="#b25">[26]</ref> are proposed successively to detect pedestrians.</p><p>As the fast development of deep learning, deep pedestrian detection methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have achieved great progress due to the excellent feature learning capability by deep convolutional networks. Initially, convolutional neural networks (CNN) are directly used to extracted features to replace the traditional handcrafted features <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. With the great success of Faster R-CNN <ref type="bibr" target="#b15">[16]</ref> on object detection, researchers attempt to adapt the Faster R-CNN into pedestrian detection. To address the issue that the downstream classifier of Faster R-CNN degrades the detection performance, RPN+BF <ref type="bibr" target="#b28">[29]</ref> utilizes a boosted forest to substitute for the original classifier of Faster R-CNN and proposes the effective bootstrapping for mining hard negatives. Meanwhile, Adapted Faster R-CNN <ref type="bibr" target="#b8">[9]</ref> proposes five improvements to the original Faster R-CNN and exhibits better performance than traditional pedestrian detectors. To eliminate the imbalance between positive samples and negative samples, SDS-RCNN <ref type="bibr" target="#b6">[7]</ref> jointly learns pedestrian detection and semantic segmentation by infusing a bounding-box aware semantic segmentation layer into the end of the feature extractor, which enforces the feature focus on the pedestrian's region whilst ignoring the background. Similar to SDS-RCNN, HyperLearner <ref type="bibr" target="#b7">[8]</ref> integrates the channel feature generated by a semantic segmentation network into the original feature produced by the standard pedestrian detector. Due to the speed advantage of the one-stage object detector, many pedestrian detectors based on one-stage object detection algorithms such as ALFNet <ref type="bibr" target="#b18">[19]</ref> and GDFL <ref type="bibr" target="#b29">[30]</ref> are proposed to balance between the detection accuracy and speed of pedestrian detection.</p><p>2) Occluded Pedestrian Detection: Occluded pedestrian detection is a challenging task in pedestrian detection due to the information lack of invisible parts of occluded pedestrians and the diversity of occlusions patterns. It aims to solve the problem of mutual occlusions between occluded pedestrians or occlusions caused by background. The first type of pedestrian detectors for solving occlusions mainly utilize the visible parts of occluded pedestrians or divide occluded pedestrian into multiple parts including visible and invisible parts. Bi-box <ref type="bibr" target="#b30">[31]</ref> pedestrian detector combines the visible part detection with the full-body detection to simultaneously predict the visible part and full-body part of occluded pedestrian. OR-CNN <ref type="bibr" target="#b17">[18]</ref> proposes an aggregation loss to encourage positive proposals to be close to their corresponding ground-truth and focuses on highlighting the visible body parts by suppressing the occluded parts. PCN <ref type="bibr" target="#b9">[10]</ref> proposes to predict the score maps of different body parts with a recurrent neural network and designs the context branch for adaptive context selection. Other parts-based methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> learn a series of part detectors to handle the specific visual patterns of occlusions.</p><p>The second type of pedestrian detectors for dealing with occlusions are attention-based methods, which learn robust features with the guidance of the constructed attention maps. Considering that many channels in feature map can be related to different body parts, Zhang et al. <ref type="bibr" target="#b33">[34]</ref> proposes a channel-wise branch for reweighting the feature map to highlight the visible parts and suppress occluded parts. MGAN <ref type="bibr" target="#b10">[11]</ref> introduces a mask-guided attention branch to greatly eliminate the impact of occluded parts whilst highlighting the visible parts. Other pedestrian detectors for solving occlusions include methods based on feature transformation <ref type="bibr" target="#b34">[35]</ref>, methods using temporal cue <ref type="bibr" target="#b14">[15]</ref>.</p><p>Both FRCN+A+DT <ref type="bibr" target="#b34">[35]</ref> and InterNet <ref type="bibr" target="#b35">[36]</ref> are proposed to perform feature transformation to improve the performance of pedestrian detection, which is similar to our method. However, our method substantially differs from these two methods. Our methods differs from FRCN+A+DT <ref type="bibr" target="#b34">[35]</ref> in three aspects. Firstly, our method constructs an exemplar dictionary for pedestrians whilst <ref type="bibr" target="#b34">[35]</ref> learns one single reference representation for both the pedestrians and the background. Considering the diversity of pedestrian appearance, the exemplar dictionary is able to model such diversities in a more fine-grained manner than the way of learning one single representation. Secondly and most importantly, the whole optimization framework for learning the feature transformation is entirely different between two methods. Our method employs contrastive learning to maximize the margin between positive pairs and negative pairs. In contrast, <ref type="bibr" target="#b34">[35]</ref> focuses on minimizing the semantic distance between each sample to the corresponding reference representation. Thirdly, the constructed exemplar dictionary is further leveraged to evaluate the quality of pedestrian proposals during inference by measuring the semantic distance between the proposal and the exemplar dictionary. InterNet <ref type="bibr" target="#b35">[36]</ref> is designed for object detection. It learns a representative prototype for each class and minimizes the intra-class distance by the proposed interwiner loss. InterNet differs from our method w.r.t. both the target task and the optimization framework.</p><p>3) Multi-scale Pedestrian Detection: Detecting different scales of objects is another challenge in object detection. As a particular type of object detection, pedestrian detection also needs to deal with different scales of pedestrians, especially the small-scale pedestrians which tend to be blurred and noisy. Many methods follow divide-and-conquer algorithm to detect different scales of pedestrians. For instance, SAF R-CNN <ref type="bibr" target="#b5">[6]</ref> trains a large-scale sub-network and a small-scale sub-network to deal with various sizes of pedestrian instances in the image. MS-CNN <ref type="bibr" target="#b36">[37]</ref> explores the different depth of feature maps from feature extractor to generate different sizes of proposals, which is followed by a detection network guided by context reasoning. Based on the assumption that different scales of pedestrians bodies can be modeled as 2D Gaussian kernel with various scale variance, TLL <ref type="bibr" target="#b37">[38]</ref> designs a unified fully convolutional network to locate the somatic topological line of pedestrians with line annotation for detecting multi-scale pedestrians. Recently, ascribing the poor performance of detect small-scale pedestrians to the problem of inaccurate location, Cao et al. <ref type="bibr" target="#b38">[39]</ref> proposes a location bootstrap module for re-weighting the regression loss. The loss of the predicted bounding box far from the corresponding ground-truth is stressed with high weight while the loss of the predicted bounding box near the corresponding ground-truth is ignored with low weight.</p><p>Unlike the aforementioned methods for pedestrian detection, we aim to address the challenge of detecting pedestrians with substantial appearance diversities by performing contrastive learning to guide the feature learning to eliminate the appearance diversities in the learned feature space while maximizing the distance between pedestrians and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contrastive Learning</head><p>Contrastive learning aims to guide the feature learning by minimizing the distance between positive pairs and maximizing the distance between negative pairs in the feature space, our EGCL performs contrastive learning to learn a feature transformation module Ft in such a way that the semantic distance between pedestrians in the transformed feature space is minimized whilst the distance between pedestrians and background is maximized. The exemplar dictionary is constructed not only for composing high-quality training pairs for contrastive learning, but also for refining the confidence score of predicted proposals by ECI module.</p><p>typically implemented in the form of contrastive loss <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. One prominent application of contrastive learning is for the self-supervised learning, which seeks to learn robust feature representation from large-scale unlabeled image dataset using a pretext task <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Self-supervised contrastive learning initially aims to eliminate the performance gap between unsupervised learning and supervised learning in image classification by training different pretext tasks. SimCLR <ref type="bibr" target="#b41">[42]</ref> learns effective representation by minimizing the discrepancy between differently augmented views of the same data input. MoCo (v1/v2) <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref> utilizes a momentum-based moving average of the query encoder to solve the inconsistency of the dictionary keys of negative samples. Recently, considering that previous works for contrastive learning yield sub-optimal performance when transferred to object detection due to the difference between image classification and object detection, DetCo <ref type="bibr" target="#b43">[44]</ref> analyses the essential reasons of inconsistency of classification and detection and introduces instance discrimination as a special pretext task for object detection. Meanwhile, apart from computing contrastive loss in high-level features, DetCo also performs contrastive learning on the low-level features for object detection.</p><p>Contrastive learning framework typically demands a large amount of positive pairs and negative pairs for training <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, which requires a lot of computation resources. To alleviate this problem, we propose a novel exemplar-based offline-online training strategy to train our contrastive learning framework for pedestrians detection, which constructs an exemplar dictionary covering representative pedestrian appearances and thereby utilizes the exemplar dictionary to construct high-quality training pairs for efficient contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXEMPLAR-GUIDED CONTRASTIVE LEARNING</head><p>We aim to optimize feature learning for pedestrian detection in such a way that the distance between pedestrians with various appearances is minimized whilst maximizing the distance between pedestrians and background. To this end, we propose to perform contrastive learning to optimize the feature learning by viewing pedestrian detection as a binary (pedestrian or background) classification problem. To facilitate the efficiency and effectiveness of contrastive learning, we extract an exemplar dictionary covering representative pedestrian appearances as prior knowledge to guide the contrastive learning. Besides, the pedestrian exemplar dictionary is also leveraged to refine the confidence scores of proposals during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pedestrian Detection Framework</head><p>Our proposed contrastive learning framework serves as an auxiliary functional module for optimizing feature learning for pedestrian detection, which can be seamlessly integrated into any existing proposal-based detection framework. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, we build our contrastive learning module upon the Adapted Faster R-CNN <ref type="bibr" target="#b8">[9]</ref> as an instantiation.</p><p>Adapted Faster R-CNN is a pedestrian detection method designed based on Faster R-CNN <ref type="bibr" target="#b3">[4]</ref>. Thus it has similar twostage modeling process as object detection performed by Faster R-CNN. In the first stage, a Region Proposal Network (RPN) is trained to select a set of high-quality region proposals for potential pedestrians. In the second stage, pedestrian detection is conducted by a classification head and a regression head to predict the class (true or false) of the selected proposals and the corresponding bounding boxes (offsets to the ground-truth), respectively. Two stages are both performed in the deep feature space projected by the feature learning head F h , which is typically a pre-trained deep neural networks such as VGG-16 <ref type="bibr" target="#b47">[48]</ref> in Adapted Faster R-CNN.</p><p>To detect pedestrians with diverse appearance, we learn a contrastive feature transformation module F t by contrastive learning to project the feature space of the feature learning head F h into a new feature space. The distance between pedestrians is minimized to eliminate the appearance diversities whilst the distance between pedestrians and background is maximized in this projected feature space. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the contrastive feature transformation module F t is embedded between the RPN module and the Fast R-CNN module to perform feature transformation for each of selected region proposals by the RPN. Another reasonable position for F t is between the feature learning head F h and the RPN, in which case F t performs feature transformation on the feature maps for the whole image. We conduct experiments to investigate the effect of the position of F t (before or after RPN) on the performance of pedestrian detection in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construction of Exemplar Dictionary</head><p>The exemplar dictionary is expected to cover full range of appearance diversities of pedestrians. To this end, we perform clustering on the pedestrian images cropped from the training dataset and select the samples closest to each cluster center to compose the exemplar dictionary. Such construction is based on the hypothesis that the pedestrians cropped from a sufficiently large dataset can approximate the pedestrian distribution in real world.</p><p>We employ VGG-16 pre-trained on ImageNet <ref type="bibr" target="#b48">[49]</ref>, which is the same as feature learning head F h in <ref type="figure" target="#fig_0">Figure 2</ref>, to extract features for cropped pedestrian images as input features for clustering. Then we perform clustering on the extracted features of all cropped pedestrian images using k-means algorithm <ref type="bibr" target="#b49">[50]</ref> and collect the samples closest to each cluster center to obtain the exemplar dictionary E:</p><formula xml:id="formula_0">E = k-means({F h (I i )}; K), i = 1, . . . , N,<label>(1)</label></formula><p>where I i is the i-th pedestrian image (total N images) cropped from the training set. Here the number of clusters K is a hyper-parameter to control the size of the constructed exemplar dictionary and balance between training efficiency and effectiveness of contrastive learning. Generally, coarse clustering (small K) results in a small dictionary with highly representative exemplars, which favors efficient training for contrastive learning but cannot cover full range of pedestrian diversities. In contrast, fine-grained clustering (large K) yields a more comprehensive exemplar dictionary but more time is required for contrastive learning to converge. Besides, oversized K may involve low-quality exemplars with low representativeness. <ref type="figure">Figure 3</ref> presents a t-SNE <ref type="bibr" target="#b50">[51]</ref> map of cropped pedestrian images and the selected samples (red dots) by clustering for constructing the exemplar dictionary, which shows that the selected exemplars span the whole distribution of pedestrian samples to be a representative set of various pedestrians. Besides, the visualized exemplars, which are randomly selected from the exemplar dictionary, exhibit diverse pedestrian appearances. <ref type="figure">Fig. 3</ref>. Left: t-SNE map of cropped pedestrian images (blue dots) from the training set of CityPersons <ref type="bibr" target="#b8">[9]</ref> and the selected samples (red dots) for constructing the exemplar dictionary. Right: visualized exemplars randomly selected from the exemplar dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exemplars samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-level Contrastive Learning</head><p>We conduct multi-level contrastive learning to learn the feature transformation module F t , utilizing the obtained exemplar dictionary to construct effective training data for contrastive learning.</p><p>1) Contrastive learning framework: <ref type="figure" target="#fig_1">Figure 4</ref> presents the contrastive learning framework, which is a siamese structure consisting of two parts: the feature transformation module F t and the projection head P. Given a triplet input consisting of three types of image features, namely an exemplar, a positive proposal and B negative proposals, the feature transformation module F t transforms all three types of input features into a new feature space while the projection head P projects the features into a low-dimensional vector by two fullyconnected layers. Then the positive proposal and the exemplar compose the positive training pair while the negative proposals and the exemplar compose B negative training pairs for contrastive learning. Note that both positive proposals and negative proposals are obtained by the RPN module based on a predefined IoU overlap threshold. In our implementation, all negative proposals obtained within a batch are used for composing each of the training triplets, thus the value of B is equal to the number of the negative proposals generated in current batch, which could vary in different batches. Here all input features are extracted from the feature learning head F h of the backbone network in <ref type="figure" target="#fig_0">Figure 2</ref>, namely the VGG-16 network. Since the input features are already learned in deep feature space, we design the feature transformation module F t as a shallow convolutional network, which is composed of three convolutional layers together with activation layers (ReLU in our implementation).</p><p>We adopt InfoNCE <ref type="bibr" target="#b51">[52]</ref>, a widely used contrastive loss function, to supervise our contrastive learning: proposal and the i-th negative proposal in a triplet input, respectively. sim refers to a kernel function measuring the similarity between paired vectors and we opt for dot-product for sim due to its computational efficiency. ? is a temperature hyper-parameter <ref type="bibr" target="#b46">[47]</ref> to tune the sharpness of the exponential function. Such loss function tries to maximize the similarity of the positive pair while suppressing the similarity values of B negative pairs. Note that the exemplars e in different triplet input can be different since they are randomly selected from the exemplar dictionary.</p><formula xml:id="formula_1">L CL = ? log exp(sim(e,<label>s</label></formula><p>2) Multi-level contrastive learning mechanism: A critical issue in object detection is how to deal with different size of objects in a unified framework, which is also challenging in pedestrian detection due to the similar problem formulation. To perform contrastive learning that is robust to different size of pedestrians, we perform multi-level contrastive learning by constructing pyramidal feature pairs using different depth of feature maps for a same training pair, either a positive pair or a negative pair.</p><p>Formally, for a training pair consisting a proposal (positive or negative) and an exemplar, we construct multi-level feature pairs by cropping the feature maps for both the proposal and the exemplar from different blocks of the feature learning head F h . As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we utilize the features from the 2-th block to the 5-th block of F h in our implementation. Deeperlevel features correspond to larger receptive field in the input image and thus are responsible for detecting larger pedestrians. Each level of feature pair has its own contrastive loss, thus the total contrastive loss is defined as:</p><formula xml:id="formula_2">L CL = 5 i=2 w i L i CL ,<label>(3)</label></formula><p>where w i , i = 2, . . . , 5 are hyper-parameters for balancing between contrastive losses for different levels of features. In practice, we set w 2 to be 1 and tune other hyper-parameters on a held-out small set.</p><p>3) Offline-online contrastive learning: To perform contrastive learning effectively, we adopt an offline-online learning strategy: 1) we first train a basic contrastive learner in an offline manner, which can distinguish between pedestrians and background roughly; 2) then we perform online contrastive learning together with the whole pedestrian detection framework, especially focusing on hard-pair contrastive learning for proposals being processed.</p><p>The offline contrastive learning is performed first before the training of the whole pedestrian detection framework. Thus the input features for offline contrastive learning are extracted from the feature learning head F h that is only pre-trained on ImageNet without any re-training for pedestrian detection. The training pairs are constructed by randomly selecting cropping pedestrian images as positive proposals and randomly cropped background images as negative proposals from training data. After the offline contrastive learning, the feature transformation module F t can roughly perform binary classification between pedestrians and background.</p><p>The online contrastive learning is performed after the offline training stage and is conducted jointly with the training of the whole pedestrian detection framework. Thus the parameters of the whole model including the feature learning head F h , feature transformation module F t and other modules are jointly optimized under the supervision of the contrastive learning loss and the losses for pedestrian detection (described subsequently). The positive proposals and negative proposals are all from RPN module on the same image that is being processed for pedestrian detection, which are more challenging to distinguish. Hence, the online contrastive learning further improves the performance of feature transformation module by hard-pair training. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the feature transformation module F t is embedded after the RoI Align layer to perform transformation on the features of selected region proposals by the RPN module. Then the transformed proposal features are fed into the Fast R-CNN module to predict the confidence score and the bounding box offsets:</p><formula xml:id="formula_3">F trans = F t RPN(F h (I)) ,<label>(4)</label></formula><p>where F trans is the transformed proposal features by F t and I is the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Collaborative Pedestrian Detection with Exemplar-Contrastive Inference</head><p>The pedestrian detection is typically performed by a classification head and a regression head to predict the confidence score and bounding box given the features of a pedestrian proposal, respectively. To better evaluate the quality of the proposal, we further leverage the constructed exemplar dictionary to perform exemplar-contrastive inference to measure the semantic distance between the proposal and the exemplar dictionary.</p><p>We measure two kinds of semantic distance between a proposal and the exemplar dictionary: 1) the distance between the proposal and the closest exemplar in the dictionary, 2) average distance from the proposal to the whole exemplar dictionary, which is considered to avoid the overfitting of the proposal to a marginal exemplar (in the cases that the closest exemplar happens to be a marginal exemplar). We employ HNSW <ref type="bibr" target="#b52">[53]</ref> algorithm to build a hierarchical graph in a coarseto-fine manner for the constructed exemplar dictionary (shown in <ref type="figure" target="#fig_2">Figure 5</ref>), which is used for efficient navigation to locate the semantically nearest exemplar. The distance D c between a proposal s and its nearest exemplar e c in the HNSW graph is measured by the opposite similarity value calculated by dot product:</p><formula xml:id="formula_4">e c = HNSW(E), D c = 1 ? ?(s ? e c ),<label>(5)</label></formula><p>where E is the constructed exemplar dictionary. Sigmoid function ? constrains the value into the interval [0, 1]. Both e c and s are output features from the projection head P in the contrastive learning framework. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, we measure the average distance D a between the proposal and the whole exemplar dictionary by:</p><formula xml:id="formula_5">D a = 1 HNSW top-layer (E) e?HNSWtop-layer(E) (1 ? ?(s ? e)). (6)</formula><p>Here we approximate the whole distribution of E by the top layer of HNSW graph with sparse sampling for efficiency. Our method perform exemplar-contrastive inference for evaluating the quality of the proposal s by combining two distances D c and D a . Consequently, our method performs collaborative confidence evaluation along with the classification head:</p><formula xml:id="formula_6">C = (1 ? ? ? ?)P + ?D c + ?D a ,<label>(7)</label></formula><p>where P is the predicted confidence by the classification head and C is the final estimated proposal confidence score. ? ? [0, 1] and ? ? [0, 1] are hyper-parameters to balance between different terms. In our implementation, we construct an individual HNSW graph for each level of features for the exemplar dictionary and then select an appropriate HNSW graph according to size of the proposal: the NHSW graph constructed from the deeper feature map are prepared for the larger scale of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. End-to-End Parameter Learning</head><p>The contrastive learning framework is first performed in an offline manner under the supervision of contrastive loss in Equation 3, then the online contrastive learning is performed with the whole pedestrian detection framework together, in which the parameters of the whole model are trained in an end-to-end manner. Specifically, the whole model is supervised by both the detection loss L det and contrastive loss L CL jointly:</p><formula xml:id="formula_7">L = L det + ?L CL ,<label>(8)</label></formula><p>where ? is a balancing weight between two terms. The detection loss L det comprises the losses for RPN stage and the losses for Fast R-CNN stage:</p><p>L det = L rpn cls + L rpn reg + L rcnn cls + L rcnn reg .</p><p>Herein, L rpn cls and L rpn reg refer to the classification loss and regression loss in RPN stage respectively. Similar notations apply to the L rcnn cls and L rcnn reg for Fast R-CNN module.</p><p>IV. EXPERIMENTS In this section, we conduct extensive experiments to evaluate the performance of our proposed EGCL model on three benchmark datasets including both daytime and nighttime pedestrian detection scenarios. We first perform ablation study to investigate the effectiveness of each key component of our EGCL. Then we make qualitative comparison between our EGCL and the baseline model (Adapted Faster R-CNN). In the last set of experiments, we compare our model with stateof-the-art methods for pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup 1) Datasets:</head><p>We evaluate our proposed method on three standard benchmark datasets including CityPersons <ref type="bibr" target="#b8">[9]</ref>, NightOwls <ref type="bibr" target="#b53">[54]</ref> and TJU-DHD-pedestrian <ref type="bibr" target="#b54">[55]</ref> involving both daytime and nighttime pedestrian detection. CityPersons is collected for daytime pedestrian detection. It comprises 2,975, 500 and 1,525 images for training, validation and test, respectively. NightOwls is a nighttime pedestrian dataset, in which all the images are captured in the night and dawn time. It contains 128k images for training, 51k images for validation, and 103k images for test. Covering more complex and diverse scenes than CityPersons and NightOwls, TJU-DHD-pedestrian is a challenging dataset for both daytime and nighttime pedestrian detection. It contains 75,246 images with 373,241 labeled pedestrians, which are mixed of daytime and nighttime images. It has two subsets with different scenes, namely TJU-Pedcampus and TJU-Ped-traffic. Following Pang et al. <ref type="bibr" target="#b54">[55]</ref>, we conduct experiments on these two subsets separately.</p><p>2) Evaluation metrics: Following the standard pedestrian evaluation protocol <ref type="bibr" target="#b55">[56]</ref>, we choose the log-average Miss Rate over Following the routine setting <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, the pedestrians whose bounding box height are at least 50 pixels are taken for evaluation for CityPersons. On NightOwls dataset, following <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b53">[54]</ref> and the official NightOwls evaluation application programming interface (API) 1 , we report the evaluation results on the  Reasonable subset (non-occluded pedestrians with height &gt;= 50 pixels), Reasonable small (non-occluded pedestrians with height between 50 pixels and 75 pixels), Reasonable occ subsets (occluded pedestrians with height &gt;= 50 pixels), and All (pedestrians with height &gt;= 20 pixels). On TJU-DHDpedestrian dataset, we report evaluation results on R, HO, R+HO and All subsets, which is the same with Pang et al. <ref type="bibr" target="#b54">[55]</ref>.</p><p>3) Implementation details: For both datasets, we train our framework on the reasonable subset of training data and evaluate on validation sets (test sets of these datasets are not publicly accessible). The backbone network (VGG-16) of our framework is initialized with ImageNet pre-trained model. We use Adam solver <ref type="bibr" target="#b56">[57]</ref> as optimizer and the mini-batch size is set to 1 image. Our framework is trained with 60k steps of gradient descent, in which the initial learning rate of the first 30k steps is set to 1 ? 10 ?4 and is further decayed to 1 ? 10 ?5 for the left 30k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>We first perform ablation study to investigate the effectiveness of each key component of our EGCL model including the Feature Transformation module (FT), Offline-Online Contrastive Learning (OOCL) and Exemplar-Contrastive Inference (ECI). Specifically, we conduct experiments which begin with the baseline model (Adapted Faster R-CNN) and then incrementally augment the model with each component of EGCL. Besides, we also conduct experiments to investigate the effectiveness of the constructed exemplar dictionary. <ref type="table" target="#tab_1">Table I  and Table II present</ref>   <ref type="figure">Fig. 7</ref>. Performance in terms of M R ?2 of our EGCL on R subset, with (red lines) and without (blue lines) the ECI module, as a function of the size of the constructed exemplar dictionary K on the CityPersons and NightOwls dataset, respectively. The performance of the baseline model is also presented for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison with the Baseline (Adapted Faster R-CNN):</head><p>The experimental results in <ref type="table" target="#tab_1">Table I and Table II</ref> show that our EGCL model achieves substantial improvement comparing to the Baseline model (Adapted Faster R-CNN) on all subsets of both CityPersons dataset and NightOwls dataset. Considering the performance on R (Reasonable) subset, which is commonly used for performance comparison, EGCL boosts the performance of pedestrian detection on from 15.40 to 11.79 and from 18.80 to 15.93 (in terms of M R ?2 ) on CityPersons and NightOwls respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Effect of the Feature Transformation module (FT):</head><p>The large performance gap between 'Baseline' and 'Baseline+FT' on both CityPersons and NightOwls datasets reveals the effectiveness of the feature transformation module of our EGCL. Note that we only perform online contrastive learning to train FT module in the setting of 'Baseline+FT'. The FT module transforms the initial feature space into a new feature space, in which the semantic distance between pedestrians is minimized while the semantic distance between pedestrians and background is maximized.</p><p>The comparison between 'Baseline+FT * ' and 'Baseline+FT' indicates that embedding the FT module after the RPN module performs better than the other way that before the RPN. We surmise that the FT module placed before the RPN may degenerate the perceptive precision of pedestrians by RPN since the input feature maps of RPN have larger receptive field and thus coarser localization resulted from the convolutional operations in the FT module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Effect of Offline-Online Contrastive Learning (OOCL):</head><p>Comparing the performance between 'Baseline+FT' and 'Base-line+FT+OOCL', we observe that performing offline contrastive learning before online learning can further boost the performance. It validates the theoretical analysis that offline learning is performed to train the feature transformation module (FT) to be a rough classifier between pedestrians and background while online contrastive learning further improves the performance of FT module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Effect of Exemplar-Contrastive Inference (ECI):</head><p>The constructed exemplar dictionary is used to not only compose effective training pairs for contrastive learning, but also refine the confidence scores of proposals by proposed exemplarcontrastive inference (ECI). The results in <ref type="table" target="#tab_1">Table I and Table II</ref> validate the effectiveness of ECI on both two datasets. 5) Effect of the constructed exemplar dictionary: To investigate the effectiveness of the constructed exemplar dictionary, we compare the performance of contrastive learning using and not using the exemplar dictionary for constructing the training pairs. <ref type="figure" target="#fig_4">Figure 6</ref> illustrates the comparison between two ways w.r.t. the loss convergence and performance respectively. Using the constructed exemplar dictionary, the loss of contrastive learning converges faster and reaches a lower value than that without exemplar dictionary. Further, exemplar dictionary also empowers contrastive learning to achieve a better performance in terms of M R ?2 , which demonstrates the effectiveness of exemplar dictionary for contrastive learning. Effect of the size of exemplar dictionary K. The size of the exemplar dictionary K is a hyper-parameter to be tuned in our experiments. Typically, small K leads to an exemplar dictionary which has highly representative exemplars but cannot cover full range of pedestrian diversities. In contrast, oversized K may involve low-quality exemplars with low representativeness.   <ref type="figure">Figure 7</ref> presents the performance of our EGCL as a function of K on CityPersons and NightOwls datasets respectively. The experiments are conducted under two different settings: 1) the ECI module is unmounted, under which setting the effect of different K on sole contrastive learning is evaluated; 2) the ECI module is activated and the experiments are designed to investigate the effect of different K on both contrastive learning and the exemplar-contrastive inference. We make following observations from the results. 1) Under both settings, the performance of our model is being improved at the beginning as the increase of K and reaches an optimal point, then the performance begins to degrade when provided larger size of exemplar dictionary. These results are consistent with our theoretical analysis.</p><p>2) The ECI module consistently improves the performance of our model when setting K to different values, which indicates the robustness of the proposed ECI module. 3) Though the performance of our model fluctuates with varying values of K, the performance is always better than the baseline model (Adapted Faster R-CNN), which again reveals the effectiveness of the ECI module. Since the 'reasonable' samples account for the vast majority of pedestrians in the training data and real-world scenarios as well, the exemplar dictionary contains mostly 'reasonable' exemplars and only 20% occluded exemplars if it is constructed uniformly based on the training data. However, dealing with the heavy-occluded pedestrians remains a crucial challenge in pedestrian detection. To improve the performance of our EGCL in detecting the heavy-occluded pedestrians, we increases the ratio of occluded samples in the exemplar dictionary by simply replicating the existing occluded exemplars. <ref type="table" target="#tab_1">Table III</ref> shows the performance of our EGCL on CityPersons dataset with increasing ratio of occluded samples in the exemplar dictionary. We observe that increasing the ratio of the occluded exemplars  <ref type="figure">Fig. 11</ref>. Visualization of the feature maps learned by the Baseline model (Adapted Faster R-CNN) and by our EGCL model for randomly selected samples from CityPersons <ref type="bibr" target="#b8">[9]</ref> validation dataset. Left: the feature maps of whole images (C5 block of the feature learning head F h before RPN module) are visualized for both the baseline model and our EGCL. Right: the feature maps (resized to 7 ? 7) for cropped region proposals (RoIs by RPN module) are visualized for both models. Note that the last sample of RoI visualization is a hard negative sample which tends to be falsely recognized as a pedestrian by the baseline. indeed improves the performance of our EGCL to detect the heavy-occluded pedestrians ('HO' and 'R+HO' subsets). However, too large ratio of the occluded exemplars results in the performance degradation of our model on the 'Reasonable' subset (R).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Tuning of hyper-parameters:</head><p>To illustrate the effect of the hyper-parameters involved in our method on the performance, we report the performance of our method with different settings of hyper-parameters. To be specific, <ref type="figure" target="#fig_5">Figure 8</ref> visualizes the distribution map of performance as a function of w i , i = 3, . . . , 5 in Equation 3 when performing grid search for 3 parameters. Note that w 2 is fixed to be 1 to tune other balancing weights. <ref type="figure">Figure 9</ref> shows the effect of ? in Equation 8 on the performance while <ref type="figure" target="#fig_7">Figure 10</ref> presents the performance of our model when varying the values of ? and ? in Equation 7 during grid search. In practice, we tune all these hyper-parameters on a held-out small set split from the training set, on which we can obtain the consistent correlations between the performance of our model and the settings of these hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7)</head><p>Computational complexity: To analyze the computational complexity of our model, we measure the inference time of our model with different configurations in <ref type="table" target="#tab_1">Table IV</ref>. Comparing the inference time between the baseline and our model without ECI, we observe that the contrastive learning incurs little extra time while yielding substantial performance improvement. On the other hand, ECI module is distinctly more time-consuming than the contrastive learning. The time cost of the ECI module increases roughly linearly with the size of exemplar dictionary K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Study on Contrastive learning</head><p>1) Visualization of learned feature maps: To obtain more insights into the effectiveness of our EGCL, we make a qualitative comparison between our EGCL and the baseline model (Adapted Faster R-CNN) by visualizing the feature maps of both models. <ref type="figure">Figure 11</ref> visualizes both the feature maps for the whole images (C5 block of feature learning head F h ) and that for cropped region proposals (RoIs). Our EGCL is able to detect pedestrians more precisely since it minimizes the semantic difference between pedestrians and maximizes the distance between pedestrians and background in the feature space. Besides, the untransformed feature maps for RoIs before the feature transformation module of our EGCL are also visualized in <ref type="figure">Figure 11</ref> for reference. An interesting observation is that our model is able to correctly deny the confusing (hard) negative sample in the last row of RoI visualization, which tends to be falsely recognized as a pedestrian by the baseline and the untransformed feature maps of our model.</p><p>2) Separability of RoI features: We further apply t-SNE <ref type="bibr" target="#b50">[51]</ref> to RoI features by different models to compare the their capability to distinguish between positive proposals (IoU &gt; 0.6) and hard negative proposals (IoU &lt; 0.4). <ref type="figure" target="#fig_10">Figure 13</ref> presents t-SNE maps for the baseline model and our model (including two cases: before and after the feature transformation module (FT)). The t-SNE map of RoI features after the FT module  by our EGCL shows more separability than other two t-SNE maps.</p><p>3) Visualization of pedestrian detection: We visualize the results of pedestrian detection by our EGCL and the baseline model in <ref type="figure" target="#fig_0">Figure 12</ref> to have a qualitative comparison. We first present the detection results of both models on three examples that are randomly selected from the training set in <ref type="figure" target="#fig_0">Figure 12</ref> (a). The results show that our model is able to detect all pedestrians precisely, even those with quite small size (in the second sample) or occluded heavily (in the third sample) which are missed by the baseline model. Besides, the baseline falsely captures the reflection of the pedestrian in the mirror (in the first sample) while our model can discriminate the reflection from real pedestrians. Limitations. <ref type="figure" target="#fig_0">Figure 12</ref> (b) presents two challenging examples, on which both our model and the baseline fails to detect pedestrians correctly. In the first sample, our EGCL misrecognizes a vivid human statue as a pedestrian. In this case, it is quite challenging to distinguish it from genuine pedestrians relying on sole statue features. A potential solution that can be explored in the future work is to model the semantic correlation between the statue and its surrounding objects like stone pedestal, and thereby make inference more correctly. In the second example, there are two pedestrians which are highly overlapped. Our model cannot detect the occluded pedestrian probably due to the removal of highly overlapped proposals by NMS algorithm. Those two examples reveals two potential limitations of our model, which we intend to address in the future work.</p><p>D. Comparison with State-of-the-art methods 1) CityPersons dataset: To evaluate the performance of our EGCL on daytime pedestrian detection, we compare our EGCL model with other state-of-the-art methods for pedestrian detection on CityPersons dataset. These methods include F.RCNN+ATT-vbb <ref type="bibr" target="#b33">[34]</ref>, F.RCNN+ATTpart <ref type="bibr" target="#b33">[34]</ref>, Adapted Faster R-CNN <ref type="bibr" target="#b8">[9]</ref>, OR-CNN <ref type="bibr" target="#b17">[18]</ref>, Adaptive-NMS <ref type="bibr" target="#b57">[58]</ref>, MGAN <ref type="bibr" target="#b10">[11]</ref>, HGPD <ref type="bibr" target="#b58">[59]</ref>, TLL <ref type="bibr" target="#b37">[38]</ref>, R 2 NMS <ref type="bibr" target="#b13">[14]</ref>, TLL+MRF <ref type="bibr" target="#b37">[38]</ref>, Replusion loss <ref type="bibr" target="#b16">[17]</ref>, ALFNet <ref type="bibr" target="#b18">[19]</ref>, NOH-NMS <ref type="bibr" target="#b60">[61]</ref>, FRCN+A+DT <ref type="bibr" target="#b34">[35]</ref>,Case <ref type="bibr" target="#b12">[13]</ref> and CrowdDet <ref type="bibr" target="#b59">[60]</ref>. We also adapts InterNet <ref type="bibr" target="#b35">[36]</ref> from object detection to pedestrian detection and evaluate its performance. Note that these state-ofthe-art pedestrian detectors are trained on different backbones (VGG16 or ResNet-50) for feature learning head and different input scales (?1 or ?1.3). For a fair comparison, as shown in <ref type="table" target="#tab_5">Table V</ref>, we compare our methods with these state-of-theart methods in three settings w.r.t. the adopted backbone and the input scale. Using ?1 scale of input images with VGG16 as the backbone, our EGCL achieves the best performance on most of subsets and performs on par with the first-rank method on R subset. In particular, our method outperforms the second place substantially on both HO and R+HO subsets, which reveals the advantages of our model in detecting heavyoccluded pedestrians. In the setting of adopting ResNet-50 as the backbone and using ?1 scale of input images, our EGCL performs best on all subsets except Partial subset. It is worth noting that our EGCL outperforms the second place by 1.2% on R subset, which is the most important evaluating metric for pedestrian detection. Our EGCL also performs well in the setting of using ?1.3 scale of input image and ResNet-50 as backbone. These favorable results validate the effectiveness of our model.</p><p>2) NightOwls dataset: Detecting pedestrians in nighttime is more challenging than in daytime due to difficult discrimination  <ref type="bibr" target="#b15">[16]</ref> VGG-16 20.00 Adapted Faster R-CNN <ref type="bibr" target="#b8">[9]</ref> VGG-16 18.81 RPN+BF <ref type="bibr" target="#b28">[29]</ref> VGG-16 23.26 SDS-RCNN <ref type="bibr" target="#b6">[7]</ref> VGG-16 17.80 TFAN <ref type="bibr" target="#b14">[15]</ref> ResNet-50 16.50 EGCL (ours) VGG- <ref type="bibr">16 15.93</ref> between pedestrians and background in blurred and lowcontrast circumstances. To validate the effectiveness of our model in nighttime pedestrian detection, we compare our EGCL model with state-of-the-art pedestrian detectors on NightOwls dataset. These methods include ACF <ref type="bibr" target="#b23">[24]</ref>, Checkerboards <ref type="bibr" target="#b25">[26]</ref>, Vanilla Faster R-CNN <ref type="bibr" target="#b15">[16]</ref>, Adapted Faster R-CNN <ref type="bibr" target="#b8">[9]</ref>, RPN+BF <ref type="bibr" target="#b28">[29]</ref>, SDS-RCNN <ref type="bibr" target="#b6">[7]</ref> and recent TFAN <ref type="bibr" target="#b14">[15]</ref>. The results in <ref type="table" target="#tab_1">Table VI</ref> show that our model outperforms other methods, which reveals the advantage of our model in nighttime detection over other methods. 3) TJU-DHD-pedestrian dataset: In the last set of experiments, we compare our EGCL with other methods on TJU-DHD-pedestrian dataset, which is mixed with daytime and nighttime images covering more challenging scenes for pedestrian detection. We compare our EGCL with the state-ofthe-art methods that have been evaluated previously on TJU-DHD-pedestrian dataset, including RetinaNet <ref type="bibr" target="#b61">[62]</ref>, FCOS <ref type="bibr" target="#b62">[63]</ref> and FPN <ref type="bibr" target="#b63">[64]</ref>. Specifically, we conduct experiments on the two subsets with different scenes separately, namely TJU-Pedcampus and TJU-Ped-traffic. The results in <ref type="table" target="#tab_1">Table VII</ref> show that our model achieves the best results on all subsets of both 'campus' and 'traffic' sub-datasets, which validates the effectiveness of our method. Note that only methods with reported results on this dataset are listed. Here CrowdDet is used as the baseline for our model to have a fair comparison. Our EGCL performs better than CrowdDet on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented the Exemplar-Guided Contrastive Learing (EGCL) model for pedestrian detection. EGCL learns a feature transformation module by contrastive learning to project the initial feature space into a new feature space, in which the semantic distance between pedestrians is minimized to eliminate the appearance diversities of pedestrians while the semantic distance between pedestrians and background is maximized. Extensive experiments on both daytime and nighttime pedestrian detection validate the effectiveness of the proposed EGCL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the proposed Exemplar-guided contrastive learning network (EGCL) for pedestrian detection. Built upon the adapted Faster R-CNN,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>pos )/? ) exp(sim(e, s pos )/? ) + B i=1 exp(sim(e, s i neg )/? ) , (2) where e, s pos , s i neg are the normalized vectorial representations (output from the project head P) of the exemplar, the positive PP Structure of the contrastive learning framework (CL), where Ft and P refer to the feature transformation module and the projection head, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Constructed HNSW graph for the exemplar dictionary. Blue dots refer to exemplars and red dot is the semantically closest exemplar for a proposal denoted by black star. The red solid lines indicate two types of distance Da and Dc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>False Positive Per Image (FPPI) with the range of [ 10 ?2 ,10 0 ] (denoted as M R ?2 ) as the evaluation metric. Lower value of M R ?2 indicates better performance of pedestrian detection. For experiments on CityPersons dataset, we report evaluation results on 6 subsets according to the visible ratio (in the area of pedestrian bounding boxes) of each pedestrian: R (reasonable subset with visible ratio in [0.65,1]), HO (heavy occlusion subset with visible ratio in [0.2,0.65]), R+HO (reasonable and heavy occlusion subset with visible ratio in [0.2, 1]), Bare with visible ratio [0.9, 1.0], Partial with visible ratio [0.65,0.9] and Heavy with visible ratio [0, 0.65].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>the experimental results of ablation study on CityPersons and NightOwls, respectively. Comparisons between contrastive learning with and without the constructed exemplar dictionary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Performance of our EGCL in terms of M R ?2 on R subset of CityPersons validation dataset as a function of w i , i = 3,. . . ,5 in Equation 3 (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>MR 2 &amp;LW\3HUVRQVFig. 9 .</head><label>29</label><figDesc>Performance of our EGCL in terms of M R ?2 on CityPersons validation dataset as a function of ? in Equation 8 (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Performance of our EGCL in terms of M R ?2 on R subset of CityPersons validation dataset as a function of ? and ? in Equation 7 (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization of detection results by our model and the baseline. (a) Our model is able to detect all pedestrians correctly whilst the baseline cannot recognize the pedestrians with small-scale or heavily occluded pedestrians. (b) Two challenging examples on which both our model and the baseline fail to detect pedestrians correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>The t-SNE map of RoI features by different models, in which the blue dots and the red dots refer to the positive proposals (IoU &gt; 0.6) and negative proposals (IoU &lt; 0.4). Left and Middle: RoI features before and after the feature transformation module (FT) of our trained EGCL respectively; Right: RoI features of the baseline model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>facilitate the learn-arXiv:2111.08974v3 [cs.CV] 9 Jul 2022</figDesc><table><row><cell>Input Images</cell><cell>Baseline</cell><cell>EGCL(ours)</cell><cell>RoIs</cell><cell>Baseline</cell><cell>EGCL(ours)</cell><cell>RoIs</cell><cell>Baseline</cell><cell>EGCL(ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>High</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Low</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY OF OUR EGCL MODEL ON CITYPERSONS VALIDATION SET IN TERMS OF M R ?2 (LOWER IS BETTER). BASELINE REFERS TO ADAPTED FASTER R-CNN. NOTE THAT THE FT MODULE IS EMBEDDED BEFORE THE RPN MODULE IN THE ASTERISKED MODEL (BASELINE+FT * ) WHILE OTHER MODELS MOUNT THE FT MODULE AFTER THE RPN. FOR ALL RESULTS, LOWER IS BETTER.</figDesc><table><row><cell>Methods</cell><cell>Input Scale</cell><cell>R</cell><cell>HO R+HO Heavy Partial Bare</cell></row><row><cell>Baseline [9]</cell><cell>?1</cell><cell cols="2">15.40 64.80 41.45 55.00 18.90 9.30</cell></row><row><cell>Baseline+FT  *</cell><cell>?1</cell><cell cols="2">13.79 55.82 31.55 54.44 14.52 8.28</cell></row><row><cell>Baseline+FT</cell><cell>?1</cell><cell cols="2">12.81 53.58 30.57 53.21 14.15 7.28</cell></row><row><cell>Baseline+FT+OOCL</cell><cell>?1</cell><cell cols="2">12.42 52.25 29.37 52.47 13.21 6.78</cell></row><row><cell>Baseline+FT+OOCL+ECI (EGCL)</cell><cell>?1</cell><cell cols="2">11.51 50.06 27.93 51.14 11.91 6.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY OF OUR EGCL MODEL ON NIGHTOWLS VALIDATION SET IN TERMS OF M R ?2 (LOWER IS BETTER). BASELINE REFERS TO ADAPTED FASTER R-CNN. NOTE THAT THE PERFORMANCE OF BASELINE MODEL IS ACHIEVED BASED ON OUR IMPLEMENTATION SINCE NO OFFICIAL RESULTS ARE REPORTED. FOR ALL RESULTS, LOWER IS BETTER.</figDesc><table><row><cell>Methods</cell><cell cols="4">Reasonable Reasonable small Reasonable occ All</cell></row><row><cell>Baseline [9]</cell><cell>18.80</cell><cell>26.36</cell><cell>58.90</cell><cell>31.45</cell></row><row><cell>Baseline+FT*</cell><cell>17.92</cell><cell>27.08</cell><cell>53.11</cell><cell>30.34</cell></row><row><cell>Baseline+FT</cell><cell>17.65</cell><cell>26.45</cell><cell>51.46</cell><cell>30.28</cell></row><row><cell>Baseline+FT+OOCL</cell><cell>16.99</cell><cell>28.05</cell><cell>52.02</cell><cell>29.95</cell></row><row><cell>Baseline+FT+OOCL+ECI (EGCL)</cell><cell>15.93</cell><cell>26.25</cell><cell>50.36</cell><cell>28.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF OUR EGCL IN TERMS OF M R ?2 ON CITYPERSONS DATASET WITH INCREASING RATIO OF OCCLUDED SAMPLES IN THE CONSTRUCTED EXEMPLAR DICTIONARY (LOWER IS BETTER).</figDesc><table><row><cell cols="3">Occluded Total Occluded Ratio</cell><cell>HO</cell><cell>R+HO</cell><cell>R</cell></row><row><cell>200</cell><cell>800</cell><cell>0.25</cell><cell>51.3</cell><cell>28.2</cell><cell>11.7</cell></row><row><cell>400</cell><cell>1000</cell><cell>0.40</cell><cell>50.0</cell><cell>27.9</cell><cell>11.5</cell></row><row><cell>800</cell><cell>1400</cell><cell>0.57</cell><cell>49.9</cell><cell>27.2</cell><cell>12.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>IN TERMS OF M R ?2 ON R SUBSET AND COMPUTATIONAL COMPLEXITY IN TERMS OF INFERENCE TIME PER IMAGE (IN SECOND) OF OUR EGCL ON CITYPERSONS DATASET WITH INCREASING SIZE OF EXEMPLAR DICTIONARY K. THE BASELINE METHOD AND OUR EGCL WITHOUT ECI MODULE ARE ALSO PRESENTED FOR REFERENCE.</figDesc><table><row><cell>Methods</cell><cell>K</cell><cell>Speed</cell><cell>R</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>0.23</cell><cell>15.40</cell></row><row><cell>w/o ECI</cell><cell>800</cell><cell>0.25</cell><cell>12.42</cell></row><row><cell></cell><cell>200</cell><cell>0.49</cell><cell>12.52</cell></row><row><cell></cell><cell>400</cell><cell>0.57</cell><cell>12.27</cell></row><row><cell>w/ ECI</cell><cell>600</cell><cell>0.66</cell><cell>13.71</cell></row><row><cell></cell><cell>800</cell><cell>0.76</cell><cell>11.51</cell></row><row><cell></cell><cell>1000</cell><cell>0.87</cell><cell>13.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>PERFORMANCE (IN TERMS OF M R ?2 ) OF OUR EGCL AND OTHER METHODS ON CITYPERSONS VALIDATION SUBSET (LOWER IS BETTER). TO HAVE A FAIR COMPARISON, THE EXPERIMENTS ARE CONDUCTED IN THREE DIFFERENT SETTINGS W.R.T. THE USED BACKBONE AND THE SCALE OF INPUT IMAGES. IN TERMS OF M R ?2 ) OF OUR EGCL AND OTHER METHODS ON NIGHTOWLS VALIDATION SUBSET (LOWER IS BETTER).</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Backbone</cell><cell>Input Scale</cell><cell>R</cell><cell>HO</cell><cell cols="4">R+HO Heavy Partial Bare</cell></row><row><cell cols="2">F.RCNN+ATT-vbb [34]</cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">16.4 57.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">F.RCNN+ATT-part [34]</cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">16.0 56.7</cell><cell>38.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Adapted FasterRCNN [9]</cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">15.4 64.8</cell><cell>41.5</cell><cell>55.0</cell><cell>18.9</cell><cell>9.3</cell></row><row><cell>OR-CNN [18]</cell><cell></cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">12.8 55.7</cell><cell>-</cell><cell>55.7</cell><cell>15.3</cell><cell>6.7</cell></row><row><cell cols="2">Adaptive-NMS [58]</cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">11.9 55.2</cell><cell>-</cell><cell>55.2</cell><cell>12.6</cell><cell>6.2</cell></row><row><cell>MGAN [11]</cell><cell></cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">11.5 51.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HGPD [59]</cell><cell></cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">11.3 51.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>R 2 NMS [14]</cell><cell></cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">11.1 53.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Case [13]</cell><cell></cell><cell>VGG-16</cell><cell>?1</cell><cell>11.0</cell><cell>50.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EGCL (ours)</cell><cell></cell><cell>VGG-16</cell><cell>?1</cell><cell cols="2">11.5 50.0</cell><cell>27.9</cell><cell>51.1</cell><cell>11.9</cell><cell>6.1</cell></row><row><cell>InterNet [36]</cell><cell></cell><cell>ResNet-50</cell><cell>?1</cell><cell cols="2">17.9 70.2</cell><cell>48.2</cell><cell>60.4</cell><cell>23.7</cell><cell>13.5</cell></row><row><cell>TLL [38]</cell><cell></cell><cell>ResNet-50</cell><cell>?1</cell><cell>15.5</cell><cell>-</cell><cell>-</cell><cell>53.6</cell><cell>17.2</cell><cell>10.0</cell></row><row><cell>TLL+MRF [38]</cell><cell></cell><cell>ResNet-50</cell><cell>?1</cell><cell cols="2">14.4 52.0</cell><cell>-</cell><cell>52.0</cell><cell>15.9</cell><cell>9.2</cell></row><row><cell cols="2">Repulsion Loss [17]</cell><cell>ResNet-50</cell><cell>?1</cell><cell cols="2">13.2 56.9</cell><cell>-</cell><cell>56.9</cell><cell>16.8</cell><cell>7.6</cell></row><row><cell>ALFNet [19]</cell><cell></cell><cell>ResNet-50</cell><cell>?1</cell><cell cols="2">12.0 43.8</cell><cell>26.3</cell><cell>51.9</cell><cell>11.4</cell><cell>8.4</cell></row><row><cell>CrowdDet [60]</cell><cell></cell><cell>ResNet-50</cell><cell>?1</cell><cell cols="2">12.1 40.0</cell><cell>25.4</cell><cell>47.7</cell><cell>12.9</cell><cell>7.5</cell></row><row><cell cols="2">CrowdDet+EGCL (ours)</cell><cell>ResNet-50</cell><cell>?1</cell><cell cols="2">10.9 39.3</cell><cell>24.8</cell><cell>46.4</cell><cell>11.6</cell><cell>7.4</cell></row><row><cell cols="2">FRCN+A+DT [35]</cell><cell>VGG-16</cell><cell>?1.3</cell><cell>11.1</cell><cell>-</cell><cell>-</cell><cell>44.3</cell><cell>11.2</cell><cell>6.9</cell></row><row><cell>NOH-NMS [61]</cell><cell></cell><cell>ResNet-50</cell><cell>?1.3</cell><cell>10.8</cell><cell>-</cell><cell>-</cell><cell>53.0</cell><cell>11.2</cell><cell>6.6</cell></row><row><cell>CrowdDet [60]</cell><cell></cell><cell>ResNet-50</cell><cell>?1.3</cell><cell cols="2">10.7 38.0</cell><cell>24.3</cell><cell>45.8</cell><cell>10.7</cell><cell>7.3</cell></row><row><cell>MGAN [11]</cell><cell></cell><cell>VGG-16</cell><cell>?1.3</cell><cell cols="2">10.3 49.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Case [13]</cell><cell></cell><cell>VGG-16</cell><cell>?1.3</cell><cell>9.6</cell><cell>48.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CrowdDet+EGCL (ours)</cell><cell>ResNet-50</cell><cell>?1.3</cell><cell>10.5</cell><cell>37.2</cell><cell>23.8</cell><cell>45.3</cell><cell>10.2</cell><cell>6.8</cell></row><row><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PERFORMANCE ( Methods</cell><cell>Backbone</cell><cell>Reasonable</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACF [24]</cell><cell>-</cell><cell>51.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Checkerboards [26]</cell><cell>-</cell><cell>39.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla Faster R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>( IN TERMS OF M R ?2 ) OF OUR EGCL AND OTHER METHODS ON TJU-DHD-PEDESTRIAN DATASET INCLUDING TWO SUB-DATASETS (LOWER IS BETTER).</figDesc><table><row><cell>Subset</cell><cell>Methods</cell><cell>R</cell><cell>HO</cell><cell>R+HO</cell><cell>ALL</cell></row><row><cell></cell><cell>RetinaNet [62]</cell><cell cols="2">34.73 71.31</cell><cell>42.26</cell><cell>44.34</cell></row><row><cell></cell><cell>FCOS [63]</cell><cell cols="2">31.89 69.04</cell><cell>39.38</cell><cell>41.62</cell></row><row><cell>TJU-Ped-campus</cell><cell>FPN [64]</cell><cell cols="2">27.92 67.52</cell><cell>35.67</cell><cell>38.08</cell></row><row><cell></cell><cell>CrowdDet [60]</cell><cell cols="2">25.73 66.38</cell><cell>33.65</cell><cell>35.90</cell></row><row><cell></cell><cell>CrowdDet+EGCL (ours)</cell><cell cols="2">24.84 65.27</cell><cell>32.39</cell><cell>34.87</cell></row><row><cell></cell><cell>RetinaNet [62]</cell><cell cols="2">23.89 61.60</cell><cell>28.45</cell><cell>41.40</cell></row><row><cell></cell><cell>FCOS [63]</cell><cell cols="2">24.35 63.73</cell><cell>28.86</cell><cell>40.02</cell></row><row><cell>TJU-Ped-traffic</cell><cell>FPN [64]</cell><cell cols="2">22.30 60.30</cell><cell>26.71</cell><cell>37.78</cell></row><row><cell></cell><cell>CrowdDet [60]</cell><cell cols="2">20.82 61.22</cell><cell>25.28</cell><cell>36.94</cell></row><row><cell></cell><cell>CrowdDet+EGCL (ours)</cell><cell cols="2">19.73 60.05</cell><cell>24.19</cell><cell>35.76</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://gitlab.com/vgg/nightowlsapi</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance evaluation of object detection algorithms for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="774" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fused dnn: A deep neural network fusion approach to fast and robust pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WACV. IEEE</publisher>
			<biblScope unit="page" from="953" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Illuminating pedestrians via simultaneous detection &amp; segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4950" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3127" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pcn: Part and context information for pedestrian detection with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04483</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive refinement network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><forename type="middle">C H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Count-and similarity-aware r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="88" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nms by representative region: Towards crowded pedestrian detection by proposal pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="750" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporalcontext enhanced detection of heavily occluded pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7774" to="7783" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="637" to="653" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="618" to="634" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6154" to="6162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multilayer channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3210" to="3220" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graininess-aware deep feature learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="732" to="747" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="135" to="151" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-label learning of part detectors for heavily occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3486" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6995" to="7003" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative feature transformation for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9557" to="9566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Feature intertwiner for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11851</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Small-scale pedestrian detection based on somatic topology localization and temporal feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01438</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Taking a look at smallscale pedestrians and occluded pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3143" to="3152" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nightowls: A pedestrians at night dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scharfenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Piegert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mistr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Prokofyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="691" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tju-dhd: A diverse high-resolution dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning hierarchical graph for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detection in crowded scenes: One proposal, multiple predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Nohnms: Improving pedestrian detection by nearby objects hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2020</title>
		<imprint>
			<biblScope unit="page" from="1967" to="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
