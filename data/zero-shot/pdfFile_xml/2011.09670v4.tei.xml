<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Label Encoding for Boundary Discontinuity Free Rotation Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Hou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cnhouliping17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Label Encoding for Boundary Discontinuity Free Rotation Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rotation detection serves as a fundamental building block in many visual applications involving aerial image, scene text, and face etc. Differing from the dominant regression-based approaches for orientation estimation, this paper explores a relatively less-studied methodology based on classification. The hope is to inherently dismiss the boundary discontinuity issue as encountered by the regression-based detectors. We propose new tech-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Rotation detection has recently attracted increasing attention for their important utility across different scenarios, including aerial images, scene text, and faces etc., which is relatively less studied compared with the vast literature * Corresponding author is Junchi Yan.</p><p>in horizental object detectors that do not estimate the exact rotation while only output the horizontal bounding box.</p><p>Many mainstream rotation detection algorithms (including aerial image <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b56">57]</ref>, scene text <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref> and face <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref>) are derived based on the vanilla detection algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Among them, the rotation detection algorithm based on five parameters ([x, y, h, w, ?]) dominates. Similar to the coordinate regression method in horizontal detection, angle parameter is also predicted by regression. Although gratifying results have been achieved, there are still some fundamental flaws in the orientation estimation based on regression. Angle prediction based on regression often introduces boundary discontinuity <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, mainly including periodicity of angle (PoA) and exchangeability of edges (EoE). The main reason for the former is the bounded periodic nature of the angle parameter, while the latter is related to the definition of the bounding box. In general, the root cause is that the ideal predictions are beyond the defined range. Due to the sharp increase in the loss at the boundary, the regression form of the model at the boundary and non-boundary can not be consistent. Therefore, the model has to predict the angle in a more complicated form at the boundary, which increases the burden of the model and also increases the difficulty of prediction at the boundary. This is fatal for the rotation object detection that require high precision, especially for objects with large aspect ratios.</p><p>Most existing works aim to eliminate the sudden loss increase by adding constraints on the loss function or changing the way of calculation, such as IoU Smooth L1 Loss <ref type="bibr" target="#b58">[59]</ref> and Modulated Loss <ref type="bibr" target="#b36">[37]</ref>, as shown in <ref type="table">Table 1</ref>. The advantage is that they can borrow the well developed baselines from horizontal object detectors and reuse the related techniques to boost the detection performance. However, such ad-hoc techniques applied on the regression-based detectors cannot guarantee the full dismiss of boundary discontinuity behavior. <ref type="bibr">To</ref>   <ref type="table">Table 1</ref>: Comparison between different solutions for periodicity of angle (PoA), exchangeability of edges (EoE) and square-like problem (SLP) on DOTA val set. The indicates that the method suffers the corresponding problem.</p><p>(CSL) <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> argues to apply angle classification instead of regression to address PoA. Then, CSL-based method combines with the long-side definition (five-parameter with 180 ? angular range) of bounding box to further tackle the EoE problem. The use of 'CSL+180 ? ' 1 leads to a natural solution to get rid of the boundary discontinuity issue. As angle classification based detectors are still in its early stage, there are still many limitations e.g. very heavy prediction layer and difficulty in handling square-like objects. The former problem is initially explored by the study <ref type="bibr" target="#b56">[57]</ref>. This paper is one of the classification-based endeavors in pushing forward this frontier, with two specific technical advancements as follows.</p><p>First, we design two Densely Coded Labels (DCL) in contrast to the Sparsely Coded Label (SCL, including CSL, One-Hot encoding), which has empirically led to notable training time reduction with meanwhile improved detection accuracy. To make DCL as sensitive to angle distance as CSL, we calculate the decimal difference between the predicted angle and the angle label as a angle distance aware weight. However, this weight will reintroduce the PoA problem, and we find that the long-side definition method is not conducive to the training of square-like objects. Based on the findings of the above two issues, we design Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW). ADARSW can eliminate the PoA and can be adaptively adjusted according to the object's aspect ratio, which can greatly reduce the burden of model training. Combing 'DCL+180 ? +ADARSW' as a whole, extensive experiments and visual analysis on different datasets and detectors prove that DCL-based method can be a better baseline choice than the angle regression-based and CSLbased methods. In summary, our work makes the following contributions:</p><p>1) To improve the robustness especially for objects with small aspect ratio, we propose Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which further improves accuracy by making our proposed DCL-based detector sensitive to angular distance and object's aspect ratio. In contrast, the existing CSL-based detector suffers from its <ref type="bibr" target="#b0">1</ref> Unless otherwise specified, the CSL-based method mentioned in this paper is based on the long-side definition method. long-side definition for detecting square-like objects.</p><p>2) We compare the impact of two classic Densely Coded Labels (DCL) by introducing them to the angle classification task for potential speedup, namely Binary Coded Label (BCL) and Gray Coded Label (GCL), which are more compact than existing CSL. Though GCL has been partly studied in <ref type="bibr" target="#b56">[57]</ref>, while this paper presents a more thorough investigation especially for BCL. We empirically show that DCL, especially BCL can lead to notable training speed boost (about three times) as well as detection accuracy.</p><p>3) Extensive experiments and visual analysis on different datasets and detectors prove the efficacy of our techniques. It outperforms state-of-the-art CSL-based detector <ref type="bibr" target="#b55">[56]</ref> by: 77.37% vs. 76.17% on DOTA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Horizental Object Detection Object detection is a fundamental task in the field of computer vision, and it has developed rapidly in recent years. Classic convolutional neural networks (CNN) based detectors can mainly be divided into two categories: two-stage object detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref> and single-stage object detectors <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref>. Two-stage methods are based on region proposal and have achieved promising results on some benchmarks, whereas singlestage approaches simplify detection as a regression problem to maintain a faster speed. Compared to anchor-based methods, many anchor-free based methods have become extremely popular in recent years. CornerNet <ref type="bibr" target="#b19">[20]</ref>, FCOS <ref type="bibr" target="#b41">[42]</ref>, CenterNet <ref type="bibr" target="#b7">[8]</ref> and ExtremeNet <ref type="bibr" target="#b65">[66]</ref> attempt to predict some keypoints of objects such as corners or extreme points, which are then grouped into bounding boxes. What is even more surprising is that DETR <ref type="bibr" target="#b2">[3]</ref> has constructed a new object detection paradigm based on transformer <ref type="bibr" target="#b42">[43]</ref>, which achieves anchor free and non maximum suppression (NMS) free at the same time. However, horizental object detectors cannot provide accurate orientation and scale information, so they cannot be directly applied to some specific scenes, such as aerial images and scene text.</p><p>Rotation Object Detection Rotation detectors are mainly applied in the aerial images and scene text. Recent advances in multi-oriented object detection are mainly driven by adaption of classical object detectors using rotated bounding boxes or quadrangles to represent multi-oriented objects. In the aerial imagery scene, ICN <ref type="bibr" target="#b1">[2]</ref>, ROI-Transformer <ref type="bibr" target="#b6">[7]</ref>, CAD-Net <ref type="bibr" target="#b60">[61]</ref>, SCRDet <ref type="bibr" target="#b58">[59]</ref>, R 3 Det <ref type="bibr" target="#b52">[53]</ref>, and CSL <ref type="bibr" target="#b55">[56]</ref> achieve promising performance. Gliding Vertex <ref type="bibr" target="#b50">[51]</ref> and RSDet <ref type="bibr" target="#b36">[37]</ref> achieve more accurate object detection through quadrilateral regression prediction. RRPN <ref type="bibr" target="#b32">[33]</ref>, TextBoxes++ <ref type="bibr" target="#b23">[24]</ref> and RRD <ref type="bibr" target="#b24">[25]</ref> and FOTS <ref type="bibr" target="#b29">[30]</ref> are some advanced methods for scene text detection. However, most of the above regression-based arbitrary-oriented methods focus on the prediction of angle using regression yet ignore the boundary discontinuity. Although SCRDet and RSDet solve the boundary discontinuity from the perspective of the loss function, they are not truly boundary discontinuity free methods. CSL-based detector is a new boundary discontinuity free rotation detector, which transforms angular prediction from a regression problem to a classification problem. However, CSL-based method needs to face two obvious shortcomings: heavy prediction layer and unfriendly to square-like objects. In this paper, we aim to solve the above problems from the two perspectives of encoding form and loss function weight. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mitigating Boundary Discontinuity by Classification</head><p>The boundary discontinuity <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> usually refers to the sharp loss increase of the regression-based rotation detector at the boundary situation, which makes the model unable to perform regression prediction in the same ideal and simple form at the boundary as at the non-boundary. The reasons for the boundary discontinuity are related to the definition of the object bounding box. The work <ref type="bibr" target="#b55">[56]</ref> summarizes several commonly used bounding box definitions and the causes of boundary discontinuity corresponding to these methods. Details are as follows:</p><p>1) Five-parameter method with 90 ? angular range (OpenCV definition method): mainly including periodicity of angular (PoA) and exchangeability of edges (EoE).</p><p>2) Five-parameter method with 180 ? angular range (long-side definition method): mainly suffer from periodicity of angular.</p><p>3) Eight-parameter method: sorting of the four corners.</p><p>The main cause of boundary discontinuity based on regression methods is that the ideal predictions are beyond the defined range. As a consequence, the detection result at the boundary of the detector that has not solved the boundary discontinuity often shows inaccurate angle prediction, as shown in the red bounding boxes in <ref type="figure" target="#fig_0">Figure 1</ref>(a). Different solutions are proposed, such as constraining the loss function <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59]</ref>, changing the angle prediction form <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, etc. Although some progress has been made, these methods have their own weaknesses. Many current methods have not notice or eliminate boundary discontinuity from method design. A true boundary discontinuity free detector will provide a more robust high-performance baseline, so designing such detector is a valuable research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>In this section, we first give a retrospection to the recent classification-based rotation detectors namely Circular Smooth Label (CSL) <ref type="bibr" target="#b55">[56]</ref>, pointing out its limitation in achieving a cost-efficient detector. Then we propose our socalled Densely Coded Label (DCL) technique to improve  the efficiency and also develop the Angle Distance and Aspect Ratio Sensitive Weighting technique, to improve its sensitivity to small aspect ratio objects. <ref type="figure" target="#fig_1">Figure 2</ref> shows the relationship between various angle encoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Rethinking on Sparsely Coded Label Encoding</head><p>Instead of using the regression-based loss function, the Circular Smooth Label (CSL) <ref type="bibr" target="#b55">[56]</ref> detectors have been recently proposed which transform rotation detection to a classification task such that the boundary issue naturally disappear. The CSL-based detectors adopt the so-called Sparsely Coded Label (SCL) encoding technique <ref type="bibr" target="#b55">[56]</ref> to discretize the angle into a finite number of intervals, and then predicts a discrete angle by classification 2 . Equation 1 describes the angle prediction process in CSL:</p><formula xml:id="formula_0">Encode: CSL(?Round((? gt ? 90)/?)) Decode: 90 ? ?(Argmax(Sigmoid(logits)) + 0.5)<label>(1)</label></formula><p>where ? gt presents the angle decimal label, ? = AR/C ? indicates the angle discretization granularity. AR and C ? represents angle range (the default value is 180) and the number of angle categories, respectively. can well eliminate the impact of boundary discontinuity, it also brings two thorny problems that hurt the efficiency and efficacy: i) a very heavy prediction layer and ii) unfriendliness to objects with small aspect ratio.</p><p>Thick prediction layer. Equation 2 compares the pre-   diction layer thickness of three angle prediction methods:</p><formula xml:id="formula_1">Th reg. =A Th onehot =Th csl = A ? AR/?<label>(2)</label></formula><p>where A indicates the number of anchors.</p><p>Taking A = 21, AR = 180, w = 1 as an example, the thickness of the prediction layer required by CSL and One-Hot is 3, 780, while the thickness of regressionbased approach is only 21. From the perspective of GFlops and Param, detectors based on CSL have increased by about 82.96% and 23.42% respectively. In addition, the training time of RetinaNet-CSL is three times longer than regression-based detector. The detailed statistical results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Unfriendliness to small aspect ratio objects. Fiveparameter method with 180 ? angular range is a widely used rectangular definition (long-side definition method, [x, y, h, w, ?]) without EoE problem. The ? is determined by the long side (h) of the rectangle and x-axis. However, this definition method is not suitable for square-like box and will suffer a special problem, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>. <ref type="figure" target="#fig_5">Figure 4</ref>(a)-4(b) are ground truth and candidate prediction bounding box with an aspect ratio close to 1, and their angles are 70.6 ? and ?19.7 ? , respectively. By calculating the Intersection-over-Union (IoU) and regression (e.g. smooth l1) or classification (e.g. CSL) loss of these two boxes, we find that the IoU between them is close to 1, but a relatively large loss value is produced. This loss value mainly comes from the angle parameter. Therefore, the prediction results  in <ref type="figure" target="#fig_5">Figure 4</ref>(b) are not allowed by the model, which is too harsh and increases the model's difficulty in predicting objects with small aspect ratio. In fact, this phenomenon becomes less noticeable as the aspect ratio increases. For the definition of square-like objects, using the OpenCV definition method with a period of 90 ? can effectively avoid this problem, but it will introduce EoE problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Densely Coded Label</head><p>The introduction of excessive amount of parameters and the unfriendliness to small aspect ratio objects seriously hurts the applicability of classification based rotation detectors. In this section, we will solve the above problems from the two perspectives of encoding form and loss function weight.</p><p>Binary Coded Label (BCL) <ref type="bibr" target="#b13">[14]</ref> and Gray Coded Label (GCL) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b56">57]</ref> are two Densely Coded Label (DCL) methods commonly used in the field of electronic communication. Their advantage is that they can represent a larger range of values with less coding length. Thus, they can effectively solve the problem of excessively long coding length in CSL and One-Hot based methods. The prediction layer thickness of BCL and GCL based methods are calculated as follows:</p><formula xml:id="formula_2">Th bcl = Th gcl TH dcl =A ? log 2 (AR/?)<label>(3)</label></formula><p>Under the same setting of A = 21, AR = 180, w = 1, the coding length of DCL is only 168. According to <ref type="table" target="#tab_2">Table  2</ref>, GFlops and Param only increase by 3.24% and 0.92%. The training time is almost the same as the regression-based method. The performance of DCL-based method is not drop but rises, and the specific performance comparison can quickly refer to <ref type="figure" target="#fig_0">Figure 1</ref>(b)-1(d) and <ref type="table" target="#tab_6">Table 4</ref>.</p><p>Algorithm 1-2 describe the pseudo codes that generate all n-bit gray and binary coded labels. BCL processes the angle by binarization to obtain a string of codes represented by multiple '0' and '1'. Although the coding length is greatly reduced, there may be huge changes in the coding results between adjacent values, that is, there is no classification tolerance mentioned in the CSL. For example, the three-bit binary coding results of the values '3' and '4' are '011' and '100', respectively. It can be seen that all three positions have changed, resulting in a very large difference in the loss value of the two angle predictions. GCL can solve this problem <ref type="bibr" target="#b56">[57]</ref>. In the encoding of a group of num-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Binary Coded Label (BCL)</head><p>Input: angle range AR, discretization granularity ?.</p><p>Output: A list L containing all binary coded labels; L = [], n = log 2 (AR/?) # coding length; for i in AR do bcl = Bin(i, n) # generate n-bit binary code; L.append(bcl); end for Return L;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Gray Coded Label (GCL)</head><p>Input: coding length n = log 2 (AR/?) (AR means angle range, ? represents angle discretization granularity).</p><p>Output: A list L containing all gray coded labels. if n = 1 then</p><formula xml:id="formula_3">Return [ 0 , 1 ]; else L1 = GCL(n ? 1) # recursive call; L2, L3, L4 = L1.reverse(), [], []; for s1 in L1 do L3.append( 0 + s1); end for for s2 in L2 do L3.append( 1 + s2); end for L = concat(L3, L4); end if Return L;</formula><p>bers, if any two adjacent codes differ only by one binary number, then this kind of encoding is called Gray Code. Due to only one digit is different between the maximum number and the minimum number, it is also called Cyclic Code. The coding results of '3' and '4' in the GCL method are '010' and '110'. <ref type="table" target="#tab_4">Table 3</ref> compares the coding results of BCL and GCL. The shortcomings of GCL are also obvious. Although the encoding forms between adjacent angles are not much different, which makes GCL also have a certain classification tolerance, the encoding differences of angles with large differences are not very significant, such as '1 (001)' and '6 (101)'. In summary, these two methods are agnostic or partially agnostic to the angle distance.</p><p>In the DCL-based method, only the number of categories is a power of 2 to ensure that each coding corresponds to a valid angle. For example, if the 180 degree range is divided into 2 8 = 256 categories, then the range of each division interval is ? = 180/256 = 0.703125 ? . According to the M ax(error) = ?/2 and E(error) = ?/4 proposed in work <ref type="bibr" target="#b55">[56]</ref>, the maximum and expected accuracy error are only 0.3515625 ? and 0.17578125 ? , respectively whose influence on final detecton accuracy can be negligible. How-ever, the above condition is not necessary. We find that even with some redundant invalid codes, there is no significant drop in final performance. Equation 4 specifies the encoding and decoding process of DCL (take BCL as an example):</p><p>Encode: Bin(?Round((? gt ? 90)/?)) Decode: 90 ? ?Int(Round(Sigmoid(logits))) (4) <ref type="figure" target="#fig_3">Figure 3(a)</ref> gives an example which also takes BCL as an embodiment. In the decoding process, the threshold for converting the predicted logits into binary coding is 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Angle Distance and Aspect Ratio Sensitive Weighting</head><p>To make the model sensitive to the distance of the angle, we calculate the decimal difference between the predicted angle and the angle label as an angle distance aware weight. The specific formula is designated as follows:</p><formula xml:id="formula_4">W (??) = log(|??| + 1) = log(|? gt ? ? pred | + 1) ? pred =Decode dcl (logits)<label>(5)</label></formula><p>where ?? denotes the decimal difference between the predicted angle (? pred ) and the angle label (? gt ). logits represents the prediction vector of the angle. However, the above-mentioned angle distance aware weight reintroduces the PoA problem. Take ? gt = ?90, ? pred = 89 as an example, although the angle of the two bounding boxes are very close, a very large weight is calculated. Therefore, we consider adding a periodic trigonometric function to solve this problem. As discussed in Section 4.1, the square-like object is not suitable to be defined by the long-side definition method. We propose an Angle Distance and Aspect Ratio Sensitive Weighting as ADARSW, as shown in Equation <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_5">W ADARSW (??) =| sin(?(??))| = | sin(?(? gt ? ? pred ))| ? = 1, (h gt /w gt ) &gt; r 2, otherwise<label>(6)</label></formula><p>where h gt and w gt are the long and short sides of ground truth. r is the aspect ratio threshold, the default value is 1.5.</p><p>When the object has a certain aspect ratio, the period of | sin(?(? gt ? ? pred ))| is set to 180 ? (? = 1), and when the object is square-like, the period becomes 90 ? (? = 2). Thus, the model can solve the PoA and can flexibly adjust the training strategy for different aspect ratio objects. The DCL-based angle classification loss is as follows:</p><formula xml:id="formula_6">L dcl (? gt , logits) =FL(Encode dcl (? gt ), logits) ? W ADARSW (??)<label>(7)</label></formula><p>where FL indicates focal loss <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss Function</head><p>For RetinaNet-based rotation detection, we use five parameters (x, y, h, w, ?) to represent arbitrary-oriented rectangle. Ranging in [??/2, ?/2), the ? is determined by the long side (h) of the rectangle and x-axis. For DCL based method, it calls for an additional angle classification prediction layer. The other four parameters are predicted by regression, the regression formula is as follows:</p><formula xml:id="formula_7">t x = (x ? x a )/w a , t y = (y ? y a )/h a t w = log(w/w a ), t h = log(h/h a ) t x = (x ? x a )/w a , t y = (y ? y a )/h a t w = log(w /w a ), t h = log(h /h a )<label>(8)</label></formula><p>where x, y, h, w denote the box's center coordinates, height and width respectively. Variables x, x a , x are for the ground-truth box, anchor box, and predicted box, respectively (likewise for y, w, h).</p><p>The multi-task loss is used which is defined as follows:</p><formula xml:id="formula_8">L = ? 1 N N n=1 obj n j?{x,y,h,w} L reg (v nj , v nj ) + ? 2 N N n=1 obj n L dcl (? gt , logits) + ? 3 N N n=1 L cls (p n , t n )<label>(9)</label></formula><p>where N indicates the number of anchors, obj n is a binary value (obj n = 1 for foreground and obj n = 0 for background, no regression for background). v * j denotes the predicted offset vectors, v * j is the targets vector of ground truth. t n represents the label of object, p n is the probability distribution of various classes calculated by sigmoid function. The hyper-parameter ? 1 , ? 2 , ? 3 control the trade-off and are set to {1, 0.5, 0.1} by default. The classification loss L cls is focal loss <ref type="bibr" target="#b26">[27]</ref>. The regression loss L reg is smooth L1 loss as used in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We use Tensorflow <ref type="bibr" target="#b0">[1]</ref> to implement the proposed methods on a server with GeForce RTX 2080 Ti and 11G memory. The experiments in this article are initialized by ResNet50 <ref type="bibr" target="#b12">[13]</ref> by default unless otherwise specified. We perform experiments on both aerial benchmarks and scene text benchmarks to verify the generality of our techniques. Weight decay and momentum are set 0.0001 and 0.9, respectively. We employ MomentumOptimizer over 4 GPUs with a total of 4 images per minibatch (1 images per GPU). With all these processes, we obtain about 20,000 training and 7,000 validation patches. UCAS-AOD [67] contains 1,510 aerial images of approximately 659 ? 1, 280 pixels, with two categories of 14,596 instances in total. In line with <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49]</ref>, we randomly select 1,110 for training and 400 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Protocls</head><p>ICDAR2015 <ref type="bibr" target="#b18">[19]</ref> is the Challenge 4 of ICDAR 2015 Robust Reading Competition, which is commonly used for oriented scene text detection and spotting. This dataset includes 1,000 training images and 500 testing images. ICDAR 2017 MLT <ref type="bibr" target="#b33">[34]</ref> is a multi-lingual text dataset, which includes 7,200 training images, 1,800 validation images and 9,000 testing images. The dataset is composed of complete scene images in 9 languages, and text regions in this dataset can be in arbitrary orientations, being more diverse and challenging.</p><p>HRSC2016 <ref type="bibr" target="#b31">[32]</ref> contains images from two scenarios including ships on sea and ships close inshore. All images are collected from six famous harbors. The training, validation and test set include 436, 181 and 444 images, respectively.</p><p>All the used datasets are trained by 20 epochs in total, and learning rate is reduced tenfold at 12 epochs and 16    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>Comparison of four object orientation detectors. Table 4 compares the performance of a regression based detector: RetinaNet-Reg and three classification based detectors: RetinaNet-CSL, RetinaNet-BCL and RetinaNet-GCL. We mainly focus on comparing five categories with large aspect ratios and more boundary conditions. It can be clearly seen that classification based detector outperforms those based on regression, with about 1.5%-2.3% and 3.6%-5.5% gain in overall performance (mAP 50 ) and five categories performance (5-mAP 50 ). More importantly, the performance of the DCL-based detector is nearly three times faster than the CSL-based detector, and the performance can still be further improved by about 2% and 0.8% in 5-mAP 50 and mAP 50 . <ref type="figure" target="#fig_0">Figure 1</ref> shows the visual qualitative comparison of the four methods under boundary conditions. We can fully draw the   conclusion that the orientation estimation based on classification is a boundary discontinuity free method. Angle discretization granularity. In general, the smaller ?, the higher theoretical upper bound of the model's performance. However, the decrease of ? will lead to an increase in the number of angle categories, which poses a challenge to the angle classification performance of the model. Therefore, we need to explore the impact of ? on the detection performance under different IoU thresholds, and find a suitable range of ?. In order to get the performance indicators under different IoU threshold, we conduct experiments on the DOTA validation set, and the number of image iterations per epoch is 20k. According to <ref type="table" target="#tab_7">Table  5</ref>, when the number of angle categories is between 32 and 128, the performance of the model reaches its peak. If the number of categories is too small, the theoretical accuracy loss is too large, resulting in a sharp drop in performance; if the number of categories is too large, the angle classification network of the model cannot be effectively processed and the performance will decrease slightly. <ref type="figure">Figure 5</ref> shows the comparison of angle estimates under different ?.</p><p>Redundant invalid coding. To make each code have a corresponding different angle value, the number of categories must be a power of 2 in the DCL-based method. However, this is not required. When we only set 180 categories, about 76 codings are invalid, but BCL-based method can still achieve good performance, at 36.35% as shown in <ref type="table" target="#tab_7">Table 5</ref>. We also artificially increase the length based on the theoretical shortest code length to increase the proportion of invalid codes, and the performance is only slightly reduced.</p><p>Angle Distance and Aspect Ratio Sensitive Weighting. We mainly focus on comparing ten categories with small aspect ratio in  ADARSW to the BCL and GCL based methods, which and increase by 2.3% and 2.13% in ten categories performance (10-mAP 50 ). The overall performance has also increased to 67.39% and 67.02%. After ADARSW is used, the model can predict the bounding box as shown in <ref type="figure" target="#fig_5">Figure 4(b)</ref>.</p><p>Comparison on more datasets and detectors. <ref type="table" target="#tab_10">Table 7</ref> further verifies the performance advantage of DCL based methods than CSL and regression based method on more datasets, including the text dataset ICDAR2015, MLT, and another remote sensing dataset UCAS-AOD. It is worth noting that the comparison results are based on large backbone, data augmentation, and multi-scale training and testing. We can still draw the conclusions: classification is better than regression for orientation estimation; DCL outperforms CSL in most cases. In order to verify the portability of DCL, we also conduct experiments on R 3 Det. As shown in <ref type="table" target="#tab_11">Table 8</ref>, DCL can still make R 3 Det get 1.8% improvement under the use of large backbone and data augmentation.</p><p>Visual analysis of angular features. To further analyze the angle classification ability of the model, we use the principal component analysis (PCA) <ref type="bibr" target="#b47">[48]</ref> to visualize each positive angle feature vector. We show the visualization results when the number of angle categories are 4 and 8, as shown in <ref type="figure">Figure 6</ref>. This proves that it is feasible to use classification for orientation estimation, even if only the simplest cross-entropy loss function is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the State-of-the-Art</head><p>We choose DOTA as the main comparison dataset due to the complexity of the aerial image and the large number of small, cluttered and rotated objects. As shown in <ref type="table" target="#tab_11">Table  8</ref>, through data augmentation, multi-scale training and testing commonly used by other advanced methods, RetinaNet-DCL-ResNet152 and R 3 Det-DCL-ResNet152 can achieve competitive performance, about 74.06% and 77.37%, respectively.</p><p>The HRSC2016 contains lots of large aspect ratio ship instances with arbitrary orientation, which poses a huge challenge to the positioning accuracy of the detector. Experimental results show that our model achieves state-ofthe-art performances, about 89.46% (96.41%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper develops the line of research in classification based methodology for rotation detection in two folds: i) for the prediction layer, two Densely Coded Labels (DCL) techniques are devised by shortening the code length to achieve a more light-weighted prediction layer. They both accelerate the training speed of the recently proposed Sparsely Coded Label model in orientation classification based detectors notably. ii) We further propose the technique called Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which further improves the performance by making DCL-based detector sensitive to angular distance and object's aspect ratio. Extensive experiments on different detectors and datasets show competitive performance regarding with both accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of four rotation detectors in the boundary case. Red bounding boxes indicate some bad detection cases (zoom in for better view). CSL and DCL based methods (including BCL and GCL) are totally boundary discontinuity free.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The relationship between the various angle encoding methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) DCL: Binary Coded Label (b) SCL: One-Hot Label (c) SCL: Circular Smooth Label</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Examples of encoding and decoding process of One-Hot, CSL-Gaussian and BCL for angle prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 (</head><label>3</label><figDesc>b) and Figure 3(c) show examples of encoding and decoding process of One-Hot and CSL-Gaussian for angle prediction, both of which are embodiments of the SCL encoding. Although the combined techniques 'CSL+180 ? '</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Illustration for the limitation of the long-edge definition method on the square-like objects. High IoU, but large training loss due to angle difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>DOTA [ 49 ]Figure 5 :</head><label>495</label><figDesc>is comprised of 2,806 large aerial images from different sensors and platforms. Objects in DOTA (a) ? = 180/4 (b) ? = 180/32 (c) ? = 180/128 (d) ? = 180/256 Visualization of detection results under different angle discretization granularity ?. The red and green bounding box indicate ground truth and prediction. exhibit a wide variety of scales, orientations, and shapes. These images are then annotated by experts using 15 object categories. The short names for categories are defined as (abbreviation-full name): PL-Plane, BD-Baseball diamond, BR-Bridge, GTF-Ground field track, SV-Small vehicle, LV-Large vehicle, SH-Ship, TC-Tennis court, BC-Basketball court, ST-Storage tank, SBF-Soccer-ball field, RA-Roundabout, HA-Harbor, SP-Swimming pool, and HC-Helicopter. The fully annotated DOTA benchmark contains 188,282 instances, each of which is labeled by an arbitrary quadrilateral. Half of the original images are randomly selected as the training set, 1/6 as the validation set, and 1/3 as the testing set. We divide the images into 600 ? 600 subimages with an overlap of 150 pixels and scale it to 800 ? 800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>8 Figure 6 :</head><label>86</label><figDesc>(a) ? = 180/4 (b) ? = 180/Angular feature visualization of the RetinaNet-DCL. The red dotted lines divide the different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>give a more elegant and effective solution, the more recent work called Circular Smooth Label Method Box Def. Angle Pred. PoA EoE SLP Speed mAP50:95 RetinaNet Long-Side Def.</figDesc><table><row><cell></cell><cell></cell><cell>Reg.</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>31.49</cell></row><row><cell>RetinaNet</cell><cell>OpenCV Def.</cell><cell>Reg.</cell><cell></cell><cell></cell><cell>?</cell><cell>?1x</cell><cell>34.50</cell></row><row><cell>IoU-Smooth L1 Loss [59]</cell><cell>OpenCV Def.</cell><cell>Reg.</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?1x</cell><cell>36.23</cell></row><row><cell>Modulated Loss [37]</cell><cell>OpenCV Def.</cell><cell>Reg.</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?1x</cell><cell>34.61</cell></row><row><cell>CSL [56]</cell><cell>Long-Side Def.</cell><cell>Cls.: SCL</cell><cell>?</cell><cell>?</cell><cell></cell><cell>? 1 3 x</cell><cell>35.04</cell></row><row><cell>DCL (BCL)</cell><cell>Long-Side Def.</cell><cell>Cls.: DCL</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?1x</cell><cell>36.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison of GFlops and Param over ro- tation detectors, under the same setting and hyperpa- rameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The three-digit binary code and gray code corresponding to the decimal number.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Reg 38.31 60.48 49.77 68.29 51.28 53.63 64.17 RetinaNet-CSL 40.55 66.77 51.50 73.60 53.76 57.24 (+3.61) 65.69 (+1.52) RetinaNet-BCL 41.58 67.98 57.34 74.66 54.28 59.17 (+5.54) 66.53 (+2.36) RetinaNet-GCL 42.55 68.38 56.40 73.53 54.36 59.04 (+5.41) 66.27 (2.10)</figDesc><table><row><cell>Method</cell><cell>BR</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>HA</cell><cell>5-mAP50</cell><cell>mAP50</cell></row><row><cell>RetinaNet-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of four orientation detectors on DOTA test dataset. 5-mAP 50 means the performance of the five categories listed. The number in parentheses indicates the performance gain compared to the RetinaNet-Reg.</figDesc><table><row><cell>Method</cell><cell>?</cell><cell>BR</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>HA</cell><cell cols="2">5-mAP50 mAP50 mAP75 mAP50:95</cell></row><row><cell>Reg</cell><cell>-</cell><cell cols="5">34.52 51.42 50.32 73.37 55.93</cell><cell>53.12</cell><cell>62.21</cell><cell>26.07</cell><cell>31.49</cell></row><row><cell>CSL</cell><cell cols="6">180/180 35.94 53.42 61.06 81.81 62.14</cell><cell>58.87</cell><cell>64.40</cell><cell>32.58</cell><cell>35.04</cell></row><row><cell></cell><cell>180/4</cell><cell cols="5">30.74 40.54 50.98 72.07 59.54</cell><cell>50.77</cell><cell>62.38</cell><cell>24.88</cell><cell>31.01</cell></row><row><cell></cell><cell>180/8</cell><cell cols="5">36.65 52.58 60.46 82.24 61.60</cell><cell>58.71</cell><cell>66.17</cell><cell>33.14</cell><cell>35.77</cell></row><row><cell></cell><cell cols="6">180/32 39.83 54.41 60.62 80.81 60.32</cell><cell>59.20</cell><cell>65.93</cell><cell>35.66</cell><cell>36.71</cell></row><row><cell>BCL</cell><cell cols="6">180/64 38.22 54.70 60.16 80.75 60.11 180/128 36.76 53.73 61.35 82.52 58.42</cell><cell>58.79 58.56</cell><cell>65.00 65.14</cell><cell>34.31 34.28</cell><cell>36.00 35.69</cell></row><row><cell></cell><cell cols="6">180/180 37.42 53.72 58.70 80.73 63.31</cell><cell>58.78</cell><cell>65.83</cell><cell>33.94</cell><cell>36.35</cell></row><row><cell></cell><cell cols="6">180/256 37.66 53.83 60.66 80.43 60.74</cell><cell>58.66</cell><cell>64.97</cell><cell>33.52</cell><cell>35.21</cell></row><row><cell></cell><cell cols="6">180/512 37.93 53.85 58.52 80.04 60.87</cell><cell>58.24</cell><cell>64.88</cell><cell>33.09</cell><cell>34.99</cell></row><row><cell></cell><cell>180/4</cell><cell cols="5">30.90 41.20 48.30 72.93 60.16</cell><cell>50.70</cell><cell>62.98</cell><cell>23.83</cell><cell>30.81</cell></row><row><cell></cell><cell>180/8</cell><cell cols="5">36.88 51.10 59.81 82.40 61.57</cell><cell>58.35</cell><cell>65.23</cell><cell>33.92</cell><cell>35.29</cell></row><row><cell></cell><cell cols="6">180/32 38.04 54.77 60.88 82.75 61.24</cell><cell>59.54</cell><cell>65.11</cell><cell>34.67</cell><cell>36.15</cell></row><row><cell>GCL</cell><cell cols="6">180/64 38.05 54.36 60.59 81.84 60.39</cell><cell>59.05</cell><cell>64.78</cell><cell>33.23</cell><cell>35.67</cell></row><row><cell></cell><cell cols="6">180/128 37.74 54.36 59.43 81.15 60.51</cell><cell>58.64</cell><cell>66.13</cell><cell>33.65</cell><cell>36.34</cell></row><row><cell></cell><cell cols="6">180/256 35.81 53.78 58.35 81.45 59.84</cell><cell>57.85</cell><cell>64.87</cell><cell>33.77</cell><cell>35.97</cell></row><row><cell></cell><cell cols="6">180/512 37.99 54.23 61.61 80.84 62.13</cell><cell>59.36</cell><cell>64.34</cell><cell>34.08</cell><cell>35.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of detection results under different angle discretization granularities denoted by ?. 71.62 65.18 90.70 76.32 78.47 52.26 60.25 66.61 49.15 69.92 66.53 88.92 72.11 66.32 90.79 79.86 79.03 54.11 63.18 67.86 60.04 72.22 67.39 GCL 88.52 73.58 64.38 90.80 77.66 76.38 50.84 59.46 65.83 48.42 69.59 66.27 88.96 75.20 65.24 90.78 79.13 77.95 55.60 61.90 66.18 56.27</figDesc><table><row><cell>Method ADARSW</cell><cell>PL</cell><cell>BD</cell><cell>GTF</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>SP</cell><cell>HC</cell><cell cols="2">10-mAP50 mAP50</cell></row><row><cell>BCL</cell><cell cols="11">88.63 71.72</cell><cell>67.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The verification of ADARSW on small aspect ratio objects in the DOTA dataset.</figDesc><table><row><cell>epochs, respectively. The initial learning rates for Reti-</cell></row><row><cell>naNet is 5e-4. The number of image iterations per epoch for</cell></row><row><cell>DOTA, UCAS-AOD, HRSC2016, ICDAR2015, and MLT</cell></row><row><cell>are 27k, 5k, 10k, 10k, 10k and 10k respectively, and dou-</cell></row><row><cell>bled if data augmentation and multi-scale training are used.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>+1.43) 88.09/92.93 90.38/97.22 89.23 (+0.38) 95.07 (+0.91) 58.32 73.62 65.08 (+1.07) RetinaNet-BCL 81.61 84.79 83.17 (+0.79) 88.15/92.35 90.57/97.86 89.36 (+0.51) 95.10 (+0.94) 58.91 73.14 65.26 (+1.25)</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">ICDAR2015</cell><cell></cell><cell cols="2">UCAS-AOD</cell><cell></cell><cell></cell><cell>MLT</cell></row><row><cell></cell><cell cols="2">Recall Precision</cell><cell>Hmean</cell><cell>car(07/12)</cell><cell>plane(07/12)</cell><cell>mAP 50 (07)</cell><cell>mAP 50 (12)</cell><cell cols="2">Recall Precision</cell><cell>Hmean</cell></row><row><cell>RetinaNet-Reg</cell><cell>81.49</cell><cell>83.29</cell><cell>82.38</cell><cell cols="2">87.28/90.79 90.42/97.52</cell><cell>88.85</cell><cell>94.16</cell><cell>55.70</cell><cell>75.24</cell><cell>64.01</cell></row><row><cell cols="2">RetinaNet-CSL 80.50</cell><cell>87.40</cell><cell>83.81 (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison between classification-based and regression-based methods on more datasets. 2007 or 2012 in bracket means using the 2007 or 2012 evaluation metric. ResNet101, data augmentation, multi-scale training and testing are used. 69.12 17.17 63.49 34.20 37.16 36.20 89.19 69.60 58.96 49.4 52.52 46.69 44.80 46.30 85.53 54.64 75.31 70.44 73.51 77.62 90.84 86.15 86.69 69.60 68.04 73.83 71.10 68.93 76.17 SCRDet++ [58] ResNet101 90.05 84.39 55.44 73.99 77.54 71.11 86.05 90.67 87.32 87.08 69.62 68.90 73.74 71.29 65.08 64.54 39.82 32.07 49.71 65.01 52.58 81.45 44.66 78.51 46.54 56.73 64.40 64.24 36.75 57.14 TOSO [9] ResNet101 80.17 65.59 39.82 39.95 49.71 65.01 53.58 81.45 44.66 78.51 48.85 56.73 64.40 64.24 36.75 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04 BBAVectors [60] ResNet101 88.35 79.96 50.69 62.18 78.43 78.98 87.94 90.85 83.58 84.35 54.13 60.24 65.22 64.28 55.70 72.32 DRN [36] Hourglass104 89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23 R 3 Det [53] ResNet152 89.49 81.17 50.53 66.10 70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17 87.07 48.14 70.97 78.53 80.34 87.45 90.76 85.63 86.87 61.64 70.32 71.92 73.09 67.15 76.64 RetinaNet-DCL (Ours) ResNet152 89.10 84.13 50.15 73.57 71.48 58.13 78.00 90.89 86.64 86.78 67.97 67.25 65.63 74.06 67.05 74.06 R 3 Det-DCL (Ours) ResNet152 89.78 83.95 52.63 69.70 76.84 81.26 87.30 90.81 84.67 85.27 63.50 64.16 68.96 68.79 65.45 75.54 R 3 Det-DCL (Ours) ResNet101 89.14 83.93 53.05 72.55 78.13 81.97 86.94 90.36 85.98 86.94 66.19 65.66 73.72 71.53 68.69 76.97 R 3 Det-DCL (Ours) ResNet152 89.26 83.60 53.54 72.76 79.04 82.56 87.31 90.67 86.59 86.98 67.49 66.88 73.29 70.56 69.99 77.37</figDesc><table><row><cell></cell><cell>Method</cell><cell>Backbone</cell><cell>MS</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP50</cell></row><row><cell></cell><cell>FR-O [49]</cell><cell>ResNet101</cell><cell></cell><cell cols="16">79.09 52.93</cell></row><row><cell></cell><cell>R-DFPN [54]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88</cell><cell>57.94</cell></row><row><cell></cell><cell>R 2 CNN [17]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22</cell><cell>60.67</cell></row><row><cell></cell><cell>RRPN [33]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58</cell><cell>61.01</cell></row><row><cell></cell><cell>ICN [2]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20</cell><cell>68.20</cell></row><row><cell></cell><cell>RADet [23]</cell><cell>ResNeXt101 [50]</cell><cell></cell><cell cols="15">79.45 76.99 48.05 65.83 65.46 74.40 68.86 89.70 78.14 74.97 49.92 64.63 66.14 71.58 62.16</cell><cell>69.09</cell></row><row><cell></cell><cell>RoI-Transformer [7]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67</cell><cell>69.56</cell></row><row><cell></cell><cell>CAD-Net [61]</cell><cell>ResNet101</cell><cell></cell><cell>87.8</cell><cell>82.4</cell><cell>49.4</cell><cell>73.5</cell><cell>71.1</cell><cell>63.5</cell><cell>76.7</cell><cell>90.9</cell><cell>79.2</cell><cell>73.3</cell><cell>48.4</cell><cell>60.9</cell><cell>62.0</cell><cell>67.0</cell><cell>62.2</cell><cell>69.9</cell></row><row><cell>Two-stage methods</cell><cell>AOOD [69] Cascade-FF [15] SCRDet [59] SARD [46] GLS-Net [21] FADet [22] MFIAR-Net [52]</cell><cell>DPN [4] ResNet152 ResNet101 ResNet101 ResNet101 ResNet101 ResNet152</cell><cell></cell><cell cols="15">89.99 81.25 44.50 73.20 68.90 60.33 66.86 90.89 80.99 86.23 64.98 63.88 65.24 68.36 62.13 89.9 80.4 51.7 77.4 68.2 75.2 75.6 90.8 78.8 84.4 62.3 64.6 57.7 69.4 50.1 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 89.93 84.11 54.19 72.04 68.41 61.18 66.00 90.82 87.79 86.59 65.65 64.04 66.68 68.84 68.03 88.65 77.40 51.20 71.03 73.30 72.16 84.68 90.87 80.43 85.38 58.33 62.27 67.58 70.69 60.42 90.21 79.58 45.49 76.41 73.18 68.27 79.56 90.83 83.40 84.68 53.40 65.42 74.17 69.69 64.86 89.62 84.03 52.41 70.30 70.13 67.64 77.81 90.85 85.40 86.22 63.21 64.14 68.31 70.21 62.11</cell><cell>71.18 71.8 72.61 72.95 72.96 73.28 73.49</cell></row><row><cell></cell><cell>Gliding Vertex [51]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32</cell><cell>75.02</cell></row><row><cell></cell><cell>Mask OBB [44]</cell><cell>ResNeXt101</cell><cell></cell><cell cols="15">89.56 85.95 54.21 72.90 76.52 74.16 85.63 89.85 83.81 86.48 54.89 69.64 73.94 69.06 63.32</cell><cell>75.33</cell></row><row><cell></cell><cell>FFA [11]</cell><cell>ResNet101</cell><cell></cell><cell>90.1</cell><cell>82.7</cell><cell>54.2</cell><cell>75.2</cell><cell>71.0</cell><cell>79.9</cell><cell>83.5</cell><cell>90.7</cell><cell>83.9</cell><cell>84.6</cell><cell>61.2</cell><cell>68.0</cell><cell>70.7</cell><cell>76.0</cell><cell>63.7</cell><cell>75.7</cell></row><row><cell></cell><cell>APE [68]</cell><cell>ResNeXt101</cell><cell></cell><cell cols="15">89.96 83.62 53.42 76.03 74.01 77.16 79.45 90.83 87.15 84.51 67.72 60.33 74.61 71.84 65.55</cell><cell>75.75</cell></row><row><cell></cell><cell>CenterMap OBB [45]</cell><cell>ResNet101</cell><cell></cell><cell cols="15">89.83 84.41 54.60 70.25 77.66 78.32 87.19 90.66 84.89 85.27 56.46 69.23 74.13 71.56 66.06</cell><cell>76.03</cell></row><row><cell></cell><cell>FPN-CSL [56]</cell><cell>ResNet152</cell><cell></cell><cell cols="16">90.25 76.81</cell></row><row><cell></cell><cell>IENet [28]</cell><cell>ResNet101</cell><cell></cell><cell cols="16">80.20 57.92</cell></row><row><cell></cell><cell>PIoU [5]</cell><cell>DLA-34 [5]</cell><cell></cell><cell>80.9</cell><cell>69.7</cell><cell>24.1</cell><cell>60.2</cell><cell>38.3</cell><cell>64.4</cell><cell>64.8</cell><cell>90.9</cell><cell>77.2</cell><cell>70.4</cell><cell>46.5</cell><cell>37.1</cell><cell>57.1</cell><cell>61.9</cell><cell>64.0</cell><cell>60.5</cell></row><row><cell>Single-stage methods</cell><cell>P-RSDet [64] O 2 -DNet [47] RSDet [37] PolarDet [63]</cell><cell>ResNet101 Hourglass104 [35] ResNet152 ResNet101</cell><cell></cell><cell cols="16">89.02 73.65 47.33 72.03 70.58 73.71 72.76 90.82 80.12 81.32 59.45 57.87 60.79 65.21 52.59 89.31 73.74 69.82 90.1 82.0 53.8 68.5 70.2 78.7 73.6 91.2 87.1 84.7 64.3 68.2 66.1 69.3 63.7 74.1 89.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Detection accuracy on different objects (AP 50 ) and overall performance (mAP 50 ) evaluation on DOTA. MS indicates that multi-scale training or testing is used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6</head><label>6</label><figDesc>to verify the effectiveness of ADARSW. Under the same environment and hyperparameters, we add</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">mAP (07) mAP (12)</cell></row><row><cell>R 2 CNN [17]</cell><cell>ResNet101</cell><cell>73.07</cell><cell>79.73</cell></row><row><cell>RC1 &amp; RC2 [32]</cell><cell>VGG16</cell><cell>75.7</cell><cell>-</cell></row><row><cell>RRPN [33]</cell><cell>ResNet101</cell><cell>79.08</cell><cell>85.64</cell></row><row><cell>R 2 PN [62]</cell><cell>VGG16</cell><cell>79.6</cell><cell>-</cell></row><row><cell>RetinaNet-H [53]</cell><cell>ResNet101</cell><cell>82.89</cell><cell>89.27</cell></row><row><cell>RRD [25]</cell><cell>VGG16</cell><cell>84.3</cell><cell>-</cell></row><row><cell>RoI-Transformer [7]</cell><cell>ResNet101</cell><cell>86.20</cell><cell>-</cell></row><row><cell>Gliding Vertex [51]</cell><cell>ResNet101</cell><cell>88.20</cell><cell>-</cell></row><row><cell>BBAVectors [60]</cell><cell>ResNet101</cell><cell>88.6</cell><cell>-</cell></row><row><cell>DRN [36]</cell><cell>Hourglass104</cell><cell>-</cell><cell>92.70</cell></row><row><cell>CenterMap OBB [45]</cell><cell>ResNet50</cell><cell>-</cell><cell>92.8</cell></row><row><cell>SBD [31]</cell><cell>ResNet50</cell><cell>-</cell><cell>93.70</cell></row><row><cell>RetinaNet-R [53]</cell><cell>ResNet101</cell><cell>89.18</cell><cell>95.21</cell></row><row><cell>R 3 Det [53]</cell><cell>ResNet101</cell><cell>89.26</cell><cell>96.01</cell></row><row><cell>R 3 Det-DCL (Ours)</cell><cell>ResNet101</cell><cell>89.46</cell><cell>96.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Detection accuracy on HRSC2016. 07 (12) means using the 2007(2012) evaluation metric.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">CSL can only solve the PoA, and the EoE problem can be solved by the 180 ? angular definition method (recall the discussion in Section ??).(a) Ground Truth (b) Prediction after using ADARSW</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Piou loss: Towards accurate oriented object detection in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kean</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toso: Student&apos;st distribution aided one-stage orientation target detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengming</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4057" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pulse code communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gray</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangluan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="294" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Origins of the binary code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="76" to="83" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascade detector with feature fusion for arbitrary-oriented objects in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="686" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A densely connected endto-end neural network for multiscale and multiscene sar ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20881" to="20892" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object detection based on global-local saliency constraint in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1435</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature-attentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Radet: Refine feature pyramid network and multi-layer attention network for arbitrary-oriented object detection of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghua</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">389</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ienet: Interacting embranchment one stage anchor free detector for orientation aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengming</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Omnidirectional scene text detection with sequential-free box discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08299</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rotation invariant neural network-based face detection. In Proceedings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Henry A Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-time rotation-invariant face detection with progressive calibration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2295" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mask obb: A semantic attentionbased mask oriented bounding box representation for multicategory object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sard: Towards scale-aware rotated object detection in aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="173855" to="173865" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Principal component analysis. Chemometrics and intelligent laboratory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-scale feature integrated attention-based rotation network for object detection in vhr aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1686</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50839" to="50849" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">On the arbitrary-oriented object detection: Classification based approaches revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05597</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Scrdet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13316</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Oriented object detection in aerial images with box boundary-aware vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07043</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Polardet: A fast, more precise detector for rotated target in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenshen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjia</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08720</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Objects detection for remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haigang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adaptive period embedding for representing oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Arbitraryoriented object detection via dense feature fusion and attention model for remote sensing super-resolution image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanting</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunkun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
