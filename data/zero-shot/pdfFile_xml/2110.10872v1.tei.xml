<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HENet: Forcing a Network to Think More for Font Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Mu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youdong</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">SHIYI MU</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">SHUGONG XU</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">YOUDONG DING</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HENet: Forcing a Network to Think More for Font Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>2021. HENet: Forcing a Network to Think More for Font Recognition. In AISS &apos;21: 3rd International Conference on Advanced Information Science and System, November 26-28, 2021, AISS, NY . ACM, New York, NY, USA, 8 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Object recognition Additional Key Words and Phrases: neural networks</term>
					<term>font recognition</term>
					<term>pluggable ACM Reference Format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although lots of progress were made in Text Recognition /OCR in recent years, the task of font recognition is remaining challenging.</p><p>The main challenge lies in the subtle difference between these similar fonts, which is hard to distinguish. This paper proposes a novel font recognizer with a pluggable module solving the font recognition task. The pluggable module hides the most discriminative accessible features and forces the network to consider other complicated features to solve the hard examples of similar fonts, called HE Block. Compared with the available public font recognition systems, our proposed method does not require any interactions at the inference stage. Extensive experiments demonstrate that HENet achieves encouraging performance, including on character-level dataset Explor all and word-level dataset AdobeVFR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As is known to us, different contents in the scanned document are always written in various fonts. Thus, font information in scanned document images contains rich semantic information among texts that is very useful for understanding essential information of the whole document image, such as bank application forms, receipts, and insurance claim forms. In addition, font is essential for many media workers. They often need to create a poster or write an article with attractive fonts. The best choice to find a suitable font for these media workers is to take a photo of the desired text style and seek help on font-recognizing websites. Most font websites demand complicated interactions for users and a series of pre-processing steps on images, but the recognition results are still inferior. The proposed method provides a solution without tedious pre-processing processes and interactions for font images, which still achieves a satisfactory performance.</p><p>With the rapid development of deep learning algorithms, great success has been achieved on many computer vision tasks(e.g., image classification, object detection, and optical character recognition). Font recognition can be regarded as a special task of image classification, which takes a raw image as input, and subsequently learns its class-specific feature representation through a CNN-based network. Finally, the result can be predicted with the class-specific feature representation through the classifier layer. The current research on the use of CNNs for deep learning in computer vision has got many encouraging results, including AlexNet <ref type="bibr" target="#b4">[5]</ref> , ResNet <ref type="bibr" target="#b2">[3]</ref> , VGGNet <ref type="bibr" target="#b7">[8]</ref> , GoogleNet <ref type="bibr" target="#b9">[10]</ref> and others.</p><p>However, directly using these networks on font recognition definitely can not get a satisfactory performance. Some task-specific methods are needed for the task of font recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class: HENet</head><p>Our Proposed Method <ref type="figure">Fig. 1</ref>. Comparison between the pipeline of public font website and our proposed method. Most public font website requires users to enter each character for segmentation, while our pipeline does not need all these processes which is an end to end system.</p><p>In contrast to the general image classification, font recognition is more challenging due to the larger space of candidate fonts(i.e., Available datasets usually contain thousands of font classes). Furthermore, texts in different fonts always have a similar overall appearance, only varying in specific stroke style details of individual characters. These differences in fonts can not even be distinguished for an ordinary person. As the popular deep learning-based methods generally are data-driven, the collection of labeled fonts proves to be very difficult. What is more, an ordinary person is not qualified for the labeling task on error-prone font images. All of the above difficulties require the network to explore complicated features under accessible features on limited input font images. Inspired by the great success of deep learning models in various computer vision tasks (e.g., image classification). We proposed a font recognition method for Latin alphabets based on the Convolutional Neural Network called HENet. As shown in <ref type="figure">Fig. 1</ref>, HENet does not depend on any information about character segmentation or character recognition and obtains a significant performance improvement on Explor_all Dataset covering thousands of font classes. To summarize, our contributions are as follows:</p><p>? We present a new approach for font recognition, which is called HENet. The network is end to end and does not depend on any extra input information apart from the text image, which is convenient for people to use to a large extent.</p><p>? We propose a pluggable module named HE Block to improve the accuracy of font recognition for HENet.The HE Block suppresses features with the most prominent response values and compels the network to find more complicated features to make a correct prediction on similar fonts. ? We conduct several experiments, which demonstrate that HENet achieves high accuracy on the character-level dataset Explor_all and word-level dataset Adobe_VFR.</p><p>? We further validate the effectiveness and generalization on different backbone networks and different datasets through some experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Font Recognition</head><p>Font recognition has been explored in the past few years as part of the document analysis system. Therefore many online font recognizers that require users to upload a font image and enter characters one by one for algorithms like pattern matching, have been released with these researches on font recognition, as shown in <ref type="figure">Fig. 1</ref>. These font interfaces, such as rightknights and likefont, need user interactions and relatively complicated image processing steps.</p><p>Researchers generally regard the visual font recognition task as a special image classification task with thousands of similar classes to be distinguished. Carlos et al. <ref type="bibr" target="#b0">[1]</ref> proposed global texture analysis on font recognition that employs the sliding window analysis method to obtain the features of the document, using fourth and third-order moments.</p><p>Tao et al. <ref type="bibr" target="#b10">[11]</ref> applied LBP descriptor-based Chinese character interesting points for representing discriminative font information to build a Chinese font recognition system. The above studies contain two stages(feature extraction and classifier design) that are not end-to-end and need prior knowledge about the data domain.</p><p>Recently, deep learning-based approaches have been emphasized significantly in the fields of computer vision.</p><p>The researchers who study on font recognition change their attention from the traditional two-stage method to new CNN-based high-performance algorithms. To the best of our knowledge, there are researches made on font recognition from different aspects. Wang et al. <ref type="bibr" target="#b11">[12]</ref> proposed such a method that utilizes a Convolutional Neural Network and a domain adaptation technique based on SCAE. The method requires a large amount of unlabeled real-world data and millions of synthetic data, which results in a colossal training cost. <ref type="bibr" target="#b11">[12]</ref> have also collected a font dataset named AdobeVFR. We also evaluate our method on this word-level font dataset. Huang et al. <ref type="bibr" target="#b3">[4]</ref> propose a font recognizer for Chinese characters and Chinese text blocks, which is made up of a modified inception module and convolutions.</p><p>Due to the lack of diversity of the font data, the method of dropregion was proposed to generate a large number of stochastic variant font samples whose local regions are selectively disrupted. Zhang et.al <ref type="bibr" target="#b13">[14]</ref>proposed a method on Chinese calligraphy styles which combines Squeeze-and-excitation block and Haar transform layer in Convolution networks. Yang et.al <ref type="bibr" target="#b12">[13]</ref>proposed a Hangul font cluster recognizer to address the issues caused by indistinguishable fonts and untrained new fonts. Goel N et al. <ref type="bibr" target="#b1">[2]</ref> combines the recent few-shot learning technique like prototype network with font recognition to achieve the effect of identifying new fonts with very few annotated samples per font. Our proposed HENet utilizes a novel module to regularize the feature extraction network, partially restraining the most prominent feature response in class activation maps to explore complicated features for hard examples. Therefore, the subtle difference in similar fonts can be found and is utilized to make a correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we introduce our proposed font recognizing method, which aims to solve the indistinguishable nature of those similar font classes. The architecture of the proposed HENet, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, contains three components, the backbone of feature extraction, HE Block, and font classifier. The feature extraction network learns a preliminary feature representation from the raw font image. These extracted features are sent to HE Block to suppress some of the most discriminative features, forcing the backbone of feature extraction to think more and find other informative  features of strokes and text styles. Eventually, the font classifier can predict a more accurate result by mining the information hidden under the most discriminative feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HE Block</head><p>As the task of font recognition is a multi-classification task with C classes as shown in <ref type="figure" target="#fig_1">Fig. 2. (x, y)</ref>  The concept of HE Block means "Hide and Enhance". In the training phase, we utilize the HE Block to hide a part of the highest response of the category-specific activation map, that is to say, suppress the accessible feature(the alignment and style of the global text). Then the feature extraction is obliged to learn other helpful subtle differences(the stroke details) to make the correct prediction, which helps enhance the performance of font recognition.</p><p>3.1.1 Mask Creation. The process of creating a mask for the category-specific activation map is introduced in this section. For an input activation map ? ? ? , ? ? ? denotes the mask whose values are binary, representing whether the corresponding location in F needs to be hidden. For every element M_c in M, 1 means that the value of the corresponding location in M_c will be hidden, while 0 means the activation at this place will not change completely.</p><p>Maximum Suppression. First, we randomly suppress the maximum responses in the category-specific activation map. These features mainly pay attention to the global texture information that is easy to learn during the current training period. After these global texture features are suppressed, the ability of the whole recognizer correspondingly decreases a bit in this iteration. However, the regularization of the cross-entropy loss forced the network to think more and look for other complicated information like subtle stroke features from other regions, making the network explore the semantics of the input image further. Let ? ? be the maximum mask, corresponding to the ? class of the activation map F, can be expressed by formula as follows:</p><formula xml:id="formula_0">( , ) = 1 if F c (i, j) = max(F c ) 0 otherwise<label>(1)</label></formula><p>Here, max( ) means the maximum response of the category-specific activation map . We randomly hide the max values of ? channel in the whole activation map F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Weight of Mask.</head><p>Suppose we set the weight of the mask as 0. In that case, it will not achieve excellent performance since the complete loss of the most prominent discriminate feature necessarily leads to a significant drop in performance. Let ? be the output of the HE Block, which is denoted as follows:</p><formula xml:id="formula_1">? ( , ) = ( , ) if M c (i, j) = 0 * ( , ) if M c (i, j) = 1 (2)</formula><p>where means the weight of the initial activation map . In other words, as the input of the final classifier, the feature map is substituted with their initial values by the weight when the mask of the corresponding location has a value of 0. In the section of experiment, we set the weight as 0.5, which gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Strategy for Font Recognition</head><p>It is worth mentioning that we need to replace the global average pooling and fully connected layer in the origin backbone network with 1 ? 1 convolution to apply our method, whose output channel is the same as the number of classes. Correspondingly, the output of our backbone network for feature extraction has a size of ? ? , instead of ? 1 ? 1. After the process of HE Block, we resend the feature map to an average pooling layer and get confidence score of prediction on the font image, which is shown as follows:</p><formula xml:id="formula_2">= ( ? )<label>(3)</label></formula><p>Where means the confidence score for each class. Here, AvgPool denotes the operation of average pooling layer.</p><p>Our approach is an end-to-end font recognizer. In the training stage, the activation maps are sent to our proposed HE Block. Then we get the confidence score for each font class through the global average pooling layer, which is used to make a prediction. The accessible global feature is hidden by HE block. Thus the network learns other class-sensitive complicated features covered up by accessible features, which effectively enhances the performance of the recognizer.</p><p>While in the testing phase, HE Block is discarded so that the whole activation map can directly pass through the global average pooling. And then, all of the exploited discriminative features learned by the previous network in the training phase make their effort together to make the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Font recognition is a challenging task due to the subtle difference between the similar candidate font classes, which increases recognition difficulty. We adopt the font dataset explor_all released by O'Donovan et al. <ref type="bibr" target="#b5">[6]</ref> which consists of 69192 images belonging to 1116 font classes to evaluate our algorithm on character-level font recognition. In the dataset Explor_all, each font class contains 62 images(10 numbers and 52 Latin alphabets). In our experiment, we split the whole dataset for training and testing. We randomly choose 10 images in each font class for testing and treat the rest as a training set. The task becomes more challenging because the network has not seen characters divided into the test set in the training period. Thus the network has to learn the class-specific feature for each font class, which requires the network to explore the feature space of font classes further and has the ability to generalize.</p><p>What's more, we also evaluate our method on the word-level font dataset AdobeVFR, which was released by Wang et al. <ref type="bibr" target="#b11">[12]</ref>. The dataset of AdobeVFR comprises 2383 font classes, and each font among them contains 1000 synthetic images.</p><p>The content of word in these images was sampled from a large corpus. Compared to the character-level font dataset, the word-level dataset has more features to be extracted for font recognition. However, the real-world test set has an extremely great difficulty due to the domain shift. We use AdobeVFR to evaluate the font recognizing performance on word images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use the resolution of 64?64 for our character-level font recognition experiments, while the resolution of 64?256 for our word-level font recognition experiments due to the special ratio of the word images. We trained our network whose backbone is modified Resnet18 <ref type="bibr" target="#b2">[3]</ref>. Momentum SGD optimizer is utilized with an initial learning rate of 0.001, which decays by 0.8 for every 5 epochs. We set weight decay as 10 ?4 . Our algorithm is implemented using Pytorch <ref type="bibr" target="#b6">[7]</ref> with a GeForce RTX 2080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>Our method does not require any extra parameters and helps the network find those subtle stroke features that are difficult to learn. Our results are compared with the most recent font recognition approaches on the Explor_all Dataset with a similar experimental setting. We also compared our method with dropout because they both improve performance by masking some features. Dropout randomly masks some neurons for fully connected layers, while our HE block masks the maximum response and partially preserves information preservation. The comparisons results with these state-of-the-art methods are shown in <ref type="table">Table 1</ref> We observe that our method achieves the best accuracy on Explor_all Dataset. We achieve 86.31% top-1 accuracy surpassing DropRegion <ref type="bibr" target="#b3">[4]</ref>(85.71%). DropRegion <ref type="bibr" target="#b3">[4]</ref> obtain a good performance because they modify the inception module by adding 3 branches and uses the augmentation method dropregion. Our method weakens accessible features <ref type="table">Table 1</ref>. Experimental results on Explor_all between our network and other state-of-the-art font recognition models are shown. Our method outperforms existing approaches recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy(top-1) Accuracy(top-5) CalliNet <ref type="bibr" target="#b13">[14]</ref> 65.37 92.50 Dropout <ref type="bibr" target="#b8">[9]</ref> 72.33 94.87 Hanfont <ref type="bibr" target="#b12">[13]</ref> 76.86 95.88 DropRegion <ref type="bibr" target="#b3">[4]</ref> 85.71 98.29 ours 86.31 98.48 like texture and overall appearance, forcing the network to learn more complicated knowledge like subtle stroke differences. It helps the recognizer perform better on distinguishing similar font classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To comprehensively analyze our method, we conduct related experiments to verify the setting of the parameters. We conduct our ablation studies on Explor_all dataset.</p><p>Here, we first confirm the effectiveness of our proposed HE Block on different backbones, and all of them achieve a significant performance improvement. As shown in Then, we show our ablation study analysis on the weight of mask . Top-1 and Top-5 accuracy for different weight settings is shown in <ref type="table">Table 3</ref>. It demonstrates that setting as 0.5 leads to the best performance among experiments and outperforming the network without HE Block( = 1 ) by 0.86%. For top-5 accuracy, setting the weight to 0.7 gives the best performance, which also makes an improvement of 0.44%.</p><p>Finally, we also evaluate our algorithm on the AdobeVFR dataset, which is a word-level font dataset. In <ref type="table" target="#tab_3">Table 4</ref>, the experiment demonstrates that HE Block improves top-1 performance on synthetic validation set as well as the real-world test set by 0.61% and 1.14%. Although The real-world test set is extraordinarily challenging and the training set contains no real-world data, our method still brings considerable improvement on word-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In our paper, we proposed a method to get better performance on font recognition whose categories are very similar.</p><p>Our proposed HE Block suppresses the prominently accessible feature so that the whole network can think more and learn more complicated stroke details. What's more, the utilization of HE Block is pluggable during the training phase</p><p>and does not bring any extra computation cost during inference time. We conduct related experiments with different backbone networks, proving that our method works on different backbones and surpasses the state-of-the-art font recognition models. In our future work, we will make a large-scale real-world font dataset containing both Chinese and Latin characters as a public benchmark for font recognition. Then, we will further explore the popular transformer-based model to obtain global and local strokes information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our overall architecture. Our pipeline contains three components in the training phase: a backbone for feature extraction, HE Block, and a classifier. The HE Block suppresses the most discriminative regions of the category-specific activation maps and forces the network to think more and find alternative complicated features. As a result, the network achieves the best performance by differentiating the similar font classes with more informative regions. The font class can be predicted without HE Block in the testing phase, which does not add extra computation cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>denotes the image-label pair from the dataset (X, Y), where X and Y are the image dataset and the label collection of all C classes. The input of the HE Block ? ? ? is the class-specific activation map that is extracted from the previous network. It needs to be pointed out that F = { : ?[1, C]}, where ? ? is an individual activation map which represents the ? class. H and W denote the size(height and width) of the output from the feature extraction network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>the details of the HE Block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>HE Block performance with different backbones HE Block 85.45 98.24 82.87 97.56 78.70 96.25 78.73 96.01 w/ HE Block 86.31 98.68 83.31 97.60 79.26 96.74 79.17 96.61 Ablation study on weight of mask. Setting weight to 0.5 makes the best performance.</figDesc><table><row><cell>Backbone</cell><cell>ResNet18</cell><cell>ResNet34</cell><cell>ResNet50</cell><cell>ResNet101</cell></row><row><cell>Accuracy</cell><cell cols="4">top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5</cell></row><row><cell cols="4">w/o Weight of Mask Accuracy(top-1) Accuracy(top-5)</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>85.45</cell><cell>98.24</cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell>84.61</cell><cell>98.21</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell>85.44</cell><cell>98.43</cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell>85.63</cell><cell>98.68</cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell>85.91</cell><cell>98.43</cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>86.31</cell><cell>98.48</cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell>86.18</cell><cell>98.43</cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell>85.07</cell><cell>98.38</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>, we can see that in the experiments of different backbone networks, HE Block improves top-1 accuracy by 0.86% for ResNet18, 0.44% for ResNet34, 0.56% for ResNet50, 0.44% for ResNet101.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Evaluation on AdobeVFR dataset.</figDesc><table><row><cell>Accuracy</cell><cell cols="4">syn(top-1) syn(top-5) real(top-1) real(top-5)</cell></row><row><cell>ResNet18</cell><cell>97.62</cell><cell>99.95</cell><cell>46.27</cell><cell>63.35</cell></row><row><cell>ResNet18+HE Block</cell><cell>98.23</cell><cell>99.98</cell><cell>47.41</cell><cell>65.11</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-order statistical texture analysis--font recognition applied</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Avil?s-Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Rangel-Kuoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Reyes-Ayala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrade-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Escarela-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Font-ProtoNet: Prototypical Network-Based Font Identification of Document Images in Low Data Regime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DropRegion training of inception font network for high-performance Chinese font recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haobin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploratory font selection using crowdsourced attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?nis</forename><surname>Peter O&amp;apos;donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>L?beks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2670313" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going Deeper With Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse Discriminative Information Preservation for Chinese character font categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepFont: Identify Your Font from An Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HanFont: large-scale adaptive Hangul font recognizer using CNN and font clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyeok</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heebeom</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyobin</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Injung</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10032-019-00337-w</idno>
		<ptr target="https://doi.org/10.1007/s10032-019-00337-w" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel CNN structure for fine-grained classification of Chinese calligraphy styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiulong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10032-019-00324-1</idno>
		<ptr target="https://doi.org/10.1007/s10032-019-00324-1" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="177" to="188" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
