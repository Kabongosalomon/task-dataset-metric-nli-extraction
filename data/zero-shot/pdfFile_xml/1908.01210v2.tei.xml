<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
							<email>jacobson@cs.toronto.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>sfidler@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as a distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision. Our project website is: https://nv-tlabs.github.io/DIB-R/ * authors contributed equally 33rd</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D visual perception contributes invaluable information when understanding and interacting with the real world. However, the raw sensory input to both human and machine visual processing streams are 2D projections (images), formed by the complex interactions of 3D geometry with light. Enabling machine learning models to understand the image formation process could facilitate disentanglement of geometry from the lighting effects, which is key in achieving invariance and robustness.</p><p>The process of generating a 2D image from a 3D model is called rendering. Rendering is a well understood process in graphics with different algorithms developed over the years. Making these pipelines amenable to deep learning requires us to differentiate through them.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, the authors introduced a differentiable ray tracer which builds on Monte Carlo ray tracing, and can thus deal with secondary lighting effects such as shadows and indirect light. Most of the existing work focuses on rasterization-based renderers, which, while simpler in nature as they geometrically project 3D objects onto the image plane and cannot support more advanced lighting effects, have been demonstrated to work well in a variety of ML applications such as single-image 3D prediction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Here, we follow this line of work.</p><p>Existing rasterization-based approaches typically compute approximate gradients <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref> which impacts performance. Furthermore, current differentiable rasterizertion methods fail to support differentiation with respect to many informative scene properties, such as textures and lighting, leading to low fidelity rendering, and less informative learning signals <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> .</p><p>In this paper, we present DIB-R, an approach to differentiable rendering, which, by viewing rasterization as a combination of local interpolation and global aggregation, allows for the gradients of this process to be computed analytically over the entire image. When performing rasterization of a foreground pixel, similar to <ref type="bibr" target="#b3">[4]</ref>, we define its value as a weighted interpolation of the relevant vertex attributes of the foreground face which encloses it. To better capture shape and occlusion information in learning settings we define the rasterization of background pixels through a distance-based aggregation of global face information. With this definition the gradients of produced images can be passed back through a variety of vertex shaders, and computed with respect to all influencing vertex attributes such as positions, colors, texture, light; as well as camera positions. Our differentiable rasterization's design further permits the inclusion of several well known lighting models.</p><p>We wrap our DIB-R around a simple neural network in which the properties of an initial polygon sphere are predicted with respect to some conditioning input. We showcase this framework in a number of challenging machine learning applications focusing on 3D shape and texture recovery, across which we achieve both numerical and visual state-of-the art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Differentiable Rasterization: OpenDR <ref type="bibr" target="#b21">[22]</ref>, the first in the series of differentiable rasterizationbased renderers, approximates gradients with respect to pixel positions using first-order Taylor approximation, and uses automatic differentiation to back-propagate through the user-specified forward rendering program. In this approach, gradients are non-zero only in a small band around the edges of the mesh faces, which is bound to affect performance. <ref type="bibr" target="#b13">[14]</ref> hand-designs an approximate gradient definition for the movement of faces across image pixels. The use of approximated gradients, and lack of full color information results in noisy 3D predictions, without concave surface features. To analytically compute gradients, Paparazzi <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>, propose to back-propagate the image gradients to the face normals, and then pass them to vertex positions via chain rule. However, their gradient computation is limited to a particular lighting model (Spherical Harmonics), and the use of face normals further prevents their approach to be applied to smooth shading. <ref type="bibr" target="#b24">[25]</ref> designs a C ? smooth differetiable renderer for estimating 3D geometry, while neglecting lighting and texture. <ref type="bibr" target="#b30">[31]</ref> supports per-vertex color and approximates the gradient near boundary with blurring, which produces wired effects and can not cover the full image. <ref type="bibr" target="#b10">[11]</ref> focus on rendering of point cloud and adopts a differentiable reprojection loss to constrain the distribution of predited point clouds, which loses point connectivity and cannot handle texture and lighting. <ref type="bibr" target="#b19">[20]</ref> introduces a probabilistic formulation of rasterization, where each pixel is softly assigned to all faces of the mesh. While inducing a higher computational cost, this clever trick allows gradients to be computed analytically. Parallel to our work, SoftRas-Color <ref type="bibr" target="#b20">[21]</ref> extended this framework to incorporate vertex colors and support texture and lighting theoretically. However, in <ref type="bibr" target="#b20">[21]</ref> each pixel would be influenced by all the faces and thus might have blurry problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SoftRas-Mesh recently proposed in</head><p>The key difference between the parallel work of <ref type="bibr" target="#b20">[21]</ref> and ours is that, similarly to <ref type="bibr" target="#b3">[4]</ref>, we specify each foreground pixel to the most front face and compute analytic gradients of foreground pixels by viewing rasterization as interpolation of local mesh properties. This allows our rendering effect the same as OpenGL pipeline and naturally supports optimization with respect to all vertex attributes, and additionally enables the extension of our pipeline to a variety of different lighting models. In contrast to <ref type="bibr" target="#b3">[4]</ref>, which also uses an interpolation-based approach, but applied to the entire image, our rasterization module allows for soft assignment of background pixels through an aggregation of global features.</p><p>Adverserial 3D Object Generation: Generation of 3D shapes through deep learning has been approached using a Generative Adverserial Network (GAN) <ref type="bibr" target="#b4">[5]</ref> in a plethora of work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30]</ref>. While these approaches require full 3D supervision, differentiable rendering frameworks allow learning 3D object distributions using only 2D supervision <ref type="bibr" target="#b9">[10]</ref>. We showcase our model in the same application, where we are the first to learn a generator for both shape and texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DIB-R: Differentiable Interpolation-based Renderer</head><p>In this section, we introduce our DIB-R. Treating foreground rasterization as an interpolation of vertex attributes allows realistic images to be produced, whose gradients can be fully back-propagated through all predicted vertex attributes, while defining background rasterization as an aggregation of global information during learning allows for better understanding of shape and occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rendering Pipeline</head><p>Many popular rendering APIs, such as OpenGL <ref type="bibr" target="#b35">[36]</ref> and DirectX3D <ref type="bibr" target="#b22">[23]</ref>, decompose the process of rendering 3D scenes into a set of sequential user-defined programs, referred to as shaders. While there exist many different shader types, the vertex, rasterization, and fragment shaders the three most important steps for establishing a complete rendering pipeline. When rendering an image from a 3D polygon mesh, first, the vertex shader projects each 3D vertex in the scene onto the defined 2D image plane. Rasterization is then used to determine which pixels are covered and in what manner, by the primitives these vertices define. Finally, the fragment shader computes how each pixel is colored by the primitives which cover it.</p><p>The vertex and fragment shaders can easily be defined such that they are entirely differentiable. By projecting 3D points onto the 2D image plane by multiplying with the corresponding 3D model, view and projection matrices, the vertex shader operation is directly differentiable. In the fragment shader, pixel colors are decided by a combination of local properties including assigned vertex colors, textures, material properties, and lighting. While the processes through which this information are combined can vary with respect to the chosen rendering model, in most cases this can be accomplished through the application of fully differentiable arithmetic operations. All that remains for our rendering pipeline is the rasterization shader, which presents the main challenge, due to the inherently non-differentiable operations which it requires. In the following section we describe our method for rasterizing scenes such that the derivatives of this operation can be analytically determined. Consider first only the foreground pixels that are covered by one or more faces. Here, in contrast to standard rendering, where a pixel's value is assigned from the closest face that covers it, we treat foreground rasterization as an interpolation of vertex attributes <ref type="bibr" target="#b3">[4]</ref>. For every foreground pixel we perform a z-buffering test <ref type="bibr" target="#b5">[6]</ref>, and assign it to the closest covering face. Each pixel is influenced exclusively by this face. Shown in <ref type="figure">Fig. 1</ref>, a pixel at position p i is covered by face f j with three vertices v 0 , v 1 , v 2 , and each vertex has its own attributes: u 0 , u 1 , u 2 , respectively. p i and v i are 2D coordinates on the image plane while u i are scalars. We compute the value of this pixel, I i , using barycentric interpolation of the face's vertex attributes:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Differentiable Rasterization</head><formula xml:id="formula_0">I i = w 0 u 0 + w 1 u 1 + w 2 u 2 ,<label>(1)</label></formula><p>where weights w 0 , w 1 and w 2 are calculated over the vertex and pixel positions using a differentiable functions ? k (provided in Appendix):</p><formula xml:id="formula_1">w k = ? k ( v 0 , v 1 , v 2 , p i ), k = 0, 1, 2.</formula><p>(2) While barycentric interpolation has been widely used in OpenGL pipeline. Here, we derive the differentiable reformulation. With this approach, it is easy to back-propagate gradients from a loss function L, defined on the output image, through pixel value I i to vertex attributes u k via chain rule:</p><formula xml:id="formula_2">?I i ?u k = w k , ?I i ? v k = 2 m=0 ?I i ?w m ?? m ? v k ,<label>(3)</label></formula><formula xml:id="formula_3">?L ?u k = N i=1 ?L ?I i ?I i ?u k , ?L ? v k = N i=1 ?L ?I i ?I i ? v k ,<label>(4)</label></formula><p>where N is the number of pixels covered by the face. Now consider pixels which no faces cover, which we refer to as background pixels. Notice that in the formulation above, the gradients from background pixels cannot back-propagate to any mesh attributes. However, the background pixels provide a strong constraint on the 3D shape, and thus the gradient from them provide a useful signal when learning geometry. Take, for example, pixel p i at position p i which lies outside of face f j , in <ref type="figure">Fig 1.</ref> We want this pixel to still provide a useful learning signal. In addition, information from occluded faces an entirely ignored despite their potential future influence.</p><p>Inspired by the silhouette rasterizetion of <ref type="bibr" target="#b19">[20]</ref>, we define a distance-related probability A j i , that softly assigns face f j to pixel p i as:</p><formula xml:id="formula_4">A j i = exp(? d(p i , f j ) ? ),<label>(5)</label></formula><p>where d(p i , f j ) is the distance function from pixel p i to face f j in the projected 2D space, and ? is a hyper-parameter that controls the smoothness of the probability (details provided in Appendix). We then combine the probabilistic influence of all faces on a particular pixel in the following way:</p><formula xml:id="formula_5">A i = 1 ? n j=1 (1 ? A j i ).<label>(6)</label></formula><p>where n is the number of all the faces. The combination of all A i into their respective pixel positions makes up our alpha channel prediction. With definition, any background pixel can pass its gradients back to positions of all the faces (including those ignored in the foreground pixels due to occlusion) with influence proportional to the distance between them in alpha channel. As all foreground pixels have a minimum distance to some face of 0, they must receive an alpha value of 1, and so gradients will only be passed back through the colour channels as defined above.</p><p>In summary, foreground pixels, which are covered by a specific face, back-propagate gradients though interpolation while background pixels, which are not covered by any face, softly back propagate gradients to all the faces based on distance. In this way, we can analytically determine the gradients for all aspects of our rasterization process and have achieved a fully differentiable rendering pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rendering Models</head><p>In Equation 1, we define pixel values I i by the interpolation of abstract vertex attributes u 0 , u 1 and u 2 . As our renderer expects a mesh input, vertex position is naturally one such attribute, but we also simultaneously support a large array of other vertex attributes as shown in the Appendix. In the following section we outline the vertex attributes the rasterization can interpolate over and then back-propagate through, and the rendering models which the support of these attributes allows. A complete overview of this information is shown in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Basic Models</head><p>Our DIB-R supports basic rendering models where we draw the image directly with either vertex colors or texture. To define the basic colours of the mesh we support vertex attributes as either vertex colour or u,v coordinates over a learned or predefined texture map. Pixel values are determined through bi-linear interpolation of the vertex colours, or projected texture coordinates, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Lighting Models</head><p>We also support 3 different local illumination models: Phong <ref type="bibr" target="#b25">[26]</ref>, Lambertian <ref type="bibr" target="#b15">[16]</ref> and Spherical Harmonics <ref type="bibr" target="#b26">[27]</ref>, where the lighting effect is related to normal, light and eye directions.</p><p>To unify all the different lighting models, we decompose image color, I, into a combination of mesh color I c and lighting factors I l and I s :</p><formula xml:id="formula_6">I = I l I c + I s .<label>(7)</label></formula><p>I c denotes the interpolated vertex colour or texture map values extracted directly from the vertex attributes without any lighting effect, I l and I s donate the lighting factors decided by specific lighting model chosen, where I l will be merged with mesh colour and I s is additional lighting effect that does not rely on I c . We first interpolate light-related attributes such as normals, light or eye directions in rasterization, then apply different lighting models in our fragment shader.</p><p>Phong and Lambertian Models: In the Phong Model, image colour I is decided by vertex normals, light directions, eye directions and material properties through the following equations:</p><formula xml:id="formula_7">I l = k d ( L ? N ), I s = k s ( R ? V ) ? ,<label>(8)</label></formula><p>where, k d , k s and ? are: diffuse reflection, specular reflection, and shininess constants. L, N , V and R are directions of light, normal, eye and reflectance, respectively, which are all interpolated vertex attributes. This results in the following definition for image colour under the Phong model:</p><formula xml:id="formula_8">I P hong = I c k d ( L ? N ) + k s ( R ? V ) ? .<label>(9)</label></formula><p>As a slight simplification of full Phong shading we do not adopt ambient light and set light colour at a constant value of 1. The Lambertian model can be viewed as a further simplification of the Phong Model, where we only consider diffuse reflection, and set I s as zero:  </p><formula xml:id="formula_9">I Lambertian = I c k d ( L ? N ).<label>(10)</label></formula><formula xml:id="formula_10">I SphericalHarmonic = I c n?1 l=0 l m=?l w m l Y m l ( N ),<label>(11)</label></formula><p>where Y l m is an orthonormal basis for spherical functions analogous to a Fourier series where l is the frequency and w m l is the corresponding coefficient for the specific basis. To be specific, we set l as 3 and thus predict 9 coefficients in total. By adjusting different w m l , different lighting effects are simulated. For more details please refer to <ref type="bibr" target="#b26">[27]</ref>, Section 2. Optimization Results. The design of our differentiable renderer allows for optimization over all defined vertex attributes and a variety of rendering models, which we perform a sanity check for in <ref type="figure" target="#fig_1">Fig. 2</ref>. Here, we optimize the L-1 loss between the target images (second row) and predicted rendered images. Note that <ref type="bibr" target="#b16">[17]</ref> should be strictly better than us since it supports ray tracing. However, among rasterization-based renderers we are the first to support optimization of all vertex attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications of DIB-R</head><p>We demonstrate the effectiveness of our framework through three challenging ML applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predicting 3D Objects from Single Images Geometry and color:</head><p>We first apply our approach to the task of predicting a 3D mesh from a single image using only 2D supervision. Taking as input a single RGBA image, with RGB values I and alpha values S, a Convolutional Neural Network F , parameterized by learnable weights ?, predicts the position and color value for each vertex in a mesh with a specified topology (sphere in our case). We then use a renderer R (specified by shader functions ?) to render the mesh predicted by F (I, S; ?) to a 2D silhouetteS and the colored image?. This prediction pipeline and the architecture details for our mesh prediction network F are provided in Appendix. When training this system we separate our losses with respect to the silhouette prediction,S, and the color prediction,?. We use an Intersection-Over-Union (IOU) loss for the silhouette prediction 2 :</p><formula xml:id="formula_11">L IOU (?) = E I 1 ? ||S S || 1 ||S +S ? S S || 1 ,<label>(12)</label></formula><p>where denotes element-wise product. Note that {?,S} = R(F (I, S; ?)) depend on the network's parameters ? via our DIB-R. We further use an L-1 loss for the colored image:</p><formula xml:id="formula_12">L col (?) = E I ||I ??|| 1 .<label>(13)</label></formula><p>When rendering our predicted mesh, we not only use the ground truth camera positions and compare against the original image, but also render from a random second view and compare against the ground truth renderings from this new view <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. This multi-view loss ensures that the network does not only concentrate on the mesh properties in the known perspective. We also regularize the mesh prediction with a smoothness loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, L sm , and a Laplacian loss <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, L lap , which penalize the difference in normals for neighboring faces and the change in relative positions of neighboring vertices, respectively. The full explanation of these regularizers is provided in Appendix.</p><p>The final loss function is then a weighted sum of these four losses: Given an input image, we predict geometry, texture and lighting. During training we render the prediction with a known camera. We use 2D image loss between input image and rendered prediction to to train our prediction networks. Note that the prediction can vary in different rendering models, e.g. texture can be vertex color or a texture map while the lighting can be Lambertian, Phong or Spherical Harmonics.</p><formula xml:id="formula_13">L 1 = L IOU + ? col L col + ? sm L sm + ? lap L lap .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIB-Renderer</head><p>Geometry, Texture, and Light. We next apply our method to an extension of the previous task, where a texture map is predicted instead of vertex colors, and lighting parameters are regressed to produce higher quality mesh predictions. Our neural network F is modified to predict vertex positions, a texture map, and various lighting information, depending on the lighting model used. Our full learning pipeline is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> and more details are included in the Appendix. We apply the same losses as in the previous section.</p><p>To increase the photo-realism of our predictions, we also leverage an adversarial framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. This is accomplished by training a discriminator network, D(?), to differentiate between real images, I, and rendered mesh predictions,?, while our prediction network, F , simultaneously learns to make these predictions. We adopt the W-GAN <ref type="bibr" target="#b1">[2]</ref> loss formulation with gradient penalty as in <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_14">L adv (?, ?) = E I D(I; ?) ? D(?; ?) , L gp (?) = E? (||?? D(?; ?)|| 2 ? 1) 2 .<label>(15)</label></formula><p>Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>, we additionally use a perceptual loss and discriminator feature matching loss to make training more stable:</p><formula xml:id="formula_15">L percep (?) = E I M V i=1 1 N V i ||V i (I) ? V i (?)|| 1 + M D i=1 1 N D i ||D i (I; ?) ? D i (?; ?)|| 1 ,<label>(16)</label></formula><p>where V i denotes the i-th layer of a pre-trained VGG network, V , with N V i elements, D i denotes the i-th layer in the discriminator D with N D i elements, and the numbers of layers in network V and D are M V and M D , respectively. Our full objective function for this task is then:</p><formula xml:id="formula_16">? * , ? * = arg min ? arg max ? (? adv L adv ? ? gp L gp ) + ? per L percep + L 1 .<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D GAN of Textured Shapes via 2D supervision</head><p>In our second application, we further demonstrate the power of our method by training a Generative Adversarial Network (GAN) <ref type="bibr" target="#b4">[5]</ref> to produce 3D textured shapes using only 2D supervision. We train a network F GAN to predict vertex positions and a texture map, and exploit a discriminator D(?) I to differentiate between real images, and rendered predictions. The network F GAN is modified so as to take normally distributed noise as input, in place of an image.</p><p>While empirically the above GAN is able to recover accurate shapes, it fails to produce meaningful textures. We suspect that disentangling both shape and texture by an image-based discriminator is a hard learning task. To facilitate texture modeling, we train a second discriminator D(?) t , which operates over texture map predictions. However, as our dataset does not contain true texture maps which can be mapped onto a deformed sphere, for ground truth textures we instead use the textures produced from our network trained to predict texture and lighting from images (Sec 4.1). To produce these texture maps, we pass every image in our training set through our texture and lighting prediction network, and extract the predicted texture. Our second discriminator then learns to differentiate between textures generated by F GAN , and the extracted learned textures. We train F GAN via W-GAN with gradient penalty <ref type="bibr" target="#b6">[7]</ref>, and use both discriminators to provide a learning signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset:</p><p>As in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>, our dataset comprises 13 object categories from the ShapeNet dataset <ref type="bibr" target="#b2">[3]</ref>. We use the same split of objects into our training and test set as <ref type="bibr" target="#b32">[33]</ref>. We render each model from 24 different views to create our dataset of RGB images used for 2D supervision. To demonstrate the multiple rendering models which DIB-R supports, we render each image with 4 different rendering models: 1) basic model without lighting effects, 2) with Lambertian reflectance, 3) with Phong shading, and 4) with Spherical Harmonics. Further details are provided in the Appendix.  <ref type="figure">Figure 4</ref>: Qualitative results on single image 3D object prediction. First and fifth column is the ground-truth image, the second and sixth columns are the prediction from our model, the third and seventh column are results from SoftRas-Mesh <ref type="bibr" target="#b19">[20]</ref>, the rest two columns are results from N3MR <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predicting 3D Objects from Single Images: Geometry and Color</head><p>Experimental settings: In our experiments, we set ? col = 1, ? sm = 0.001, and ? lap = 0.01. We train one network on all 13 categories. The network is optimized using the Adam optimizer <ref type="bibr" target="#b14">[15]</ref>, with ? = 0.0001, ? 1 = 0.9, and ? 2 = 0.999. The batch size is 64, and the dimension of input image is 64 ? 64. We compare our method with the two most related differentiable renderers, N3MR <ref type="bibr" target="#b13">[14]</ref> and SoftRas-Mesh <ref type="bibr" target="#b19">[20]</ref>, using the same network configurations, training data split and hyperparameters. For quantitative comparison, we first voxelize the predicted mesh into 32 3 volume using a standard voxelization tool binvox 3 provided by ShapeNet <ref type="bibr" target="#b2">[3]</ref> and then evaluate using 3D IOU, a standard metric in 3D reconstruction. We additionally measure the F-score following <ref type="bibr" target="#b31">[32]</ref> between the predicted mesh and ground truth mesh. The tolerance for F-score is set to 0.02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>: <ref type="table" target="#tab_1">Table 1</ref> provides an evaluation. Our DIB-R significantly outperforms other methods, in almost all categories on both metrics. We surpass SoftRas-Mesh/N3MR with 1.92/4.23 points and 5.98/1.23 points in terms of 3D IOU and F-score, respectively. As the only difference in this experiment is the renderer, the quantitative results demonstrate the superior performance of our method. Qualitative examples are shown in <ref type="figure">Fig 4.</ref> Our DIB-R faithfully reconstructs both the fine-detailed color and the geometry of the 3D shape, compared to SoftRas-Mesh and N3MR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predicting 3D</head><p>Objects from Single Images: Geometry, Texture and Light Experimental settings: We adopt a UNet <ref type="bibr" target="#b27">[28]</ref> architecture to predict texture maps. As we deform a mesh from a sphere template, similar to <ref type="bibr" target="#b12">[13]</ref>, we use 2D spherical coordinates as the UV coordinates. The dimension of the input image and predicted texture is 256 ? 256. We use a 6-layer ResNet <ref type="bibr" target="#b8">[9]</ref> architecture to regress the XYZ directions of light at each vertex from the features at the bottleneck layer of UNet network. We only use the Lambertian model and qualitatively compare with N3MR <ref type="bibr" target="#b13">[14]</ref> by measuring the reconstruction error on rendered images under identical settings. Note that SoftRas-Mesh <ref type="bibr" target="#b19">[20]</ref> does not support texture and lighting and so no comparison can be made. In the following sections, we only perform experiments on the car class, which has more diverse texture.</p><p>Results: We provide results in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="figure" target="#fig_3">Fig 5.</ref> As ShapeNet does not provide groundtruth UV texture, we compute the L-1 difference on the rendered image using the predicted texture/texture+lighting and the GT image. Compared to N3MR <ref type="bibr" target="#b13">[14]</ref>, we achieve significantly better results both quantitatively and qualitatively. We obtain about 40% lower L-1 difference on texture and 60% smaller angle difference on lighting direction than N3MR. We also obtain significantly better visual results, in terms of the shape, texture and lighting, as shown in <ref type="figure" target="#fig_3">Fig 5 and</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Texture and Lighting Results With Adversarial Loss</head><p>We now evaluate the effect of adding the adverserial loss to the previous experiment. We demonstrate our DIB-Render with Phong and Spherical Harmonic lighting models. For Phong model we keep diffuse and specular reflectance constant as 1 and 0.4 respectively and predict lighting direction together with shininess constant ? while for Spherical Harmonic model we predict 9 coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental settings:</head><p>We first train the model without adversarial loss for 50000 iterations then fine-tune it with adversarial loss for extra 15000 steps. The detailed network architecture is provided in the Appendix. We set ? adv = 0.5, ? gp = 0.5, and ? per = 1. We fix the learning rate for the discriminator to 1e ?5 and optimize using Adam <ref type="bibr" target="#b14">[15]</ref>, with ? = 0.0001, ? 1 = 0.5, and ? 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results and Separation Study:</head><p>We show qualitative results in <ref type="figure">Fig. 6</ref>. Notice that the network disentangles texture and light quite well, and produces accurate 3D shape. Furthermore, the adverserial loss helps in making the predicted texture look more crisp compared to <ref type="figure" target="#fig_3">Fig 5.</ref> To further study texture and light disentanglement, we render test input images with the same car model but vary the lighting direction ( <ref type="figure">Fig. 7)</ref>. Our predictions recover the shape, texture map and lighting directions. Further examples are provided in <ref type="figure">Fig. 8</ref>. On the left, we fix the lighting and texture but render the car in different camera views, to illustrate consistency of prediction across viewpoints. On the right of the figure, we render images with different shininess constants, but fixed lighting direction, texture and camera view. Here, we find that our model is not able to accurately predict the shininess constant. In this case, the texture map erroneously compensates for the shininess effect. This might be because the shininess effect is not significant enough to be learned by a neural network though 2D supervision. We hope to resolve this issue in future work.   <ref type="figure">Figure 9</ref>: Qualitative examples on CUBbird dataset <ref type="bibr" target="#b34">[35]</ref> and PASCAL3D+ Car dataset <ref type="bibr" target="#b37">[38]</ref> Figure 10: Samples from our 3D GAN trained on car images, from 2 viewpoints (shown in each column). <ref type="figure">Figure 11</ref>: Renderings of objects produced by interpolating between latent codes of our 3D GAN, from 2 views</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Real Images</head><p>Experimental settings: We next test our method on real images. Since real images generally do not have multiview captures for the same object, it would be very hard to infer precise light because light and texture would be entangled together. Following CMR <ref type="bibr" target="#b12">[13]</ref>, we adopt CUB bird dataset <ref type="bibr" target="#b34">[35]</ref> and PASCAL3D+ car dataset <ref type="bibr" target="#b37">[38]</ref>, predicting shape and texture from a single view image. Results: We compare our method with CMR <ref type="bibr" target="#b12">[13]</ref>. Instead of predicting texture flow, we predict texture map directly. Both two methods use GT cameras estimated from structure from motion. <ref type="table" target="#tab_5">Table  3</ref> provides quantitative evaluation of predicted texture and shape on CUB bird dataset. Our show better shape and key points predictions than CMR. While the loss of texture predictions are the same, <ref type="figure">Fig. 9</ref> shows qualitative improvements our method provides. Our textures are of higher fidelity and more realistic. This is because we predict a whole image as the texture map while CMR <ref type="bibr" target="#b12">[13]</ref> adopts N3MR <ref type="bibr" target="#b13">[14]</ref>, which uses face color as the texture and so the restricted face size results in blurriness. For car prediction, both two methods clearly posses poor artifacts. This is because the segmentation in PASCAL3D+ car dataset is estimated from Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>, which is not perfect. In addition, the car textures have more details than birds, which make it very hard to learn good shape and texture. Despite these facts, the visual quality our of predictions continues to display a marked improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">3D GAN of Textured Shapes via 2D Supervision</head><p>Experimental settings: We first train the networks to only predict shape, by only passing gradients back through the silhouette prediction. We then fix the shape prediction and only train to produce textures. We perform this experiment on the car class. Images from 4 primary views are rendered for each predicted mesh in each training iteration, and concatenated together when passed to rendered image discriminator to force generation of objects in a canonical pose. Results: We show the results of randomly sampling from our learned distribution of car shapes and textures in <ref type="figure">Fig 10.</ref> This figure demonstrates the high quality of of shape and texture generations, in addition to their diversity. We also show the result of rendering meshes produced from interpolation between latent codes in <ref type="figure">Fig 11,</ref> to demonstrate the robust nature of our learned distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a complete rasterization-based differentiable renderer for which gradients can be computed analytically. Our framework, when wrapped around a neural network, learns to predict shape, texture, and light from single images. We further showcase our framework to learn a generator of 3D textured shapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Illustration of our Differentiable Rasterization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>We perform a sanity check for our DIB-R by running optimization over several mesh attributes. We optimize w.r.t. different attributes in different rendering models: (a,b) vertex position and color in the vertex color rendering model, (c,d) texture and texture coordinates in the texture rendering model, (e,f) vertex and camera position in Lambertian model, (g) lighting in the Spherical Harmonic model, (h) material in the Phong model. Spherical Harmonic Model: Here, I l is determined by normals while I s is set to 0:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Full architecture of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative examples for 3D shape, texture and light prediction. Col. 1-3: 1) GT rendered image with texture+light, 2) texture only rendered image, 3) light map. Col 4-6: our predictions. Col: 7-9: N3MR<ref type="bibr" target="#b13">[14]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :Figure 8 :</head><label>678</label><figDesc>Qualitative examples for 3D shape, texture and light prediction, when exploiting adverserial loss. Purple rectangle: Input image. Left Example: Phong Lighting. Right Example: Spherical Harmonics. First col: Texture and Light. Second col: Texture. Third col: Light. Forth to Sixth col: Texture; different views. Ours GT Light &amp; Texture Separation Study. Purple rectangle: Input image, which are rendered with the same car model but different lighting directions. Each three columns visualize Texture + Light, Texture, Light. Ours GT Light &amp; Texture Separation Study. Purple rect: Input image. Left: Input images are with same light and texture but vary views. Right: Input images are with the same texture but with different shininess constants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on single image 3D object prediction reported with 3D IOU (%) / F-score (%). 58.5/80.6 45.7/55.3 74.1/46.3 71.3/53.3 41.4/39.1 55.5/43.8 36.7/46.4 67.4/35.0 55.7/83.6 60.2/39.2 39.1/46.9 76.2/74.2 59.4/66.9 57.0/54.7 SoftR. [20] 58.4/71.9 44.9/49.9 73.6/41.5 77.1/51.1 49.7/40.8 54.7/41.7 39.1/39.1 68.4/29.8 62.0/82.8 63.6/39.3 45.3/37.1 75.5/68.6 58.9/55.4 59.3/49.9 Ours 57.0/75.7 49.8/55.6 76.3/52.2 78.8/53.6 52.7/44.7 58.8/46.4 40.3/45.9 72.6/38.8 56.1/82.0 67.7/43.1 50.8/51.5 74.3/73.3 60.9/63.2 61.2/55.8</figDesc><table><row><cell>Category Airplane Bench Dresser</cell><cell>Car</cell><cell>Chair</cell><cell>Display Lamp Speaker</cell><cell>Rifle</cell><cell>Sofa</cell><cell>Table</cell><cell>Phone</cell><cell>Vessel</cell><cell>Mean</cell></row><row><cell>N3MR [14]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results for texture and light prediction. Tex- ture/Texture+Light shows L-1 loss on the rendered image for texture/texture+lighting. Lighting shows the angle be- tween predicted lighting and GT lighting. Lower is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on CUB bird dataset<ref type="bibr" target="#b34">[35]</ref>. Texture and 2D IOU show L-1 loss and 2D IOU loss between predictions and GT, lower is better. Key point evaluates percentage of predicted key points lying in the threshold of 0.1, higher is better.</figDesc><table><row><cell>input</cell><cell>ours</cell><cell>CMR</cell><cell>input</cell><cell>ours</cell><cell>CMR</cell><cell>input</cell><cell>ours</cell><cell>CMR</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We denote E I E I?p data (I)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02392</idno>
		<title level="m">Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8377" to="8386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical z-buffer visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ned</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 20th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to generate and reconstruct 3d meshes with only 2d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09259</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2802" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="371" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lambert</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Photometria. 1760</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">222</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Paparazzi: Surface editing by way of multi-view image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ti Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">221</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond pixel norm-balls: Parametric adversaries using an analytically differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ti Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05567</idno>
		<title level="m">Soft rasterizer: Differentiable rendering for unsupervised single-view mesh reconstruction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to 3D game programming with DirectX 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Luna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LLC</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Stylus Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11149</idno>
		<title level="m">Pix2vex: Image-to-geometry reconstruction using a smooth differentiable renderer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Bui Tuong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GEOMetrics: Exploiting geometric structure for graph-encoded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved adversarial systems for 3d object generation and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised 3d shape learning from image collections in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attila</forename><surname>Szab?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10519</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What do single-view 3d reconstruction networks learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3405" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">OpenGL programming guide: the official guide to learning OpenGL, version 1.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Shreiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning shape priors for single-view 3d completion and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="673" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d object reconstruction from a single depth view with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d-aware scene manipulation via inverse graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
