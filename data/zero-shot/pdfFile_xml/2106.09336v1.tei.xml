<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
							<email>mihaiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
							<email>andreiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
							<email>egbazavan@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<email>wfreeman@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>sukthankar@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>sminchisescu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Automatic 3d pose and shape reconstruction results with THUNDR. (Left) Input image. (Middle) Reconstructed 3d meshes projected on the camera plane and overlayed on the image. (Right) Different viewpoint showing our intermediate predicted marker representation (in green) and the reconstructed surface geometry. THUNDR provides automatic 3D scene placement of the reconstructed humans under a perspective camera model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present THUNDR, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images. Key to our methodology is an intermediate 3d marker representation, where we aim to combine the predictive power of model-free-output architectures and the regularizing, anthropometrically-preserving properties of a statistical human surface model like GHUM-a recently introduced, expressive full body statistical 3d human model, trained endto-end. Our novel transformer-based prediction pipeline can focus on image regions relevant to the task, supports selfsupervised regimes, and ensures that solutions are consistent with human anthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both the fully-supervised and the self-supervised models, for the task of inferring 3d human shape, joint positions, and global translation. More-over, we observe very solid 3d reconstruction performance for difficult human poses collected in the wild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The significant recent progress in 3d human sensing is supported by the development of statistical human surface models and the emergence of different forms of supervised and self-supervised visual inference methods. The use of statistical human pose and shape models offers advantages in the use of an anatomical and semantically meaningful human body representation, during both learning and inference. Human anthropometry could be used to regularize a learning and inference process, which, in the absence of such constraints, and given the ambiguity of 3d lifting from monocular images, could easily run haywire. This is especially true for unfamiliar and complex poses not previously seen in a 'training set'-as they never all are. Semantic models offer not only correspondences with image detector responses (specific body keypoints or semantic segmentation maps) which can give essential alignment signals for 3d self-supervision, but can also help rule out 3d solutions that may otherwise entirely break the symmetry of the body, the relative proportions of limbs, the consistency of the surface in terms of non self-intersection, or the anatomical joint angle limits.</p><p>The choice of evaluation metrics has an important role, too. For now, by far the most used representation-perceived as 'model-independent'-are the 'body joints', a popular concept, neither by virtue of its anatomical clarity (as that point idealization could be bio-mechanically argued against), nor-for computer vision, and more practically-given its lack of ground-truth observability. In practice, human 'body joints' are obtained either by fitting proprietary articulated 3d body models to marker data (internal models of the Mocap system, where the assumptions and error models are not always available) or by human annotators eye-balling joint positions in images, followed by multi-view triangulation to obtain pseudo-ground truth. While the latter have proven extremely useful in bootstraping initial 3d predictors, the joint-click positioning cannot be considered an accurate anatomical reality, in any single image, and even less so, consistently, over a large corpora, especially as for many non-frontal-parallel poses 'joint locations' are difficult to correctly identify, visually. While some form of 3d body joint prediction error seems unavoidable under the current ground-truth and state of the art metrics, a safeguard could be to operate primarily with visually grounded structures and obtain joint estimates using statistical body models, based on their surface estimates, as just a final step.</p><p>In this paper, we rely on the visual reality of 3d body surface markers (in some conceptualization, a 'model-free' representation) and that of a 3d statistical body (a 'modelbased' concept) as pillars in designing a hybrid 3d visual learning and reconstruction pipeline. Markers have the additional advantages of being useful for registration between different parametric models, can be conveniently relied-upon for fitting, and can be used as a reduced representation of body shape and pose, as we will here show. Our model combines multiple novel transformer refinement stages for efficiency and localization of key predictive features, and relies on combining 'model-free' and 'model-based' losses for both accuracy and for results consistent with human anthropometry. Quantitative results in major benchmarks indicate state of the art performance. Extensive qualitative testing in the wild supports the overall feasibility, and the quality of 3d reconstructions produced by THUNDR, under both supervised and self-supervised regimes. Related Work: There is considerable prior work in 3d human sensing which we only briefly mention here without aiming at a full literature review <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref>. Methods sometimes referred as 'model-based' <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9]</ref> rely on statistical human body models like SMPL or GHUM, whereas others sometimes referred to as 'model-free' <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38]</ref> rely on predicting a set of markers or mesh positions, without forms of statistical surface or kinematic regularization based on human anthropometry. While the second class of techniques tend to perform better in benchmarks (which are mostly emphasizing the prediction of 3d joint locations and occasionally joint angles), the former tend to be more semantically and anatomically intuitive, easier to deploy in the context of self-supervised learning, and more robust in environments, or for poses, not encountered during training. In this work we aim to leverage the advantages of both methods: predicting visually observable sets of markers, and yet regularize estimates using statistical kinematic pose and shape models. Moreover, additional innovations in the use of multiple layers of refining visual transformers, produce significant computational efficiency and accuracy gains in benchmarks, for self-supervised learning, and in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>In this section we review our methodology including the 3d statistical body models, the marker based-modeling, as well as the proposed THUNDR learning and inference architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Statistical 3D Human Body Models</head><p>We use a recently introduced statistical 3d human body model called GHUM <ref type="bibr" target="#b32">[33]</ref>, to represent the pose and the shape of the human body. The model has been trained endto-end, in a deep learning framework, using a large corpus of human shapes and motions. The model has generative body shape and facial expressions ? = (? b , ? f ) represented using deep variational auto-encoders and generative pose ? = (? b , ? lh , ? rh ) for the body, left and right hands respectively represented as normalizing flows <ref type="bibr" target="#b34">[35]</ref>. The pelvis translation and rotation are controlled separately, and represented by a 6d rotation representation <ref type="bibr" target="#b39">[40]</ref> r ? R 6?1 and a translation vector t ? R 3?1 w.r.t the origin (0, 0, 0). The mesh consists of of N v = 10, 168 vertices and N t = 20, 332 triangles. To pose the mesh, we apply the GHUM network V(? b , ? b , r, t) ? R Nv?3 to obtain the posed vertices. We omit the facial expressions and left and right hand poses, as we here focus on main body pose and shape. We also drop the b subscript for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Model</head><p>We adopt a pinhole camera model, with intrinsics C = [f x , f y , c x , c y ] and associated perspective projection operation x 2d = ?(x 3d , C), where x 3d ? R 3?1 . Because we work with cropped images, we also adapt our intrinsics, such that projecting the same 3d points -either in the cropped image or the original, full image -would give the same alignment. The transformation of image intrinsics C into corresponding crop intrinsics C c is given by</p><formula xml:id="formula_0">[C c 1] = K[C 1] ,<label>(1)</label></formula><p>where K ? R 5?5 is the scale and translation matrix, adapting the image intrinsics C. By using a perspective camera model, we ensure that reconstructions are obtained in camera space. Hence, we have meaningful translation and relative positioning of one subject in the scene (or relative positioning of multiple subjects) when reconstructing from monocular images. A perspective model is a much more accurate and general representation of the imaging transformation compared to an orthographic one. It is in our view desirable in all cases, in order to go beyond showing just model projections or reconstructions in a human-centred coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Marker-based Modelling</head><p>Current model-based architectures directly predict specific shape or pose parameters from a raw image. Inspired by model-free methods where weaker constraints are applied on outputs, we adopt an intermediate representation given by 3d surface markers. These can capture human shape and pose and we can predict them directly in a 3d camera space.</p><p>However, training purely model-free methods based on surface markers (as opposed to joint locations), faces additional challenges for both supervised and unsupervised learning. First as such markers are different from joint positions, very few datasets have labels for them. Markers may be available for motion capture datasets in both 2d and 3d, but training a reliable detector is not necessarily easy especially if one seeks generalization outside the lab, where most marker-based systems operate. For self-supervised learning, where additional forms of semantic (body part segmenation) analysis are often necessary, the lack of a statistical body model would render such potentially useful signals unavailable. Finally-and especially when learning with small supervised training sets or for exploratory self-supervised learning-, the lack of regularization given by a body model could lead to 3d predictions with inconsistent anthropometry, further derailing a convergent learning process.</p><p>Our approach is to use a 3d surface marker set as an intermediate representation proxy, controlled by both surface (mesh) properties and the parameters of a statistical 3d human pose and shape model (GHUM). For practical considerations, and without loss of generality, we adopt the Human3.6M marker set that consists of N m = 53 units, see <ref type="figure">fig. 2</ref> for details. We next describe two network heads, which given any 3d markers M ? R Nm?3 achieve the following: (i) reconstruct the GHUM mesh through a simple architecture V d (M) ? R Nv?3 , and (ii) recover the corresponding GHUM parameters (?, ?, r, t) from M, so we can also recover an anthropometric mesh equivalent to V d , V p (?, ?, r, t).</p><p>Training the Marker-based Poser (MP) The markers are essentially free 3 dof variables, but they follow the given surface placement description, in our case, the VICON protocol.</p><p>To train a network that maps markers to vertices, we need a dataset of corresponding markers and vertices.</p><p>We take a synthetic sampling approach based on our GHUM model. Given generative codes for pose and shape ?, ? ? N (0; I), r drawn from the Haar distribution on SO In our experiments, we noticed that injecting noise at this point, i.e. M + N (0; I), supports the more accurate retrieval of the full mesh given real, imprecise markers that one could find in motion capture datasets such as CMU or Human3.6M, or as produced by an image-based marker regressor. An overview of the poser function, denoted MP is given in <ref type="figure">fig. 2</ref>. We denote V d the mesh that is directly predicted from markers. We denote V p the mesh that is parametrically obtained by posing the GHUM model given parameters ( ?, ?, r, t) regressed from markers. For training, we use the loss</p><formula xml:id="formula_1">L = L p (V, V p ) + L d (V, V d ),<label>(2)</label></formula><p>where L P and L V are the mean-per-vertex errors, computed using a L 2 metric, between the input mesh, and the parametric and direct meshes, respectively. We also experimented with supervising ( ?, ?, r, t) directly, but learning was not successful. To make the training process easier, we subtract the mean marker position (computed as the 3d centroid of each M) before regressing ?, ? and r and we obtained lower reconstruction errors using this modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">THUNDR</head><p>In <ref type="figure" target="#fig_1">fig. 3</ref> we show an overview of our proposed hybrid learning architecture for monocular 3d body pose and shape estimation. Our architecture is different from existing pose and shape estimation methods, that directly regress the parameters of a human model (i.e. SMPL or GHUM) from a single feature representation of an image. We instead regress an intermediate 3d representation in the form of surface landmarks (markers) and regularize it in training using a statistical body model. Moreover, we preserve the spatial structure of high-level image features by avoiding pooling operations, and relying instead on self-attention to enrich our representation <ref type="bibr" target="#b30">[31]</ref>. We draw inspiration from vision transformers <ref type="bibr" target="#b7">[8]</ref>, as we also use a hybrid convolutional-transformer architecture, and from <ref type="bibr" target="#b35">[36]</ref>, as we explore the idea of iteratively refining estimates by relying on cascaded, input-sensitive processing blocks, with homogeneous parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Our marker poser is based on a constrained marker-prediction pipeline which auto-encodes an initially generated, body mesh that is consistent with the human anthropometry V into a set of markers M via a linear layer characterized by a matrix W. The markers are then used to predict both the GHUM parameters, resulting in a mesh Vp (we center the markers before regressing ?, ? and r) and a free-form mesh V d . Training losses ensure the consistency between V, Vp and V d . We also show a detail of the cannonical marker placement, as attached on the GHUM model. Notice slight left/right and more pronounced front/back placement asymmetries that help disambiguate the model side and facing direction.</p><p>Our network receives as input a cropped image I ? R W ?H?3 of a person, together with the pseudo groundtruth camera intrinsics C c ? R 1?4 of the crop (see ? 2.1). We apply a convolutional neural network (CNN) on the input image and extract a downsampled feature map representation F ? R W 32 ? H 32 ?D . We flatten the feature map along the spatial dimensions to get a sequence of N = W 32 ? H 32 tokens. We append to each token the camera intrinsics and get our input feature sequence F s ? R N ?(D+4) . This sequence is linearly embedded by means of matrix E ? R (D+4)?D , where D is the embedding dimensionality, and concatenate it with an extra learnable [markers] token, F m ? R 1?D . Next, learnable positional embeddings E pos ? R (N +1)?D are added to the sequence. Different from standard transformer architectures, we use a single transformer encoder layer <ref type="bibr" target="#b30">[31]</ref>, TL, to iteratively refine our input representation for a number of L steps. We collect at each stage l ? {1 . . . L}, a refinement update ?M l ? R Nm?3 , with N m the number of markers, from each transformed representation Z l using a shared MLP applied on the representation of the [markers] token,</p><formula xml:id="formula_2">Z 0 = F m F s E + E pos (3) Z l = TL(Z l?1 )<label>(4)</label></formula><formula xml:id="formula_3">?M l = MLP(Z 0 l ).<label>(5)</label></formula><p>The refinement updates ?M l are added to the default marker coordinates, M 0 , as</p><formula xml:id="formula_4">M L = M 0 + ?? L l=1 ?M l ,<label>(6)</label></formula><p>where ? is a parameter controlling the step size. M 0 are computed based on the default GHUM parameters, (? 0 , ? 0 , r 0 , t 0 ), and camera intrinsics. That is, we find the optimal translation t * 0 such that the corresponding posed mesh projects in the center of the image <ref type="bibr" target="#b35">[36]</ref>. Finally, M 0 is computed as</p><formula xml:id="formula_5">M 0 = WV(? 0 , ? 0 , r 0 , t * 0 ).<label>(7)</label></formula><p>We apply the pre-trained marker-based poser MP (see ? 2.2) on M L in order to recover the GHUM mesh and parameters, {V d , ?, ?, r, t}. We also compute the mesh geometry using the standard GHUM poser from the regressed model parameters, V p ( ?, ?, r, t). During training, we use a mixed regime based on both weak 2d supervision losses and full 3d supervision losses, where data is available.</p><p>We include regularization losses for pose and shape, as</p><formula xml:id="formula_6">L ps = ? 2 2 + ? 2 2 .<label>(8)</label></formula><p>For this constraint to also affect the predicted markers M L in a direct manner, we must formulate a consistency loss between the two representations. We set a novel loss that measures the mean per-marker position error (i.e. MPMPE) between the predicted markers and the markers on the surface of V p , i.e. M p = WV p , as</p><formula xml:id="formula_7">L m = 1 N m Nm i=1 M i L ? M i p 2 .<label>(9)</label></formula><p>We use a standard 2d reprojection loss measured with respect to either annotated or predicted keypoints, j ? R K?2 , weighted by a per-keypoint confidence score s ? R K?1 , with K the number of keypoints. From our directly regressed mesh V d we extract 3d joints J via the standard GHUM regressor and project them using camera intrinsics C c to predict 2d keypoints</p><formula xml:id="formula_8">L k = 1 K K i=1 s i j i ? ?(J i (V d ), C c ) 2 .<label>(10)</label></formula><p>Similarly to <ref type="bibr" target="#b35">[36]</ref>, we use a soft differentiable rasterizer <ref type="bibr" target="#b20">[21]</ref> to compute a body part alignment loss with respect to either ground-truth or predicted body part maps B ? R W ?H?15 , with 15 different body part labels</p><formula xml:id="formula_9">L b = 1 W * H W * H i=1 B i ? R(V d , C c ) i 1 ,<label>(11)</label></formula><p>where R is the rasterized image of the 3d body parts of V d , projected using camera intrinsics C c . Given access to 3d supervision with ground-truth vertices V gt and joints J gt , we use standard vertex and 3d keypoints losses:</p><formula xml:id="formula_10">L f = ? v L v (V d , V gt ) + ? j L j (J, J gt ),</formula><p>with L v the MPVE (mean per vertex error) metric and L j the MPJPE (mean per joint position error) metric. Parameters ? v and ? j control the importance of each loss.</p><p>Finally, we can write our full loss function, as follows</p><formula xml:id="formula_11">L = ? ps L ps + ? m L m + ? k L k + ? b L b + L f<label>(12)</label></formula><p>where ? are used to weigh the different loss components. The fully supervised loss L f is only used if there exists 3d ground truth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Datasets We use two datasets containing images in-thewild, COCO2017 <ref type="bibr" target="#b19">[20]</ref> (30,000 images) and OpenImages <ref type="bibr" target="#b17">[18]</ref> (24,000 images) for our weakly-supervised training (WS). We use the 2d keypoint annotations where available, otherwise we rely on a 2d pose detector to supplement missing annotations and use an additional confidence score per keypoint.</p><p>For the fully-supervised (FS) experiments, we use two standard datasets Human3.6M <ref type="bibr" target="#b11">[12]</ref> and 3DPW <ref type="bibr" target="#b31">[32]</ref>. Because the ground-truth of 3DPW is provided as SMPL <ref type="bibr" target="#b21">[22]</ref> 3d meshes, we use GHUM fits to these meshes to report the vertex-to-vertex errors. The MPJPE metrics are reported on the 3d joints regressed from the ground-truth SMPL meshes, as standard in the literature. Differently from existing methods, we use less 3d supervision, with superior results. We did not include additional datasets such as MuCo-3DHP <ref type="bibr" target="#b22">[23]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b23">[24]</ref> or UP3D <ref type="bibr" target="#b18">[19]</ref>, but we believe they could be helpful in further increasing our reconstruction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>In all our experiments we use a ResNet50 <ref type="bibr" target="#b10">[11]</ref> backbone pretrained for the ImageNet <ref type="bibr" target="#b6">[7]</ref> image classification task. Our complete architecture has 25M parameters, 23.5M for the backbone and 1.5M for the transformer layer and the MLP regressor. We use L = 4 stages, step size ? = 0.1, an embedding size 256 and 8 heads for the MultiHeadAttention layer. We train the network for 50 epochs, with batch size of 32, base learning rate of 1e ? 4 and exponential decay 0.99. Our marker poser MP has 8.5M parameters and consists of MLPs with a hidden layer size of 256. The network is trained for 1M steps with a batch size of 128. All our networks were trained on a single V100 GPU with 16GB of memory. Our code is implemented in TensorFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Pose and Shape Reconstruction</head><p>For this task, we report several common error metrics that are used for evaluating the error of 3d reconstruction. Most commonly used for 3d joint errors are mean per joint position error (MPJPE) and MJPE-PA, which is MPJPE after rigid alignment of the prediction with ground truth via Procrustes Analysis. The latter metric, removes global misalignment (i.e. scale and rotation) and mainly evaluates the quality of the reconstructed 3d pose. For evaluating 3d shape we use the MPVPE metric between the vertices of the predicted and ground-truth meshes, respectively.</p><p>We evaluate our networks on the two datasets that provide 3d ground-truth information, Human3.6M and 3DPW. For the Human3.6M dataset, there are three commonly used evaluation protocols in the literature. Protocols P1 and P2 consider splitting the official training set into new training and testing subsets, with subjects S1, S5-S8 for training and S9 and S11 for evaluation. P1 evaluates on all available camera views in testing, while P2 only on a single predefined camera view (we consider this to be a highly inconclusive protocol due to its small size and design but report on it in order to compare to other methods). The third and most representative protocol we consider is the official one, where we evaluate on the hold-out test dataset of 900K samples. We also submit predictions on the official website for other methods (where code and models are available) to get comparable results. To be fair in our comparison with other methods, we do not retrain on the whole official training dataset. We show results for all protocols in tables 1, 3 and 5. For P1, we report results for both the weakly supervised regime (WS) and for the mixed regime (WS+FS) in order to compare with prior work. For the official protocol, we report only the MPJPE rounded to the nearest integer, as this is the format the results are returned by the official site. On all protocols and in all training regimes, we obtain stateof-the-art results. In table 4 we report errors on the testing split of the 3DPW dataset. We obtain better results than the prior state-of-the-art, in both the WS+FS regime and the WS an input image, we first use a CNN to extract a feature map F ? R W ?H?D , where W and H represent the spatial extent, and D the number of channels per feature. In this example W = H = 4. We serialize the feature map and concatenate to each feature the camera intrinsics of the image, C. Next, we take our sequence, linearly embed it and add positional encoding. We also add an extra learnable [markers] token to the input. This representation is iteratively transformed L times through the same transformer encoder layer with learnable weights ?. At each transformation stage l, we gather the representation of the [markers] token, feed it through an MLP and regress the marker coordinates refinement ?M l . (Bottom) We compute the default marker coordinates M0 as a function of the image camera intrinsics and default GHUM model parameters. The regressed marker coordinates displacements are added to it and the result represents the final estimated marker coordinates ML. We use the pre-trained marker-based poser MP to get our predicted ghum model vertices and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MPJPE-PA MPJPE Translation Error HMR (WS) <ref type="bibr" target="#b14">[15]</ref> 67.45 106.84 NR HUND (SS) <ref type="bibr" target="#b35">[36]</ref> 66  regime.</p><p>In <ref type="figure" target="#fig_2">fig. 4</ref> we show qualitative reconstructions from THUNDR in-the-wild where one can observe that direct mesh reconstructions V d have better image alignment in general. We also show the image attention map for the [markers] token, aggregated over all transformer layers. Notice how the network learns to focus on faces, hands and feet. Ablation Studies In table 2, we ablate different methodological choices in our proposed architecture in the weakly supervised regime and report results on protocol P2 of Hu-man3.6M dataset. First, we change THUNDR to directly regress GHUM parameters (i.e. ?, ?, r, t) from the input image, skipping our intermediate marker representation and removing the marker poser MP. The convolutional-transformer architecture stays mostly the same, with some minor mod-  Mesh Fitting We test our marker-based poser on the Hu-man3.6M dataset, for which the authors shared 3D marker positions for the training data. We fit an associated GHUM mesh in two ways: (i) by minimizing an energy that takes into account 3d marker ground truth, 2d reprojection errors for all GHUM 3d body joints (including hands and face) and a semantic alignment cost, and ( ii) by simply running our trained marker poser on the ground-truth 3d marker positions to produce a mesh V d . For a sequence fitting example, see our Sup. Mat. First, we compute the mean per-marker error for the models V gt obtained from energy optimization to ground-truth makers M gt (i.e. those recovered from mocap data). This gives an error of 38.4 mm, with an average processing rate of 0.15 frames/second. Second, we compute the errors of markers placed on the predicted mesh V d given ground-truth marker positions. This achieves a slightly higher error of 44.3 mm, but with an average processing rate of 100 frames/second, when ran sequentially. Note that our marker poser has never seen the marker sequences of Human3.6M during training, as the marker poser was trained with samples drawn from a normalizing flow prior based on the CMU motion capture dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>Method MPJPE-PA MPJPE HMR <ref type="bibr" target="#b14">[15]</ref> 56.8 88.0 GraphCMR <ref type="bibr" target="#b16">[17]</ref> 50.1 NR Pose2Mesh <ref type="bibr" target="#b4">[5]</ref> 47.0 64.9 I2L-MeshNet <ref type="bibr" target="#b24">[25]</ref> 41.7 55.7 SPIN <ref type="bibr" target="#b15">[16]</ref> 41.1 NR THUNDR 34.9 48.0  <ref type="table">Table 4</ref>: Performance of different pose and shape estimation methods on the 3DPW dataset.*Shape evaluation is done on GHUM.</p><p>Method MPJPE HMR <ref type="bibr" target="#b14">[15]</ref> 89 SPIN <ref type="bibr" target="#b15">[16]</ref> 68 HUND <ref type="bibr" target="#b35">[36]</ref> 66 THUNDR 53 Ethical Considerations Our methodology aims to decrease bias by introducing flexible forms of self-supervision which would allow, in principle, for system bootstrapping and adaptation to new domains and fair, diverse subject distributions, for which labeled data may be difficult or impossible to collect upfront. Applications like visual surveillance and person identification would not be effectively supported currently, given that model's output does not provide sufficient detail for these purposes. This is equally true of the creation of potentially adversely-impacting deepfakes, as we do not include an appearance model or a joint audio-visual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We have presented THUNDR, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images. Faced with the difficult issue of handling not directly observable human body joints, on which nevertheless many error metrics are based, and aiming at both reconstruction accuracy and good self-supervised learning and generalization under anthropometric human body constraints, we propose a novel model that combines a surface-marker representation with 3d statistical body regularization. The model is designed around a learnable pipeline that refines multiple transformer layers for computational efficiency and for precise, task-sensitive, image feature localization. We demonstrate state-of-the-art results on Human3.6M and 3DPW, in both the fully-supervised and the self-supervised regimes. In our extensive qualitative assessment (see Sup. Mat.) we observe accurate 3d reconstruction performance for difficult human poses collected under challenging imaging conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(3), and t uniformly sampled from a (?20 . . . 20) ? (?20 . . . 20) ? (?20 . . . 20) meters box, we produce a posed GHUM sample mesh V(?, ?, r, t). The associated markers can be retrieved by a simple (fixed) linear regression matrix W ? R Nv?Nm , such that M = WV(?, ?, r, t).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of our proposed THUNDR architecture, to estimate the parameters of a generative human model (GHUM). (Top) Given</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results of THUNDR on images in the wild. From top to bottom: (i) input image (ii) direct mesh reconstructions V d (iii) parameteric mesh reconstructions Vp. Notice that direct mesh reconstruction aligns better, particularly the feet and the limbs. (iv) reconstructions seen from a different viewpoint with regressed marker representations shown in green. (v) the image attention map for the [markers] token, aggregated over all transformer layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of different pose and shape estimation methods on the Human3.6M dataset, with training/testing under protocol P1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on different variations of THUNDR:THUNDR-GHUM directly regresses GHUM parameters from the image and THUNDR-Vp is our standard version were we instead evaluate on the predicted parameteric mesh Vp. in our full method we only use the direct mesh V d , we also show the errors if we instead evaluate on the parameteric mesh V p (we denote this by THUNDR-V p ). These results are also better than THUNDR-GHUM, but worse than THUNDR. This again suggests the utility of our intermediate representation and the importance of working with two separate mesh reconstructions.Marker PoserWe present more details on the training of the marker poser and its additional benefits, outside of the transformer-based 3d pose reconstruction architecture. During training, we experiment with 6 levels of Gaussian noise added to the markers, as ? {0, 2, 5, 10, 20, 50} mm. We ablate each one of the trained marker poser models on the Hu-man3.6M ground-truth marker data. The best performance in reconstruction is achieved for the network with = 5 mm. This model is used in all of our other experiments. During training, the error on the direct mesh reconstruction reaches 2.5 mm MPVPE, while the parameteric mesh reconstruction reaches 3.7 mm MPVPE.</figDesc><table><row><cell>This evaluation is</cell></row><row><cell>done in a weakly supervised regime and we report error metrics on</cell></row><row><cell>Human3.6M protocol P2.</cell></row><row><cell>ifications to accommodate more output variables (i.e. we</cell></row><row><cell>use 4 extra input tokens, one for each GHUM parameter,</cell></row><row><cell>instead of 1). This performs worse than our proposed ar-</cell></row><row><cell>chitecture THUNDR and this shows that our intermediate</cell></row><row><cell>marker representation is easier to learn from image features.</cell></row><row><cell>Next, as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of different pose and shape estimation methods on the Human3.6M dataset, protocol P2.</figDesc><table><row><cell>Method</cell><cell cols="3">MPJPE-PA MPJPE MPVPE</cell></row><row><cell>HUND [36] (SS)</cell><cell>70.3</cell><cell>98.1</cell><cell>NR</cell></row><row><cell>THUNDR (WS)</cell><cell>59.9</cell><cell>86.8</cell><cell>NR</cell></row><row><cell>HMR [15]</cell><cell>81.3</cell><cell>NR</cell><cell>NR</cell></row><row><cell>GraphCMR [17]</cell><cell>70.2</cell><cell>NR</cell><cell>NR</cell></row><row><cell>SPIN [16]</cell><cell>59.2</cell><cell>NR</cell><cell>116.4</cell></row><row><cell>Pose2Mesh [5]</cell><cell>58.9</cell><cell>89.2</cell><cell>NR</cell></row><row><cell>I2L-MeshNet [25]</cell><cell>57.7</cell><cell>93.2</cell><cell>NR</cell></row><row><cell>HUND [36]</cell><cell>56.5</cell><cell>87.7</cell><cell>NR</cell></row><row><cell>THUNDR</cell><cell>51.5</cell><cell>74.8</cell><cell>*88.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance of different methods on the Human3.6M official, representative held-out test set, containing 900K samples.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://mocap.cs.cmu.edu..8" />
	</analytic>
	<monogr>
		<title level="j">Carnegie Mellon Motion Capture Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3d multi-bodies: Fitting sets of plausible 3d human models to ambiguous image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Ehrhadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00980</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="769" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ko?eck?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5243" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7708" to="7717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Estimating Articulated Human Motion with Covariance Scaled Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">IJRR</biblScope>
			<biblScope unit="page" from="371" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bo-dyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Rahul Sukthankar, and Cristian Sminchisescu. GHUM &amp; GHUML: Generative 3D human shape and articulated pose models. CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense renderand-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bill Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06910</idno>
		<title level="m">Neural descent for visual 3d human pose and shape. CVPR 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7054" to="7063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object-occluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07035</idno>
		<title level="m">On the continuity of rotation representations in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
