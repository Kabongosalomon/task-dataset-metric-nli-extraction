<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">R-Drop: Regularized Dropout for Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Liang</surname></persName>
							<email>xbliang3@stu.suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
							<email>lijuwu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<email>wangyuenlp@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Meng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>minzhang@suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">R-Drop: Regularized Dropout for Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dropout is a powerful and widely used technique to regularize the training of deep neural networks. Though effective and performing well, the randomness introduced by dropout causes unnegligible inconsistency between training and inference. In this paper, we introduce a simple consistency training strategy to regularize dropout, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the above inconsistency. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English?German translation (30.91 BLEU) and WMT14 English?French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep learning has achieved remarkable success in various areas, e.g., natural language processing, computer vision, speech/audio processing, etc. When training a deep neural network, regularization techniques <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b74">75]</ref> are indispensable to prevent over-fitting and improve the generalization ability of deep models. Among them, the dropout technique <ref type="bibr" target="#b23">[24]</ref>, the most widely used one, aims to prevent co-adaptation and performs implicit ensemble by simply dropping a certain proportion of hidden units from the neural network during training. Existing literature <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b80">81]</ref> has revealed the possible side effect of dropout that there is an unnegligible inconsistency between training and inference stage of dropout models, i.e., the randomly sampled sub model (caused by dropout) during training is inconsistent with the full model (without dropout) during inference. Through imposing L 2 regularization on the inconsistent hidden states <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b80">81]</ref>, current methods can mitigate the inconsistency problem to some extent but are far from being widely used.</p><p>In this paper, we introduce a simple yet more effective alternative to regularize the training inconsistency induced by dropout, named as R-Drop. Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same N x 1 ( | ) <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( | )</head><p>Softmax</p><formula xml:id="formula_0">( 1 || 2 )</formula><p>Softmax N x FF SA 1 ( | ) <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( | )</head><p>Softmax ( 1 || 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>Feed-Forward N x dropped units units <ref type="figure" target="#fig_0">Figure 1</ref>: The overall framework of our proposed R-Drop. We take Transformer <ref type="bibr" target="#b62">[63]</ref> structure for illustration. The left picture shows that one input x will go through the model twice and obtain two distributions P 1 and P 2 , while the right one shows two different sub models produced by dropout. data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence between the two distributions. That is, R-Drop regularizes the outputs of two sub models randomly sampled from dropout for each data sample in training. In this way, the inconsistency between the training and inference stage can be alleviated. Compared with the dropout strategy in conventional neural network training, R-Drop only adds a KL-divergence loss without any structural modifications.</p><p>From the perspective of deep neural network regularization, our proposed R-Drop can be treated as a new variation of dropout. Different from most of the previous methods that merely work on the hidden units of each layer (e.g., the standard dropout <ref type="bibr" target="#b23">[24]</ref>) or model parameters (e.g., dropconnect <ref type="bibr" target="#b63">[64]</ref>), R-Drop works on both the hidden units and the output of sub models sampled by dropout, which is much more effective. We theoretically analyze the regularization effect of R-Drop, where the result shows that R-Drop can reduce the inconsistency existed in the training and inference.</p><p>Though R-Drop regularization is simple, we find it is surprisingly effective through extensive experiments on 5 tasks with 18 datasets, spanning from natural language processing, including language modeling, neural machine translation, abstractive summarization, and language understanding, to computer vision, i.e., image classification. It creates new records on multiple datasets, such as 30.91 BLEU score on WMT14 English?German and 43.95 on WMT14 English?French translation tasks while only be simply applied to the training of the vanilla Transformer, and also achieves SOTA results on the CNN/DailyMail summarization dataset. These universal improvements clearly demonstrate the effectiveness of R-Drop.</p><p>Our main contributions are summarized as follows:</p><p>? We propose R-Drop, a simple yet effective regularization method built upon dropout, which can be universally applied to train different kinds of deep models.</p><p>? We theoretically show that our R-Drop can reduce the inconsistency between training and inference of the dropout based models.</p><p>? Through extensive experiments on 4 NLP and 1 CV tasks with a total of 18 datasets, we show that R-Drop achieves extremely strong performances, including multiple SOTA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>The overall framework of our R-Drop regularization method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Before elaborating on the details, we first present some necessary notations. Given the training dataset D = {(x i , y i )} n i=1 , the goal of the training is to learn a model P w (y|x), where n is the number of the training samples, (x i , y i ) is the labeled data pair. x i is input data and y i is the label. For example, in NLP, x i can be the source language sentence in machine translation, and y i is the corresponding target language sentence. In CV, x i can be one image, and y i is the categorical class label. The probability distribution of the mapping function is also denoted as P w (y|x), and the Kullback-Leibler (KL) divergence between two distributions P 1 and P 2 is represented by D KL (P 1 ||P 2 ). In the following, we will explain our proposed R-Drop, training algorithm, and theoretical analysis, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">R-Drop Regularization</head><p>We introduce our simple regularization method in this part. Given the training data D = {(x i , y i )} n i=1 , the main learning objective for a deep learning model is to minimize the negative log-likelihood loss function, which is as follow:</p><formula xml:id="formula_1">L nll = 1 n n i=1 ? log P w (y i |x i ).<label>(1)</label></formula><p>Since the deep neural networks are prone to over-fitting, regularization methods such as dropout <ref type="bibr" target="#b60">[61]</ref> are usually adopted during training to reduce the generalization error of the model. Specifically, dropout randomly drops part of units in each layer of the neural network to avoid co-adapting and over-fitting. Besides, dropout also approximately performs to combine exponentially many different neural network architectures efficiently <ref type="bibr" target="#b60">[61]</ref>, while model combination can always improve the model performance. Though simple and effective, there is a huge inconsistency between training and inference that hinders the model performance. That is, the training stage takes the sub model with randomly dropped units, while the inference phase adopts the full model without dropout. Also, the sub models caused by randomly sampled dropout units are also different without any constraints. Based on above observations and the randomness of the structure brought by dropout, we propose our R-Drop to regularize the output predictions of sub models from dropout.</p><p>Concretely, given the input data x i at each training step, we feed x i to go through the forward pass of the network twice. Therefore, we can obtain two distributions of the model predictions, denoted as P w 1 (y i |x i ) and P w 2 (y i |x i ). As discussed above, since the dropout operator randomly drops units in a model, the two forward passes are indeed based on two different sub models (though in the same model). As shown in the right part of <ref type="figure" target="#fig_0">Figure 1</ref>, the dropped units in each layer of the left path for the output prediction P w 1 (y i |x i ) are different from that of the right path for output distribution P w 2 (y i |x i ). Thus the distributions of P w 1 (y i |x i ) and P w 2 (y i |x i ) are different for the same input data pair (x i , y i ). Then, at this training step, our R-Drop method tries to regularize on the model predictions by minimizing the bidirectional Kullback-Leibler (KL) divergence between these two output distributions for the same sample, which is:</p><formula xml:id="formula_2">L i KL = 1 2 (D KL (P w 1 (y i |x i )||P w 2 (y i |x i )) + D KL (P w 2 (y i |x i )||P w 1 (y i |x i ))).<label>(2)</label></formula><p>With the basic negative log-likelihood learning objective L i N LL of the two forward passes:</p><formula xml:id="formula_3">L i N LL = ? log P w 1 (y i |x i ) ? log P w 2 (y i |x i ),<label>(3)</label></formula><p>the final training objective is to minimize L i for data (x i , y i ):</p><formula xml:id="formula_4">L i = L i N LL + ? ? L i KL = ? log P w 1 (y i |x i ) ? log P w 2 (y i |x i )<label>(4)</label></formula><formula xml:id="formula_5">+ ? 2 [D KL (P w 1 (y i |x i )||P w 2 (y i |x i )) + D KL (P w 2 (y i |x i )||P w 1 (y i |x i ))],</formula><p>where ? is the coefficient weight to control L i KL . In this way, our R-Drop further regularizes the model space beyond dropout and improves the generalization ability of a model. Compared Equation (1) with Equation (4), our R-Drop only adds a KL-divergence loss L i KL based on two forward passes in training. Note that our regularization methodology can be universally applied on different model structures if there exists randomness in a model (e.g., dropout) that can produce different sub models or outputs. We leave further explorations as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Algorithm</head><p>The overall training algorithm based on our R-Drop is presented in Algorithm 1. As introduced before, at each training step, Line 3-4 show that we go forward the model and obtain output distributions P w 1 (y|x) and P w 2 (y|x), then Line 5-6 calculate the negative log-likelihood and the KL-divergence randomly sample data pair (x i , y i ) ? D, <ref type="bibr">4:</ref> repeat input data twice as [x i ; x i ] and obtain the output distribution [P w 1 (y i |x i ), P w 2 (y i |x i )], <ref type="bibr">5:</ref> calculate the negative log-likelihood loss L i N LL by Equation <ref type="formula" target="#formula_3">(3)</ref>, <ref type="bibr">6:</ref> calculate the KL-divergence loss L i KL by Equation <ref type="formula" target="#formula_2">(2)</ref>, <ref type="bibr">7:</ref> update the model parameters by minimizing loss L i of Equation (4). 8: end while between the two distributions. It is worth nothing that we do not forward the input data twice, instead, we repeat the input data x and concatenate them ([x; x]) in batch-size dimension, which can make forward procedure happen in the same mini-batch to save the training cost. Finally, the model parameters are updated (Line 7) according to the loss of Equation <ref type="bibr" target="#b3">(4)</ref>. The training will continue over the data epochs till convergence. Compared to the conventional training, our implementation is similar to enlarge the batch size to be double, and one potential limitation is that the computational cost of R-Drop increases at each step. As we show in Section 4.1, similar to other regularization methods (e.g., training w/ or w/o dropout), though R-Drop needs more training to converge, the final optimum is much better with a superior performance. We also show another study of baseline with doubled batch size in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Theoretical Analysis</head><p>We analyze the regularization effect of R-Drop in this subsection. Let h l (x) ? R d denote the output of the l-th layer of a neural network with input vector x, and let ? l ? R d denote a random vector, each dimension of which is independently sampled from a Bernoulli distribution B(p): Then the dropout operation on h l (x) can be represented by h l ? l (x) = 1 p ? l h l (x), where denotes the element-wised product. Hence, the output distribution of the neural network with parameter w after applying dropout is P w ? (y|x) := softmax(linear(h L ? L (? ? ? (h 1 ? 1 (x ? 0 ))))), where ? = (? L , ? ? ? , ? 0 ). The objective for R-Drop enhanced training can be formulated as solving the following constrained optimization problem:</p><formula xml:id="formula_6">min w 1 n n i=1 E ? [? log P w ? (y i |x i )],<label>(5)</label></formula><formula xml:id="formula_7">s.t. 1 n n i=1 E ? (1) ,? (2) [D KL (P w ? (1) (y i |x i )||P w ? (2) (y i |x i )))] ? .<label>(6)</label></formula><p>More precisely, R-Drop optimizes the constrained optimization problem in Equation <ref type="formula" target="#formula_6">(5)</ref> and Equation (6) in a stochastic manner, i.e., it samples two random vectors ? <ref type="bibr" target="#b0">(1)</ref> and ? (2) (corresponding to two dropout instantiations) from Bernoulli distribution and one training instance (x i , y i ), and updates the parameter w according to the stochastic gradient ? w L i from Equation (4).</p><p>As we presented, one problem for dropout is the inconsistency between the training and inference models. Specifically, the training objective for dropout is the average loss of the sub models, i.e., min w</p><formula xml:id="formula_8">1 n n i=1 E ? [? log P w ? (y i |x i )]</formula><p>, while the full model (denoted asP w (y|x)) is used for inference. Our proposed R-Drop enhanced training reduces this inconsistency by forcing the sub structures to be similar. The following proposition uses a linear model to demonstrate that with the constraint in Equation <ref type="bibr" target="#b5">(6)</ref>, the inconsistency gap between the average loss of sub structures and the loss of the full model can be bounded (detailed proof can be found in Appendix B). Proposition 2.1. For a linear model P w (y|x) = softmax(Norm(w T x)) where Norm(?) denotes the normalization layer and x ? R d , with the constraint in Equation (6) in the main paper, we have</p><formula xml:id="formula_9">|L nll (w) ? E ? [L nll (w, ?)]| ? c ? , where L nll (w), L nll (w, ?)</formula><p>are the empirical loss calculated by the full model and a random sub model respectively, c is a constant related to the Liptschtz constant of the softmax operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discussion</head><p>The most related works with our R-Drop are ELD <ref type="bibr" target="#b40">[41]</ref> and FD <ref type="bibr" target="#b80">[81]</ref>, which also study the consistency training with dropout. However, R-Drop has key differences with them. (1) The gap control is from different views. ELD works on directly reducing the gap between the sub model with dropout (train) and the expected full model without dropout (inference), while R-Drop and FD are both working on penalizing the discrepancy between the sub models, the superiority of regularizing the sub models has been proved in FD. (2) The regularization efficiency is different. ELD only back-propagates the gradients through sub model without the full model, which is less efficient than R-Drop that updates both sub models. <ref type="formula" target="#formula_3">(3)</ref> The regularization effect is different. Both ELD and FD use the L 2 distance on hidden states as the regularization loss function. However, this is far away from the main training objective that minimizes the negative log-likelihood over model output distribution. The distance of hidden states is not in the same space as the probability distribution since log-softmax hugely affects the optimization. In comparison, R-Drop utilizes the KL-divergence between the output probability distributions as the consistency regularization, which is in the same space as the training objective. More analysis and experimental comparisons are shown in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To evaluate our approach and show its universal impact, we conduct experiments on 5 different tasks, including 4 natural language processing (NLP) and 1 computer vision (CV) tasks, which are neural machine translation (NMT) (6 datasets), abstractive summarization (1 dataset), language understanding (8 datasets), language modeling (1 dataset), and image classification (2 datasets). For convenience, we utilize 'RD' to represent R-Drop in the tables of experimental results hereinafter. More details of experimental settings for each dataset can be found in Appendix A.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Application to Neural Machine Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model &amp; Training</head><p>We take the most popular Transformer <ref type="bibr" target="#b62">[63]</ref> network as our model structure. The transformer_iwslt_de_en and transformer_vaswani_wmt_en_de_big are the configurations for IWSLT and WMT translations respectively. The weight ? is set as 5 for all translation tasks. Implementation is developed on Fairseq <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We calculate the BLEU scores on these tasks for evaluation, following <ref type="bibr" target="#b79">[80]</ref>. The IWSLT performances are shown in <ref type="table" target="#tab_2">Table 1 and the rich-resource WMT results are in Table 2</ref>. First, we can see that our R-Drop achieves more than 2.0 BLEU score improvements on 8 IWSLT translation tasks, which clearly shows the effectiveness of our method. The results on WMT translations are more impressive. After applying our simple method on the basic Transformer network, we achieve the state-of-the-art (SOTA) BLEU score on WMT14 En?De (30.91) and En?Fr (43.95) translation tasks, which surpass current SOTA models, such as the BERT-fused NMT <ref type="bibr" target="#b79">[80]</ref> model that leverages large-scale monolingual data, and the Data Diversification <ref type="bibr" target="#b47">[48]</ref> method trained with many translation models. Note that R-Drop is complementary to the above methods, and we believe stronger results can be achieved if we apply R-Drop on their methods and better backbone models beyond Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application to Language Understanding</head><p>Dataset We further evaluate our proposed approach on the language understanding tasks by finetuning the pre-trained models <ref type="bibr" target="#b2">3</ref> , which are the standard development sets of GLUE <ref type="bibr" target="#b64">[65]</ref> benchmark. The GLUE benchmark includes 8 different text classification or regression tasks, which are MNLI, MRPC, QNLI, QQP, RTE, SST-2, STS-B (regression), CoLA. The detailed statistics are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model &amp; Training</head><p>We take the BERT-base <ref type="bibr" target="#b8">[9]</ref> and strong RoBERTa-large <ref type="bibr" target="#b38">[39]</ref> pre-trained models as our backbones to perform fine-tuning, which are publicly available. For each task, different random seeds and parameter settings are required, thus we dynamically adjust the coefficient ? among {0.1, 0.5, 1.0} for each setting. Other configurations are following the previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref>. For the regression task STS-B, we use MSE instead of KL-divergence to regularize the outputs (see Appendix for MSE regularization details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The evaluation metrics for above 8 tasks are as follows: The result for STS-B is the Pearson correlation; Matthew's correlation is used for CoLA; Other tasks are measured by Accuracy. The results are presented in <ref type="table">Table 3</ref>. We can see that R-Drop achieves 1.21 points and 0.80 points (on average) improvement over the two baselines BERT-base and RoBERTa-large, respectively, which clearly demonstrate the effectiveness of R-Drop. Specifically, our RoBERTa-large + RD also surpasses the other two strong models: XLNet-large <ref type="bibr" target="#b71">[72]</ref> and ELECTRA-large <ref type="bibr" target="#b6">[7]</ref>, which are specially designed with different model architecture and pre-training task.  <ref type="table">Table 3</ref>: Fine-tuned model performances on GLUE language understanding benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Application to Summarization</head><p>Dataset Abstractive summarization task is to summarize the long sentence/document into a short sequence/sentence (through generation) with the main content remained. For this generation task, we use the CNN/Daily Mail dataset originally introduced by Hermann et al. <ref type="bibr" target="#b21">[22]</ref> to evaluate our method. This dataset contains news documents (source), and their corresponding highlights (target) crawled from CNN and Daily Mail website. It contains 287,226 documents for training, 13,368 documents for validation and 11,490 documents for test. We follow <ref type="bibr" target="#b34">[35]</ref> to preprocess the dataset.</p><p>Model &amp; Training To mostly show the effectiveness, we take the super strong pre-trained sequenceto-sequence BART <ref type="bibr" target="#b34">[35]</ref> model as our backbone and fine-tune it using our method. In this task, the coefficient weight ? is set as 0.7 to control the KL-divergence. For other hyper-parameters, we follow the setting of the original paper <ref type="bibr" target="#b34">[35]</ref> without modification.  Results The performance is evaluated by ROUGE F1 score <ref type="bibr" target="#b36">[37]</ref>. Specifically, we report the unigram ROUGE-1 (RG-1) and bigram ROUGE-2 (RG-2) overlap to assess the informativeness, and the longest common subsequence ROUGE-L (RG-L) score to assess the fluency. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. We can see that R-Drop based training outperforms the fine-tuned BART model by 0.3 points on RG-1 and RG-2 score and achieves the SOTA performance. Specifically, our result also surpasses the PEGASUS method <ref type="bibr" target="#b73">[74]</ref>, which brings a novel self-supervised paradigm carefully designed for summarization, and the previous best work BART+R3F <ref type="bibr" target="#b0">[1]</ref>, which introduces a parametric noise sampled from normal or uniform distributions. Instead, our R-Drop does not introduce any extra parameters or model structure changes during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Application to Language Modeling</head><p>Dataset We also evaluate our approach on another widely acknowledged NLP task: language modeling. The dataset we choose for this task is the commonly adopted Wikitext-103 dataset <ref type="bibr" target="#b41">[42]</ref>, which is the largest available word-level language modeling benchmark with long-term dependency.</p><p>WikiText-103 contains about 103M training tokens from 28K articles on Wikipedia, and the average length of tokens per article is about 3.6K. The data is preprocessed by following <ref type="bibr" target="#b49">[50]</ref>.</p><p>Model &amp; Training We take two models to conduct the language modeling task. One is the basic Transformer decoder <ref type="bibr" target="#b62">[63]</ref>, another is the more advanced one: Adaptive Input Transformer <ref type="bibr" target="#b4">[5]</ref>, which introduces adaptive input embeddings into the Transformer model. We use the open-source Fairseq <ref type="bibr" target="#b49">[50]</ref> toolkit, and the corresponding model configurations are transformer_lm_gpt and transformer_lm_wiki103 for Transformer and Adaptive Input Transformer. We simply set the weight ? to be 1.0 without tuning during training. Other configurations are same as <ref type="bibr" target="#b49">[50]</ref> and <ref type="bibr" target="#b4">[5]</ref>.  <ref type="table">Table 5</ref>: Perplexity results on Wikitext-103 language modeling task. Adaptive refers to Adaptive Input Transformer <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The evaluation metric for language modeling is perplexity, which can well measure the probability of a sentence. Same as <ref type="bibr" target="#b4">[5]</ref>, we report the perplexity on both valid and test sets. The results are shown in <ref type="table">Table 5</ref>. From the table, we can see that our R-Drop based training improves the perplexity on both two different model structures, e.g., 0.80 perplexity improvement on test set over Adaptive Input Transformer. Besides, more improvement can be achieved when the baseline model is not so strong, e.g., 1.79 perplexity gain on valid set and 1.68 on test set above the Transformer baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Application to Image Classification</head><p>Dataset For image classification, we conduct experiments on two widely acknowledged benchmark datasets, i.e., CIFAR-100 <ref type="bibr" target="#b31">[32]</ref> and the ILSVRC-2012 ImageNet dataset <ref type="bibr" target="#b7">[8]</ref> (denoted as ImageNet for short). CIFAR-100 dataset consists of 60k images of 100 classes, and there are 600 images per class with 500 for training and 100 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100 ImageNet</head><p>ViT-B/16 <ref type="bibr" target="#b10">[11]</ref>   The ImageNet dataset consists of 1.3M image samples of 1, 000 categorical classes. We utilize the same data preprocessing strategies with <ref type="bibr" target="#b10">[11]</ref>, where the details are given in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model &amp; Training</head><p>We choose the recent strong and popular Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> model as our backbone. More specifically, we take the two publicly released pre-trained models, ViT-B/16 and ViT-L/16, with 86M and 307M parameters respectively, and we conduct model fine-tuning on the CIFAR-100 and ImageNet datasets. During fine-tuning, the weight ? is set as 0.6 for both models, and we set other hyper-parameters/training details to be same as <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The classification performance is measured by Accuracy, and the results are presented in <ref type="table" target="#tab_9">Table 6</ref>. For CIFAR-100, we achieve about 0.65 accuracy improvement over ViT-B/16 baseline, and 0.41 points over ViT-L/16 model. Similarly, on the large-scale ImageNet dataset, consistent improvements are also obtained. These observations demonstrate that our R-Drop can still benefit the model performance even the baseline is powerful. In a word, through the above NLP tasks and this image classification task, we clearly show R-Drop is effective and can be universally applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Study</head><p>Beyond the superior experimental results, in this section, we conduct extensive studies on different perspectives to better understand our R-Drop method. The analysis experiments are performed on the IWSLT14 De?En translation task. More studies can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regularization and Cost Analysis</head><p>We first show the regularization effect of our R-Drop and study the potential limitation of training cost (as discussed in Section 2.2). Hence, we plot the curves of training/valid loss and valid BLEU along the training update number for Transformer and Transformer + RD models. Besides, we also plot the corresponding curves along the training time (minutes). The curves are shown in <ref type="figure">Figure 2</ref>. We can observe: 1) Along with the training, Transformer quickly becomes over-fitting, and the gap between train and valid loss of Transformer is large, while R-Drop has a lower valid loss. This well proves that R-Drop can provide persistent regularization during training. 2) At the early training stage, Transformer improves the BLEU score quickly but converges to bad local optima soon. In comparison, R-Drop gradually improves the BLEU score and achieves a much superior performance. Though it needs more training to converge, the final optimum is better. This is same as other regularization methods (e.g., training w/ or w/o dropout). R-Drop indeed increases the training cost at each step since it requires repeating input x for another computation in a mini-batch. Note that this is similar to batch size doubled training without KL-divergence. In Appendix C.1, we conduct this training and show that R-Drop increases negligible cost but with a much stronger performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">k-step R-Drop</head><p>The above study shows that R-Drop can achieve much stronger performance, but with a lower convergence, thus we study another training strategy that is to perform R-Drop every k steps to improve the training efficiency, instead of applying at each step. We vary k in {1, 2, 5, 10} to see the difference, where k = 1 is the current training strategy. The valid BLEU curves along with training update number and training time are presented in <ref type="figure">Figure 3</ref>. From the curves, we can conclude that though the convergence is faster with larger k, the training fails to fall into good optima, which quickly over-fits, and the BLEU scores become worse and worse when we increase k. This proves that our R-Drop at each step can well regularize the training and obtain superior performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">m-time R-Drop</head><p>Our method regularizes the model output between two distributions P w 1 (y|x) and P w 2 (y|x), and it is also interesting to see whether more improvements can be achieved if we regularize m distributions for the same input data, where m = 2 is the current setting. Therefore, we extend our R-Drop to be: LKL = ? m * (m?1) i =j i,j?1,??? ,m DKL(P w i (y|x)||P w j (y|x)), and we take m = 3 for a feasible implementation. The BLEU score for IWSLT14 De?En test set is 37.30 when m = 3, which is similar to that when m = 2 (37.25 BLEU score). This reflects that R-Drop already has a strong regularization effect between two distributions, without the necessity of stronger regularization. Besides the above studies, we investigate R-Drop from another perspective, i.e., the dropout values. In current training, the two distributions are based on the same dropout value (e.g., 0.3 for IWSLT translations). In this study, we utilize two different dropout values for the two output distributions during training (e.g., 0.1 for P w 1 (y|x), 0.3 for P w 2 (y|x)) to see the difference. We choose the two dropout rates from {0.1, 0.2, 0.3, 0.4, 0.5} with total 15 (C 2 5 for two different rates + C 1 5 for two same rates) combinations. The results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Among these different results, we can see that: 1) Dropout rates with the same value (0.3, 0.3) is the best choice (current setting), 2) R-Drop can stably achieve strong results when the two dropout rates are in a reasonable range (0.3 ? 0.5) without a big performance difference. One interesting point is that even the two dropout values are both 0.5, which means half of the units are expected to be dropped, R-Drop can still obtain a satisfied result (36.48 BLEU) compared with the baseline Transformer (34.64 BLEU). These results all confirm the advantage of our R-Drop, and we are interested in studying more in the future. 37.20 ? = 10 36.95 <ref type="table">Table 7</ref>: BLEU scores with different ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Two Dropout Rates</head><p>Further, we investigate the impact of the KL-divergence loss weight ?. As mentioned in Section 3.1, we set ? = 5 for NMT experiments.</p><p>Here we vary the ? in {1, 3, 5, 7, 10} and conduct experiments. As shown in <ref type="table">Table 7</ref>, small ? (e.g., 1) can not perform as good as large ? (e.g., 5), which means we should pay more attention to the KLdivergence regularization. However, too much regularization (? = 10) is also not good, and the best balanced choice is ? = 5. Note that the choice of ? is distinct for different tasks (e.g., NMT, language understanding), which depends on how easy the over-fitting happens caused by the specific data size and model size of each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Regularization Methods. Bigger models always tend to have better performance, especially for various large-scale pre-trained models, e.g., Vision Transformer <ref type="bibr" target="#b10">[11]</ref>, Swin Transformer <ref type="bibr" target="#b39">[40]</ref>, GPT families <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b5">6]</ref>, BERT <ref type="bibr" target="#b8">[9]</ref>, BART <ref type="bibr" target="#b34">[35]</ref>, Switch Transformers <ref type="bibr" target="#b13">[14]</ref>, etc. With millions and even billions of parameters, these deep models are prone to over-fitting, thus requiring regularization strategies to improve their generalization ability <ref type="bibr" target="#b33">[34]</ref>. To tackle with over-fitting, many regularization techniques have been proposed, e.g., weight decay <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b66">67]</ref>, dropout <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b60">61]</ref>, normalization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b70">71]</ref>, adding noise <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52]</ref>, layer-wise pre-training and initialization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>, label-smoothing <ref type="bibr" target="#b61">[62]</ref>, and so on. Among which, dropout and its variants are most popular owing to its effectiveness and moderate cost as well as good compatibility with other regularization methods <ref type="bibr" target="#b45">[46]</ref>, which has been successfully applied to regularize a wide range of neural network architectures <ref type="bibr" target="#b50">[51]</ref>, e.g., convolutional neural network layers <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b9">10]</ref>, recurrent neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b42">43]</ref>, Transformer <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b68">69]</ref>. The success of dropout methods can be interpreted by preventing co-adaptation of neurons and performing an implicit ensemble of sub models from dropout. Owing to the effect in promoting sparsity of weights and stochastic nature, dropout methods are also adapted to other applications, e.g., contrastive learning for sentence representation learning <ref type="bibr" target="#b17">[18]</ref>, neural network compression <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> and model uncertainty estimation <ref type="bibr" target="#b15">[16]</ref>.</p><p>Unlike previous researches of designing specific dropout variants or adapting dropout to different applications, we consider to further regularize the model on the success of dropout. Specifically, any two sub models sampled from dropout are encouraged to produce consistent model prediction for an input data by utilizing KL-divergence in the training stage. That is, we conduct regularization on the model output level. In doing so, the sub model outputs produced by the randomness of dropout are regularized to reduce the parameter freedom, which will enhance generalization in inference.</p><p>Consistency Training. Besides regularization methods, our work also relates to a few works of consistency training on dropout models or data augmentation. Among them, the most representative methods are ELD <ref type="bibr" target="#b40">[41]</ref>, FD <ref type="bibr" target="#b80">[81]</ref>, and Cutoff <ref type="bibr" target="#b59">[60]</ref>. As discussed in Section 2.4, ELD only focuses on the inconsistency between the sub model with dropout (train) and the expected full-model without dropout (inference), while FD works between the sub models only (consistence between two sub models). Both ELD and FD utilize L 2 to regularize the hidden space. Instead, our R-Drop performs consistency training on dropout from the output space with a more effective bidirectional KL loss. Unlike the above consistency training method on sub models, Cutoff resembles launching consistency training from a data perspective by regularizing the inconsistency between the original data the augmented samples with part of the information within an input sentence being erased.</p><p>Self-distillation. Minimizing the KL-divergence between the output distributions of two different models correlates with knowledge distillation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b77">78]</ref>, where the two models refer to teacher and student, respectively. In our setting, the teacher and student are the dropout instantiations of the same model, thus it resembles self-knowledge distillation <ref type="bibr" target="#b43">[44]</ref> scenario. Different from existing method that exploits dark knowledge from the model itself <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> or distills knowledge between different layers <ref type="bibr" target="#b74">[75]</ref>, our strategy can be regarded as an instance-wise self-knowledge distillation, i.e., each pair of sampled sub models perform distillation between each other for the same input, which also relates to mutual learning <ref type="bibr" target="#b75">[76]</ref> but ours is much more efficient without extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we proposed a simple yet very effective consistency training method built upon dropout, namely R-Drop, which minimizes the bidirectional KL-divergence of the output distributions of any pair of sub models sampled from dropout in model training. Experimental results on 18 popular deep learning datasets show that not only can our R-Drop effectively enhance strong models, e.g., ViT, BART, Roberta-large, but also work well on large-scale datasets and even achieve SOTA performances when combined with vanilla Transformer on WMT14 English?German and English?French translations. Due to the limitation of computational resources, for pre-training related tasks, we only tested R-Drop on downstream task fine-tuning in this work. We will test it on pre-training in the future. In this work, we focused on Transformer based models. We will apply R-Drop to other network architectures such as convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Experimental Settings</head><p>We provide more detailed settings for the experiments of each task in this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Neural Machine Translation</head><p>For all the NMT tasks, we use the public datasets from IWSLT competitions 4 and WMT competitions <ref type="bibr" target="#b4">5</ref> . We tokenize all the datasets with byte-pair-encoding (BPE) <ref type="bibr" target="#b58">[59]</ref> approach with the dictionary built jointly upon the source and target sentence pairs except the IWSLT17 En?Zh translation dataset that is built separately. After tokenization, the resulted vocabularies for IWSLT datasets are near 10k, while for WMT datasets, the vocabulary size is about 32k.</p><p>To train the Transformer based NMT models, we use transformer_iwslt_de_en configuration for IWSLT translations, which has 6 layers in both encoder and decoder, embedding size 512, feed-forward size 1, 024, attention heads 4, dropout value 0.3, weight decay 0.0001. For the WMT experiments, the transformer_vaswani_wmt_en_de_big setting has 6 layers in encoder and decoder, embedding size 1, 024, feed-forward size 4, 096, attention heads 16, dropout value 0.1, attention dropout 0.1 and relu dropout 0.1. The training is optimized with Adam <ref type="bibr" target="#b28">[29]</ref> with ? 1 = 0.9, ? 2 = 0.98, = 10 ?9 . The learning rate scheduler is inverse_sqrt with default learning rate 0.0005 and warmup steps 4, 000. Label smoothing <ref type="bibr" target="#b61">[62]</ref> is adopted with value 0.1. Our code implementation is based on open-source Fairseq 6 . We train the IWSLT translations on 1 GEFORCE RTX 3090 card and the WMT translations on 8 GEFORCE RTX 3090 cards.</p><p>To evaluate the performance, we use multi-bleu.perl 7 to evaluate IWSLT14 En?De and all WMT tasks for a fair comparison with previous works <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b48">49]</ref>. For other NMT tasks, we use sacre-bleu 8 <ref type="bibr" target="#b52">[53]</ref> for evaluation. When inference, we follow <ref type="bibr" target="#b62">[63]</ref> to use beam size 4 and length penalty 0.6 for WMT14 En?De, beam size 5 and penalty 1.0 for other tasks. We further report the sacre-bleu evaluated BLEU score on WMT14 En?De and En?Fr tasks to show advanced comparisons, where the corresponded results are 29.5 and 41.8, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Abstrative Summarization</head><p>For summarization, we take the pre-trained BART <ref type="bibr" target="#b34">[35]</ref> model as backbone and fine-tune on the CNN/DailyMail dataset <ref type="bibr" target="#b8">9</ref> . BART is a pre-trained sequence-to-sequence model based on the masked source input and autoregressive target output, which contains 12 layers of Transformer encoder and 12 layers of Transformer decoder, the embedding size is 1, 024, and the feed-forward size is 4, 096. Dropout value is 0.1. During fine-tuning, we follow the hyper-parameters used in <ref type="bibr" target="#b34">[35]</ref>. The pre-trained model and the backbone implementations are all from Fairseq 10 . The training is conducted on 8 GEFORCE RTX 3090 GPU cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Language Modeling</head><p>For language modeling, we train on the Transformer decoder <ref type="bibr" target="#b62">[63]</ref> and Adaptive Input Transformer <ref type="bibr" target="#b4">[5]</ref> models. The configuration for Transformer is transformer_lm_gpt, which contains 12 layers with embedding size 768 and feed-forward size 3, 072, attention heads 12. Dropout and attention dropout are 0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Language Understanding</head><p>For language understanding tasks, we follow the popular pre-training and fine-tuning methodology, and the fine-tuned sets are the GLUE <ref type="bibr" target="#b64">[65]</ref> benchmark. We follow previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref> to work on the 8 tasks, including singe-sentence classification tasks (CoLA, SST-2), sentence-pair classification tasks (MNLI, QNLI, RTE, QQP, MRPC), and the sentence-pair regression task (STS-B). The detailed data statistics can be found from the original paper <ref type="bibr" target="#b64">[65]</ref>.</p><p>The pre-trained BERT-base model is the Transformer <ref type="bibr" target="#b62">[63]</ref> encoder network, which contains 12 layers with embedding size 768, feed-forward size 3, 072 and attention heads 12. Correspondingly, the Roberta-large model contains 24 layers with embedding size 1, 024, feed-forward size 4, 096 and attention heads 16. During fine-tuning, we use Adam <ref type="bibr" target="#b28">[29]</ref> as our optimizer with ? 1 = 0.9, ? 2 = 0.98, = 10 ?6 , and L 2 weight decay of 0.01. We select the learning rate in range {5 ? 10 ?6 , 10 ?5 } and batch size in {8, 16, 32}. Other hyper-parameter settings are mostly same as previous works <ref type="bibr" target="#b38">[39]</ref>. The pre-trained model and the backbone implementations are all from Huggingface Transformers 12 . We report the specific settings of several important hyper-parameters in <ref type="table" target="#tab_11">Table 8</ref>, including the dropout value. The fine-tuning experiments are conducted on 1 GEFORCE RTX 3090 GPU card.</p><p>Further, to give a clear comparison of our R-Drop based fine-tuning and vanilla fine-tuning, we plot the performance changes from different random seeds over the pre-trained BERT model on each GLUE task. The curves are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. We can see that consistent improvements are achieved on different random seeds, which means our R-Drop can robustly help improve the model generalization and model performance. Besides, we also provide some task performances of different ? values, shown in <ref type="table">Table 9</ref>. This result demonstrated that ? indeed is a sensitive hyper-parameter for each GLUE task, while ? = 1.0 is a good choice for most tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSE Regularization</head><p>Our R-Drop is presented under the KL-divergence between two distributions. To extend our method into the regression task, such as STS-B in GLUE, we introduce the MSE-based regularization. For input data (x, y), we forward the x two times similarly as in classification and obtain the two predicted values y 1 and y 2 . Then we regularize these two predicted values with MSE  <ref type="table">Table 9</ref>: Comparison of the effect of different ? for some GLUE tasks.</p><p>as follow:</p><formula xml:id="formula_10">L mser = ||y 1 ? y 2 || 2 ,<label>(7)</label></formula><p>and we add L mser with conventional MSE loss: L mse = ||y ? y 1 || 2 + ||y ? y 2 || 2 . The final optimization objective is:</p><formula xml:id="formula_11">L = L mse + ?L mser .<label>(8)</label></formula><p>A.5 Image Classification</p><p>The image classification task is evaluated with the recent popular Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> model, which is the same as Transformer but with the image patch data as input. We take the two publicly released models <ref type="bibr" target="#b12">13</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theoretical Discussion of R-Drop</head><p>In this section, we provide the proof for Proposition 2.1 in the main paper and make some discussions. Proof: Here, the normalization operator normalize the row of the weight matrix w to be 1. According to the Lipschitz continuity of the loss, we have</p><formula xml:id="formula_12">|L nll (w) ? E ? [L nll (w, ?)]| ? c 1 ? 1 n n i=1 E ? w T x i ? 1 p ? (w T x i ) ? (9) = c 1 ? 1 n n i=1 (1 ? p) w T x i<label>(10)</label></formula><p>where c 1 is the Lipshitz constant. According to the condition 1</p><formula xml:id="formula_13">n n i=1 E ? (1) ,? (2) [D KL (P w ? (1) (y i |x i )||P w ? (2) (y i |x i )))] ? , we have 1 2n n i=1 E ? (1) ,? (2) P w ? (1) (y i |x i ) ? P w ? (2) (y i |x i )) 1 ? 2 ,<label>(11)</label></formula><p>because the relation between the KL-divergence and the total variation distance. Since we constrain the norm of w, for fixed x, the softmax operator is a bijection from w T x to softmax w T x . Suppose c 2 is the Lipschitz constant of the inverse function from softmax w T x to w T x, we have</p><formula xml:id="formula_14">1 2n n i=1 E ? (1) ,? (2) 1 p ? w T x i ? 1 ? 1 p ? w T x i ? 2 ? c 2 2<label>(12)</label></formula><p>For the left term in Eq.(5), we have 1</p><formula xml:id="formula_15">2np n i=1 E ? (1) ,? (2) w T x i ? 1 ? w T x i ? 2 = 1?p n n i=1 w T x i , because ? (1) , ? (2) independently follow Bernoulli distribution. Then we have 1 n n i=1 w T x i ? c2 1?p 2 . Combined with Eq.(4), we have |L nll (w) ? E ? [L nll (w, ?)]| ? c 1 c 2 2<label>(13)</label></formula><p>Let c = 1 2 c 1 c 2 , we can get the result.</p><p>C More Studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Batch Size Doubled Training</head><p>As discussed in Section 2.2, we implement the algorithm by repeating input data x once and concatenating the x with repeated one in the same mini-batch to forward once. This is similar to enlarging the batch size to be double at each step. The difference is that half of the data are the same as the other half, while directly doubling the batch size, the data in the same mini-batch are all different. Therefore, we are still interested in the result of directly doubling the batch size to see the performance. We conduct experiments on IWSLT14 De?En translation with Transformer, and the batch size is enlarged from 4, 096 to be 8, 192. The result is 34.93 BLEU score. We can see that though slight improvement is achieved (compared to baseline 34.64), it falls far behind our strong performance 37.25. For the detailed training cost for each step, we present the number here: Transformer + Double Batch costs near 9ms per step, while Transformer + DR costs about 10ms per step. The additional cost is from the KL-divergence loss backward computation. We can see the cost is 1.1 times, which is a negligible cost. We also plot the valid BLEU curves along with the training for this study. The curves are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Compared to this batch size doubled training and our R-Drop, we can clearly see the advantage of R-Drop. With similar training costs, R-Drop gradually improves the performance to a much stronger one. In the figures, we also plot the curve for Transformer with the original batch size training (e.g., 4, 096) for a better comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Importance of KL-Divergence</head><p>Our method introduces a KL-divergence between the two distributions from the same sample. In this study, we specifically investigate the importance of KL-divergence loss. Thus, this ablation removes   the L KL loss between the two distributions and only keeps the L N LL loss for training. Similar to other studies, we work on IWSLT14 De?En translation, and the model is Transformer. The result is also 34.93 BLEU score (same as above enlarged batch size), which is slightly better than the Transformer baseline (34.64), but far worse from our R-Drop based training result 37.25 BLEU score. This result well demonstrates the importance and effectiveness of our introduced KL-divergence loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Training Time and Efficiency</head><p>To fairly compare the training time between baseline and R-Drop model for the same batch size, We reduce the batch size for the R-Drop model and provide the BLEU score along with training time on IWSLT14 De?En translation tasks, which is shown in <ref type="table" target="#tab_2">Table 10</ref>. From the table, we can see that though R-Drop needs more training to converge, the final model is much better than the baseline. We can see that the time cost is comparable for reduced half-batch training when reaching the BLEU score equal to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Experiments for ELD and FD</head><p>To give a better understanding of the advantages of R-Drop over hidden space regularization, we also conduct experimental studies. Since FD <ref type="bibr" target="#b80">[81]</ref> has shown its advantage over ELD <ref type="bibr" target="#b40">[41]</ref>, we mainly present experimental comparisons between FD and R-Drop. The experiments are image classification on the cifar-10 dataset (used in FD work) and IWSLT14 De?En Translation (used in R-Drop work). The cifar10 experiments are conducted on the released code <ref type="bibr" target="#b13">14</ref> , and we add R-Drop regularization as our implementation. For the image classification task, the R-Drop model hyper-parameter ? needs to reduce linearly with the learning rate implemented by torch.optim.lr_scheduler.MultiStepLR. The IWSLT experiments are on our released code, and we implement the FD same as its released code. From <ref type="table" target="#tab_2">Table 11</ref>, we can clearly see that R-Drop is superior to FD on both tasks, which can prove the advantages of the KL-divergence consistency regularization.  We also compare our R-Drop based training with deep ensembling and weight average methods. (1) Deep ensembling usually utilizes multiple models with different parameters, which incurs additional inference costs, including both numbers of model parameters and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Ensemble and Weight Averaging</head><p>(2) Parameter/weight averaging directly averages the multiple model parameters, and it can only be useful when the parameters are not far away from each other. In contrast, R-Drop aims to make sub models consistent within dropout-based training. It is simple yet effective by adding a consistent regularization loss without any other modifications. The model parameters are not increased, and the inference cost is the same as a single model. We train several independent models with different parameters (each with dropout) and then do deep ensembling, weight averaging on these full models. The results are show in <ref type="table" target="#tab_2">Table 12</ref> . Obviously, R-Drop achieves great performance with a single full model (e.g., 37.25), much better than weight averaging and deep ensembling. Ensemble methods improve the independently trained full models, but the best result is still from R-Drop (at least with 6 models ensembling here). Weight averaging can improve the performance of the base dropout model by averaging nearby checkpoints with the same random seed, but the improvement is relatively small compared with R-Drop. Besides, the weights average obtains an extremely low result for different random seeds trained models (BLEU=0) due to the far difference of model parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>? l i = 1 ,</head><label>1</label><figDesc>with probability p, 0, with probability 1-p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Loss/BLEU curves along with model training. R-Drop with different step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>R-Drop with two different dropout rate combinations. Among the 25 numbers, 15 are different since the table is symmetric and triangular.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results on 8 GLUE tasks with different random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition B. 1 .</head><label>1</label><figDesc>For a linear model P w (y|x) = softmax(Norm(w T x)) where Norm(?) denotes the normalization layer and x ? R d , with the constraint in Equation(6)in the main paper, we have|L nll (w) ? E ? [L nll (w, ?)]| ? c ? , where L nll (w), L nll (w, ?)are the empirical loss calculated by the full model and a random sub model respectively, c is a constant related to the Liptschtz constant of the softmax operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Results of R-Drop and Transformer with a doubled batch size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">En?De En?Fr</cell></row><row><cell>Transformer [63]</cell><cell>29.12</cell><cell>42.69</cell></row><row><cell>MUSE [77]</cell><cell>29.90</cell><cell>43.50</cell></row><row><cell>Depth Growing [70]</cell><cell>30.07</cell><cell>43.27</cell></row><row><cell>Transformer-Admin [38]</cell><cell>30.10</cell><cell>43.80</cell></row><row><cell>Data Diversification [48]</cell><cell>30.70</cell><cell>43.70</cell></row><row><cell>BERT-fused NMT [80]</cell><cell>30.75</cell><cell>43.78</cell></row><row><cell>Transformer + RD</cell><cell>30.91</cell><cell>43.95</cell></row></table><note>BLEU scores on 8 IWSLT machine translation tasks. We first evaluate the NMT tasks, which is very important in NLP. To best show the effectiveness of our method, experiments are conducted on both low-resource and rich-resource translation tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores on WMT14 En?De and En?Fr machine translation tasks. the widely acknowledged WMT translation tasks, and we take the WMT14 English?German and English?French tasks. The IWSLT datasets contain about 170k training sentence pairs, 7k valid pairs, and 7k test pairs. The WMT data sizes are 4.5M , 36M for En?De and En?Fr respectively, valid and test data are from the corresponding newstest data.</figDesc><table><row><cell cols="2">Datasets The datasets of low-resource sce-</cell></row><row><cell cols="2">nario are from IWSLT competitions, which in-</cell></row><row><cell cols="2">clude IWSLT14 English?German (En?De),</cell></row><row><cell cols="2">English?Spanish (En?Es), and IWSLT17</cell></row><row><cell cols="2">English?French (En?Fr), English?Chinese</cell></row><row><cell>(En?Zh) translations.</cell><cell>The rich-resource</cell></row><row><cell>datasets come from</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Method RG-1 RG-2 RG-L Transformer [63] 39.50 16.06 36.63 ProphetNet [54] 44.02 21.17 41.30 BART [35] 44.16 21.28 40.90 PEGASUS [74] 44.17 21.47 41.11 BART + R3F [1] 44.38 21.53 41.17 BART + RD 44.51 21.58 41.24</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>ROUGE results on CNN/Daily</cell></row><row><cell>Mail summarization dataset. RG-1, RG-</cell></row><row><cell>2, RG-L stand for ROUGE-1, ROUGE-2,</cell></row><row><cell>and ROUGE-L scores.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Accuracy on CIFAR-100 and Ima- geNet classification tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>1. For Adaptive Input Transformer, the configuration is transformer_lm_wiki103 with 16 layers, embedding size 1, 024, feed-forward size 4, 096, attention heads 16, dropout 0.3, attention dropout 0.1, gelu dropout 0.1 and adaptive softmax dropout 0.2. We train Transformer model for 50k steps and Adaptive Input Transformer for 286k steps. The development is based on the code base Fairseq 11 . The training is on 8 Tesla V100 GPU cards.</figDesc><table><row><cell>Hyper-parameter</cell><cell cols="6">CoLA MRPC RTE SST-2 MNLI QNLI</cell><cell>QQP</cell><cell>STS-B</cell></row><row><cell>Learning Rate</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell>Max Update</cell><cell>5336</cell><cell>2296</cell><cell cols="5">3120 20935 123873 33112 113272</cell><cell>3598</cell></row><row><cell>Max Sentence (Batch)</cell><cell>16</cell><cell>16</cell><cell>8</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>16</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Coefficient ?</cell><cell>0.5</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>0.5</cell><cell>1.0</cell><cell>0.5</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameters when fine-tuning our models on GLUE benchmark.</figDesc><table><row><cell>Mcc Acc</cell><cell>58 60 62 64 56 83 84 85 86</cell><cell>1</cell><cell cols="2">2 BERT 3 CoLA</cell><cell>4</cell><cell cols="3">5 BERT+RD 90 82 84 86 88 Acc 1 80 90 91 92 93 Acc</cell><cell>2</cell><cell>3 MRPC</cell><cell>4</cell><cell>5</cell><cell>Acc Acc</cell><cell>74 68 70 72 66 64 90 91 93 92</cell><cell>1</cell><cell>2</cell><cell>3 RTE</cell><cell>4</cell><cell>5</cell><cell>Acc P corr</cell><cell>94 93 92 91 92 88 90</cell><cell>1</cell><cell>2</cell><cell>3 SST-2</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell>82</cell><cell>1</cell><cell>2</cell><cell>3 MNLI</cell><cell>4</cell><cell>5</cell><cell>89</cell><cell>1</cell><cell>2</cell><cell>3 QNLI</cell><cell>4</cell><cell>5</cell><cell></cell><cell>89</cell><cell>1</cell><cell>2</cell><cell>3 QQP</cell><cell>4</cell><cell>5</cell><cell></cell><cell>86</cell><cell>1</cell><cell>2</cell><cell>3 STS-B</cell><cell>4</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>MRPC 84.30 86.03 86.51 85.78 SST-2 92.54 92.77 93.02 92.43 MNLI 84.20 84.48 83.44 82.27 QNLI 91.21 91.92 92.01 91.12</figDesc><table><row><cell>?</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>classes and 14M images in total. ViT-B/16 is a Transformer model with 12 Transformer encoder layers, embedding size 768, feed-forward size 3, 072 and attention heads 12, while ViT-L/16 with 24 layers, 1, 024 embedding size, 4, 096 feed-forward size and 16 attention heads. We only conduct the fine-tuning stage experiments on CIFAR-100 and ImageNet. Note that the ImageNet results are computed without additional techniques (Polyak averaging and 512 resolution images) used to achieve results in<ref type="bibr" target="#b10">[11]</ref>. During fine-tuning, the dropout values are 0.1 for both models. Fine-tuning is on 8 GEFORCE RTX 3090 GPU cards.</figDesc><table><row><cell>, ViT-B/16 and ViT-L/16, which are pre-trained on ImageNet-21k [8]</cell></row><row><cell>dataset with 21k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Comparison Baseline and R-Drop (reduced half-batch training) BLEU score along with training time on IWSLT14 De?En translation.</figDesc><table><row><cell>Model</cell><cell cols="2">Acc (CIFAR-100) BLEU (IWSLT14 De?En)</cell></row><row><cell>Baseline</cell><cell>77.1</cell><cell>34.78</cell></row><row><cell>FD [81]</cell><cell>77.6</cell><cell>35.04</cell></row><row><cell>R-Drop</cell><cell>78.13</cell><cell>37.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Comparison of Baseline, FD and R-Drop on IWSLT14 De?En and CIFAR-100 tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Transformer full-model deep ensembling 34.78 36.06 36.37 36.59 36.80 36.92 Transformer full-model weight averaging (checkpoint) 34.78 35.00 35.21 35.26 35.29 35.44 R-Drop full-model weight averaging (checkpoint) 37.25 37.22 37.27 37.30 37.22 37.31</figDesc><table><row><cell>Model Settings</cell><cell>Model Number</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell cols="2">Transformer full-model weight averaging (model)</cell><cell>34.78</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Comparison of BLEU scores achieved by model deep ensembling or weight averaging with different model (trained with different random seed) and epoch checkpoint (trained with same seed).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We apply our R-Drop on the fine-tuning stage only in this work. R-Drop can also be applied during pre-training. Due to the computational cost, we leave this as future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/jeonsworld/ViT-pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://github.com/akamaster/pytorch_resnet_cifar10</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank the reviewers for their constructive comments. Juntao Li is the corresponding author. This work was supported by the National Science Foundation of China (NSFC No. 62036004).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Better fine-tuning by reducing representational collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03156</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards understanding ensemble, knowledge distillation and self-distillation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09816</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3084" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Seed: Self-supervised distillation for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-knowledge distillation in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangchul</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>RANLP 2019)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03340</idno>
		<title level="m">Teaching machines to read and comprehend</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simplifying neural nets by discovering flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shakeout: A new regularized deep neural network training scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Labach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahrokh</forename><surname>Valaee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13310</idno>
		<title level="m">Survey of dropout methods for deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixkd: Towards efficient distillation of large-scale language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weituo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Manual and automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Automatic Summarization</title>
		<meeting>the ACL-02 Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep transformers for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07772</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dropout with expectation-linear regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingkai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08017</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-distillation amplifies regularization in hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05715</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2498" to="2507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A survey of regularization strategies for deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Berangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrouz</forename><surname>Minaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3947" to="3986" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structured bayesian pruning via log-normal multiplicative noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6778" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Data diversification: A simple strategy for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Ti</forename><surname>Aw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10018" to="10029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autodropout</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01761</idno>
		<title level="m">Learning dropout patterns to regularize deep networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1831</idno>
		<title level="m">Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in autoencoders and deep networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting bleu scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2401" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weight normalization: a simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recurrent dropout without memory loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1757" to="1766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat data augmentation approach for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13818</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2082" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards dropout training for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Not all attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04692</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Depth growing for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5558" to="5563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Dropattention: A regularization method for fully-connected self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zehui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Muse: Parallel multi-scale attention for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09483</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Scheduled drophead: A regularization method for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Incorporating bert into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Fraternal dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dendi</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://iwslt.org/5https://www.statmt.org/wmt14/translation-task.html6https://github.com/pytorch/fairseq/tree/master/examples/translation7https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl8https://github.com/mjpost/sacrebleu9https://github.com/abisee/cnn-dailymail" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
