<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Regression Network for Video Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>ganchuang1990@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Regression Network for Video Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of video grounding from natural language queries. The key challenge in this task is that one training video might only contain a few annotated starting/ending frames that can be used as positive examples for model training. Most conventional approaches directly train a binary classifier using such imbalance data, thus achieving inferior results. The key idea of this paper is to use the distances between the frame within the ground truth and the starting (ending) frame as dense supervisions to improve the video grounding accuracy. Specifically, we design a novel dense regression network (DRN) to regress the distances from each frame to the starting (ending) frame of the video segment described by the query. We also propose a simple but effective IoU regression head module to explicitly consider the localization quality of the grounding results (i.e., the IoU between the predicted location and the ground truth). Experimental results show that our approach significantly outperforms state-of-the-arts on three datasets (i.e., Charades-STA, ActivityNet-Captions, and TACoS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query: A man reaches out and then pets the fish.</head><p>Video:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video grounding is an important yet challenging task in computer vision, which requires the machine to watch a video and localize the starting and ending time of the target video segment that corresponds to the given query, as shown in <ref type="figure">Figure 1</ref>. This task has drawn increasing attention over the past few years due to its vast potential applications in video understanding <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b5">6]</ref>, video retrieval <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8]</ref>, and human-computer interaction <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50</ref>], etc.</p><p>The task, however, is very challenging due to several reasons: 1) It is nontrivial to build connections between <ref type="bibr">Figure 1</ref>. An illustrative example of the video grounding task. Given a video and a query, the video grounding task aims to identify the starting and ending time of the video segment described by the query. One key challenge of this task is how to leverage dense supervision upon sparsely annotated starting and ending frames. the query and complex video contents; 2) Localizing actions of interest precisely in a video with complex backgrounds is very difficult. More critically, a video can often contain many thousands of frames, but it may have only a few annotated starting/ending frames (namely the positive training examples), making the problem even more challenging. Previous approaches often adopt a two-stage pipeline <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10]</ref>, where they generate the proposals and rank them according to their similarities with the query. However, this pipeline incurs two issues: 1) One video often contains thousands of proposals, resulting in a heavy computation cost when comparing proposal-query pairs. 2) The performance highly relies on the quality of proposals. To address the above issues, one-stage video grounding methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b10">11]</ref> have been studied. Yuan et al. <ref type="bibr" target="#b42">[43]</ref> propose to learn a representation of the video-query pair and use a multi-layer perceptron (MLP) to regress the starting and ending time. Chen et al. <ref type="bibr" target="#b4">[5]</ref> and Ghosh et al. <ref type="bibr" target="#b10">[11]</ref> attempt to predict two probabilities at each frame, which indicate whether this frame is a starting (or ending) frame of the target video segment. The grounding result is obtained by selecting the frame with the largest starting (or ending) probability. However, the existing two-stage and one-stage methods have one common issue: they neglect the rich information from the frames within the ground truth.</p><p>Recently, anchor-free approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref> for one-stage object detection become increasingly popular because of their simplicity and effectiveness. In this vein, Tian et al. <ref type="bibr" target="#b35">[36]</ref> propose the FCOS framework to solve object detection in a per-pixel prediction fashion. Specifically, FCOS trains a regression network to directly predict the distance from each pixel in the object to the object's boundary. This idea is helpful for video grounding. If we train a model to predict the distance from each frame to the ground truth boundary, then all the frames within the ground truth can be leveraged as positive training samples. In this way, the number of positive samples is sufficiently increased and thus benefits the training.</p><p>In this paper, we propose a dense regression network for video grounding, which consists of four modules, including a video-query interaction module, a location regression head, a semantic matching head, and an IoU regression head. The main idea is as straightforward as training a regression module to directly regress the ground truth boundary from each frame within the ground truth. In the training, all frames within the ground truth are selected as positive samples. By doing so, the sparse annotation is able to be used to generate more positive training samples sufficiently, which boosts grounding performance eventually.</p><p>For each video-query pair, our model produces dense predictions (i.e., one predicted temporal bounding box for each frame) while we are only interested in the one that matches the query best. To select the best grounding result, we focus on two perspectives: 1) Does the box match the query semantically? 2) Does the box match the temporal boundary of the ground truth? Specifically, we train a semantic matching head to predict a score for each box, which indicates whether the content in the box matches the query semantically. However, this score cannot directly reflect the localization quality (i.e., the IoU with the ground truth), which is of vital importance for video grounding. This motivates us to further consider the localization quality of each prediction. To do so, one may use the "centerness" assumption in FCOS, which, however, is empirically found inapplicable for video grounding (see <ref type="table" target="#tab_4">Table 5</ref>). In this paper, we train an IoU regression head to directly estimate the IoU between the predicted box and the ground truth. Last, we combine the matching score and the IoU score to find the best grounding result. It is worth noting that the dense regression network works in a one-stage manner. We evaluate our proposed method on three popular benchmarks for video grounding, i.e., Charades-STA <ref type="bibr" target="#b8">[9]</ref>, ActivityNet-Captions <ref type="bibr" target="#b24">[25]</ref> and TACoS <ref type="bibr" target="#b31">[32]</ref>.</p><p>To sum up, our contributions are as follows:</p><p>? We propose a dense regression network for one-stage video grounding. We provide a new perspective to leverage dense supervision from the sparse annotations in video grounding.</p><p>? To explicitly consider the localization quality of the predictions, we propose a simple but effective IoU regression head and integrate it into our one-stage paradigm.</p><p>? We verified the effectiveness of our proposed method on three video grounding datasets. On ActivityNet-Captions especially, our method obtains the accuracy of 42.49%, which significantly outperforms the stateof-the-art, i.e., 36.90% by He et al. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Video grounding. Recently, great progress has been achieved in deep learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">51]</ref>, which facilitates the development of video grounding. Existing methods on this task can be grouped into two categories (i.e., two-stage and one-stage). Most two-stage methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> resort to a propose-andrank pipeline, where they first generate proposals and then rank them relying on the similarity between proposal and query. Gao et al. <ref type="bibr" target="#b8">[9]</ref> and Hendricks et al. <ref type="bibr" target="#b16">[17]</ref> propose to use the sliding windows as proposals and then perform a comparison between each proposal and the input query in a joint multi-modal embedding space. To improve the quality of the proposals, Xu et al. <ref type="bibr" target="#b39">[40]</ref> incorporate a query into a neural network to generate the query-guided proposals. Zhang et al. <ref type="bibr" target="#b45">[46]</ref> explicitly model temporal relations among proposals using a graph. The two-stage methods are straightforward but have two limitations: 1) Comparing all the proposal-query pairs leads to a heavy computation cost;</p><p>2) The performance highly relies on the quality of proposals. Our method is able to avoid the above limitations since the candidate proposals are not required.</p><p>To perform video grounding more efficiently, many methods that go beyond the propose-and-rank pipeline have been studied. He et al. <ref type="bibr" target="#b15">[16]</ref> and Wang et al. <ref type="bibr" target="#b38">[39]</ref> propose a reinforcement learning method for video grounding task. In the work by He et al. <ref type="bibr" target="#b15">[16]</ref>, the agent adjusts the boundary of a temporal sliding window according to the learned policy. At the same time, Yuan et al. <ref type="bibr" target="#b42">[43]</ref> propose the attentionbased grounding approach which directly predicts the temporal coordinates of the video segment that described by the input query. Ghosh et al. <ref type="bibr" target="#b10">[11]</ref> and Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose to select the starting and ending frames by leveraging cross-modal interactions between text and video. Specifically, they predict two probabilities at each frame, which indicate whether this frame is a starting (or ending) frame of the ground truth video segment. Unlike the previous work by Chen et al. <ref type="bibr" target="#b3">[4]</ref> and Ghosh et al. <ref type="bibr" target="#b10">[11]</ref> where only the starting and ending frame are selected as positive training samples, our method is able to leverage much more positive training samples, which significantly boosts the grounding performance.   <ref type="figure">Figure 2</ref>. Schematic of our dense regression network. We use the video-query interaction module to fuse the features from the video and query. By constructing the feature pyramid, we obtain hierarchical feature maps and forward them to the grounding module. At each location t, the grounding module predicts a temporal bounding box, along with a semantic matching score and an IoU score for ranking.</p><p>Anchor-free object detection. Anchor-free object detectors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref>  propose FoveaBox <ref type="bibr" target="#b23">[24]</ref> to predict category-sensitive semantic maps for the object existing possibility and produce a category-agnostic bounding box at each position. Tian et al. devise FCOS <ref type="bibr" target="#b35">[36]</ref> to make full use of the pixels in a ground truth bounding box to train the model and propose centerness to suppress the low-quality predictions. Our work is related to FCOS since we also directly predict the distance from each frame to the ground truth boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>Notation. Let V = {I t ? R H?W ?3 } T t=1 be an untrimmed video, where I t denotes the frame at time slot t with height H and width W . We denote the query with N words as Q = {w n } N n=1 , where w n is the n-th word in the query. Problem Definition. Given a video V and a query Q, video grounding requires the machine to localize a video segment (i.e., a temporal bounding box b = (t s , t e )) starting at t s and ending at t e , which corresponds to the query. This task is very challenging since it is difficult to localize actions of interest precisely in a video with complex contents. More critically, only a few frames are annotated in one video, making the training samples extremely imbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General scheme</head><p>We focus on solving the problem that existing video grounding methods neglect the rich information from the frames within the ground truth, which, however, is able to significantly improve the localization accuracy. To this end, we propose a dense regression network to regress the starting (or ending) frame of the video segment described by the query for each frame. In this way, we are able to select every frame within the ground-truth as a positive training sample, which significantly benefits the training of our video grounding model.</p><p>Formally, we forward the video frames {I t } T t=1 and the query {w n } N n=1 to the video-query interaction module G for extracting the multi-scale feature maps. Then, each feature map is processed by the grounding module, which consists of three components, i.e., location regression head M loc , semantic matching head M match and IoU regression head M iou . The location regression head predicts a temporal bounding boxb t at the t-th frame by computing</p><formula xml:id="formula_0">{b t } T t=1 = {(t ?d t,s , t +d t,e )} T t=1 , {(d t,s ,d t,e )} T t=1 = M loc (G({I t } T t=1 , {w n } N n=1 )),<label>(1)</label></formula><p>where (d t,s ,d t,e ) are the predicted distances to the starting and ending frame. With the predicted boxes {b t } T t=1 at hand, our target is to select the box that matches the query best. To this end, we propose two heads in the grounding module. The semantic matching head predicts a scorem t indicating whether the content in the boxb t matches the query semantically. However, this score cannot directly reflect the localization quality (i.e., the IoU with the ground truth), which, however, is also very important for video grounding. Therefore, we propose the IoU regression head to predict a score? t for directly estimating the IoU between b t and the corresponding ground truth. The schematic of our approach is shown in <ref type="figure">Figure 2</ref>. For simplicity, we denote our model as dense regression network (DRN).</p><p>Inference details. Given an input video, we forward it through the network and obtain a boxb t , a semantic matching scorem t as well as an IoU score? t for each frame I t .</p><p>The final grounding result is obtained by choosing the box with the highestm t ?? t .</p><p>In the following, we will introduce the details of the video-query interaction module in Section B. Then, we detail the location regression head, the semantic matching head and the IoU regression head in Sections 3.3, 3.4, and 3.5, respectively. Last, we introduce the training details of our model in Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-level video-query interaction module</head><p>Building connections between vision and language is a crucial step for video grounding. To learn better visionlanguage representations, we propose a multi-level videoquery interaction module. Given a video with T frames, we use some feature extractor (e.g., C3D <ref type="bibr" target="#b36">[37]</ref>) to obtain the video feature F ? R T ?c , where c is the channel dimension. Then, the vision-language representations are produced by using multi-level fusion and temporal location embedding.</p><p>Multi-level fusion. The target video segments described by the query often have large scale variance in video grounding. For example on Charades-STA dataset <ref type="bibr" target="#b8">[9]</ref>, the shortest ground truth is 2.4s while the longest is 180.8s. To handle this issue, we follow Lin et al. <ref type="bibr" target="#b26">[27]</ref> to obtain a set of hierarchical feature maps from multiple levels. Since the model may focus on different parts of the input query at each level, we follow <ref type="bibr" target="#b17">[18]</ref> to fuse the query and the video features at different levels. Specifically, we encode the query Q = {w n } N n=1 into {h n } N n=1 and a global representation g by using a bi-directional LSTM as:</p><formula xml:id="formula_1">h 1 , h 2 , . . . , h N = BiLSTM(Q) and g = [h 1 ; h N ], (2) where h n = [ ? h n ; ?</formula><p>h n ] is the concatenation of the forward and backward hidden states of the LSTM for the n-th word. For the i-th level, a textual attention ? i,n is computed over the words, and the query feature q i is computed as:</p><formula xml:id="formula_2">q i = N n=1 ? i,n ? h n , ? i,n = Softmax(W 1 (h n (W (i) 2 ReLU(W 3 g)))),<label>(3)</label></formula><p>where is element-wise multiplication. W 1 and W 3 are the parameters shared across different levels but W (i) 2 is learned separately for each level i. Given the input visual feature M i ? R Ti?c of a vision-language fusion module, we first duplicate q i for T i times to obtain a feature map D i ? R Ti?c , where T i is the temporal resolution at the i-th level. Then, we perform element-wise multiplication to fuse M i and D i , leading to a set of feature maps</p><formula xml:id="formula_3">{C i ? R Ti?c } L i=1</formula><p>, where L is set to 3 in our paper. Last, we obtain the feature maps {P i ? R Ti?c } L i=1 for the grounding module by using FPN. We put more details in the supplementary material.</p><p>Temporal location embedding. We find that the queries often contain some words for referring temporal orders, such as "after" and "before". Therefore, we seek to fuse the temporal information of the video with the visual features. The temporal location of the t-th frame (or segment)</p><formula xml:id="formula_4">is l t = [ t?0.5 T , t+0.5 T , 1 T ].</formula><p>The location embedding l t is concatenated with the output of the vision-query fusion module that fuses the video feature F and the query feature. Note that the concatenation is performed along the channel dimension, resulting in the feature map C 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Location regression head</head><p>With the vision-language representation P (we omit index i for better readability), we propose a location regression head to predict the distance from each frame to the starting (or ending) frame of the video segment that corresponds to the query. We implement it as two 1D convolution layers with two output channels in the last layer. For each location t on the feature map P, if it falls inside the ground truth, then this location is considered as a training sample. Then, we have a vector d t = (d t,s , d t,e ) being the regression target at location t. Here, d t,s and d t,e denote the distance from location t to the corresponding boundary and are computed as</p><formula xml:id="formula_5">d t,s = t ? t s , d t,e = t e ? t,<label>(4)</label></formula><p>where t s and t e is the starting and ending frames of the ground truth, respectively. It is worth noting that d t,s and d t,e are all positive real values since the positive location t falls in the ground truth (i.e., t s &lt; t &lt; t e ). For those locations fall outside the ground truth, we do not use them to train the location regression head as in <ref type="bibr" target="#b35">[36]</ref>. It is worth mentioning that the FPN <ref type="bibr" target="#b26">[27]</ref> exploited in our video-query interaction module could also help the location regression head. The intuition is that all the positive locations from different feature maps can be used to train the location regression head, which further increases the number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantic matching head</head><p>For each video-query pair, the location regression head predicts a temporal bounding boxb t at each location t. Then, how to select the box that matches the query best is the key to perform video grounding.</p><p>Since the target of video grounding is to localize the video segments described by the query, it is straightforward to evaluate whether the content inb t matches the query semantically. To this end, we devise a semantic matching head to predict a scorem t for each predicted boxb t . The semantic matching head is implemented as two 1D convolution layers with one output channel in the last layer. If location t falls in the ground truth, its label is set as m t = 1. For those locations fall outside the ground truth, we consider them as negative training samples, i.e., m t = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">IoU regression head</head><p>The semantic matching scorem t indicates whether the content in the boxb t matches the query semantically. However, we also care about whetherb t matches the ground truth temporal boundary, which can be measured by the localization quality (i.e., the IoU with the ground truth).</p><p>To find the box with the best localization quality, one may use the "centerness" technique in FCOS <ref type="bibr" target="#b35">[36]</ref>. In short, "centerness" is introduced for object detection to suppress the low-quality detected objects based on a hand-crafted assumption-the location closer to the center of objects will predict a box with higher localization quality (i.e., a larger IoU with the ground truth). However, we empirically found that this assumption is inapplicable to video grounding. Specifically, we conduct an experiment to find out which location predicts the best box (i.e., has the largest IoU with the ground truth). For each video-query pair, we select the predicted box that has the largest IoU with the ground truth. Then, we divide the ground truth into three portions evenly and sum up the number of locations that predicts the best box for each portion. Experimental results show that More than 46% of the predictions are not predicted by the central locations of the ground truth.</p><p>In this paper, we propose to explicitly consider the localization quality of the predicted boxb t in the training and testing. The main idea is as straightforward as predicting a score at each location t to estimate the IoU betweenb t and the corresponding ground truth. To do so, we train a three-layer convolution network as the IoU regression head in the grounding module, as shown in <ref type="figure">Figure 2</ref>. Note that the input of the IoU regression head is the concatenation of the feature maps obtained from the first convolution layer of the semantic matching head and the location regression head. The training target u t is obtained by calculating the IoU betweenb t and the corresponding ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training details</head><p>We define the training loss function for the location regression head as follows:</p><formula xml:id="formula_6">L loc = 1 N pos T t=1 1 t gt L 1 (d t ,d t ),<label>(5)</label></formula><p>where we use the IoU regression loss <ref type="bibr" target="#b40">[41]</ref> as L 1 following Tian et al. <ref type="bibr" target="#b35">[36]</ref>. N pos is the number of positive samples.</p><p>1 t gt is the indicator function, being 1 if location t falls in the ground truth and 0 otherwise. The training loss function for the semantic matching head is defined as:</p><formula xml:id="formula_7">L match = 1 N pos T t=1 L 2 (m t ,m t ),<label>(6)</label></formula><p>where we adopt the focal loss <ref type="bibr" target="#b27">[28]</ref> as L 2 since it is effective when handling the class imbalance issue. To train the IoU regression head for predicting the IoU between the predicted box and ground truth, we define the training loss function as follows:</p><formula xml:id="formula_8">L iou = T t=1 L 3 (u t ,? t ),<label>(7)</label></formula><p>where we choose to use the Smooth-L1 loss <ref type="bibr" target="#b11">[12]</ref> as L 3 because it is less sensitive to outliers. With randomly initialized parameters, the location regression head often fails to produce high-quality temporal bounding boxes for training the IoU regression head. Thus, we propose a three-step strategy to train the proposed DRN, which consists of a video-query interaction module G, a semantic matching head M match , an IoU regression head M iou and a location regression head M loc . Specifically, in the first step, we fix the parameters of the IoU regression head and train the DRN by minimizing Equations <ref type="formula" target="#formula_6">(5)</ref> and <ref type="bibr" target="#b5">(6)</ref>. In the second step, we fix the parameters in DRN except for the IoU regression head and train the DRN by minimizing Equation <ref type="bibr" target="#b6">(7)</ref>. In the third step, we fine-tune the whole model in an end-to-end manner 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Charades-STA is a benchmark dataset for the video grounding task, which is built upon the Charades <ref type="bibr" target="#b32">[33]</ref> dataset. The Charades dataset is collected for video action recognition and video captioning. Gao et al. <ref type="bibr" target="#b8">[9]</ref> adapt the Charades dataset to the video grounding task by collecting the query annotations. The Charades-STA dataset contains 6672 videos and involves 16128 video-query pairs, where 12408 pairs are used for training and 3720 for testing. The duration of the videos is 29.76 seconds on average. Each video has 2.4 annotated moments and the duration of each moment is 8.2 seconds. We follow the same split of the dataset as in Gao et al. <ref type="bibr" target="#b8">[9]</ref> for fair comparisons. ActivityNet-Captions (ANet-Captions) is collected for the dense video captioning task. It is also a popular benchmark for video grounding since the captions can be used as queries. ANet-Captions consists of 20K videos with 100K queries. The videos are associated with 200 activity classes, where the content is more diverse compared to Charades-STA. On average, each video contains 3.65 queries, and each query has an average length of 13.48 words. The average duration of the videos is around 2 minutes. The Ac-tivityNet Captions dataset is split into the training set, validation set, testing set with a 2:1:1 ratio, including 37421, 17505 and 17031 video-query pairs separately. The public split of the dataset contains a training set and two validation sets val 1 and val 2. The testing set is withheld for competition. We train our model on the training set and evaluate it on val 1 and val 2 separately for fair comparisons. TACoS dataset is collected by Regneri et al. <ref type="bibr" target="#b31">[32]</ref> for video grounding and dense video captioning tasks. It consists of 127 videos on cooking activities with an average length of 4.79 minutes. For the video grounding task, TACoS dataset contains 18818 video-query pairs. Compared to Activi-tyNet Captions dataset, TACoS has more temporally annotated video segments with queries per video. Each video has 148 queries on average. Moreover, TACoS dataset is very challenging since the queries in TACoS dataset span over only a few seconds even a few frames. We follow the same split of the dataset as Gao et al. <ref type="bibr" target="#b8">[9]</ref> for fair comparisons, which has 10146, 4589, and 4083 video-query pairs for training, validation, and testing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Evaluation metric. For fair comparisons, we follow Gao et al. <ref type="bibr" target="#b8">[9]</ref> to compute "R@n, IoU=m" as the evaluation metric. To be specific, it represents the percentage of testing samples that have at least one correct grounding prediction (i.e., the IoU between the prediction and the ground truth is larger than m) in the top-n predictions. Video Feature Extractor. We use the C3D <ref type="bibr" target="#b36">[37]</ref> network pre-trained on Sports-1M <ref type="bibr" target="#b21">[22]</ref> as the feature extractor. The C3D network takes 16 frames as input and the outputs of the fc6 layer with dimensions of 4096 are used as a feature vector. We also extract the I3D <ref type="bibr" target="#b2">[3]</ref> and VGG <ref type="bibr" target="#b33">[34]</ref> features to conduct experiments on Charades-STA. More details about the feature extractor are put in the supplementary material. Language Feature. We transform each word of language sentences into lowercase. We use pre-trained GloVe word vectors to initialize word embeddings with the dimension of 300. A one-layer bi-directional LSTM with 512 hidden units serves as the query encoder. Training settings. The learning rate in the first training step is 0.001 and we decay it by a factor of 100 for the second step. During fine-tuning, we set the learning rate as 10 ?6 . We set batch size as 32 and use Adam <ref type="bibr" target="#b22">[23]</ref> as the optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with state-of-the-arts</head><p>Comparisons on Charades-STA. We compare our model with the state-of-the-art methods in <ref type="table" target="#tab_6">Table 1</ref>. Our DRN reaches the highest scores over all IoU thresholds. Particularly, when using the same C3D features, our DRN outperforms the previously best method (i.e., R-W-M [16]) by 8.7% absolute improvement, in terms of R@1, IoU=0.5. For fair comparisons with MAN <ref type="bibr" target="#b45">[46]</ref> and ExCL <ref type="bibr" target="#b10">[11]</ref>, we perform additional experiments by using the same features (i.e., VGG and I3D) as reported in their papers. Our DRN outperforms them by 1.66% and 8.99%, respectively.  <ref type="table">Table 3</ref>. It is worth noting that this dataset is very challenging since each video may correspond to multiple queries (148 queries </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation studies</head><p>In this section, we will perform complete and in-depth ablation studies to evaluate the effect of each component of our model. More details about the structures and training configurations of the baseline methods (such as DRN-Center) can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">How does location regression help?</head><p>Compared with other one-stage video grounding methods, the key to our DRN is to leverage more positive samples for the training. Here, we implement three variants of our methods: DRN-Half, DRN-Random and DRN-Center. The three baselines are the same as the original DRN (DRN-All) except that they only select a subset of frames within the ground truth as the positive training samples. Specifically, DRN-Half randomly chooses 50% of the frames within the ground truth to train the model. DRN-Random and DRN-Center are the extreme cases of our location regression settings, where they only randomly select one frame or the center frame within the ground truth as the positive training sample. By comparing the performance of the variants with our DRN, we justify the importance of increasing the number of positive training samples to train a one-stage video grounding model. <ref type="table" target="#tab_3">Table 4</ref> shows that all of these variants decrease the performance significantly. It verifies the effectiveness of our dense regression network, which is able to mine more positive training samples from sparse annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Does IoU regression help video grounding?</head><p>As discussed in Section 3.5, besides the IoU regression head, using "centerness" technique is another way to assess the localization quality. Here, we implement a variant of our model by replacing the IoU regression head with the centerness head in FCOS <ref type="bibr" target="#b35">[36]</ref>. Specifically, the centerness head is trained to predict a centerness score at each frame. The frame closer to the ground truth's center is expected to have a larger centerness value. In the inference, we follow <ref type="bibr" target="#b35">[36]</ref> to multiply the centerness score and matching score to obtain the final score for each predicted box. We also implement a baseline by removing the IoU regression head from our model and directly use the matching score to rank the predictions. <ref type="table" target="#tab_4">Table 5</ref> reveals that the IoU regression head consistently improves the performance on both datasets. These results demonstrate that the matching score is not sufficient to evaluate the localization quality. Predicting the IoU between the predicted box and ground truth is straightforward and helpful for video grounding. Using centerness slightly decreases the grounding accuracy since the centerness assumption is not suitable for video grounding. We also visualize the qualitative results in <ref type="figure" target="#fig_2">Figure 3</ref>. In the top example, the two grounding results are both predicted by the frames within the ground truth, while the IoU regression head helps to select the one that has a larger IoU. In the bottom example, the background context is similar and the query is complex. Despite such difficulty, the IoU regression head still helps to select a better grounding result. More visualization results are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Does multi-level fusion help?</head><p>The multi-level fusion (MLF) technique extracts different representations of the same query at different levels and fuses them with the video feature. Here, we implement a baseline by removing MLF from our DRN. Specifically, we only fuse the visual feature and the query feature at the first level (i.e., C 1 in <ref type="figure">Figure 2</ref>). From <ref type="table">Table 6</ref>, applying MLF to our model is able to lift the video grounding performance    <ref type="figure">78%</ref>). In addition, we implement another baseline MLF-Same by using the same query feature to fuse the video feature at different levels. In our experiments, the MLF-Same baseline performs worse than our DRN on Charades-STA (44.76% vs. 45.40%), revealing that extracting different query features at different levels is able to improve the video-query representations and boost the grounding performance eventually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">How does the location embedding help?</head><p>To evaluate the effectiveness of the temporal location embedding in our model, we conduct an ablation study by directly forwarding the video features into the network without concatenating with the location embeddings. The results in <ref type="table">Table 6</ref> conclude that the location embedding makes the localization more precisely. One possible reason is that the model is able to learn the temporal orders of the video contents through the location embeddings. To further study the effect of the location embedding, we collect a "temporal" subset of samples from the ANet-Captions dataset. In particular, we are interested in the query that contains four commonly used temporal words (i.e., before, after, while, then). The subset consists of 7176 training samples and 3620 testing samples. We use two settings to evaluate our model: 1) train on full ANet-Captions dataset and test on the temporal subset; 2) train and test on the temporal subset. From <ref type="table" target="#tab_5">Table 7</ref>, using location embedding consistently improves the performance in both settings. Especially when training and testing the model on the temporal subset, the performance gain increases to 1.7%, further verifying the effectiveness of the location embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have proposed a dense regression network for video grounding. By training the model to predict the distance from each frame to the starting (ending) frame of the video segment described by the query, the number of positive training samples is significantly increased, which boosts the performance of video grounding. Moreover, we have devised a simple but effective IoU regression head to explicitly consider the quality of localization results for video grounding. Our DRN outperforms the state-ofthe-art methods on three benchmarks, i.e., Charades-STA, ActivithNet-Captions and TACoS. It would be interesting to extend our DRN for temporal action localization and dense video captioning, and we leave it for our future work.</p><p>In the supplementary material, we first give the training details of our DRN in Section A. Then, we illustrate the details of the video-query interaction module in Section B. Next, we detail the grounding module in Section C, followed by more qualitative results in Section D. Last, we provide more details of "centerness" in Section E. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More details about our DRN</head><p>The training details of our proposed DRN are shown in Algorithm 1. With randomly initialized parameters, the location regression head often fails to produce high-quality temporal bounding box for training the IoU regression head. Thus, we propose a three-step strategy to train the proposed DRN. Specifically, in the first step, we fix the parameters of the IoU regression head and train the DRN by minimizing Equations <ref type="formula" target="#formula_6">(5)</ref> and <ref type="bibr" target="#b5">(6)</ref>. In the second step, we fix the parameters in DRN except for the IoU regression head and train the DRN by minimizing Equation <ref type="bibr" target="#b6">(7)</ref>. In the third step, we fine-tune the whole model in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of video-query interaction module</head><p>The video-query interaction module consists of two parts, as shown in <ref type="figure">Figure A</ref>. The first part serves as a data preprocessor, which takes the query sentences, video frames and temporal coordinates as input and outputs the query feature and video feature (C 1 ). The second part is a fully convolutional network with vision-language fusion modules. It is used to fuse the video feature and query feature and construct a feature pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Video feature extractor</head><p>Instead of predicting a temporal bounding box at each frame, we exploit a more efficient way to implement our dense regression network. Specifically, we divide a video into K segments evenly. Thus, the temporal resolution of the video comes to K, which significantly reduces the computation in our model. Then, we use our model to predict a temporal bounding box w.r.t. the central frame of each segment. We set K as 32 for Charades-STA and ActivityNet Captions, and 128 for TACoS dataset. Three types of feature extractor are detailed as follows: C3D. We use C3D <ref type="bibr" target="#b36">[37]</ref> pre-trained on sport1M <ref type="bibr" target="#b21">[22]</ref> to extract features. The C3D network takes 16 consecutive frames (a snippet) as input and the output of fc6 layer is used as a snippet-level feature vector. The feature of each segment is obtained by performing max-pooling among the snippet-level features that correspond to the segment. VGG. We use VGG16-BN <ref type="bibr" target="#b33">[34]</ref> pre-trained on ImageNet. VGG16-BN takes one frame as input and the output of fc7 layer is used as the frame-level feature. The segment feature is obtained by performing max-pooling among the framelevel features that correspond to the segment. predict regression offsetd t using Equation <ref type="formula" target="#formula_0">(1)</ref> 4:</p><p>update DRN by minimizing Equations <ref type="formula" target="#formula_6">(5)</ref> and <ref type="formula" target="#formula_7">(6)</ref>  predict bounding boxb t using Equation <ref type="formula" target="#formula_0">(1)</ref> 3:</p><p>predict IoU betweenb t and ground truth <ref type="bibr">4:</ref> update DRN by minimizing Equation <ref type="formula" target="#formula_8">(7)</ref>  predict bounding boxb t using Equation <ref type="formula" target="#formula_0">(1)   4:</ref> predict IoU betweenb t and ground truth <ref type="bibr">5:</ref> update DRN by minimizing Equations (5), (6), (7) 6: end while Output: Trained DRN I3D. We use I3D <ref type="bibr" target="#b2">[3]</ref> pre-trained on Kinetics to extract features. The I3D network takes 64 consecutive frames (a snippet) as input and outputs a snippet-level feature vector. The feature of each segment is obtained by performing maxpooling among the snippet-level features that correspond to the segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Query feature extractor</head><p>First, each word in the input query sentence is mapped into a 300-dim vector using pre-trained GloVe word embeddings. Then, the word embeddings of the query sentence are fed into a one-layer bi-directional LSTM with 512 units. Last, the sequence of hidden states is used as query features. The hidden states of the first and the last word are concatenated, leading to the global representation g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Location embedding</head><p>The input temporal coordinates of the k-th segment is a 3D vector, i.e., ( k?0.5 K , k+0.5 K , 1 K ). We forward it to a linear layer, leading to a 256D location embedding. The location embedding is then concatenated with the video features along the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Vision-Language Fusion Module</head><p>We apply the textual attention mechanism to the input query feature and obtain the attended feature. Then, the attended query features and the features from a lower level of   the pyramid are fused by using element-wise multiplication. The details are shown in <ref type="figure">Figure B</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Language Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video-Query Interaction Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Language Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More details about grounding module</head><p>The grounding module involves three components, including semantic matching head, location regression head and IoU regression head. Both of the semantic matching head and location regression head consist of two 1D convolution layers, and IoU regression head contains three 1D convolution layers. The details are shown in <ref type="figure" target="#fig_5">Figure C</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More visualization examples</head><p>We show more qualitative results of the IoU regression head in <ref type="figure">Figure D</ref>. The IoU regression head helps to select the prediction that has a larger IoU with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More details about centerness E.1. Details of centerness baseline</head><p>To compare our IoU regression with the centerness in FCOS <ref type="bibr" target="#b27">[28]</ref>, we conduct an experiment by replacing the loss function of IoU regression head with a centerness loss as in <ref type="bibr" target="#b27">[28]</ref>. Specifically, we train the model to predict a centerness score for each location. The training target is defined as: centerness * = min(d * t,s , d * t,e ) max(d * t,s , d * t,e )</p><p>where d * t,s , d * t,e are the distances between location t and the starting frame, the ending frame of ground truth boundary respectively. We follow <ref type="bibr" target="#b27">[28]</ref> to adopt the binary crossentropy loss as the loss function for centerness in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Results of the centerness assumption</head><p>The centerness assumption <ref type="bibr" target="#b27">[28]</ref> is that the location closer to the center of objects will predict a box with higher localization quality (i.e., a larger IoU with the ground truth). We conduct an experiment to find out which location predicts the best box. In our experiment, we train a model using the semantic matching loss and location regression loss. For each video-query pair, we select the predicted box that has the largest IoU with the ground truth. Then, we divide the ground truth into three portions evenly and sum up the number of the locations that predicts the best box for each portion. From <ref type="figure">Figure E</ref>, more than 48% of the predictions are not predicted by the central locations of the ground truth.  <ref type="figure">Figure E</ref>. The location distribution of the "best location" on two datasets. Here, the "best location" denotes the location that predicts the best grounding result for each video-query pair. We show the statistics of their relative locations w.r.t. the ground truth, which has been divided into three portions evenly. Here, we only focus on the locations within the ground truth since few locations fall outside of the ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>C</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>TimeQuery:</head><label></label><figDesc>The child continues using the toy on the clothes while looking up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on ActivityNet Captions dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 :</head><label>5</label><figDesc>end while Step2: Fix the parameters of G, M match , and M loc 1: while not converges do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A .Figure B .</head><label>AB</label><figDesc>The details of Video-Query Interaction Module. Note that "Conv1D (b, s)" denotes a 1D convolution layer with a kernel size of b and a stride of s. All the convolution layers are followed by batch normalization and ReLU. The details of Vision-Language-Fusion Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure C .</head><label>C</label><figDesc>The details of Grounding Module. The input Pi is from the i-th level in the feature pyramid with a temporal dimension of K 2 i?1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>TimeQuery:</head><label></label><figDesc>Butter, brown sugar, sugar, vanilla, eggs are mixed and whisked together in a glass. little boythrows arrow with a bow , then he raise his right arm in signal success. injured player is taken to the dressing room by two men , then the teams continue playing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Video-Query Interaction Module Semantic Matching Head conv IoU Regression Head Vision-Language Fusion Location Regression Head C conv x3 conv</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>+ ,</cell><cell>/ ,</cell><cell></cell><cell>1</cell><cell>?</cell><cell>'</cell><cell>?</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>?</cell><cell>0.9</cell><cell>?</cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Matching Score</cell></row><row><cell></cell><cell></cell><cell>+ -</cell><cell>/ -</cell><cell></cell><cell>1</cell><cell>?</cell><cell>'</cell><cell>?</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell>?</cell><cell>0.8</cell><cell>?</cell><cell>0.4</cell></row><row><cell>1</cell><cell>t</cell><cell>T</cell><cell></cell><cell></cell><cell cols="3">IoU with GT</cell></row><row><cell></cell><cell>Video Feature</cell><cell>+ .</cell><cell>/ .</cell><cell></cell><cell>1</cell><cell cols="2">' ? ?</cell><cell>%</cell></row><row><cell></cell><cell>Extractor</cell><cell>Concatenate</cell><cell>Temporal Location</cell><cell>conv</cell><cell>conv</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Vision-Language Fusion</cell><cell>Embedding</cell><cell></cell><cell cols="3">Temporal bounding box</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 3 .</head><label>13</label><figDesc>Comparisons with state-of-the-arts on Charades-STA. Comparisons on TACoS using C3D features.</figDesc><table><row><cell>Methods</cell><cell cols="2">Feature</cell><cell cols="4">R@1 IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7 R@1 R@5 R@5</cell></row><row><cell>CTRL [9]</cell><cell></cell><cell>C3D</cell><cell></cell><cell>23.63</cell><cell>8.89</cell><cell>58.92</cell><cell>29.52</cell></row><row><cell>SMRL [39]</cell><cell></cell><cell>C3D</cell><cell></cell><cell>24.36</cell><cell>11.17</cell><cell>61.25</cell><cell>32.08</cell></row><row><cell>MAC [10]</cell><cell></cell><cell>C3D</cell><cell></cell><cell>30.48</cell><cell>12.20</cell><cell>64.84</cell><cell>35.13</cell></row><row><cell>T-to-C [40]</cell><cell></cell><cell>C3D</cell><cell></cell><cell>35.60</cell><cell>15.80</cell><cell>79.40</cell><cell>45.40</cell></row><row><cell cols="3">R-W-M [16] C3D</cell><cell></cell><cell>36.70</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DRN (ours)</cell><cell></cell><cell>C3D</cell><cell></cell><cell>45.40</cell><cell>26.40</cell><cell>88.01</cell><cell>55.38</cell></row><row><cell>ExCL [11]</cell><cell></cell><cell>I3D</cell><cell></cell><cell>44.10</cell><cell>22.40</cell><cell>-</cell><cell>-</cell></row><row><cell>DRN (ours)</cell><cell></cell><cell>I3D</cell><cell></cell><cell>53.09</cell><cell>31.75</cell><cell>89.06</cell><cell>60.05</cell></row><row><cell>SAP [7]</cell><cell></cell><cell>VGG</cell><cell></cell><cell>27.42</cell><cell>13.36</cell><cell>66.37</cell><cell>38.15</cell></row><row><cell>MAN [46]</cell><cell></cell><cell>VGG</cell><cell></cell><cell>41.24</cell><cell>20.54</cell><cell>83.21</cell><cell>51.85</cell></row><row><cell cols="3">DRN (ours) VGG</cell><cell></cell><cell>42.90</cell><cell>23.68</cell><cell>87.80</cell><cell>54.87</cell></row><row><cell cols="7">Table 2. Comparisons on ANet-Captions using C3D features. (  *  )</cell></row><row><cell cols="7">indicates the method that uses val 2 split as the testing set, while</cell></row><row><cell cols="5">other methods use the val 1 split.</cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell cols="5">R@1 IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7 R@1 R@5 R@5</cell></row><row><cell>CTRL [9]</cell><cell></cell><cell cols="2">14.00</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>ACRN [29]</cell><cell></cell><cell cols="2">16.17</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>T-to-C[40]</cell><cell></cell><cell cols="2">27.70</cell><cell cols="2">13.60</cell><cell>59.20</cell><cell>38.30</cell></row><row><cell>R-W-M [16]</cell><cell></cell><cell cols="2">36.90</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>DRN (ours)</cell><cell></cell><cell cols="2">42.49</cell><cell cols="2">22.25</cell><cell>71.85</cell><cell>45.96</cell></row><row><cell>TGN  *  [4]</cell><cell></cell><cell cols="2">27.93</cell><cell>-</cell><cell></cell><cell>44.20</cell><cell>-</cell></row><row><cell>ABLR  *  [43]</cell><cell></cell><cell cols="2">36.79</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>CMIN  *  [49]</cell><cell></cell><cell cols="2">43.40</cell><cell cols="2">23.88</cell><cell>67.95</cell><cell>50.73</cell></row><row><cell cols="2">DRN  *  (ours)</cell><cell cols="2">45.45</cell><cell cols="2">24.36</cell><cell>77.97</cell><cell>50.30</cell></row><row><cell cols="3">Methods</cell><cell></cell><cell cols="3">R@1 IoU=0.5 IoU=0.5 R@5</cell></row><row><cell cols="4">ABLR [43]</cell><cell cols="2">9.40</cell><cell>-</cell></row><row><cell cols="3">CTRL [9]</cell><cell></cell><cell cols="2">13.30</cell><cell>25.42</cell></row><row><cell cols="4">ACRN [29]</cell><cell cols="2">14.62</cell><cell>24.88</cell></row><row><cell cols="4">SMRL [39]</cell><cell cols="2">15.95</cell><cell>27.84</cell></row><row><cell cols="4">CMIN [49]</cell><cell cols="2">18.05</cell><cell>27.02</cell></row><row><cell cols="3">TGN [4]</cell><cell></cell><cell cols="2">18.90</cell><cell>31.02</cell></row><row><cell cols="3">MAC [10]</cell><cell></cell><cell cols="2">20.01</cell><cell>30.66</cell></row><row><cell cols="4">DRN (ours)</cell><cell cols="2">23.17</cell><cell>33.36</cell></row><row><cell cols="7">Comparisons on ActivityNet-Captions. Table 2 reports</cell></row><row><cell cols="7">the video grounding results of various methods. We follow</cell></row><row><cell cols="7">the previous methods to use C3D features for fair compar-</cell></row><row><cell cols="7">isons. Since previous methods use different testing splits,</cell></row><row><cell cols="7">we report the performance of our model on both val 1 and</cell></row><row><cell cols="7">val 2. Regarding R@1, IoU=0.5, our method outperforms</cell></row><row><cell cols="7">R-W-M [16] by 5.59% absolute improvement on val 1 split</cell></row><row><cell cols="7">and exceeds CMIN [49] by 2.05% on val 2 split.</cell></row></table><note>Comparisons on TACoS. We compare our DRN with state- of-the-art methods with the same C3D features in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on the number of positive training samples on Charades, measured by R@1 and R@5 when IoU=0.5.</figDesc><table><row><cell>Methods</cell><cell>R@1 IoU=0.5</cell><cell>Gain</cell><cell>R@5 IoU=0.5</cell><cell>Gain</cell></row><row><cell>DRN-Center</cell><cell>38.36</cell><cell>-</cell><cell>83.36</cell><cell>-</cell></row><row><cell>DRN-Random</cell><cell>40.88</cell><cell>2.52</cell><cell>84.11</cell><cell>0.75</cell></row><row><cell>DRN-Half</cell><cell>42.79</cell><cell>4.43</cell><cell>85.88</cell><cell>2.52</cell></row><row><cell>DRN-All</cell><cell>45.40</cell><cell>7.04</cell><cell>88.01</cell><cell>4.65</cell></row><row><cell cols="5">on average). Despite its difficulty, our method reaches the</cell></row><row><cell cols="5">highest score in terms of both R@1 and R@5 when IoU =</cell></row><row><cell cols="5">0.5 and outperforms previous best result by a large margin</cell></row><row><cell cols="2">(i.e., 23.17% vs. 20.01%).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of the IoU regression head on Charades-STA and ActivityNet-Captions, measured by R@1 when IoU=0.5.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Methods</cell><cell></cell><cell>R@1 IoU=0.5</cell></row><row><cell></cell><cell cols="4">w/o IoU regression head</cell><cell>44.13</cell></row><row><cell>Charades-STA</cell><cell cols="2">w/ Centerness</cell><cell></cell><cell>44.02</cell></row><row><cell></cell><cell cols="3">w/ IoU regression head</cell><cell>45.40</cell></row><row><cell></cell><cell cols="4">w/o IoU regression head</cell><cell>40.44</cell></row><row><cell>ANet-Captions</cell><cell cols="2">w/ Centerness</cell><cell></cell><cell>39.83</cell></row><row><cell></cell><cell cols="3">w/ IoU regression head</cell><cell>42.49</cell></row><row><cell cols="5">Table 6. Ablation study of multi-level fusion (MLF) and location</cell></row><row><cell cols="5">embedding on Charades-STA, measured by R@1 when IoU=0.5.</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="3">Components MLF location IoU=0.5 R@1</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>43.04</cell></row><row><cell cols="2">Charades-STA</cell><cell>?</cell><cell>?</cell><cell>43.79 43.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.40</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>39.78</cell></row><row><cell cols="2">ANet-Captions</cell><cell>?</cell><cell>?</cell><cell>40.61 40.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Ablation study of the location embedding on the collected subset of ANet-Captions, measured by R@1 when IoU=0.5.</figDesc><table><row><cell>Train</cell><cell>Test</cell><cell>Methods</cell><cell>R@1 IoU=0.5</cell></row><row><cell cols="2">Full-set Full-set</cell><cell>w/o location w/ location</cell><cell>40.61 42.49</cell></row><row><cell cols="2">Full-set Sub-set</cell><cell>w/o location w/ location</cell><cell>47.38 48.37</cell></row><row><cell cols="2">Sub-set Sub-set</cell><cell>w/o location w/ location</cell><cell>43.28 44.97</cell></row><row><cell cols="4">on both Charades-STA (43.79% vs. 43.04%) and ANet-</cell></row><row><cell cols="3">Captions datasets (40.61% vs. 39.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 1</head><label>1</label><figDesc>Training details of DRN.Input: Video V = {I t } T t=1 ; query Q = {w n } N n=1Step1: Fix the parameters of M iou 1: while not converges do</figDesc><table><row><cell>2:</cell><cell>predict matching scorem t</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Fine-tune G, M match , M loc , and M iou jointly 1: while not converges do</figDesc><table><row><cell cols="2">5: end while</cell></row><row><cell>Step3: 2:</cell><cell>predict matching scorem t</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>27.21% 22.07% 48.01% 51.96% 24.78% 25.97%</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>[0, 1/3)</cell></row><row><cell></cell><cell>[1/3, 2/3)</cell></row><row><cell></cell><cell>[2/3, 1]</cell></row><row><cell>Charades-STA</cell><cell>ANet-Captions</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We put the training algorithm in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements. This work was partially supported by</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial learning with local coordinate coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-marginal wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Langyuan</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Localizing natural language in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8175" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relation attention for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic proposal for activity localization in videos via sentence query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8199" to="8206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual encoding for zeroexample video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mac: Mining activity concepts for language-based temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="245" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Excl: Extractive clip localization using natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuva</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, Qinfeng Shi, and Mingkui Tan. Multi-way backpropagation for training compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-embedding generative adversarial networks for high resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nat: Neural architecture transformer for accurate and compact architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="735" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Read, watch, and move: Reinforcement learning for temporally grounding natural language descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Localizing moments in video with temporal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1380" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-level visual-semantic alignments with relationwise dual attention network for image and text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Location-aware graph convolutional networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attentive moment retrieval in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-modal moment localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominikus</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic hand gesture recognition using vision-based approach for human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyeeta</forename><surname>Singha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarjit</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabul</forename><surname>Hussain Laskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1129" to="1141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Languagedriven temporal activity localization: A semantic matching reinforcement learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilevel language and vision integration for text-to-clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9062" to="9069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9159" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From whole slide imaging to microscopy: Deep microscopy adaptation network for histopathology cancer image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collaborative unsupervised domain adaptation for medical image diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging meets NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-modal interaction networks for query-based moment retrieval in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Vision-language navigation with self-supervised auxiliary reasoning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discrimination-aware channel pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="875" to="886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
