<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Emotion Recognition: A multi-task approach using deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-29">October 29, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakash</forename><surname>Saroop</surname></persName>
							<email>aakash.saroop@somaiya.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Mathamsetty</surname></persName>
							<email>sashank.m@somaiya.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Vasani</surname></persName>
							<email>vaibhav.vasani@somaiya.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering K. J. Somaiya College of Engineering Mumbai</orgName>
								<address>
									<settlement>Maharashtra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering K. J. Somaiya College of Engineering Mumbai, Maharashtra</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering K. J. Somaiya College of Engineering Mumbai, Maharashtra</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Emotion Recognition: A multi-task approach using deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-29">October 29, 2021</date>
						</imprint>
					</monogr>
					<note>*Corresponding author 2 nd Pathik Ghugare Department of Computer Engineering K. J. Somaiya College of Engineering Mumbai, Maharashtra</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial Emotion Recognition</term>
					<term>Multi-task learning</term>
					<term>Multi-output model</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Image Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial Emotion Recognition is an inherently difficult problem, due to vast differences in facial structures of individuals and ambiguity in the emotion displayed by a person. Recently, a lot of work is being done 1 arXiv:2110.15028v1 [cs.CV] 28 Oct 2021 in the field of Facial Emotion Recognition, and the performance of the CNNs for this task has been inferior compared to the results achieved by CNNs in other fields like Object detection, Facial recognition etc. In this paper, we propose a multi-task learning algorithm, in which a single CNN detects gender, age and race of the subject along with their emotion. We validate this proposed methodology using two datasets containing realworld images. The results show that this approach is significantly better than the current State of the art algorithms for this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As AI continues to become an increasing integral part of our lives, the need for machines to understand the state of the human emotions is of critical importance. This has the potential to take human computer interaction to the next level, with a direct impact in the field of voice assistants, mental health therapy, recommendation systems etc.</p><p>Since there are unique local languages and local moral orders, different cultures can use the same emotion and expression in very different ways <ref type="bibr" target="#b0">[1]</ref>. This makes facial emotion recognition is an ambiguous task as each person picks up visual cues differently. The State of the art algorithms for Facial Emotion Recognition (FER) give much lower accuracy on a reasonably large dataset obtained from real world images taken in a non controlled environment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">17]</ref>, as compared to other computer vision tasks like object detection <ref type="bibr" target="#b14">[18]</ref>, image classification <ref type="bibr" target="#b15">[19]</ref> etc.</p><p>In this paper, we hypothesise that a Convolutional Neural Network, subjected to multi-task learning on the same data, i.e. learning to predict emotion, age, gender and race of a subject from the same facial image would perform better on individual tasks, by transferring knowledge across different domains. The summary of contributions through this paper is:</p><p>? Validation that the proposed multi-task learning approach of combining FER with age, gender and race classification outperforms the multi-task approach of combining FER and facial action unit detection and the conventional single-task learning approach.</p><p>? Presenting a CNN architecture which gives outputs corresponding to all the labels mentioned above.</p><p>The rest of this paper is organised as follows: Related work has been reviewed in Section II. The proposed approach has been discussed in Section III. Experimental setup has been shared in Section IV. Experimental results are shared in Section V. Further discussions on the observations made are discussed in Section VI. Conclusion and future work are suggested in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>The work done in the field of FER can be broadly divided into two categories based on whether the features were hand-crafted or generate through a Neural Network. Before the widespread use of CNNs in the field of computer vision, FER used to be carried out by identifying facial components or landmarks from the facial region <ref type="bibr" target="#b19">[23,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b21">25,</ref><ref type="bibr" target="#b22">26]</ref>. Recent work in the field of Facial Emotion Recognition deals with using Real World Images instead of taking images in a controlled environment. This leads to several pre-processing methods being used on the images obtained . Some of these approaches are used for preprocessing the images, before feeding them to a CNN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. In <ref type="bibr" target="#b1">[2]</ref>, feature extraction approaches such as HoG, LBP are used to extract features from the image, which requires relatively lower computing power and memory than Deep Learning approaches. In <ref type="bibr" target="#b4">[5]</ref>, Milad Zadeh et. al. discusses the use of pre-processing method of Gabor filters to increase the accuracy of Facial emotion recognition. In this paper, a very small dataset <ref type="bibr" target="#b8">[12]</ref> of 213 images which lead to overfitting on the Neural Network. <ref type="bibr" target="#b10">[14]</ref> draws the Bezier curve on the eye and mouth and classifies the emotion of the characteristic with Hausdroff distance.</p><p>Ever since the in popularity of Deep Learning, CNNs have been used for FER, which provide the output by enabling "end-to-end" learning to occur in the pipeline directly from the input images <ref type="bibr" target="#b23">[27]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, Mehdipour Ghazi et. al. discusses an approach to deal with Emotion recognition using DL under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localisation. They have proposed that using preprocessing methods for pose and illumination normalisation along with pre-trained deep learning models or accounting for these variations during training substantially resolve this weakness present in the dataset. <ref type="bibr" target="#b11">[15]</ref> approaches the problem of emotion classification based on thermal images of the face. The ideas presented in this paper have been heavily built upon the work done in <ref type="bibr" target="#b2">[3]</ref>. Gerard Pons et. al. take the work of multi-task, multi-label and multidomain learning with residual convolutional networks for emotion recognition. They had claimed that using the same CNN, when trained to predict both emotion and facial action units, it performed better at each individual task. In this paper, it was suggested that other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Aprroach</head><p>Here the approach of multi-task learning, i.e. a single model generating labels for emotion, gender, age and race, is to be compared with the approach of singletask learning, i.e. a model predicting only emotion of a subject, as shown in Figure1. The CNN architecture used for this study is shown in Figure2. More details about the CNN can be found in the appendix.</p><p>Each of the classification tasks have a separate output layer, which means that each output layer has a softmax function which is independent from the In the pre-processing stage, the images have been subjected to pose normalisation. This has been achieved by identifying the eye centres of the faces presented in the images, and the images being rotated such that the line joining the eye centres becomes horizontal. The eye detection has been carried out using the Cascade classifier of opencv library <ref type="bibr" target="#b12">[16]</ref>. There were a few cases in which the pre-trained algorithm was incorrectly identifying the eye centres and the images were being rotated by an angle of large magnitude, distorting the images <ref type="figure">(Figure 3</ref>). To counter this, the max range to which the image can be rotated has been capped at 10 degrees. In the images presented in both the data sets, the faces only have minor rotation presented. Capping the maximum rotation at 10 degrees ensures that the examples in which eye centers are identified correctly are normalised, but the examples in which eyes are not correctly identified, are not rotated by a large magnitude.</p><p>The CNN architecture has been strongly built upon [6], with the change in dropout rates to improve the accuracy. The dropout rates in [6] were 0.4, 0.4, 0.5, 0.6 in increasing layers of the neural network. In the neural network On the FER dataset, the cross validation accuracy of the previous model was 63%. Upon making the changes, the accuracy has increased to 67%. The reasoning for why this change having performed better than the original model could be: in deeper layers of the Neural Network, each layer learns to detect more complex patterns <ref type="bibr" target="#b5">[7]</ref>, which are of increasing importance in our classification task, and need to have higher probability of being retained in each pass through the neural network. The second last layer of the neural network provides an input to 4 separate output layers, one for each of the classification tasks. So, except the output layer, all the classification tasks have the same weights for the rest of the neural network. For each of the classification tasks, categorical cross entropy is used to calculate the loss of each task. Hence there are 4 loss functions. To calculate the loss function of the entire Neural network, adequate weights needs to be assigned to the loss function of each of the tasks, to get a meaningful result in each of the separate tasks. The weights assigned to emotion, age, race and gender loss were 2, 4, 1.5, 0.1 respectively.</p><p>Using the above weights, the overall loss is calculated. Then the model backpropagates this loss to improve its performance in subsequent iterations. To ensure that the CNN gives optimal performance, two callbacks were used:</p><p>Early stopping: The validation accuracy for emotion was calculated for each epoch, during the training of the neural network. This is used to determine when the model started overfitting on the data. When overfitting has been identified, further training of the model is stopped and the weights from the epoch which provided the best validation accuracy for the data, have been restored to the final model.</p><p>Reduce on Plateau: If the validation accuracy for emotion output fails to improve for 5 epochs, the learning rate is reduced to 0.2 times the previous model to continue training of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this paper the following datasets have been used: RAF-DB <ref type="bibr" target="#b7">[9]</ref>: Real-world Affective Faces Database (RAF-DB) is a large-scale facial expression database with around 30K great-diverse facial images downloaded from the Internet. Based on the crowdsourcing annotation, each image has been independently labeled by about 40 annotators. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc. RAF-DB has large diversities, large quantities, and rich annotations. FER <ref type="bibr" target="#b6">[8]</ref>: The FER dataset classifies facial expressions from 35,685 examples of 48x48 pixel grayscale images of faces. Images are categorized based on the emotion shown in the facial expressions (happiness, neutral, sadness, anger, surprise, disgust, fear).</p><p>The model was given the data in the form of {x i , y i }, where x i is a 50x50 grayscale image containing a human face, and y i is the list of numpy arrays containing the labels corresponding to the image. A sample output i: [[1, 0, 0, 0, 0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0, 0, 0]] Signifies that the person present in the image is surprised, male, caucasian and has an age of 0-3.</p><p>The multi label CNN was trained using a batch size of 32 images. The initial learning rate was set to 3e-4 and the number of epochs was equal to 100. All the models were implemented in Tensorflow <ref type="bibr" target="#b9">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>The validation accuracies recorded can be seen in <ref type="table">Table 1</ref>. In the multi la-Dataset Single Label Output (Emotion) Multi Label Output RAFDb 0.4538 0.7926 FER 0.6791 0.5312* * FER has labels only corresponding to emotions. <ref type="table">Table 1</ref>: Emotion Validation Accuracies bel output model, the validation accuracy of the model after being trained on RAFDB is 79.26%. After the same model is further trained on FER, its accuracy reduces to 53.12%. Hence, for our final model, the training was stopped after just RAFDB and the final accuracy is 79.26%. This is an improvement from the accuracy reported in the multi-task model of <ref type="bibr" target="#b2">[3]</ref>, which gives an accuracy of 45.9% for emotion classification on the SFEW dataset. This shows that the hypothesis of this paper is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head><p>RAFDB (Basic Emotions) FER (After training on RAFDB) emotion 0.7926 0.5312 gender 0.7832 N/A* race/ ethnicity 0.8610 N/A* age 0.7476 N/A* * Not applicable. In the Multi-label model, weights for other losses set to 0. For each of the datasets, 90% of the images were used as training data and 10% of the images were used as cross-validation data. As seen in <ref type="figure" target="#fig_3">Figure 4</ref>, as the accuracy of the model begins to plateau multiple times throughout the training, the learning rate of the model is reduced before further training. However, when the learning rate becomes less than 1e-6, further training is stopped and the model weights from the epoch having the best validation accuracy are restored to the final model.  In addition to this, while trying to perform transfer learning on neural networks which had been pre-trained on facial recognition, the following accuracies were obtained for single task learning of facial emotions on the FER dataset:   <ref type="bibr" target="#b16">[20]</ref>: 18.03% ResNet50v2 <ref type="bibr" target="#b17">[21]</ref>: 48.08% FaceNet <ref type="bibr" target="#b18">[22]</ref>: 30.88% These are all lower than the accuracy obtained on the neural network used for single task learning without any transfer learning, which was 63%.Hence the final model used did not use transfer learning. Sample outputs of the model are shown in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>In case of multi-task learning the validation accuracy is 79% when trained for all the labels on RAF-Db, which reduces to 53% when the same model, pre-trained on RAF-Db was further trained single task learning on FER, as this dataset has labels only corresponding to emotions. Hence the final multi-task learning model, which gives the best results, has been trained only on RAF-Db.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future work</head><p>In this paper we developed an approach to perform multi-task learning, i.e. predicting gender, age and race along with emotion. The results obtained were drastically better than the classical approach of single task learning using the same CNN architecture. This paper was an effort at improving the accuracy of CNNs for Facial Emotion Recognition. Future work will consist of:</p><p>? The approach of multi-task learning can be tested on various CNN architectures for facial emotion recognition.</p><p>? To effectively train more data having all the labels. Labels for age, gender and race can be generated using pre-trained open source models for the FER dataset and used as a part of the training set.</p><p>? Various filters such as Gabor, HOG, LBP, SIFT can be applied during the preprocessing step and their effect on the results can be studied.</p><p>? Finding the optimal values of the weights of the loss function of the 4 branches of the CNN, instead of hardcoding the values.</p><p>? Finding the optimal values of the weights of the loss function of the 4 branches of the CNN, instead of hardcoding the values. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the model softmax function of other output layers: ?(z) i = e zi K j=1 e zj (1) ? = softmax K = number of classes of the classifier e zi = standard exponential function for vector e zj = standard exponential function for vector For emotion recognition, the model classifies the input image in one of 7 classes consisting of the basic human emotions: Surprise, fear, disgust, happy, sad, angry and neutral. Gender is classified as male, female or unsure. Race is classified as Caucasian, African-American or Asian. For age estimation, the classes are divided into a range of ages: 0-3, 4-19, 20-39, 40-69 and 70+. The datasets used are FER, containing labels for only emotion and RAF-Db containing labels corresponding to all 4 tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Architecture of the model (a) Showing the original image in the dataset with incorrectly identified eye centers. (b) Showing the distortion to the pre-processed image architecture presented in this paper, the dropout layers are now monotonically decreasing : 0.6, 0.5, 0.4, 0.4 in the increasing layers of the neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy and loss evolution of the label emotion.(a) On RAF-Db (b) On FER after training on RAF-Db VGG16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sample Outputs of the model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Cross Validation Accuracies</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Not applicable. In the Multi-label model, weights for other losses set to 0.</figDesc><table><row><cell>Label</cell><cell cols="2">RAFDB (Basic Emotions) FER (After training on RAFDB)</cell></row><row><cell>emotion</cell><cell>0.6956</cell><cell>1.2371</cell></row><row><cell>gender</cell><cell>0.5110</cell><cell>N/A*</cell></row><row><cell>race/ ethnicity</cell><cell>0.4524</cell><cell>N/A*</cell></row><row><cell>age</cell><cell>0.9003</cell><cell>N/A*</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Loss values Some parts of the code were taken from [6, 10, 11].</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Li, Shan and Deng, Weihong and Du, JunPing for giving us access to the RAF-Db dataset. We are also grateful to K. J. Somaiya College of Engineering for giving us the opportunity to work in this research area.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The social constructionist viewpoint&quot;. The social construction of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rom</forename><surname>Harre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Blackwell</publisher>
			<biblScope unit="page" from="2" to="14" />
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Brief Review of Facial Emotion Recognition Based on Visual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ko</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18020401</idno>
		<ptr target="https://doi.org/10.3390/s18020401" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Masip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive analysis of deep learning based representation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdipour</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazim Kemal</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast facial emotion recognition using convolutional neural networks and Gabor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><forename type="middle">Mohammad</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Majidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Conference on Knowledge Based Engineering and Innovation (KBEI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Special Issue on &quot;Deep Learning of Representations</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Japanese Female Facial Expression (JAFFE) Dataset [Data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miyuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Gyoba</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3451524</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3451524" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Dan Man?, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognition of facial emotion through face analysis based on quadratic bezier curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Hwan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian Journal of Science and Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human emotion recognition from facial thermal image based on fused statistical feature and multi-class SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anushree</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual IEEE India Conference (INDICON)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<title level="m">The OpenCV Library. Dr. Dobb&amp;#x27;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facial emotion recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sarvakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Senkamalavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Materials Today: Proceedings</title>
		<imprint>
			<date type="published" when="2021-08-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fedvision: An online visual object detection platform powered by federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lican</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13172" to="13179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis of artificial intelligence based image classification techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shakya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Innovative Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>JIIP).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the IEEE conference on computer vision and pattern recognition 2016</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the IEEE conference on computer vision and pattern recognition 2015</title>
		<imprint>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Development of a Facial Emotion Recognition Method based on combining AAM with DBN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In2010 International Conference on Cyberworlds</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-10-20" />
			<biblScope unit="page" from="87" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric feature-based facial expression recognition in image sequences using multi-class adaboost and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7714" to="7748" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A real time facial expression classification system using local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In2012 4th International conference on intelligent human computer interaction (IHCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-12-27" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Facial expression recognition based on local region specific features and support vector machines. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="7803" to="7824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep structured learning for facial expression intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="143" to="54" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Appendix: The appendix contains the model architecture of the CNN used</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
