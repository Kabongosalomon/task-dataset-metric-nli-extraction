<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Deep Learning-based 6D Bin Pose Estimation in 3D Scans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Gajdo?ech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skeletex Research</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics, Physics and Informatics</orgName>
								<orgName type="institution">Comenius University Bratislava</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Kocur</surname></persName>
							<email>kocur@fmph.uniba.sk</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics, Physics and Informatics</orgName>
								<orgName type="institution">Comenius University Bratislava</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Brno University of Technology</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Stuchl?k</surname></persName>
							<email>stuchlik@skeletex.xyz</email>
							<affiliation key="aff0">
								<orgName type="institution">Skeletex Research</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Hudec</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Informatics and Information Technologies</orgName>
								<orgName type="institution">Slovak Technical University Bratislava</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Madaras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skeletex Research</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics, Physics and Informatics</orgName>
								<orgName type="institution">Comenius University Bratislava</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Deep Learning-based 6D Bin Pose Estimation in 3D Scans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision</term>
					<term>Bin Pose Estimation</term>
					<term>6D Pose Estimation</term>
					<term>Deep Learning</term>
					<term>Point Clouds</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An automated robotic system needs to be as robust as possible and fail-safe in general while having relatively high precision and repeatability. Although deep learning-based methods are becoming research standard on how to approach 3D scan and image processing tasks, the industry standard for processing this data is still analytically-based. Our paper claims that analytical methods are less robust and harder for testing, updating, and maintaining. This paper focuses on a specific task of 6D pose estimation of a bin in 3D scans. Therefore, we present a high-quality dataset composed of synthetic data and real scans captured by a structured-light scanner with precise annotations. Additionally, we propose two different methods for 6D bin pose estimation, an analytical method as the industrial standard and a baseline data-driven method. Both approaches are crossevaluated, and our experiments show that augmenting the training on real scans with synthetic data improves our proposed data-driven neural model. This position paper is preliminary, as proposed methods are trained and evaluated on a relatively small initial dataset which we plan to extend in the future. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Capturing a scene with 3D scanners is a standard for automatized systems analyzing a scene. To pick mechanical parts from a bin by a robotic arm equipped with a gripper, the parts need to be localized. First, the localization of bin is essential to restrain the robot from collisions. Then, the kinematics of the robot is optimized for path planning. The problem of bin localization can be defined as a 6 DoF pose estimation of a template 3D model of the bin in the 3D scan.</p><p>Nowadays, analytical methods are still the industrial standard for the processing of 3D scans. On the contrary, the academic and research standards have evolved to data-driven or hybrid approaches. Analytical computation of bin transformation in captured point clouds might be vulnerable to missing critical information in the captured scans, like corners and edges, yielding lower robustness than expected. The computation precision of a hard-defined analytical ala https://orcid.org/0000-0002-8646-2147 b https://orcid.org/0000-0001-8752-2685 c https://orcid.org/0000-0001-8556-8364 d https://orcid.org/0000-0002-1659-0362 e https://orcid.org/0000-0003-3917-4510 gorithm might be higher but at the cost of lower robustness if a key content is missing. In applications of automated intelligent systems, it may be interesting to lower its precision to increase the robustness in some scenarios. The other possible approach is to split the pipeline into two steps -the first part of the pipeline orients on the robustness and raw datadriven localization. The second part focuses on the precision-based analytical solution starting from the predicted pose estimations, thus having the robustness properties inherited from the data-driven approach.</p><p>In this paper, we present a novel dataset containing high-quality real and synthetic 3D scans of different bins in various poses containing a variety of items captured by structured light scanners. We publish the dataset 1 for further research. We propose an analytical method and a conceptually simple deep convolutional neural network for 6D bin pose estimation. We experimentally evaluate it and show that our network is more robust than the analytical method. Our method achieves better accuracy than existing 6D pose estimation methods. We also show the inclusion of synthetic data into the training process is beneficial. We experimentally verify that cases of successful pose approximations done by our network can be further refined in post-processing with iterative closest point (ICP), substantially increasing the portion of data with close-to-zero final error. We present this work as a position paper. We nevertheless feel that the preliminary results presented in this paper show promise, and we intend to continue this research by collecting a larger dataset and performing a more thorough evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Finding the 6D pose of an object is one of the classical computer vision problems tackled using various methods over the years. Existing algorithms for images and point clouds categorize into two main groups, analytical algorithms <ref type="bibr" target="#b19">(Stein and Medioni, 1992;</ref><ref type="bibr" target="#b13">Katsoulas, 2003b)</ref>, and data-driven algorithms. Data-driven algorithms can be further split into feature-based methods <ref type="bibr" target="#b20">(Vidal et al., 2018;</ref><ref type="bibr" target="#b2">Drost et al., 2010)</ref> and Deep Neural Networks (DNN)-based methods <ref type="bibr" target="#b15">(Park et al., 2019;</ref><ref type="bibr" target="#b1">Bukschat and Vetter, 2020)</ref>.</p><p>On the one hand, the feature-based methods are optimized using only the 3D object model, as they match pairs of points between the model and the captured scene. DNN-based methods, on the other hand, are trained on large sets of actual 3D scenes to generalize the solution. Moreover, a hybrid method can be composed of a sequence of data-driven steps and the final analytical step, with the ICP-like methods being the widely-used analytical post-processing step <ref type="bibr" target="#b0">(Besl and McKay, 1992;</ref><ref type="bibr" target="#b23">Xiang et al., 2017)</ref>. BOP challenge is trying to capture the state-of-the-art in this area, comparing traditional and data-driven methods on benchmark datasets .</p><p>Even though the problem of finding 3D translation and 3D rotation of rigid objects is very general, it is nevertheless dependant on the input data. Most of the widely adopted datasets consist of RGBD images of textured objects with complex geometries from a single device with known internal camera parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Analytical and Feature-based Methods</head><p>A traditional approach of registering objects has been detecting the local descriptors combined into shapebased primitives and searching for their corresponding pairs on 3D CAD models. The simplest case is Hough transform applied to detect lines <ref type="bibr" target="#b13">(Katsoulas, 2003b)</ref>. The efforts to enhance the algorithm to re-duce the number of possible detections resulted in specifying that lines have to be orthogonal to represent the shape borders <ref type="bibr" target="#b12">(Katsoulas, 2003a)</ref>. Similar to Hough transform, the RANSAC algorithm extracts the geometric description of the object by fitting the corresponding shape primitives into the 3D data. The non-deterministic algorithm is used in a sequence of standard steps. <ref type="bibr" target="#b4">(Guo et al., 2020)</ref> enhance the algorithm by using shape primitives to approximate the objects. <ref type="bibr" target="#b21">(Vock et al., 2019)</ref> propose to reduce 3D points into point pair features (PPF). However, RANSAC usually ends with many false positives (e.g., floor points); therefore, an ICP is usually required for fine-tuning. PPFs are widely used in literature to estimate object points in point cloud or RGBD data <ref type="bibr" target="#b2">(Drost et al., 2010;</ref><ref type="bibr" target="#b20">Vidal et al., 2018;</ref><ref type="bibr" target="#b3">Guo et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Neural Network-based Methods</head><p>Some methods estimate 6D poses from a single RGB image either directly by modifying an existing 2D object detection framework <ref type="bibr" target="#b1">(Bukschat and Vetter, 2020)</ref> or by using a neural network to obtain 2D-3D correspondences further used in a PnP solver to obtain the final pose <ref type="bibr" target="#b15">(Park et al., 2019;</ref><ref type="bibr" target="#b24">Zakharov et al., 2019)</ref>. In contrast to RGB data, the scanners utilized in our work output only texture from a grayscale cameranot color, limiting the application of related papers even further. RGB with depth information is also commonly used as input for deep learning base pose estimation. Several methods <ref type="bibr" target="#b14">(Mitash et al., 2018;</ref><ref type="bibr" target="#b11">Hosseini Jafari et al., 2019)</ref> use deep learning models to output hypotheses which are then processed in a hypothesis validation pipeline to obtain the final poses. Other indirect methods use deep learning networks to output keypoints <ref type="bibr" target="#b6">(He et al., 2020)</ref> or object fragments  which are then used in a PnP solver to obtain the final poses.</p><p>Other deep learning approaches apply neural networks directly to compute the 6D pose. DenseFusion <ref type="bibr" target="#b22">(Wang et al., 2020)</ref> uses RGB information to obtain segmentation masks of objects. These are used to combine depth and RGB data to generate perpixel embeddings, which are then used to estimate object poses in a voting scheme. An improved version of the algorithm called MaskedFusion <ref type="bibr" target="#b16">(Pereira and Alexandre, 2020)</ref> improves accuracy by masking non-relevant data.</p><p>These approaches are trained for specific objects and require their 3D models to be available during training. We aim to be able to estimate 6D poses of arbitrary bin-shaped objects. The mentioned methods are thus not easily transferable to our scenario. Moreover, the methods are usually trained for cameras with specific internal parameters, a constraint we aim to avoid in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pose Parameterization</head><p>The pose of a rigid object can be described with a pair of a rotation matrix R ? SO(3) and a translation vector t ? R 3 . The translation vector can usually be represented directly as an output of a neural network and used in a loss function since the space R 3 has a direct continuous representation. On the other hand, there are no continuous representations of SO(3), making it difficult for neural networks to learn such representations <ref type="bibr" target="#b25">(Zhou et al., 2019)</ref>.</p><p>Rotational matrices only have 3 degrees of freedom while having 9 elements. Constraining the elements directly during the training process is impractical, so an orthogonalization procedure must be utilized <ref type="bibr" target="#b25">(Zhou et al., 2019)</ref>. Rotation can also be represented using different equivalent parameterizations such as quaternions <ref type="bibr" target="#b23">(Xiang et al., 2017;</ref><ref type="bibr" target="#b22">Wang et al., 2020;</ref><ref type="bibr" target="#b16">Pereira and Alexandre, 2020)</ref> or axis-angle vectors <ref type="bibr" target="#b1">(Bukschat and Vetter, 2020)</ref>.</p><p>Symmetric objects pose a specific problem for rotation representation. Depending on the type of symmetry, multiple different rotation parameterizations can be valid for the same pose. This might introduce problems as some loss functions can then have undesirable multiple global minima. Some approaches mitigate these issues for some types of symmetries by using losses based on distances of sampled points on object models <ref type="bibr" target="#b23">(Xiang et al., 2017;</ref><ref type="bibr" target="#b22">Wang et al., 2020;</ref><ref type="bibr" target="#b16">Pereira and Alexandre, 2020)</ref> A different approach <ref type="bibr" target="#b18">(Pitteri et al., 2019)</ref> proposes mapping all representations onto a single canonical representation used during training. Some methods avoid these issues altogether by not directly outputting the object pose but calculating it indirectly from keypoints <ref type="bibr" target="#b6">(He et al., 2020)</ref> or object fragments .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>We have collected a new dataset consisting of both real captures (scans) from Photoneo PhoXi structured light scanner devices <ref type="bibr" target="#b17">(Photoneo, 2017)</ref> annotated by hand and synthetic samples produced by our generator. See <ref type="figure" target="#fig_0">Figure 1</ref> for an example of both real and synthetic 3D scanner captures of scenes composed of mechanical parts in a bin from our dataset.</p><p>In comparison with existing datasets, some notable differences include:</p><p>? most of the captured bins are texture-less, made from uniform, single-colored materials,</p><p>? all bins are of cuboid shape with different proportions. Compared to objects with complex geometry, bins consist of flat faces with edges, which are not guaranteed to be seen in the capture due to occlusion. Surface models of these bins are not provided, just their approximate bounding boxes,</p><p>? PhoXi scanner provides high-resolution 3D geometry data, but no RGB data, with a rough and noisy gray-scale intensity image being the closest equivalent,</p><p>? captures come from different devices with various intrinsic camera parameters. We aim to work directly on 3D point clouds, which contain these parameters implicitly as opposed to RGBD images.</p><p>The original scans contain various parameters, such as gray-scale intensities and normals. We rely only on 2D single-view maps of 3D coordinates in 2064 ? 1544 resolution in our proposed approaches. We use 80% as the training data, and the remaining 20% (every fifth sample) plus a unique set of independently captured 49 samples (including 10 synthetic samples) as a test set. Due to its currently limited size, we recommend cross-validation instead of an explicit train-validation split. We plan to add more samples into the dataset, as we will further enhance our methods in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BIN POSE ESTIMATION IN 3D POINT CLOUD</head><p>The bin pose estimation is a computation process of estimating a transformation matrix that maps coordinates of a bin-space into a scanner-space. As outlined in the previous sections, the specific task of bin pose estimation differs in many key aspects from the general task of 6D pose estimation. Therefore, we have decided to propose also two methods for this task. The first method is an analytical heuristic we have developed, and the second is a CNN-based pose estimation method. We deliberately designed the methods to be conceptually simple to provide solid baselines without bells and whistles. The following subsections describe the proposed methods. Evaluation and comparison of results for a set of experiments are in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analytical Edge-based Fitting</head><p>An analytical algorithm for pose estimation is composed of a set of steps performed sequentially in the pipeline. This four-step method assumes that the top edges of the bin are closer to the camera than background objects, and at least a part of every top edge can be seen. A plane is fitted into the bin-cuts, and wall-cuts not corresponding to this plane are discarded as outliers. The remaining wall-cuts are assigned to four bin walls according to corners fitted into horizontal and vertical bin-cuts. Finally, the lines are fitted into categorized wall-cuts, which define the bin basis.</p><p>Initially, the horizontal and vertical scan-lines are defined in scan-space. Each scan-line is divided into intervals, where scan-line interval going through the whole bin is called bin-cut. Specifically, each bin-cut is composed of two wall-cuts and one interval for the floor (representing the ground of the bin).</p><p>Next, minimum depth values in camera-space in the intervals are detected, and vectors describing edge-to-edge direction are computed. The set of such vectors is computed in both directions, horizontally and vertically (see <ref type="figure" target="#fig_1">Figure 2</ref>, left).</p><p>Moreover, a mode vector direction is computed in both horizontal and vertical directions. Those mode directions are used to compute the cross product of these directions to compute the normal defining the top of the bin. At the end of the step, the wall-cuts are filtered according to the calculated plane.</p><p>Consequently, a corner detection is performed on the filtered data. Each corner is detected as a bincut endpoint, where the change of direction between neighboring bin-cut endpoints is the highest; such detection is performed in every direction, and all four corners are detected (see <ref type="figure" target="#fig_1">Figure 2, right)</ref>.</p><p>Finally, the set of detected corners categorizes wall-cuts into four categories of the bin walls. Lines are fitted into filtered wall-cuts, and the bin-space is defined using the computed plane normal and fitted lines, which can be used for the bin-space definition and calculation of the final bin-space to camera-space transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CNN-based Pose Estimation</head><p>The analytical method may fail when bin edges or corners are occluded or outside of the scanner view. Such instances may frequently occur in industrial applications when human or robotic operators manipulate bins or contain items that cover the bin edges.</p><p>To overcome these issues, we propose a datadriven approach using a convolutional neural network. We propose a simple network that can reliably estimate the pose up to a reasonable level of accuracy. This estimate provided by the network is then refined using an ICP algorithm to obtain the final bin pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Parameterization of the bin pose</head><p>The pose of the bin can be parameterized using a rotation matrix R ? SO(3) and a translation vector t ? R 3 . We represent the translation vector directly. To represent rotation, we opt to use a strategy similar to <ref type="bibr" target="#b25">(Zhou et al., 2019)</ref> and represent the rotation by using two vectors from R 3 which can be used to determine the rotation matrix uniquely except for degenerate cases discussed later. The two vectors represent the orientation of the z and y axes of the bin in the camera coordinates. We denote these vectors as v z and v y , respectively.</p><p>To obtain the rotation matrix R from the vectors v z and v y , we employ the Gram-Schmidt orthogonalization process to calculate the columns of the actual rotation matrix they represent. During the procedure, we perform the following calculations:</p><formula xml:id="formula_0">u z = v z v z ,<label>(1)</label></formula><formula xml:id="formula_1">w y = v y ? v y | u z u z ,<label>(2)</label></formula><formula xml:id="formula_2">u y = w z w z ,<label>(3)</label></formula><formula xml:id="formula_3">u x = u y ? u z .<label>(4)</label></formula><p>The vectors u x , u y , u z form an orthonormal basis of R 3 . We can then construct a matrix ( u x , u y , u z ) which is a valid rotation matrix. The fact that the matrix represents a proper rotation (e.g. det(R) = 1) is enforced by equation <ref type="formula" target="#formula_3">(4)</ref>.</p><p>Using this procedure, any two vectors u z and u y can yield a valid rotation matrix provided that they are linearly independent. We found this limitation to not be of concern in practice.</p><p>Under this parameterization, any rotation matrix can be parameterized by many pairs of such vectors, and it is thus not unique in this regard. However, this is not an issue as we use a loss function which only depends on orientations of u z and u y , which are unique. To obtain a single pair of valid vectors u z and u y , which would yield a given matrix R, we can use the third and second columns of the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Bin Symmetry</head><p>We aim to detect bins of rectangular shapes. Rectangular bins are symmetric in a 180-degree rotation around an axis parallel to the bin-base normal going through the center of the base. Therefore, there are always two valid rotation matrices for each possible bin pose, which introduces issues during training as the network is forced to learn only one correct output of two possible outputs for a similar input, resulting in the network's inability to converge.</p><p>To remedy this issue we employ a simple strategy. The two possible rotations R 1 and R 2 are related by a symmetry rotation (5) such that R 1 = R s R 2 , where</p><formula xml:id="formula_4">R s = ? ? ?1 0 0 0 ?1 0 0 0 1 ? ? .<label>(5)</label></formula><p>Therefore, the only differences between the matrices are the signs in the first two columns, which allows us always to choose one of the matrices based on the sign of the matrix elements. We always select the matrix which has a positive element in the first row and second column. If this element is zero, we use the sign of the next element below. If the value is zero again, we use the sign of the last row and second column, which has to be 1 or -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Network Architecture</head><p>In our experiments we use a standard ResNet backbone <ref type="bibr" target="#b5">(He et al., 2016)</ref> for feature extraction. We apply global average pooling on the feature maps and feed the resulting features into three separate branch-heads to output the three vectors v z , v y and t. Each head comprises two fully-connected layers, with ReLU activations used in rotational heads and Leaky ReLU activations used in the translational branch. The whole network architecture, along with output postprocessing, is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Loss Function</head><p>For a given ground truth pose defined by R and? t we first check whether to transform the rotation matrix using R s as described in subsection 4.2.2. We extract the vectors u z and u y as the third and second columns of the rotation matrix. We then train the network, which outputs three vectors u z , u y , t using a joint loss function:</p><formula xml:id="formula_5">L = L r ( u z , v z ) + L r ( u y , v y ) + ?L L1 (? t, t),<label>(6)</label></formula><p>where L L1 is the standard L1 loss, ? is a weight hyperparameter and L r is the angle between two vectors in radians:</p><formula xml:id="formula_6">L r ( u, v) = acos u| v u v + ? ,<label>(7)</label></formula><p>with ? added to prevent undefined loss for output vectors with small norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION AND FINAL EXPERIMENTS</head><p>We evaluate the analytical method proposed in Section 4.1 and the neural network described in Section 4.2 using the dataset described in Section 3. We also show the results after refinement of the network output with ICP and provide an experimental comparison of our method to existing approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Metrics</head><p>Since we do not have 3D surface reconstruction of every bin in our dataset, we rely on model-independent pose error functions, i.e. comparing just the ground truthP = (R,? t ) and estimated P = (R, t ) transformation matrices. All our ground-truth rotation matrices consider the same orientation of the cuboid bin with the longer dimension along the x-axis, therefore we can use the strategy from subsection 4.2.2 to obtain symmetriesR 1 ,R 2 and minimize the metrics. We plan to complete the dataset with model reconstructions in the future. This will allow the calculation of metrics like e ADI , e VSD , e MSSD allowing for evaluation of the actual surface alignment <ref type="bibr" target="#b7">(Hinterstoi?er et al., 2012)</ref>. Evaluating the translation t is straight-forward using the euclidean distance e TE (? t, t ) = t ?? t 2 . For comparison of rotation, we use the angular distance e RE (R, R), which is the angle between rotational axis in angle-axis representation and can be directly computed from the rotation matrices as:</p><formula xml:id="formula_7">e RE (R, R) = min R ?{R 1 ,R 2 } arcos Tr(R R ?1 ) ? 1 2 ,<label>(8)</label></formula><p>where Tr is the matrix trace operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Network Results</head><p>We have experimented with different configurations of the proposed baseline network 2 , see <ref type="table">Table 1</ref> for results. Apart from the backbone, we tried two different input resolutions, half and quarter of the raw scan, which resulted in resolutions 1032 ? 772 and 516 ? 386, respectively. ResNet18 with half-resolution of the input scan has the worst performance, probably due to the small receptive field of the network. Interestingly, ResNet34 with quarter-resolution outperformed half-resolution. Additional sub-sampling probably acted as a noise-suppression. Additionally, we have trained the best performing configuration on a subset of the dataset without the 2 https://github.com/gajdosech2/bin-detect synthetic samples. Naturally, it achieved the worst test error since this set also contains synthetic scans, which were not encountered during training. Surprisingly, it also has higher errors e TE = 7.656, e RE = 0.559 on a subset of the test data with real samples only. Configuration trained on both real and synthetic samples achieves e TE = 6.108 and e RE = 0.529 on such subset. This would suggest that the synthetic data helps the model generalize on real scans, despite the evident gap between real and synthetic samples. Apart from average values of metrics e RE , e TE , the <ref type="table">Table 1</ref> also shows average losses L z r = L r ( u z , v z ) and L y r = L r ( u z , v z ) over the validation set. The loss function has, in this case, useful interpretation even as an evaluation metric. L z r represents the error in the predicted normal of the bin's bottom face, with L y r denoting the rotation around this axis.</p><p>A qualitative sample of the hybrid two-step approach, where the data-driven method is refined with post-alignment using ICP, can be seen in <ref type="figure" target="#fig_3">Figure 4</ref>. This refinement improved the results (both e TE and e RE ) in 91 samples out of the 218 in validation + test set. In general, it improves the pose estimation if the bin model has exact size and walls are visible. However, as mentioned in Section 3, the dataset currently does not contain complete surface reconstructions of the bins, just their approximate bounding boxes. <ref type="figure" target="#fig_4">Figure 5</ref> shows the comparison between the baseline network, its results after ICP refinement, the same version trained over real data only, and our analytical method. As can be seen, the analytical method achieves reasonable error for approximately 40% samples. The remaining samples had either high errors or the method failed to estimate in 47% cases, which was treated as an infinite error. The ICP refinement achieved almost zero error in a few cases. However, samples with non-corresponding points aligned produced higher errors which can be improved by limiting the usage of ICP only for confident cases, where the number of paired-points is higher than some threshold. This would mitigate the negative effect in a few cases, lowering the average error. The hybrid method with ICP refinement lowers the minimum error of the network, matching the analytical approach while also retaining robustness. However, in some cases, the ICP fails to improve the bin pose, resulting in slightly increased overall maximum error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with existing methods</head><p>Despite the uniqueness of our data, we have trained and qualitatively evaluated existing state of the art models: DPOD <ref type="bibr" target="#b24">(Zakharov et al., 2019)</ref>, DenseFusion <ref type="bibr" target="#b22">(Wang et al., 2020)</ref>, MaskedFusion <ref type="bibr" target="#b16">(Pereira and Alexandre, 2020)</ref> and EfficientPose <ref type="bibr" target="#b1">(Bukschat and Vetter, 2020)</ref>. We performed the evaluation only on a subset of our dataset (120 samples) with a single bin model, for which we have made the required surface reconstruction as the compared methods require such data. See <ref type="figure" target="#fig_5">Figure 6</ref> for qualitative comparison and <ref type="table" target="#tab_2">Table 2</ref> for quantitative results over test set of 14 samples. We also show the performance of our proposed baseline model. The scope of this experiment is limited, and further evaluation is necessary to draw any strong conclusions. However, this preliminary experiment shows that our method can outperform the existing ones while being conceptually simpler and not requiring a model of the detected bin during training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we have introduced a task of bin pose estimation, which we identified as an essential component in many vision-based automation systems in the industry. We have collected a dataset of highquality 3D scans of various bins in different environments using scanners with various parameters. In our future work, we aim to improve the dataset by collecting more data to enable a more thorough evaluation of bin pose estimation methods. We hope that such data will be useful for further research in this area. We also propose two baseline methods for 6D bin pose estimation. The evaluation results suggest that the bin poses can be estimated reliably with a simple convolutional neural network. In many cases, the resulting poses can be further refined using ICP to improve the accuracy of poses. We see the potential for further research in this area, especially regarding the effects of different types of bin pose parametrization on the network performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>In this work we present a novel dataset containing 520 real and 370 synthetic 3D scans of bins. (Left) Synthetic sample. (Right) Real scan annotated by hand. The ground truth transformations of bin 3D model into the scanner-space is demonstrated by purple mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(From left to right) the camera space is row-wise and column-wise segmented into similar depth intervals, from which horizontal and vertical bin-cuts are constructed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of the bin-pose estimation network. The structured point cloud is fed into a ResNet backbone. The resulting features are fed into three separate heads. Each head is composed of a few fully-connected layers. One of the heads outputs the resulting translation vector t. The other two heads output intermediate vectors v z and u z . Equations (1-4) are then used to obtain the columns of the resulting rotation matrix R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(Left) final improvement of data-driven method using ICP algorithm, (right) a fail-case of the ICP, where the bin was snapped to ground points of the bin, worsening the fit. Points of raw scan are in blue, prediction of the network in pink and ICP refinement in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Vertical axes show the fraction of the test samples with the error below the value of the metrics e RE , e TE on the horizontal axes. The analytical method achieves low error on a few samples but fails to predict pose for approximately half of the cases. Using synthetic data in training improves the overall performance of the neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison on single sample: Top Left: DPOD, Top Right: EfficientPose, Bottom Left: Dense Fusion, Bottom Right: Masked Fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results over small test set of 14 samples.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Efficientpose: An efficient, accurate and scalable end-to-end 6d multi object pose estimation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bukschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient center voting for object detection and 6d pose estimation in 3d point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5072" to="5084" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose estimation and adaptable grasp configuration with point cloud registration and geometry understanding for fruit grasp planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page">105818</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoi?er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<title level="m">BOP challenge 2020 on 6D object localization. European Conference on Computer Vision Workshops (EC-CVW)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ipose: Instanceaware 6d pose estimation of partly occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
		<editor>Jawahar, C. V., Li, H., Mori, G., and Schindler, K.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="477" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Localization of piled boxes by means of the hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katsoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust extraction of vertices in range images by constraining the hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katsoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IbPRIA</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving 6d pose estimation of objects in clutter via physics-aware monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3331" to="3338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7667" to="7676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Maskedfusion: Mask-based 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Phoxi 3d scanner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Photoneo</surname></persName>
		</author>
		<ptr target="https://www.photoneo.com/products/phoxi-scan-m/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On object symmetries and 6d pose estimation from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pitteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural indexing: efficient 3-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="145" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for 6d pose estimation of free-form rigid objects using point pair features on range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Llado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">2678</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast template matching and pose estimation in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dieckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ochmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
