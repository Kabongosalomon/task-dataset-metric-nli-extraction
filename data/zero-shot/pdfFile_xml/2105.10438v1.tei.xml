<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Fine-Grained Low-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
						</author>
						<title level="a" type="main">Compositional Fine-Grained Low-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Zero-Shot Learning</term>
					<term>Few-Shot Learning</term>
					<term>Fine-Grained Recognition</term>
					<term>Visual Attention</term>
					<term>Compositional Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a novel compositional generative model for zero-and few-shot learning to recognize fine-grained classes with a few or no training samples. Our key observation is that generating holistic features for fine-grained classes fails to capture small attribute differences between classes. Therefore, we propose a feature composition framework that learns to extract attribute features from training samples and combines them to construct fine-grained features for rare and unseen classes. Feature composition allows us to not only selectively compose features of every class from only relevant training samples, but also obtain diversity among composed features via changing samples used for the composition. In addition, instead of building holistic features for classes, we use our attribute features to form dense representations capable of capturing fine-grained attribute details of classes. We propose a training scheme that uses a discriminative model to construct features that are subsequently used to train the model itself. Therefore, we directly train the discriminative model on the composed features without learning a separate generative model. We conduct experiments on four popular datasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Fine-grained recognition, which is to classify categories that are visually very similar, is an important yet challenging task with a wide range of applications from fashion industry, e.g., recognition of different types of shoe or cloth <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, to face recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and environmental conservation, e.g., recognizing endangered species of birds or plants <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, training finegrained classification systems is challenging, as collecting sufficient training samples from every class requires costly annotations by domain experts to distinguish between similar classes, e.g., 'Parakeet Auklet' and 'Least Auklet' bird species or 'Sweatpant' and 'Jeggings', see <ref type="figure" target="#fig_5">Figure 5</ref>. Thus, training samples often follow a long-tail distribution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, where many classes have few or no training samples. In this work, we aim to generalize fine-grained recognition to novel classes with a few or no training samples via capturing and transferring fine-grained knowledge from seen to novel classes without overfitting on seen classes.</p><p>Although fine-grained classification has achieved remarkable performance by using feature pooling <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and discriminative region localization <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> techniques, these works cannot generalize to novel classes, as it requires sufficient training samples from every class, and cannot leverage auxiliary information such as class semantic vectors, which is fundamental for transferring knowledge to novel classes. With the need for costly training data, conventional fine-grained classification methods cannot scale to a large number of classes. However, fine-grained classes can often be described in terms of attributes that are common among classes. Thus, using these semantic descriptions to transfer knowledge among classes can significantly reduce the amount of training samples.</p><p>Zero/few-shot classification, on the other hand, leverages auxiliary information in the form of class semantic  Notice that these classes are different only in a few attributes.</p><p>descriptions or few training samples to generalize to classes with no or insufficient samples, respectively. Recent zeroshot works rely on generative models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> to synthesize features of novel classes to augment the training set. However, methods based on Generative Adversarial Networks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref> suffer from low diversity in generated features. Likelihood-based methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> promote diversity among generated features, but their generated features are often non-discriminative. Moreover, these generative methods are not effective in the few-shot learning setting <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, where a few samples of novel classes are available. This is because they synthesize novel class features based on only class semantic information, which cannot leverage additional visual information from samples in novel classes to enhance feature generation. While <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> are designed to generate features conditioned on visual information, these works ignore class semantic descriptions and cannot synthesize features for classes without any training samples. Although <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> have studied generative models for both zero-shot and few-shot learning, they mainly focus on class semantic information, ignoring few visual samples, or require a large number of unlabeled samples from novel classes in a transductive setting.</p><p>Leveraging the remarkable performance of Convolutional Neural Networks <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, most zero/few-shot works extract image features by pooling local information from image regions into holistic representations <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Although holistic features encode discriminative information among classes, they are not trained to explicitly capture differences among attributes of different classes. Therefore, holistic features cannot describe finegrained details of novel classes.</p><p>It has been shown that fine-grained classes exhibit a compositional structure <ref type="bibr" target="#b37">[38]</ref> in which we only need to recognize basic attributes such as color, shape, or material to recognize a large number of classes expressible in terms of these attributes. Few works have explored learning compositional structures. <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> propose to reconstruct image features using attribute features, which either require at least a few samples per class, hence, cannot work in the zero-shot setting, or only work with class semantic vectors, hence, cannot work in the few-shot setting. <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> use neural networks to combine classifiers of simple attribute to recognize novel classes without training samples. However, these works learn with holistic features which fail to capture attribute details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Paper Contributions</head><p>We develop a novel framework that addresses the limitations of aforementioned methods. We propose dense attention and attribute embedding mechanisms capturing all attribute information to distinguish fine-grained differences among classes, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. To improve novel class predictions, instead of generating holistic features, as in <ref type="figure" target="#fig_3">Figure 3</ref> (left), we learn to extract attribute features from seen classes and recombine them to effectively construct all attribute features of novel classes, see <ref type="figure" target="#fig_3">Figure 3</ref> (right). Our method has several advantages over the state of the art:</p><p>-Instead of using holistic features, which lack fine-grained details, we learn to extract and generate dense features consisting of attribute features for hundreds of attributes.</p><p>-Our framework selectively composes features of novel classes from semantically related training samples. It allows specifying different sample sets used for composition that leads to the diversity of composed features. Thus, we control the composition process by constraining the samples that are used to build attribute features of novel classes.</p><p>-Instead of using a generative model to build features and then train a discriminative model, we use a discriminative model to compose features of novel classes in order to train itself. This makes the learning process efficient by removing the need for learning additional generative models.</p><p>-Our framework can be naturally adapted to take into account both semantic and visual information of novel classes to improve few-shot performances.</p><p>Paper Organization. In Section 2, we discuss about the pros and cons of related works. Then, we provide an overview on our problem setting as well as our overall approach in Section 3. We propose the dense attention mechanism for extracting fine-grained features for attributes in Section 4. Given these fine-grained features, we introduce an efficient and effective feature composition framework for zero-shot learning and extend it to few-shot learning in Section 5 and 6, respectively. Finally, we demonstrate the effectiveness of our methods on four popular datasets in Section 7 and conclude the paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Fine-grained Recognition. The goal of fine-grained recognition is to capture the small but discriminative features across different classes. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b43">[44]</ref> captures the interaction between discriminative feature maps through pooling technique while <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> propose better ways to learn global image features that capture fine-grained details. On the other hand, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b47">[48]</ref> localize discriminative parts of images through part-based supervision. To avoid the localization annotation of discriminative parts, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> localize them in a weakly supervised setting. Despite tremendous success in the fully supervised setting, these works cannot generalize to zero-shot learning, where only high-level attribute descriptions are given for novel classes, or few-shot learning with limited samples in novel classes.</p><p>Zero-shot Learning. Early works on zero-shot learning focus on learning joint embedding spaces for visual features and class semantics <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. On the other hand, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> transfer knowledge from seen to novel classes via knowledge graphs which encode richer structures than class semantics. However, most works on zero-shot learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> rely on holistic image features, which cannot capture local discriminative information from attributes. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b57">[58]</ref> manage to localize distinct visual parts for zero-shot learning despite using costly bounding-box annotations. <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref> employ attention mechanism to localize discriminative regions in a weakly-supervised setting, however, they cannot capture every attribute due to the limited number of attention models. Moreover, the predictions of these methods are often biased towards seen classes due to the lack of training samples for novel classes <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. To overcome this issue, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref> propose to reduce the probability of seen classes on samples that are out of training distribution, while <ref type="bibr" target="#b66">[67]</ref> directly calibrates predictions to favor novel classes by adding a fixed margin to prediction scores. Recent work <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref> propose calibration losses that consider adjustment of novel class probabilities as training objective functions. However, these works only regularize predictions to prevent seen class bias without transferring knowledge to novel classes.</p><p>Few-shot Learning. Related to zero-shot learning, few-shot learning aims at recognizing novel classes with few training samples. To prevent overfitting on these training samples, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b69">[70]</ref> propose to learn suitable metric spaces that generalize to novel classes. On the other hand, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref> frame few-shot learning as finding an optimal model parameters which can quickly adapt to novel classes via fine-tuning. Recently, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref> directly predict weights of novel class classifiers while <ref type="bibr" target="#b74">[75]</ref> combines both visual and semantic information to recognize novel classes. However, most works cannot localize attributes in images for finegrained recognition. Although <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref> are designed for fine-grained recognition, they either requires strong supervision of attribute locations or cannot learn in zero-shot setting as they ignore semantic information.</p><p>Feature Generation. To effectively transfer knowledge to novel classes, recent zero/few-shot learning methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b62">[63]</ref> use generative models to augment a training set with synthesized features of novel classes. In zero-shot setting, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b77">[78]</ref> employ Generative Adversarial Networks (GAN) whose generated features often lack diversity due to mode collapse <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref> and lack generalization ability due to memorization of training data <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>. On the other hand, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> propose to use Variational Autoencoders (VAE) that optimize the likelihood of every training sample to enforce diversity. However, these methods suffer from posterior collapse <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, which results in generating generic non-discriminative features. To address these issues, <ref type="bibr" target="#b17">[18]</ref> combines VAE and GAN, while <ref type="bibr" target="#b19">[20]</ref> directly learns a feature generator without any encoder or discriminator. However, these improvements are only effective for seen classes with sufficient training samples. Although cycle consistency losses <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b86">[87]</ref> directly regulate the generated features of novel classes, they often collapse these features together to align them with semantic vectors of their classes, which results in lack of visual diversity among features. For few-shot learning, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> propose to generate new poses or viewpoints of novel class samples to capture intraclass variation. However, most zero/few-shot works can only synthesize holistic features, which cannot describe finegrained attributes needed for recognizing novel classes.</p><p>Compositional Learning. Decomposing concepts into common components is a natural and simple technique for knowledge sharing <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>. <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b91">[92]</ref> learn compositional representations that generalize to classes with few samples. On the other hand, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref> learn to combine samples for data augmentation. However, these works cannot generalize to novel classes not encountered during training as they require training samples for every class. While <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref> combine attribute classifiers to recognize novel combination of attributes, they build upon holistic features, which cannot capture fine-grained attribute details. Recently, by examining various zero-shot methods, <ref type="bibr" target="#b37">[38]</ref> shows that zero-shot generalization requires capturing attribute information and their compositional structures.</p><p>This work is an extended version of our previous work <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b67">[68]</ref>. In particular, (a) we extend our framework to fewshot setting where both visual and semantic information is used for feature composition. (b) We provide further explanation on the formulation of our framework especially on dense attention mechanism. (c) We report experimental results on few-shot learning to show the effectiveness of our proposed method. (d) We provide additional quantitative and qualitative results to support our design choices on dense attention and attribute grounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>We assume that we have two disjoint sets of classes C s and C n , where C s denotes seen classes with sufficient training images, C n denotes novel classes with limited (less than m) or no training samples and C C s ? C n denotes the set of all classes. Let D s and D n be the set of training samples for seen and novel classes, respectively, and (I 1 , y 1 ), . . . , (I N , y N ) be N training samples, where I i denotes the i-th training image and y i ? C corresponds to its class. Specifically, zeroshot setting requires a model to recognize novel classes without training samples, |D n | = 0, while the few-shot setting assumes a small amount of training samples in novel classes, i.e., |D n | ? m |D s |. In order to identify novel classes in the zero-shot setting, similar to prior works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we assume access to class semantic vectors of all classes, {z c } c?Cs?Cn , at the training time. More specifically,</p><formula xml:id="formula_0">z c = [z c 1 , . . . , z c A ]</formula><p>, where z c a encodes the strength of attribute a in class c. We normalize each z c to have unit norm to prevent prediction bias toward classes with many attributes. To extract dense features from regions of attribute in images, we also use the attribute semantic vectors {v a } A a=1 as the average of GloVe representation <ref type="bibr" target="#b96">[97]</ref> of words in attribute names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-Grained Feature Generation for Novel Classes</head><p>We propose to transfer knowledge from seen to novel classes by synthesizing features of novel classes from features of seen classes in order to compensate for the lack of training samples. Specifically, our goal is to generate features for each attribute and use them to describe novel classes according to their semantic class descriptions, z. To achieve this, we develop a unified framework consisting of a dense attention mechanism and a compositional feature generation method, which extract attribute features and compose them to synthesize features of novel classes for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEARNING FINE-GRAINED FEATURES</head><p>We present a method to extract attribute features for finegrained recognition. For each attribute, we extract a spatial attention feature from the most relevant regions of an input image. These attribute features will be subsequently used to compose features of novel classes. In addition to learning the attention network for attributes, we also fine-tune the attribute semantic vectors to ground textual information into visual domain for more effective transferring knowledge from seen to novel fine-grained classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dense Attribute-Based Attention</head><p>The ability to learn visual models of attributes is crucial for transferring knowledge from seen to novel classes. Recent work either embed image features into the class semantic space <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> or generate image features from class semantic vectors <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b77">[78]</ref>. However, without localizing each attribute, they ignore discriminative visual features of fine-grained classes, obtaining holistic features that contain information from non-discriminative or irrelevant image regions.</p><p>As the first component of our method, we propose an attribute-based spatial attention model, where for each attribute, we localize the most relevant image regions to the attribute to extract an attribute feature from a given image. Recall that {v a } A a=1 is the set of attribute semantic vectors and {f r i } R r=1 = CNN ? (I i ) denotes the region features of the image i from a convolutional visual backbone parametrized by ?. For the a-th attribute, we define its attention weights of focusing on different regions of image i as,</p><formula xml:id="formula_1">?(f r i , v a ) exp(v T a W ? f r i ) r exp(v T a W ? f r i ) ,<label>(1)</label></formula><p>where W ? denotes a learnable matrix that measures the compatibility between each attribute semantic vector and the visual feature of each region. Using the set of attention weights {?(f r i , v a )} R r=1 , we compute the attribute feature for the a-th attribute as,</p><formula xml:id="formula_2">h a i R r=1 ?(f r i , v a )f r i . (2)</formula><p>Thus, h a i represents the visual feature of the image i that is relevant to the a-th attribute according to the attribute semantic vector v a . Notice that when an attribute is absent in the image, h a i captures the visual evidence used to reject the attribute in the image. For instance, the model could focus on 'back belly', and later assigns a negative score to it, to indicate the absence of 'white belly', as in <ref type="figure" target="#fig_5">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attribute Embedding</head><p>Given the set of attribute features {h a i } A a=1 for each training image i, our goal is to compute the class score of the image i belonging to a class c. During training, the class score would be optimized to be large for the ground-truth class c = y i and small for other classes c = y i . To do so, we define A attribute scores, where each score measures the strength of having each attribute in the image (recall A is the number of attributes). We fuse these scores using each class semantic vector to find the class score.</p><p>More specifically, we define the attribute score e(h a i ), which is the confidence of having the a-th attribute in the image i, by matching the attribute feature h a i with the attribute semantic vector v a ,</p><formula xml:id="formula_3">e(h a i ) v T a W e h a i .<label>(3)</label></formula><p>Here, W e is an embedding matrix that embeds the attribute feature h a i to the a-th attribute semantic space. In fact, when the attribute is visually present in an image, the associated image feature would be projected near its attribute semantic vector. We compute the class score as the sum of products between each attribute score, e(h a i ), and the strength of having the a-th attribute in the c-th class, z a c , as</p><formula xml:id="formula_4">s(H i , z c ) = A a=1 e(h a i ) ? z a c .<label>(4)</label></formula><p>We refer to H i h 1 i , . . . , h A i as the dense feature matrix, which is the collection of all attribute features for the image i. Therefore, when a class c that has attribute a (i.e., z a c &gt; 0) is present in the image i, we learn to maximize the corresponding attribute score e(h a i ). Loss Function. Finally, we train our model to minimize the cross-entropy loss between the predicted and ground-truth labels of training samples,</p><formula xml:id="formula_5">min W e,W ?,{va } A a=1 ? i log p(y i |H i , z yi ), p(y i |H i , z yi ) = exp(s(H i , z yi )) c exp(s(H i , z c )) ,<label>(5)</label></formula><p>where p(y i |H i , z yi ) is the prediction probability calculated by applying softmax normalization on the score s(H i , z yi   among all classes, we effectively allow transferring finegrained knowledge from seen to novel classes. In the experiments, through ablation studies, we show that fine-tuning the attribute semantic vectors results in significant improvement of the performance.</p><p>Although dense attention and attribute embedding are designed for fine-grained recognition, the entire model is only trained on seen class samples which leads to bias towards seen classes and fails to generalize to novel classes.</p><p>Self-Calibration Approach. To overcome this challenge, one approach would to design a calibration loss that allows to shift some of the prediction probabilities from seen to novel classes during training. More specifically, we can define</p><formula xml:id="formula_6">L cal ?? cal i log ? ? n?Cn p(n|H i , z n ) ? ? ,<label>(6)</label></formula><p>where ? cal ? 0 controls the influence of this loss during training. Thus, minimization of L cal in conjunction with the cross-entropy loss, promotes to put nonzero probability on the novel classes during training. Hence, at testing time, for an image from a novel class, the model can produce a (large) non-zero probability for the true novel class. Although this approach can mitigate seen class bias, it simply reduces seen class probabilities without any knowledge transfer between seen and novel classes. Therefore, in the next section, we propose a feature generation framework to synthesize features of novel classes, thus, effectively transferring knowledge from seen to novel classes. As we show in the experiments, our feature generation method significantly improves the performance compared to the self-calibration approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FINE-GRAINED FEATURE GENERATION FOR ZERO-SHOT LEARNING</head><p>In this section, we discuss our proposed method for generating dense features, containing all fine-grained features, of novel classes without training samples. Our framework first samples a set of candidate feature combinations from which we select the most probable combination having the largest prediction score p(n|H, z n ) for each novel class n. The proposed framework alternates between constructing features and updating a discriminative model p(y|H, z) by increasing its confidence on the features of novel classes. Algorithm 1 summarizes the steps of our method. We start by defining the compositional property of dense features and propose a framework that generates dense features for novel classes by leveraging this compositional property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dense Feature Generation</head><p>Since fine-grained classes are different in only a few attributes, we want to generate visual features encoding every attribute information. Thus, we propose to generate the dense feature matrix H, which contains all fine-grained attribute features/information, while discarding irrelevant background information. As such, we use the dense features from training images {H i } N i=1 as building blocks for our feature composition model.</p><p>Our goal is to generate dense features H for a class y with semantic vector z. Instead of directly learning and maximizing p(H|y, z), which is prone to mode/posterior collapse and cannot scale to a large number of attributes and dense features, our insight is to transform a discriminative model p(y|H, z), which is easier to train, into a generative model via application of the Bayes rule, argmax H p(H|y, z) = argmax H p(y|H, z)p(H|z), <ref type="bibr" target="#b6">(7)</ref> where the feature prior p(H|z) captures the data manifold by assigning high probability to regions containing attribute features and small or zero probability otherwise. Due to the high dimensions of features in H, estimating p(H|z) is intractable. To overcome this, we propose a compositional assumption on attribute features where only features h a i from various training samples can be combined into a valid For an novel class u with a semantic vector zn, we generate candidate combinations by sampling from p(H|zn) and use a discriminative model p(y|H, zn) to select the best combination in order to train itself.</p><p>H. This makes the solution-space more tractable since the number of combinations is finite, yet combinatorial. To enforce this assumption, let S ? D s be a subset of seen class sample indices from which we construct new features (this is the minibatch used during training). We denote by U(S) the set of all possible combinations of attribute features from samples in S, i.e.,</p><formula xml:id="formula_7">U(S) h 1 i1 , . . . , h A i A | i a ? S ?a ,<label>(8)</label></formula><p>where h a ia is the a attribute feature from sample i a used to compose H. We propose to limit the support of the feature prior to only feature combinations in U(S), i.e., p(H|z) = 0, H ? U(S).</p><p>Selecting features from U(S) is equivalent to combining attribute features across samples in S to describe novel classes, e.g., if S contains samples of a 'blue head, white belly bird' and a 'red bird', U(S) can describe a variety of birds such as a 'blue head, red belly bird' or a 'red head, white belly bird', see <ref type="figure" target="#fig_6">Figure 6</ref>. Thus, we rewrite <ref type="formula">(7)</ref> as searching for the best feature composition in U(S) via maximizing, argmax H p(H|y, z) ? argmax H?U (S) p(y|H, z)p(H|z).</p><p>Remark 2. Notice that we use class probability predictions as the criteria for selecting feature combinations. Thus, our framework is robust against missing attributes, where some attributes needed to describe a target class are absent from images in S, by resorting to the most probable feature combination in U(S).</p><p>Next, we discuss an efficient way to solve (10) via sampling. We propose to train the discriminative model directly on the composed features, leading to an efficient framework that does not need to learn a separate generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficient Feature Composition</head><p>Since optimizing (10) requires an expensive combinatorial search in U(S), we propose an efficient method to significantly reduce the search space by avoiding irrelevant combinations. To do so, we impose that H should only be composed from attribute features of semantically related samples in S with respect to a target novel class n. We define related samples as the ones whose semantic vectors best reconstruct the class semantic vector z n . We recover the set of related samples in S with respect to a novel class n, denoted by Q n (S), as</p><formula xml:id="formula_10">Q n (S) argmin B?S min ? z n ? i?S z i ? i 2 2 s. t. |B| ? k, ? i ? 0, ?i ? B,<label>(11)</label></formula><p>where B denotes any subset of S of size at most k, z i denotes the ground-truth class semantic vector of the sample i, ? i denotes the reconstruction weight, and k is the number of related samples to select. The nonnegative constraint on the combination weights ? i 's ensures that the semantic meaning of each sample does not change due to the reconstruction. We solve (11) using Nonnegative Orthogonal Matching Pursuit <ref type="bibr" target="#b97">[98]</ref>, which greedily adds samples into Q n (S) to decrease its loss until no further improvement can be made (see the supplementary materials for more details).</p><p>Given the set of related samples Q n (S), we construct a prior for dense features such that the more related a sample is to the target semantic vector, the more probable its attribute features will be used for composition,</p><formula xml:id="formula_11">p(H|z n ) A a=1 p(h a ia |z n ),<label>(12)</label></formula><p>where we assume the independence of attribute features given the semantic vector. Notice that attribute dependencies can still be captured via classifier predictions p(y|H, z) in (10) for feature generation. We propose to compute probability of using attribute features from training samples as,</p><formula xml:id="formula_12">p(h a ia |z n ) ? ? ? ? ? exp z ia zn ? i?Qn (S) exp z i zn ? , if i a ? Q n (S), 0, otherwise,<label>(13)</label></formula><p>where z ia is the ground-truth class semantic vector of sample i a used to compose attribute a, ? is a non-negative scalar that controls the probability of using attribute features from related samples. When ? 0, the prior would mostly include attribute features from the most related sample and when ? = 0, it would uniformly sample attribute features from all related samples. Notice that we measure the relatedness as the cosine similarity between the sample semantic z ia and the target semantic z. The prior also assigns zero probability to combinations of samples outside the related sample set Q n (S) to exclude these combinations. Indeed, the prior allows us to sample a set of candidate features to find the most probable feature, thus we avoid searching through all combinations in U(S). Specifically, we first samples a set of candidate combinations M n (S) from which we seek the combination that maximizes the product of an novel class probability and the prior, H n (S) argmax H?Mn(S) p(n|H, z n )p(H|z n ),</p><formula xml:id="formula_13">M n (S) {H|H ? p(H|z n )},<label>(14)</label></formula><p>where H n (S) is the most probable dense feature of class n and M n (S) is constructed by sampling b candidate combinations according to p(H|z n ) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function.</head><p>Having the composed features of novel classes {H n (S)} n?Cn , we train the discriminative model by increasing its prediction confidence on these features as novel classes while maintaining the confidence on seen class samples via the following cross-entropy loss,</p><formula xml:id="formula_14">min W e,{va} A a=1 E S ? 1 |S| i?S log p y i |H i , z yi ? 1 |C n | u?Cn log p n|H n (S), z n .<label>(15)</label></formula><p>Here, we transfer knowledge from seen to novel classes by recognizing novel combinations of features as novel classes. Thus, the discriminative model learns the existence of novel classes to avoid seen class bias. We alternate between composing features <ref type="bibr" target="#b13">(14)</ref> and minimizing the loss (15) on a random sample set S in each iteration until convergence. By randomizing the sample set S, we effectively ensure the diversity among composed features by enforcing them to be built from various sets of samples. For inference, we recognize a test image by finding the most probable class according to the discriminative model: c * = argmax c?Cs?Cn p(c|H, z c ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Compositional Feature Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FINE-GRAINED FEATURE GENERATION FOR FEW-SHOT LEARNING</head><p>In this section, we propose to extend our framework for few-shot learning where there are limited training samples of novel classes. Let H j and z j are the dense feature and semantic vector, respectively, of a training sample of novel class j ? D n . Our goal is to generate features for the novel class y j that takes into account this class training samples by conditioning the feature prior on both semantic and visual information. As such, we introduce a novel visual-semantic feature based on both H j and z j as,</p><formula xml:id="formula_15">h j = A a=1 h a j z a j ,<label>(16)</label></formula><p>whereh j is the visual-semantic feature of sample j. Here, we use the semantic information in z j to highlight the discriminative attribute features in H j , that represent the novel class y j . If attribute a is indicative for class y j , i.e., z a j &gt;&gt; 0, thenh j would mostly include this attribute feature h a j and would suppress other irrelevant attribute features with z a j = 0. We propose to synthesize features of few-shot classes by conditioning the generation process on their visual-semantic features as,</p><formula xml:id="formula_16">argmax H p(H|y j , z j ,h j ) ? argmax H?U (S) p(y j |H, z j )p(H|,h j ). (17)</formula><p>To efficiently solve the generation process, we employ a similar sampling technique as in the zero-shot setting which first samples candidates, M j (S), from the prior distribution and selects the best feature combination among them, H j (S) argmax H?Mj (S) p(y j |H, z j )p(H|h j ),</p><formula xml:id="formula_17">M j (S) {H|H ? p(H|h j )}.<label>(18)</label></formula><p>To compute the prior distribution, we identify a set of visually and semantically related samples Q j (S) which best reconstruct the visual-semantic feature of j,</p><formula xml:id="formula_18">Q j (S) argmin B?S min ? h j ? i?Bh i ? i 2 2 s. t. |B| ? k, ? i ? 0, ?i ? B.<label>(19)</label></formula><p>Given the related sample set, we simplify the prior via attribute-independence assumption p(H|h j ) A a=1 p(h a ia |h j ) similar to <ref type="bibr" target="#b11">(12)</ref>. We use the cosine similarity of visual-semantic features to express the probability of using sample i a to represent sample j,</p><formula xml:id="formula_19">p(h a ia |h j ) ? ? ? ? ? exp h iahj ? i?Q j (S) exp h ihj ? , if i a ? Q u (S). 0, otherwise.<label>(20)</label></formula><p>By using visual-semantic features, we effectively incorporate both visual and semantic information for feature generation while leveraging our efficient composition procedure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><formula xml:id="formula_20">E S,Sn ? 1 |S| i?S log p y i |H i , z yi ? 1 |S n |(1 + ?) j?Sn log p y j |H j , z j ? ? |S n |(1 + ?) j?Sn log p y j |H j (S), z j ,<label>(21)</label></formula><p>where S ? D s , S n ? D n are sets of samples from seen and novel classes, respectively, and ? ? 0 trades off the influences of learning from real or synthesized features in novel classes during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We demonstrate the effectiveness of our framework, referred to as Composer, on four popular datasets: DeepFashion <ref type="bibr" target="#b1">[2]</ref>, AWA2 <ref type="bibr" target="#b98">[99]</ref>, CUB <ref type="bibr" target="#b99">[100]</ref>, SUN <ref type="bibr" target="#b100">[101]</ref>. We first discuss the datasets, evaluation metrics, implementation details and baselines. We then present the zero-shot and generalized zero-shot performances in pre-trained and fine-tuned feature extractor settings. We show comparisons between compositional models and current generative models, effects of hyper-parameters, and ablation studies on dense attention as well as feature composition. Finally, we demonstrate the effectiveness of our framework on few-shot settings and the effect of learning from real and synthesized features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We conduct experiments on visual recognition datasets: DeepFashion <ref type="bibr" target="#b1">[2]</ref>, AWA2 <ref type="bibr" target="#b98">[99]</ref>, CUB <ref type="bibr" target="#b99">[100]</ref>, and SUN <ref type="bibr" target="#b100">[101]</ref> having different data statistics, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. DeepFashion <ref type="bibr" target="#b1">[2]</ref> contains images of fine-grained clothing categories with 36 seen classes and 10 novel classes. Due to attribute redundancy in DeepFashion, we only select top 300 most discriminative attributes by choosing attributes having low class entropy. AWA2 <ref type="bibr" target="#b98">[99]</ref> consists of animal images with 40 seen classes and 10 novel classes described by 85 attributes. CUB <ref type="bibr" target="#b99">[100]</ref> is a fine-grained bird dataset with 47 images per class for 150 seen classes and 50 novel classes. Each class is carefully annotated with 312 attributes which can be grounded in images. Finally, SUN <ref type="bibr" target="#b100">[101]</ref> is a visual scene dataset with 645 seen classes and 72 novel classes with only 16 images per class. We follow the data splits of <ref type="bibr" target="#b98">[99]</ref> for AWA2, CUB, and SUN. On DeepFashion, we partition the categories into 36 seen and 10 novel classes, in order to have a sufficient number of training samples in novel classes. We use the original training/testing split of the dataset to further divide seen classes into training and testing sets.</p><p>Evaluation Metrics: Similar to <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b98">[99]</ref>, we measure the top-1 accuracy on two settings: i) zero/few-shot learning (n ? n) where testing images are from novel classes thus the model only need to distinguish among novel classes and ii) generalized zero/few-shot learning where testing images comes from both seen and novel classes. In the latter setting, we measure the accuracy when recognizing seen classes (a ? s) and novel classes (a ? n). We compute the harmonic mean (H) between seen and novel class accuracy to measure the trade-off between these performances. In few-shot learning, we report the mean performances over 10 runs with different random seeds following <ref type="bibr" target="#b18">[19]</ref>. learns a compatibility function between visual and semantic information by accounting for the semantic similarity among classes. LFGAA+SA <ref type="bibr" target="#b103">[104]</ref> proposes a dynamic compatibility function that adapts to attributes appearing in an image. To show the effectiveness of feature composition, we report performances of our Self-Calibration approach, which uses the calibration loss (6) with ? cal = 0.1, which achieves the best performances on all datasets. On Deep-Fashion, we run each baseline using their released codes with their default settings. On the remaining datasets, we use the performances reported in their papers to ensure their best performances. For few-shot learning, we compare with two discriminative methods: WeightImprint <ref type="bibr" target="#b72">[73]</ref>, which predicts classifiers for novel classes from few samples, AM3 <ref type="bibr" target="#b74">[75]</ref>, which leverages both semantic and visual information to recognize novel classes. Moreover, we report CADA as the state-of-the-art generative model for few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Implementation Details: Following <ref type="bibr" target="#b35">[36]</ref>, we resize images to 224 ? 224 and extract features using the ResNet101 backbone <ref type="bibr" target="#b34">[35]</ref> for our method. Our setting is comparable with the above baselines except for SMA using VGG19 and LFGAA combining VGG19, GoogleNet, and ResNet101. We use the feature map of the last convolutional layer whose size is 7 ? 7 ? 2048 and treat it as features from 7 ? 7 = 49 regions. We implement our framework in PyTorch and optimize it using RMSprop <ref type="bibr" target="#b105">[106]</ref> with the default setting, learning rate of 0.0001 and batch size of 50 having an equal number of samples per class. We train the dense attention model on seen classes and use it to compose dense features for at most N att = 2000 and N comp = 4000 iterations, respectively, on a NVIDIA V100 GPU. To prevent seen class bias in zero-shot setting, we add a margin of 1 to novel class scores and ?1 to seen class scores, which reduces the dominance of seen classes similar to <ref type="bibr" target="#b66">[67]</ref>. We experiment in two settings: i) using pre-trained ImageNet features (pre-trained setting) and ii) fine-tuning the ResNet backbone on each dataset (fine-tuned setting). To measure the robustness of our method, we fix the hyperparameters Method DFashion (5691 images/class) AWA2 (588 images/class) CUB (47 images/class) SUN (16 images/class) n ? n a ? s a ? n H n ? n a ? s a ? n H n ? n a ? s a ? n H n ? n a ? s a ? n H   at ? = 5, k = 5, b = 50 (? = 10, k = 10, b = 50) for the pretrained (fine-tuned) setting on all datasets. For few-shot learning, we set ? = 0.5 and use a pre-trained ResNet101 backbone in all experiments for fair comparison with <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Zero-Shot Experiments</head><p>Zero-Shot Learning: <ref type="table" target="#tab_6">Table 2</ref> shows the zero-shot accuracy (n ? n) across different datasets. In the pre-trained setting, our method significantly outperforms other methods by at least 2.3% and 3.1% on DeepFashion and CUB, respectively while having comparable performance with the state-of-theart method on AWA2. In the fine-tuned setting, we improve at least 3.2%, 5.1%, and 2.2% on DeepFashion, AWA2, and CUB respectively. Generalized Zero-Shot Learning: <ref type="table" target="#tab_6">Table 2</ref> also shows generalized zero-shot performances. We observe that methods using dense features, Self-Calibration and ours, surpass other methods on the majority of datasets. Specifically, we improve the harmonic mean by at least 5.0%, 1.7%, 1.8% on DeepFashion, AWA2, CUB, respectively, in pre-trained setting and at least 4.6%, 2.0% on DeepFashion, AWA2, respectively, in fine-tuned setting. Our method achieves high accuracy of seen classes and significantly improves accuracy of novel classes by 7.3%,1.8%, 6.1% in pre-trained setting on DeepFashion, AWA2, CUB, respectively. Notice that finetuning on CUB, SUN with small number of samples overfits to training data due to the high capacity of dense attention. <ref type="table" target="#tab_10">Table 4</ref> shows the performances of different attention mechanism variants on generalized zero-shot learning without feature composition. We observe that without dense attention and attribute grounding, the model performs poorly at both seen and novel class recognition on AWA2 and CUB since it cannot capture fine-grained attribute details. Although having dense attention without attribute grounding improves CUB performances, this degrades AWA2 performances dues to the misalignment between attribute semantics from word embedding and visual features, as shown in <ref type="figure" target="#fig_1">Figure 10</ref>. As such, learning both dense attention and grounding attribute semantics are crucial for achieving high performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Dense Attention Mechanism:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Dense Feature Composition:</head><p>We compare recent generative methods and Attribute GANs, where we learn a separate GANs per attribute, with random composition which uniformly samples combinations from U(S) and our methods in   believe this is due to the strong regularization effect of dense feature composition which prevents overfitting on feature combinations from training samples while retaining the ability to recognize fine-grained details obtained from seen classes. Notice that Attribute GANs cannot scale to 312 attributes in CUB dataset due to its large memory consumption for training hundreds of GANs, thus we do not report its performance. Our method surpasses random composition by 3.7% (6.0%), 2.9% (2.1%) in harmonic mean (zero-shot accuracy) on AWA2 and CUB, respectively. <ref type="figure" target="#fig_7">Figure 7</ref> shows the zero-shot and generalized zero-shot performances on DeepFashion in the fine-tuned setting as functions of b, ? and k. We vary the value of one hyper-parameter while fixing the remaining hyper-parameters and measure the improvement with respect to the lowest value in each hyperparmeter range. By increasing the search budget for most probable combination via the number of candidate combinations b, we improve both zero-shot and generalized zero-shot performances compared to random composition (b = 0). The performances stabilize across a wide range of b as probable samples according to p(H|z) often have high p(y|H, z)p(H|z) probability. Increasing ? increases the similarity between composed features and features of related samples, thus our method constructs "hard" features being closer to the decision boundary of seen classes. The harmonic mean improves with ? and degrades for large values of ?, as the composed features become too similar to training features. When increasing the size of the related samples via k, we improve zero-shot accuracy the most, as composed features have richer attribute details by using more samples. Notice that our method only uses at most k = 20 related samples, as additional samples are not selected since they do not improve the reconstruction loss <ref type="bibr" target="#b10">(11)</ref>, thus the performances remain the same for larger k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Hyperparameters:</head><p>Ablation Study: We report the effectiveness of different components in our method on DeepFashion in <ref type="table" target="#tab_8">Table 3</ref> (right). We observe that the discriminative model, trained only on seen classes, fails to generalize to novel classes. Although training on random composed features improves the harmonic mean, this does not significantly improve zero-shot accuracy due to the lack of meaningful knowledge in composed features. Using only prior p(H|z) improves the harmonic mean by 2.0% but not zero-shot accuracy, as maximizing the prior is equivalent to using features from the most related sample without modifications for novel classes. We achieve the best performance when combining the classification knowledge p(y|H, z) and the prior knowledge p(H|z). Notice without varying the sample set S, the harmonic mean drops by 3.3%, showing the importance of feature diversity for zero-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute grounding:</head><p>We visualize how the attribute semantic vectors changes during training. By modifying the word embedding according to <ref type="bibr" target="#b4">(5)</ref>, we observe the semantic representations of related attributes, such as "lean", "fast" and "agility", are brought together. On the other hand, opposite attributes, such as "fast" and "slow", are separated apart in the semantic space. Thus, attribute grounding improves performances via aligning visual and textual domains.</p><p>Complexity Analysis: <ref type="figure" target="#fig_1">Figure 11</ref> shows memory/space complexity of f-Translator which use a generative model to train a discriminative model, and our method which directly trains a discriminative model. We measure only training time which excludes loading and evaluation times for 6000 iterations and GPU memory usage. By using a single model for feature generation and classification, we improve the training time by 3 folds and the memory usage by 6 folds compared to f-Translator.</p><p>Qualitative Results: <ref type="figure">Figure 9</ref> visualizes attention maps of attribute features for both present and absent attributes in composed features for novel classes. In addition to using relevant features for present attributes, our method also selects appropriate features to indicate the absence of attributes in novel classes. Thus, the discriminative model, trained on these features, learns from both present and absent attributes to recognize novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Few-Shot Experiments</head><p>For fair comparisons, we follow the setting of <ref type="bibr" target="#b18">[19]</ref>, whose implementation is publicly available. To be specific, we measure the mean few-shot and generalized few-shot performances over ten runs with one, two, five and ten shots for each novel class on CUB, AWA2 datasets in pre-trained setting. We compare with three variants of our model as Visual-Semantic Composer, which synthesizes class features based on visual-semantic featuresh, Semantic Composer, which generates features conditioned on only z, and No Composer which trains a discriminative model on only real dense features.</p><p>Few-Shot Learning: <ref type="figure" target="#fig_1">Figure 12</ref> shows the few-shot performances (n ? n) on both AWA2 and CUB datasets. We significantly improve the performances by 9.4% and 3.7% compared to AM3 on AWA2 as well as 9.6% and 7.0% compared to CADA in CUB for one-and two-shot settings, respectively. All variants of our methods significantly outperform CADA due to our ability to capture fine-grained features. Our Visual-Semantic Composer is most effective on AWA2 dataset where each class is described by a small number of 85 attributes and additional visual information is beneficial for feature generation. Thus, we achieve 5.6%, 5.2% improvements compared to Semantic Composer in oneand two-shot learning, respectively. On CUB, as Semantic Composer can leverage a large number of carefully annotated attributes for composition, it can achieve comparable performances to Visual-Semantic Composer. When given one or two training samples, WeightImprint, AM3 have low performances due to overfitting.</p><p>Generalized Few-Shot Learning: <ref type="figure" target="#fig_1">Figure 12</ref> also shows our method improves the harmonic mean by 7.0% and 5.3% on AWA2 and 7.8% and 4.7% on CUB compared to CADA given only one and two training samples of novel classes. Without learning from synthesized features of novel classes, No Composer misclassifies novel class samples as seen classes, thus cannot perform well in generalized few-shot setting despite its high few-shot performances. This shows the importance of generating novel class features to mitigate the imbalance between seen and novel training samples, which causes seen class bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Training on Real and Synthesized Features:</head><p>We further measure the effectiveness of training on real and synthesized features by varying ? in <ref type="bibr" target="#b20">(21)</ref>. We observe low few-shot performances when mostly learning from synthesized features (? = 2) since these samples cannot accurately describe novel classes. On the other hand, learning from only real features (? = 0) results in low generalized few-shot performances as the model overfits on few training samples. This shows the necessity of learning from both real and synthesized features, which prevents overfitting on few training examples and improves generalization toward novel classes. Our method is most effective when we put more weight on real features compared to synthesize features (? = 0.5) to train the discriminative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>We proposed a dense feature composition framework that extracts attribute features from training samples and recombines them to construct features of novel classes. Our framework selectively composes features of novel classes from only related training samples and alternates between different samples used for composition to improve the diversity among composed features. We employ a training scheme where a discriminative model composes features to train itself. By extensive experiments on four popular datasets, we show the effectiveness of our method on both zero-shot and few-shot settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>D. Huynh and E. Elhamifar are with the Khoury College of Computer Sciences, Northeastern University, Boston, MA, 02115.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Three fine-grained classes and their attribute descriptions from the CUB (top) and DeepFashion (bottom) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Traditional zero-shot classification (top) compresses visual features to perform global embedding with class semantic descriptions, hence, not efficiently capturing fine-grained discriminative visual information. Our method (bottom) finds local discriminative regions through dense attention based on attributes and individually embeds each attribute feature with the attribute semantic description, thus transfers knowledge to novel classes while preserving all fine-grained details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Left: Traditional generative models synthesize holistic features from random codes lacking fine-grained details. Right: Our compositional model constructs dense features from training samples. By selecting different relevant samples for composition, our method builds diverse features for novel classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Overview of our dense attention for attributes with attribute embedding. Image features of R regions are extracted and fed into our dense attention mechanism to compute attention features for all attributes. The attention features are then aligned with attribute semantic vectors to measure the scores of attributes in the image, which are combined to form the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of attention maps of top positive attribute scores (left) and negative attribute scores (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Overview of our compositional zero-shot learning framework. Given a set of samples S, we extract dense attribute features Hi from each sample i ? S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Effects of hyperparameters on harmonic mean (H) and zero-shot accuracy (n ? n) on DeepFashion in fine-tuned setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Attention visualization of attribute features from samples used for dense feature composition of target classes. Our method selects relevant attributes from related samples to describe novel classes. Visualization of a subset of attribute semantic vectors before and after visual grounding on AWA2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Comparison between f-Translator and our method in terms of time and memory complexities on DeepFashion. Effect of training on real and synthesized features for few-shot and generalized few-shot learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Input: Training set Ds, Dn, randomly initialized ?(f , v), p(y|H, z) \\1st Stage: Learn Dense Attention Model for t = 1, . . . , Natt do</figDesc><table><row><cell>Train Dense Attention Model ?(f , v) via (5)</cell></row><row><cell>end for</cell></row><row><cell>\\2nd Stage: Compose Dense Feature</cell></row><row><cell>for t = 1, . . . , Ncomp do</cell></row><row><cell>Sample S ? Ds, Sn ? Dn</cell></row><row><cell>Extract dense features {Hi}i?S?S n with ?(f , v) via (2)</cell></row><row><cell>Generate features of novel classes via (14) or (18)</cell></row><row><cell>Fine-tune p(y|H, z) via (15) or (21)</cell></row><row><cell>end for</cell></row><row><cell>Output: Optimal parameters</cell></row></table><note>1. Notice that a class can be described by different semantic vectors which reflects its visual variations. For simplicity, we use a single semantic vector per class.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of the datasets used in our experiments.</figDesc><table><row><cell>with respect to both real and synthesized features of novel</cell></row><row><cell>classes as:</cell></row><row><cell>min W e,{va} A a=1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 :</head><label>2</label><figDesc>Performances on DeepFashion, AWA2, CUB and SUN. We report zero-shot accuracy (n ? n) in the zero-shot setting and seen class accuracy (a ? s), novel class accuracy (a ? n), harmonic mean (H) in generalized zero-shot setting. * indicates the usage of extra supervision from human captions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Method f-Translator [20] f-VAEGAN-d2 [18] CADA-VAE [19] Attribute GANs Random Comp Composer (Ours)</cell><cell>n ? n 70.4 71.1 -65.1 65.5 71.5</cell><cell>AWA2 a ? s a ? n Generative models H 72.6 55.3 62.6 70.6 57.6 63.5 75.0 55.8 64.0 75.2 58.1 65.6 Compositional models 76.7 56.6 65.1 77.3 62.1 68.8</cell><cell>n ? n 58.5 61.0 --67.3 69.4</cell><cell>CUB a ? s a ? n 54.8 47.0 60.1 48.4 53.5 51.6 --64.4 51.2 56.4 63.8</cell><cell>H 50.6 53.6 52.5 -57.0 59.9</cell><cell>Method No Comp Random Comp p(H|z) Comp p(y|H, z)p(H|z) Comp (fixed S) Composer (Ours)</cell><cell>u ? u 44.6 44.9 43.9 46.9 47.3</cell><cell>DeepFashion a ? s a ? u 54.2 2.6 40.4 30.1 36.9 37.1 44.7 26.9 42.3 32.8</cell><cell>H 5.0 34.5 36.5 33.6 36.9</cell></row></table><note>(left). Random composition significantly outperforms recent methods by at least 1.1% and 3.4% harmonic mean on AWA2 and CUB, respectively while performing comparably with Attribute GANs. We</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3 :</head><label>3</label><figDesc>Left: Comparison between generative models and compositional models on AWA2 and CUB in the pre-trained setting.Right: Ablation study on DeepFashion in the fine-tuned setting.Fig. 8: Few-shot accuracy (n ? n) and harmonic mean (H) for few-shot and generalized few-shot learning in pre-trained setting.</figDesc><table><row><cell>Dense</cell><cell>Attribute</cell><cell></cell><cell>AWA2</cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell></row><row><cell>Attention</cell><cell>Grounding</cell><cell>a ? s</cell><cell>a ? n</cell><cell>H</cell><cell cols="2">a ? s a ? n</cell><cell>H</cell></row><row><cell>No</cell><cell>No</cell><cell>59.2</cell><cell>19.1</cell><cell>28.9</cell><cell>16.7</cell><cell>12.1</cell><cell>14.1</cell></row><row><cell>Yes</cell><cell>No</cell><cell>54.0</cell><cell>18.4</cell><cell>27.5</cell><cell>36.5</cell><cell>27.1</cell><cell>31.1</cell></row><row><cell>Yes</cell><cell>Yes</cell><cell>82.5</cell><cell>25.7</cell><cell>39.4</cell><cell>65.3</cell><cell>41.9</cell><cell>51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 4 :</head><label>4</label><figDesc>Ablation study of dense attention mechanism without feature composition on the CUB and AWA2 datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attentive fashion grammar network for fashion landmark detection and clothing category classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning attribute representations with localization for flexible fashion search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Ak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Tham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Link the head to the &quot;beak&quot;: Zero shot learning from noisy text description at part precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6288" to="6297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing part attributes with insufficient data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive multi-label CNN learning with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot learning with localization in realistic settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal cycle-consistent generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">f-vaegan-d2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sch?nfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning feature-tofeature translator by alternating back-propagation for generative zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative dual adversarial network for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient matching generative networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot learning via simultaneous generating and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning jake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/1904.03582</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot learning -the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Locality and compositionality in zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A causal view of compositional zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compositional zero-shot learning via fine-grained dense feature composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From red wine to red tomato: Composition with context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-attention multiclass constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Marginalized latent semantic encoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking knowledge graph propagation for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stacked semantics-guided attention model for fine-grained zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A shared multi-attention framework for multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attentive region embedding network for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning where to look: Semantic-guided multi-attention localization for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generalized zero-shot recognition based on visually semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generating visual representations for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Zeroshot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adaptive confidence smoothing for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fine-grained generalized zero-shot learning via dense attribute-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generalized zeroshot learning with deep calibration network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Meta-learning for semisupervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Revisiting posenormalization for fine-grained few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-attention meta learning for few-shot fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Towards gan benchmarks which require generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Detecting overfitting of deep generative networks via latent recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Don&apos;t blame the elbo! a linear vae perspective on posterior collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Dual adversarial semanticsconsistent network for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Attribute learning in large-scale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Measuring compositionality in representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Taskdriven modular networks for zero-shot compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning unseen concepts via hierarchical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>2014. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Stable and efficient representation learning with nonnegativity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot learning: A conditional visual classification perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Modeling inter and intra-class relations in the triplet loss for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cacheux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Attribute attention for semantic disambiguation in zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Attribute prototype network for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tijmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>COURSERA: Neural networks for machine learning 4</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
