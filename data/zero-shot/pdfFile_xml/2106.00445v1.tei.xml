<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sample Selection with Uncertainty of Losses for Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">RIKEN</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">RIKEN</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sample Selection with Uncertainty of Losses for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In learning with noisy labels, the sample selection approach is very popular, which regards small-loss data as correctly labeled during training. However, losses are generated on-the-y based on the model being trained with noisy labels, and thus large-loss data are likely but not certainly to be incorrect. There are actually two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss decreases slower than other data, since deep neural networks "learn patterns rst"; (b) it belongs to an underrepresented group of data and has not been selected yet. In this paper, we incorporate the uncertainty of losses by adopting interval estimation instead of point estimation of losses, where lower bounds of the con dence intervals of losses derived from distribution-free concentration inequalities, but not losses themselves, are used for sample selection. In this way, we also give large-loss but less selected data a try; then, we can better distinguish between the cases (a) and (b) by seeing if the losses e ectively decrease with the uncertainty after the try. As a result, we can better explore underrepresented data that are correctly labeled but seem to be mislabeled at rst glance. Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning with noisy labels is one of the most challenging problems in weakly-supervised learning, since noisy labels are ubiquitous in the real world <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b69">70]</ref>. For instance, both crowdsourcing and web crawling yield large numbers of noisy labels everyday <ref type="bibr" target="#b14">[15]</ref>. Noisy labels can severely impair the performance of deep neural networks with strong memorization capacities <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>To reduce the in uence of noisy labels, a lot of approaches have been recently proposed <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. They can be generally divided into two main categories. The rst one is to estimate the noise transition matrix <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref>, which denotes the probabilities that clean labels ip into noisy labels. However, the noise transition matrix is hard to be estimated accurately, especially when the number of classes is large <ref type="bibr" target="#b73">[74]</ref>. The second approach is sample selection, which is our focus in this paper. This approach is based on selecting possibly clean examples from a mini-batch for training <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. Intuitively, if we can exploit less noisy data for network parameter updates, the network will be more robust.</p><p>A major question in sample selection is what criteria can be used to select possibly clean examples. At the present stage, the selection based on the small-loss criteria is the most common method, and has been veri ed to be e ective in many circumstances <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b70">71]</ref>. Specically, since deep networks learn patterns rst <ref type="bibr" target="#b1">[2]</ref>, they would rst memorize training data of clean labels and then those of noisy labels with the assumption that clean labels are of the majority in a noisy class. Small-loss examples can thus be regarded as clean examples with high probability. Therefore, in each iteration, prior methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60]</ref> select the small-loss examples based on the predictions of the current network for robust training.</p><p>However, such a selection procedure is debatable, since it arguably does not consider uncertainty in selection. The uncertainty comes from two aspects. First, this procedure has uncertainty about small-loss examples. Speci cally, the procedure uses limited time intervals and only exploits the losses provided by the current predictions. For this reason, the estimation for the noisy class posterior is unstable <ref type="bibr" target="#b71">[72]</ref>, which causes the network predictions to be equally unstable. It thus takes huge risks to only use losses provided by the current predictions <ref type="figure" target="#fig_0">(Figure 1, left)</ref>. Once wrong selection is made, the inferiority of accumulated errors will arise <ref type="bibr" target="#b73">[74]</ref>. Second, this procedure has uncertainty about large-loss examples. To be speci c, deep networks learn easy examples at the beginning of training, but ignore some clean examples with large losses. Nevertheless, such examples are always critical for generalization. For instance, when learning with imbalanced data, distinguishing the examples with non-dominant labels are more pivotal during training <ref type="bibr" target="#b38">[39]</ref>. Deep networks often give large losses to such examples <ref type="figure" target="#fig_0">(Figure 1, right)</ref>. Therefore, when learning under the realistic scenes, e.g., learning with noisy imbalanced data, prior sample selection methods cannot address such an issue well.</p><p>To relieve the above issues, we study the uncertainty of losses in the sample selection procedure to combat noisy labels. To reduce the uncertainty of small-loss examples, we extend time intervals and utilize the mean of training losses at di erent training iterations. In consideration of the bad in uence of mislabeled data on training losses, we build two robust mean estimators from the perspectives of soft truncation and hard truncation w.r.t. the truncation level, respectively. Soft truncation makes the mean estimation more robust by holistically changing the behavior of losses. Hard truncation makes the mean estimation more robust by locally removing outliers from losses. To reduce the uncertainty of large-loss examples, we encourage networks to pick the sample that , due to the instability of the current prediction, the network gives a larger loss to the clean example and does not select it for updates. If we consider the mean of training losses at di erent epochs, the clean example can be equipped with a smaller loss and then selected for updates. Right: uncertainty of large-loss examples. Since the deep network learns easy examples at the beginning of training, it gives a large loss to clean imbalanced data with non-dominant labels, which causes such data unable to be selected and severely in uence generalization.</p><p>has not been selected in a conservative way. Furthermore, to address the two issues simultaneously, we derive concentration inequalities <ref type="bibr" target="#b4">[5]</ref> for robust mean estimation and further employ statistical con dence bounds <ref type="bibr" target="#b2">[3]</ref> to consider the number of times an example was selected during training.</p><p>The study of uncertainty of losses in learning with noisy labels can be justi ed as follows. In statistical learning, it is known that uncertainty is related to the quality of data <ref type="bibr" target="#b55">[56]</ref>. Philosophically, we need variety decrease for selected data and variety search for unselected data, which share a common objective, i.e., reduce the uncertainty of data to improve generalization <ref type="bibr" target="#b41">[42]</ref>. This is our original intention, since noisy labels could bring more uncertainty because of the low quality of noisy data. Nevertheless, due to the harm of noisy labels for generalization, we need to strike a good balance between variety decrease and search. Technically, our method is specially designed for handling noisy labels, which robustly uses network predictions and conservatively seeks less selected examples meanwhile to reduce the uncertainty of losses and then generalize well.</p><p>Before delving into details, we clearly emphasize our contributions in two folds. First, we reveal prior sample selection criteria in learning with noisy labels have some potential weaknesses and discuss them in detail. The new selection criteria are then proposed with detailed theoretical analyses. Second, we experimentally validate the proposed method on both synthetic noisy balanced/imbalanced datasets and real-world noisy datasets, on which it achieves superior robustness compared with the state-of-the-art methods in learning with noisy labels. The rest of the paper is organized as follows. In Section 2, we propose our robust learning paradigm step by step. Experimental results are discussed in Section 3. The conclusion is given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we rst introduce the problem setting and some background (Section 2.1). Then we discuss how to exploit training losses at di erent iterations (Section 2.2). Finally, we introduce the proposed method, which exploits training losses at di erent iterations more robustly and encourages networks to pick the sample that is less selected but could be correctly labeled (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Let X and Y be the input and output spaces. Consider a k-class classi cation problem, i.e., Y = [k], where [k] = {1, . . . , k}. In learning with noisy labels, the training data are all sampled from a corrupted distribution on X ? Y. We are given a sample with noisy labels, i.e.,S = {(x,?)}, where? is the noisy label. The aim is to learn a robust classi er that could assign clean labels to test data by only exploiting a training sample with noisy labels.</p><p>Let f : X ? R k be the classi er with learnable parameters w. At the i-th iteration during training, the parameters of the classi er f can be denoted as w i . Let : R k ? Y ? R be a surrogate loss function for k-class classi cation. We exploit the softmax cross entropy loss in this paper. Given an arbitrary training example (x,?), at the i-th iteration, we can obtain a loss i , i.e., i = (f (w i ; x),?). Hence, until the t-th iteration, we can obtain a training loss set L t about the example (x,?), i.e., L t = { 1 , . . . , t }.</p><p>In this paper, we assume that the training losses in L t conform to a Markov process, which is to represent a changing system under the assumption that future states only depend on the current state (the Markov property) <ref type="bibr" target="#b50">[51]</ref>. More speci cally, at the i-th iteration, if we exploit an optimization algorithm for parameter updates (e.g., the stochastic gradient descent algorithm <ref type="bibr" target="#b3">[4]</ref>) and omit other dependencies (e.g.,S), we will have P (w i |w i?1 , . . . , w 0 ) = P (w i |w i?1 ), which means that the future state of the classi er f only depends on the current state. Furthermore, given a training example and the parameters of the classi er f , we can determine the loss of the training example as discussed. Therefore, the training losses in L t will also conform to a Markov process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extended Time Intervals</head><p>As limited time interval cannot address the instability issue of the estimation for the noisy class posterior well <ref type="bibr" target="#b48">[49]</ref>, we extend time intervals and exploit the training losses at di erent training iterations for sample selection. One straightforward idea is to use the mean of training losses at di erent training iterations. Hence, the selection criterion could b?</p><formula xml:id="formula_0">? = 1 t t i=1 i .<label>(1)</label></formula><p>It is intuitive and reasonable to use such a selection criterion for sample selection, since the operation of averaging can mitigate the risks caused by the unstable estimation for the noisy class posterior, following better generalization. Nevertheless, such a method could arguably achieve suboptimal classi cation performance for learning with noisy labels. The main reason is that, due to the great harm of mislabeled data, part of training losses are with too large uncertainty and could be seen as outliers. Therefore, it could be biased to use the mean of training losses consisting of such outliers <ref type="bibr" target="#b11">[12]</ref>, which further in uences sample selection. More evaluations for our claims are provided in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Robust Mean Estimation and Conservative Search</head><p>We extend time intervals and meanwhile exploit the training losses at di erent training iterations more robustly. Speci cally, we build two robust mean estimators from the perspectives of soft truncation and hard truncation <ref type="bibr" target="#b6">[7]</ref>. Note that for speci c tasks, it is feasible to decide the types of robust mean estimation with statistical tests based on some assumptions <ref type="bibr" target="#b7">[8]</ref>. We leave the analysis as future work. Two distribution-free robust mean estimators are introduced as follows. Soft truncation. We extend a classical M-estimator from <ref type="bibr" target="#b6">[7]</ref> and exploit the widest possible choice of the in uence function. More speci cally, give a random variable X, let us consider a non-decreasing in uence function ? : R ? R such that ?(X) = log(1 + X + X 2 /2), X ? 0.</p><p>(</p><p>The choice of ? is inspired by the Taylor expansion of the exponential function, which can make the estimation results more robust by reducing the side e ect of extremum holistically. The illustration for this in uence function is provided in Appendix A.1. For our task, given the observations on training losses, i.e., L t = { 1 , . . . , t }, we estimate the mean robustly as follows:</p><formula xml:id="formula_2">? s = 1 t t i=1 ?( i ).<label>(3)</label></formula><p>We term the above robust mean estimator (3) the soft estimator. Hard truncation. We propose a new robust mean estimator based on hard truncation. Specifically, given the observations on training losses L t , we rst exploit the K-nearest neighbor (KNN) algorithm <ref type="bibr" target="#b29">[30]</ref> to remove some underlying outliers in L t . The number of outliers is denoted by t o (t o &lt; t), which can be adaptively determined as discussed in <ref type="bibr" target="#b78">[79]</ref>. Note that we can also employ other algorithms, e.g., principal component analysis <ref type="bibr" target="#b52">[53]</ref> and the local outlier factor <ref type="bibr" target="#b5">[6]</ref>, to identify underlying outliers in L t . The main reason we employ KNN is because of its relatively low computation costs <ref type="bibr" target="#b78">[79]</ref>.</p><p>The truncated loss observations on training losses are denoted by L t?to . We then utilize L t?to for the mean estimation. As the potential outliers are removed with high probability, the robustness of the estimation results will be enhanced. We denote such an estimated mean as? h . We have?</p><formula xml:id="formula_3">h = 1 t ? t o i ?L t?to i .<label>(4)</label></formula><p>The corresponding estimator (4) is termed the hard estimator.</p><p>We derive concentration inequalities for the soft and hard estimators respectively. The search strategy for less selected examples and overall selection criterion are then provided. Note that we do not need to explicitly quantify the mean of training losses. We only need to sort the training examples based on the proposed selection criterion and then use the selected examples for robust training.</p><p>Theorem 1. Let Z n = {z 1 , ? ? ? , z n } be an observation set with mean ? z and variance ? 2 . By exploiting the non-decreasing in uence function ?(z) = log(1 + z + z 2 /2). For any &gt; 0, we have</p><formula xml:id="formula_4">1 n n i=1 ?(z i ) ? ? z ? ? 2 (n + ? 2 log( ?1 ) n 2 ) n ? ? 2 ,<label>(5)</label></formula><p>with probability at least 1 ? 2 .</p><p>Proof can be found in Appendix A.1.</p><p>Theorem 2. Let Z n = {z 1 , . . . , z n } be a (not necessarily time homogeneous) Markov chain with mean ? z , taking values in a Polish state space ? 1 ? . . . ? ? n , and with a minimal mixing time ? min . The truncated set with hard truncation is denoted by Z no , with n o &lt; n. If |z i | is upper bounded by Z. For any 1 &gt; 0 and 2 &gt; 0, we have</p><formula xml:id="formula_5">1 n ? n o z i ?Zn\Zn o ?? z ? 1 n ? n o 2Z 2? min log 2 1 + 2Zn o n 2? min log 2n 2 ,<label>(6)</label></formula><p>with probability at least 1 ? 1 ? 2 .</p><p>Proof can be found in Appendix A.2. For our task, let the training loss be upper-bounded by L. The value of L can be determined easily by training networks on noisy datasets and observing the loss distribution <ref type="bibr" target="#b0">[1]</ref>.</p><p>Conservative search and selection criteria. In this paper, we will use the concentration inequalities <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref>  Denote the number of times one example was selected by n t (n t ? t). Let = 1 2t . For the circumstance with soft truncation, the selection criterion is</p><formula xml:id="formula_6">s =? s ? ? 2 (t + ? 2 log(2t) t 2 ) n t ? ? 2 .<label>(7)</label></formula><p>Let 1 = 2 = 1 2t , for the situation with hard truncation, by rewriting <ref type="formula" target="#formula_5">(6)</ref>, the selection criterion is</p><formula xml:id="formula_7">h =? h ? 2 ? 2? min L(t + ? 2t o ) (t ? t o ) ? t log(4t) n t .<label>(8)</label></formula><p>Note that we directly replace t with n t . If an example is rarely selected during training, n t will be far less than n, which causes the lower bounds to change drastically. Hence, we do not use the mean of all training losses, but use the mean of training losses in xed-length time intervals. More details about this can be checked in Section 3. For the selection criteria <ref type="formula" target="#formula_6">(7)</ref> and <ref type="formula" target="#formula_7">(8)</ref>, we can see that they consist of two terms and have one term with a minus sign. The rst term in Eq. (7) (or Eq. <ref type="formula" target="#formula_7">(8)</ref>) is to reduce the uncertainty of smallloss examples, where we use robust mean estimation on training losses. The second term, i.e., the statistical con dence bound, is to encourage the network to choose the less selected examples (with a small n t ). The two terms are constraining and balanced with ? 2 or ? min . To avoid introducing strong assumptions on the underlying distribution of losses <ref type="bibr" target="#b7">[8]</ref>, we tune ? and ? min with a noisy validation set. For the mislabeled data, although the model has high uncertainties on them (i.e., a small n t ) and tends to pick them, the over tting to the mislabeled data is harmful. Also, the mislabeled data and clean data are rather hard to distinguish in some cases as discussed. Thus, we should search underlying clean data in a conservative way. In this paper, we initialize ? and ? min with small values. This way can reduce the adverse e ects of mislabeled data and meanwhile select the clean examples with large losses, which helps generalize. More evaluations will be presented in Section 3.</p><p>Algorithm 1 CNLCU Algorithm. 1: Input ? 1 and ? 2 , learning rate ?, xed ? , epoch T k and T max , iteration t max ; for T = 1, 2, . . . , T max do 2: Shu le training datasetS; for t = 1, . . . , t max do 3: Fetch mini-batchS fromS; 4: ObtainS 1 = arg min S :|S |?R(T )|S| (? 1 , S ); // calculated with Eq. <ref type="bibr" target="#b6">(7)</ref> or Eq. (8) 5: ObtainS 2 = arg min S :|S |?R(T )|S| (? 2 , S ); // calculated with Eq. <ref type="bibr" target="#b6">(7)</ref> or Eq. <ref type="formula" target="#formula_5">(8)  6</ref>:</p><formula xml:id="formula_8">Update ? 1 = ? 1 ? ?? (? 1 ,S 2 ); 7: Update ? 2 = ? 2 ? ?? (? 2 ,S 1 ); end 8: Update R(T ) = 1 ? min T T k ?, ? ; end 9: Output ? 1 and ? 2 .</formula><p>The overall procedure of the proposed method, which combats noisy labels by concerning uncertainty (CNLCU), is provided in Algorithm 1. CNLCU works in a mini-batch manner since all deep learning training methods are based on stochastic gradient descent. Following <ref type="bibr" target="#b14">[15]</ref>, we exploit two networks with parameters ? 1 and ? 2 respectively to teach each other. Speci cally, when a mini-batchS is formed (Step 3), we let two networks select a small proportion of examples in this mini-batch with Eq. <ref type="bibr" target="#b6">(7)</ref> or <ref type="formula" target="#formula_7">(8)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluate the robustness of our proposed method to noisy labels with comprehensive experiments on the synthetic balanced noisy datasets (Section 3.1), synthetic imbalanced noisy datasets (Section 3.2), and real-world noisy dataset (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments on Synthetic Balanced Noisy Datasets</head><p>Datasets. We verify the e ectiveness of our method on the manually corrupted version of the following datasets: MNIST <ref type="bibr" target="#b24">[25]</ref>, F-MNIST <ref type="bibr" target="#b65">[66]</ref>, CIFAR-10 <ref type="bibr" target="#b23">[24]</ref>, and CIFAR-100 <ref type="bibr" target="#b23">[24]</ref>, because these datasets are popularly used for the evaluation of learning with noisy labels in the literature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b25">26]</ref>. The four datasets are class-balanced. The important statistics of the used synthetic datasets are summarized in Appendix B.1.</p><p>Generating noisy labels. We consider broad types of label noise: <ref type="bibr" target="#b0">(1)</ref>. Symmetric noise (abbreviated as Sym.) <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref>. (2) Asymmetric noise (abbreviated as Asym.) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b59">60]</ref>. (3) Pair ip noise (abbreviated as Pair.) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b79">80]</ref>. <ref type="bibr" target="#b3">(4)</ref>. Tridiagonal noise (abbreviated as Trid.) <ref type="bibr" target="#b76">[77]</ref>. <ref type="bibr" target="#b4">(5)</ref>. Instance noise (abbreviated as Ins.) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">64]</ref>. The noise rate is set to 20% and 40% to ensure clean labels are diagonally dominant <ref type="bibr" target="#b35">[36]</ref>. More details about above noise are provided in Appendix B.1. We leave out 10% of noisy training examples as a validation set.</p><p>Baselines. We compare the proposed method (Algorithm 1) with following methods which focus on sample selection, and implement all methods with default parameters by PyTorch, and conduct all the experiments on NVIDIA Titan Xp GPUs. <ref type="bibr" target="#b0">(1)</ref>. S2E <ref type="bibr" target="#b70">[71]</ref>, which properly controls the sample selection process so that deep networks can better bene t from the memorization effects. <ref type="bibr" target="#b1">(2)</ref>. MentorNet <ref type="bibr" target="#b18">[19]</ref>, which learns a curriculum to lter out noisy data. We use self-paced MentorNet in this paper. <ref type="bibr" target="#b2">(3)</ref>. Co-teaching <ref type="bibr" target="#b14">[15]</ref>, which trains two networks simultaneously and cross-updates parameters of peer networks. <ref type="bibr" target="#b3">(4)</ref>. SIGUA <ref type="bibr" target="#b15">[16]</ref>, which exploits stochastic integrated gradient underweighted ascent to handle noisy labels. We use self-teaching SIGUA in this paper. <ref type="bibr" target="#b4">(5)</ref>. JoCor <ref type="bibr" target="#b59">[60]</ref>, which reduces the diversity of networks to improve robustness. Other types of baselines such as adding regularization are provided in Appendix B.2. Note that we do not compare the proposed method with some state-of-the-art methods, e.g., SELF <ref type="bibr" target="#b43">[44]</ref> and DivideMix <ref type="bibr" target="#b26">[27]</ref>. It is because their proposed methods are aggregations of multiple techniques. We mainly focus on sample selectionin in learning with noisy labels. Therefore, the comparison is not fair. Here, we term our methods with soft truncation and hard truncation as CNLCU-S and CNLCU-H respectively.</p><p>Network structure and optimizer. For MNIST, F-MNIST, and CIFAR-10, we use a 9-layer CNN structure from <ref type="bibr" target="#b14">[15]</ref>. Due to the limited space, the experimental details on CIFAR-100 are provided in Appendix B.3. All network structures we used here are standard test beds for weaklysupervised learning. For all experiments, the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> (momentum=0.9) is used with an initial learning rate of 0.001, and the batch size is set to 128 and we run 200 epochs. We linearly decay learning rate to zero from 80 to 200 epochs as did in <ref type="bibr" target="#b14">[15]</ref>. We take two networks with the same architecture but di erent initializations as two classi ers as did in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b59">60]</ref>, since even with the same network and optimization method, di erent initializations can lead to di erent local optimal <ref type="bibr" target="#b14">[15]</ref>. The details of network structures can be checked in Appendix C.</p><p>For the hyper-parameters ? 2 and ? min , we determine them in the range {10 ?1 , 10 ?2 , 10 ?3 , 10 ?4 } with a noisy validation set. Here, we assume the noise level ? is known and set R(</p><formula xml:id="formula_9">T ) = 1 ? min{ T T k ?, ? } with T k =10.</formula><p>If ? is not known in advanced, it can be inferred using validation sets <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b74">75]</ref>. As for performance measurement, we use test accuracy, i.e., test accuracy = (# of correct prediction) / (# of testing). All experiments are repeated ve times. We report the mean and standard deviation of experimental results.</p><p>Experimental results. The experimental results about test accuracy are provided in <ref type="table" target="#tab_1">Table  1</ref>, 2, and 3. Speci cally, for MNIST, as can be seen, our proposed methods, i.e., CNLCU-S and CNLCU-H, produce the best results in the vast majority of cases. In some cases such as asymmetric noise, the baseline S2E outperforms ours, which bene ts the accurate estimation for the number of selected small-loss examples. For F-MNIST, the training data becomes complicated. S2E cannot achieve the accurate estimation in such situation and thus has no great performance like it got on MNIST. Our methods achieve varying degrees of lead over baselines. For CIFAR-10, our methods once again outperforms all the baseline methods. Although some baseline, e.g., Co-teaching, can work well in some cases, experimental results show that it cannot handle various noise types. In contrast, the proposed methods achieve superior robustness against broad noise types. The results mean that our methods can be better applied to actual scenarios, where the noise is diversiform.</p><p>Ablation study. We rst conduct the ablation study to analyze the sensitivity of the length of time intervals. In order to avoid too dense gures, we exploit MNIST and F-MNIST with the    Such robustness to hyperparameters means our methods can be applied in practice and does not need too much e ect to tune the hyperparameters. Furthermore, since our methods concern uncertainty from two aspects, i.e., the uncertainty  <ref type="table">Table 3</ref>: Test accuracy (%) on CIFAR-10 over the last ten epochs. The best two results are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on Synthetic Imbalanced Noisy Datasets</head><p>Experimental setup. We exploit MNIST and F-MNIST. For these two datasets, we reduce the number of training examples along with the labels from "0" to "4" to 1% of previous numbers. We term such synthetic imbalanced noisy datasets as IM-MNIST and IM-F-MNIST respectively. This setting aims to simulate the extremely imbalanced circumstance, which is common in practice. Moreover, we exploit asymmetric noise, since these types of noise can produce more imbalanced case <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b35">36]</ref>. Other settings such as the network structure and optimizer are the same as those in experiments on synthetic balanced noisy datasets. As for performance measurements, we use test accuracy. In addition, we exploit the selected ratio of training examples with the imbalanced classes, i.e., selected ratio=(# of selected imbalanced labels / # of all selected labels). Intuitively, a higher selected ratio means the proposed method can make better use of training examples with the imbalanced classes, following better generalization <ref type="bibr" target="#b20">[21]</ref>.</p><p>Experimental results. The test accuracy achieved on IM-MNIST and IM-F-MNIST is presented in <ref type="figure" target="#fig_4">Figure 2</ref>. Recall the experimental results in <ref type="table" target="#tab_1">Table 1</ref> and 2, we can see that the imbalanced issue is catastrophic to the sample selection approach when learning with noisy labels. For IM-MNIST, as can be seen, all the baselines have serious over tting in the early stages of training. The curves of test accuracy drop dramatically. As a comparison, the proposed CNLCU-S and CNLCU-H can give a try to large-loss but less selected data which are possible to be clean but equipped with imbalanced labels. Therefore, our methods always outperform baselines clearly. In the case of Asym. 10%, our methods achieve nearly 30% lead over baselines. For IM-F-MNIST, we can also see that our methods perform well and always achieve about 5% lead over all the baselines. Note that due to the huge challenge of this task, some baseline, e.g., S2E, has a large error bar. In addition, <ref type="bibr" target="#b15">16</ref> 18 20 <ref type="bibr">22 24</ref> Time Interval  <ref type="table" target="#tab_5">Table 4</ref>. The results explain well why our methods perform better on synthetic imbalanced noisy datasets, i.e., our methods can make better use of training examples with the imbalanced classes. Note that since we give a try to large-loss but less selected data in a conservative way, the selected ratio is still far away from the class prior probability on the test set, i.e., 10%. However, a little improvement of the selection ratio can bring a considerable improvement of test accuracy. These results tell us that, in the sample selection approach when learning with noisy labels, improving the selected ratio of training examples with the imbalanced classes is challenging but promising for generalization. This practical problem deserves to be studied in depth. <ref type="bibr">Dataset</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IM-MNIST IM-F-MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments on Real-world Noisy Datasets</head><p>Experimental setup. To verify the e cacy of our methods in the real-world scenario, we conduct experiments on the noisy dataset Clothing1M <ref type="bibr" target="#b66">[67]</ref>. Speci cally, for experiments on Clothing1M, we use the 1M images with noisy labels for training and 10k clean data for test respectively. Note that we do not use the 50k clean training data in all the experiments. For preprocessing, we resize the image to 256?256, crop the middle 224?224 as input, and perform normalization. The experiments on Clothing1M are performed once due to the huge computational cost. We leave 10% noisy training data as a validation set for model selection. Note that we do not exploit the resampling trick during training <ref type="bibr" target="#b26">[27]</ref>. Here, Best denotes the test accuracy of the epoch where the validation accuracy was optimal. Last denotes test accuracy of the last epoch. For the experiments on Clothing1M, we use a ResNet-18 pretrained on ImageNet as did in <ref type="bibr" target="#b59">[60]</ref>. We also use the Adam optimizer and set the batch size to 64. During the training stage, we run 15 epochs in total and set the learning rate 8 ? 10 ?4 , 5 ? 10 ?4 , and 5 ? 10 ?5 for 5 epochs each.</p><p>Experimental results. The results on Clothing1M are provided in <ref type="table" target="#tab_7">Table 5</ref>. Speci cally, the proposed methods get better results than state-of-the-art methods on Best, which achieve an improvement of +1.28% and +0.99% over the best baseline JoCor. Likewise, the proposed methods outperform all the baselines on Last. We achieve an improvement of +1.01% and +0.54% over Jo-Cor. Note that the results are a bit lower than some state-of-art methods, e.g., <ref type="bibr" target="#b72">[73]</ref> and <ref type="bibr" target="#b53">[54]</ref>, because of the following reasons. <ref type="bibr" target="#b0">(1)</ref>. We follow <ref type="bibr" target="#b59">[60]</ref> and use ResNet-18 as a backbone. The state-of-art methods <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b53">54]</ref> use ResNet-50 as a backbone. Our aim is to make the experimental results directly comparable with previous papers <ref type="bibr" target="#b59">[60]</ref> in the same area. <ref type="bibr" target="#b1">(2)</ref>. We only focus on the sample selection approach and do not employ other advanced techniques, e.g., introducing the prior distribution <ref type="bibr" target="#b53">[54]</ref> and combining semi-supervised learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we focus on promoting the prior sample selection in learning with noisy labels, which starts from concerning the uncertainty of losses during training. We robustly use the training losses at di erent iterations to reduce the uncertainty of small-loss examples, and adopt con dence interval estimation to reduce the uncertainty of large-loss examples. Experiments are conducted on benchmark datasets, demonstrating the e ectiveness of our method. We believe that this paper opens up new possibilities in the topics of using sample selection to handle noisy labels, especially in improving the robustness of models on imbalanced noisy datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theoretical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 1</head><p>For the circumstance with soft truncation,? z = 1 n n i=1 ?(z i ). As suggested in <ref type="bibr" target="#b6">[7]</ref>, we can exploit</p><formula xml:id="formula_10">? ? z and? + z such that? ? z ?? z ?? + z ,<label>(9)</label></formula><p>to derive a bound for? z . For some positive real parameter ?, we de ne</p><formula xml:id="formula_11">r(? z ) = n i=1 ? [?(z i ?? z )] = 0.<label>(10)</label></formula><p>Let us introduce the quantity</p><formula xml:id="formula_12">r(?) = 1 ?n n i=1 ? [?(z i ? ?)] .<label>(11)</label></formula><p>With the exponential moment inequality <ref type="bibr" target="#b12">[13]</ref> and the C r inequality <ref type="bibr" target="#b40">[41]</ref>, we have</p><formula xml:id="formula_13">exp{?nr(?)} ? 1 + ?(? z ? ?) + ? 2 [? 2 + (? z ? ?) 2 ] n ? exp{n?(? z ? ?) + n? 2 [? 2 + (? z ? ?) 2 ]}.<label>(12)</label></formula><p>In the same way,</p><formula xml:id="formula_14">exp{??nr(?)} ? exp{?n?(? z ? ?) + n? 2 [? 2 + (? z ? ?) 2 ]}.<label>(13)</label></formula><p>If we de ne for any ? s ? R the bounds</p><formula xml:id="formula_15">B ? (?) = ? z ? ? ? ?[? 2 + (? z ? ?) 2 ] ? log( ?1 ) ?n<label>(14)</label></formula><p>and</p><formula xml:id="formula_16">B + (?) = ? z ? ? + ?[? 2 + (? z ? ?) 2 ] + log( ?1 ) ?n .<label>(15)</label></formula><p>From <ref type="bibr" target="#b8">[9]</ref> (Lemma 2.2), we obtain that</p><formula xml:id="formula_17">P (r(?) &gt; B ? (?)) ? 1 ? and P (r(?) &lt; B + (?)) ? 1 ? .<label>(16)</label></formula><p>Let? ? z be the largest solution of the quadratic equation B ? (?) and? + z be the smallest solution of the quadratic equation B + (?). Also, to guarantee the solution of the quadratic equation, we assume</p><formula xml:id="formula_18">4? 2 ? 2 + 4 log( ?1 ) n ? 1.<label>(17)</label></formula><p>From <ref type="bibr" target="#b8">[9]</ref> (Theorem 2.6), we then hav?</p><formula xml:id="formula_19">? ? z ? ? z ? ?? 2 + log( ?1 ) ?n ? ? 1 ,<label>(18)</label></formula><p>and?</p><formula xml:id="formula_20">+ z ? ? z + ?? 2 + log( ?1 ) ?n ? ? 1 .<label>(19)</label></formula><p>With probability at least 1-2 , we have? ? z ?? z ?? + z . We can choose ? = n ? 2 . Then we have</p><formula xml:id="formula_21">|? z ? ? z | ? ? 2 (n + ? 2 log( ?1 ) n 2 ) n ? ? 2 ,<label>(20)</label></formula><p>which holds with probability at least 1-2 .</p><p>We exploit the lower bound and let = 1 2t . Then we have</p><formula xml:id="formula_22">s =? s ? ? 2 (t + ? 2 log(2t) t 2 ) n t ? ? 2 ,<label>(21)</label></formula><p>where n t denotes the number of times that the example was selected in the time intervals. Here, we provide the graph of the used in uence function for the soft estimator, which explains the mechanism of the function y = log(1 + x + x 2 /2) more clearly. The illustration is presented in <ref type="figure" target="#fig_5">Figure 3</ref>. As can be seen, when x is large and may be an outlier, the in uence function can reduce its negative impact for mean estimation. Therefore, we exploit such an in uence function for robust mean estimation, which brings better classi cation performance.</p><p>A.2 Proof of Theorem 2 <ref type="bibr">Lemma 1 ([48]</ref>). Let Z n = {z 1 , . . . , z n } be a (not necessarily time homogeneous) Markov chain with mean ? z , taking values in a Polish state space ? 1 ? . . . ? ? n , with a mixing time ? (?) (for 0 ? ? ? 1). Let</p><formula xml:id="formula_23">? min = inf 0??&lt;1 ? (?) ? 2 ? ? 1 ? ? 2 .<label>(22)</label></formula><p>For some ? ? R + , suppose that f : ? ? R satis es the following inequality:</p><formula xml:id="formula_24">f (a) ? f (b) ? n i=1 ?1[a i = b i ],<label>(23)</label></formula><p>for every a, b ? ?. Then for any ? 0, we have</p><formula xml:id="formula_25">P (|f (Z n ) ? Ef(Z n )| ? ) ? 2 exp ?2 2 ? 2 ? min .<label>(24)</label></formula><p>The detailed de nition of the mixing time for the Markov chain can be found in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. Let f be the mean function. Following the prior work on mean estimation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>, without loss of generality, we assume ? z = 0 for the underlying true distribution, and |z i | is upper bounded by Z. Then we can set ? to 4Z/n for Eq. (23). Combining the above analyses, we can revise Eq. (24) as follows:</p><formula xml:id="formula_26">P 1 n n i=1 z i ? 2Z n 2? min log 2 1 ? 1 ,<label>(25)</label></formula><p>and P max i?[n]</p><formula xml:id="formula_27">|z i | ? 2Z n 2? min log 2n 2 ? 2 ,<label>(26)</label></formula><p>for 1 &gt; 0 and 2 &gt; 0. If we remove the potential outliers Z no from Z n . Therefore, we have</p><formula xml:id="formula_28">1 n ? n o z i ?Zn\Zn o ?? z = 1 n ? n o z i ?Zn ? z i ?Zn o ? 1 n ? n o ? ? z i ?Zn + z i ?Zn o ? ? ? 1 n ? n o ? ? z i ?Zn + n o max i?[n] |z i | ? ? ? 1 n ? n o 2Z 2? min log 2 1 + 2Zn o n 2? min log 2n 2 ,<label>(27)</label></formula><p>which holds with probability at least 1 ? 1 ? 2 . For our task, we exploit the concentration inequality. Let 1 = 2 = 1 2t , and the losses be bounded by L. Next we can obtain</p><formula xml:id="formula_29">|? h ? ?| ? 2L t ? t o 2? min log(4t) + t o t 4? min log(4t) = 2 ? 2? min L(t + ? 2t o ) (t ? t o )t log(4t)<label>(28)</label></formula><p>with the probability at least 1 ? 1 t . In practice, it is easy to identify the value of L. For example, we can training deep networks on noisy datasets to observe the loss distributions. Then, we exploit the lower bound such that</p><formula xml:id="formula_30">h =? h ? 2 ? 2? min L(t + ? 2t o ) (t ? t o ) ? t log(4t) n t<label>(29)</label></formula><p>for sample selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Complementary Experimental Analyses</head><p># of training # of testing # of class size MNIST 60,000 10,000 10 28?28?1 F-MNIST 60,000 10,000 10 28?28?1 CIFAR-10 50,000 10,000 10 32?32?3 CIFAR-100 50,000 10,000 100 32?32?3 <ref type="table">Table 6</ref>: Summary of synthetic datasets used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 The Details of Datasets and Generating Noisy Labels</head><p>For the details of datasets, the important statistics of the used datasets are summarized in <ref type="table">Table 6</ref>. For the details of generating noisy labels, we exploit both class-dependent and instance-dependent label noise which include ve types of synthetic label noise to verify the e ectiveness of the proposed method. Here, we describe the details of the noise setting as follows:</p><p>(1). Class-dependent label noise:</p><p>? Symmetric noise: this kind of label noise is generated by ipping labels in each class uniformly to incorrect labels of other classes.</p><p>? Asymmetic noise : this kind of label noise is generated by ipping labels within a set of similar classes. In this paper, for MNIST, ipping 2?7, 3?8, 5?6. For F-MNIST, ipping TSHIRT?SHIRT, PULLOVER?COAT, SANDALS?SNEAKER. For CIFAR-10, ipping TRUCK?AUTOMOBILE, BIRD ?AIRPLANE, DEER?HORSE, CAT?DOG. For CIFAR-100, the 100 classes are grouped into 20 super-classes, and each has 5 sub-classes. Each class is then ipped into the next within the same super-class.</p><p>? Pair ip noise: the noise ips each class to its adjacent class.</p><p>? Tridiagonal noise: the noise corresponds to a spectral of classes where adjacent classes are easier to be mutually mislabeled, unlike the unidirectional pair ipping. It can be implemented by two consecutive pair ipping transformations in the opposite direction.    <ref type="table">Table 8</ref>: Test accuracy on F-MNIST over the last ten epochs.</p><p>(2). Instance-dependent label noise:</p><p>? Instance noise: the noise is quite realistic, where the probability that an instance is mislabeled depends on its features. We generate this type of label noise to validate the e ectiveness of the proposed method as did in <ref type="bibr" target="#b63">[64]</ref>.</p><p>We use synthetic noisy MNIST as an example and plot the noise transition matrices in <ref type="figure">Figure  4</ref>. The noise rate is set to 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison with Other Types of Baselines</head><p>As we focus on the sample selection approach in learning with noisy labels, in the main paper (Section 3.1), we fairly compare our methods with the baselines which also focus on sample selection. Here, we evaluate other types of baselines. We exploit APL <ref type="bibr" target="#b35">[36]</ref> and CDR <ref type="bibr" target="#b64">[65]</ref>, which add implicit regularization from di erent perspectives. The experiments are conducted on MNIST and F-MNIST. Other experimental settings are the same as those in the main paper. The experimental results are provided in <ref type="table" target="#tab_10">Table 7</ref> and 8, which show that the proposed methods can outperform them with respect to classi cation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Experiments on Synthetic CIFAR-100</head><p>For CIFAR-100, we use a 7-layer CNN structure from <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b70">71]</ref>. Other experimental settings are the same as those in the experiments on MNIST, F-MNIST, and CIFAR-10. The results are provided in <ref type="table" target="#tab_12">Table 9</ref>. We can see the proposed method outperforms all the baselines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Experiments for Ablation Study</head><p>We conduct the ablation study to analyze the sensitivity of the length of time intervals. The results are shown in <ref type="figure">Figure.</ref> 5 and 6. As we can seen, the proposed method, i.e., CNLCU-S and CNLCU-H are robust to the choices of hyperparameters.  Note that in this paper, we concern uncertainty from two aspects, i.e., the uncertainty about small-loss examples and the uncertainty about large-loss examples. Here, we conduct ablation study to show the e ect of removing di erent components to provide insights into what makes Test Accuracy <ref type="figure">Figure 6</ref>: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-H. The error bar for standard deviation in each gure has been shaded.</p><p>the proposed methods successful. The experiments are conducted on MNIST and F-MNIST. Other experimental settings are the same as those in the main paper (Section 3.1). Note that we employ two networks to teach each other following <ref type="bibr" target="#b14">[15]</ref>. Therefore, when we do not consider uncertainty in sample selection, the proposed methods will reduce to the baseline Co-teaching <ref type="bibr" target="#b14">[15]</ref>.</p><p>To study the e ect of concerning uncertainty about small-loss examples, we remove the concerns about large-loss examples, i.e., the network is not encouraged to choose the less selected examples for updates. We express such a setting as "without concerning about large-loss examples" (abbreviated as w/o cl). To study the e ect of concerning uncertainty about large-loss examples, we remove the concerns about small-loss examples, i.e., we only exploit the predictions of the current network. We express such a setting as "without concerning about small-loss examples" (abbreviated as w/o cs). Besides, we express the setting which directly uses non-robust mean as Co-teaching-M.</p><p>The experimental results of ablation study are provided in <ref type="table" target="#tab_1">Table 10</ref> and 11. As can be seen, both aspects of uncertainty concerns can improve the robustness of models. Therefore, combining two uncertainty concerns, we can better combat noisy labels. In addition, robust mean estimation is superior to the non-robust mean in learning with noisy labels. <ref type="table" target="#tab_1">Table 12</ref> describes the 9-layer CNN <ref type="bibr" target="#b14">[15]</ref> used on MNIST, F-MNIST, and CIFAR-10. <ref type="table" target="#tab_1">Table 13</ref> describes the 9-layer CNN <ref type="bibr" target="#b73">[74]</ref> used on CIFAR-100. Here, LReLU stands for Leaky ReLU <ref type="bibr" target="#b67">[68]</ref>. The slopes of all LReLU functions in the networks are set to 0.01. Note that that the 7/9-layer CNN is a standard and common practice in weakly supervised learning. We decided to use these CNNs, since then the experimental results are directly comparable with previous approaches in the same area, i.e., learning with noisy labels.   <ref type="table" target="#tab_1">Table 11</ref>: Test accuracy (%) on F-MNIST over last ten epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Complementary Explanation for Network Structures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrations of uncertainty of losses. Experiments are conducted on the imbalanced noisy MNIST dataset. Left: uncertainty of small-loss examples. At the beginning of training (Epochs 1 and 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to present conservative search and the overall sample selection criterion. Speci cally, we exploit their lower bounds and consider the selected number of examples during training. The selection of the examples that are less selected is encouraged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(Step 4 andStep 5). The number of instances is controlled by the function R(T ), and two networks only select R(T ) percentage of examples out of the minibatch. The value of R(T ) should be larger at the beginning of training, and be smaller when the number of epochs goes large, which can make better use of memorization e ects of deep networks<ref type="bibr" target="#b14">[15]</ref> for sample selection. Then, the selected instances are fed into its peer network for parameter updates (Step 6 and Step 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>mentioned noise settings as representative examples. For CNLCU-S, the length of time intervals is chosen in the range from 3 to 8. For CNLCU-H, the length of time intervals is chosen in the range from 10 to 15. Note that the reason for their di erent lengths is that their di erent mechanisms. Speci cally, CNLCU-S holistically changes the behavior of losses, but does not remove any loss from the loss set. We thus do not need too long length of time intervals. As a comparison, CNLCU-H needs to remove some outliers from the loss set as discussed. The length should be longer to guarantee the number of examples available for robust mean estimation. The experimental results are provided in Appendix B.4, which show the proposed CNLCU-S and CNLCU-H are robust to the choices of the length of time intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Test accuracy vs. number of epochs on IM-MNIST and IM-F-MNIST. The error bar for standard deviation in each gure has been shaded.the baseline SIGUA performs badly. It is because SIGUA exploits stochastic integrated gradient underweighted ascent on large-loss examples, which makes the examples with imbalanced classes more di cult to be selected than them in other sample selection methods.The selected ratio achieved on IM-MNIST and IM-F-MNIST is presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>y = log(1 + x + x 2 /2) The illustration of the in uence function for the soft estimator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>8 SymmetricFigure 4 :</head><label>84</label><figDesc>022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.8 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.022 0.Synthetic class-dependent transition matrices used in our experiments on MNIST. The noise rate is set to 20%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Illustrations of the hyperparameter sensitivity for the proposed CNLCU-S. The error bar for standard deviation in each gure has been shaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Noise type</cell><cell cols="2">Sym.</cell><cell cols="2">Asym.</cell><cell cols="2">Pair.</cell><cell cols="2">Trid.</cell><cell cols="2">Ins.</cell></row><row><cell>Method/Noise ratio</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell></row><row><cell>S2E</cell><cell>89.99 ?2.07</cell><cell>75.32 ?5.84</cell><cell>89.00 ?0.95</cell><cell>81.03 ?1.93</cell><cell>88.66 ?1.32</cell><cell>67.09 ?4.03</cell><cell>89.53 ?2.63</cell><cell>77.29 ?3.97</cell><cell>88.65 ?2.12</cell><cell>79.35 ?3.04</cell></row><row><cell>MentorNet</cell><cell>90.37 ?0.17</cell><cell>86.53 ?0.65</cell><cell>89.69 ?0.19</cell><cell>67.21 ?2.94</cell><cell>87.92 ?1.08</cell><cell>83.70 ?0.49</cell><cell>88.74 ?0.33</cell><cell>85.63 ?0.59</cell><cell>87.52 ?0.15</cell><cell>83.27 ?1.42</cell></row><row><cell>Co-teaching</cell><cell>91.48 ?0.10</cell><cell>88.80 ?0.29</cell><cell>91.03 ?0.14</cell><cell>68.07 ?4.58</cell><cell>90.77 ?0.23</cell><cell>86.91 ?0.71</cell><cell>91.24 ?0.11</cell><cell>89.18 ?0.36</cell><cell>90.60 ?0.12</cell><cell>87.90 ?0.45</cell></row><row><cell>SIGUA</cell><cell>87.64 ?1.29</cell><cell>87.23 ?0.72</cell><cell>76.97 ?2.59</cell><cell>45.96 ?3.40</cell><cell>69.59 ?5.75</cell><cell>68.93 ?2.80</cell><cell>79.97 ?3.23</cell><cell>76.14 ?4.24</cell><cell>76.92 ?5.09</cell><cell>74.89 ?4.84</cell></row><row><cell>JoCor</cell><cell>91.97 ?0.13</cell><cell>89.96 ?0.19</cell><cell>90.95 ?0.21</cell><cell>79.79 ?2.39</cell><cell>91.52 ?0.24</cell><cell>87.40 ?0.58</cell><cell>92.01 ?0.17</cell><cell>89.42 ?0.33</cell><cell>91.43 ?0.71</cell><cell>87.59 ?0.94</cell></row><row><cell>CNLCU-S</cell><cell>92.37 ?0.15</cell><cell>91.45 ?0.28</cell><cell>92.57 ?0.15</cell><cell>83.14 ?1.77</cell><cell>92.04 ?0.26</cell><cell>88.20 ?0.44</cell><cell>92.24 ?0.17</cell><cell>90.08 ?0.34</cell><cell>91.69 ?0.10</cell><cell>89.02 ?1.02</cell></row><row><cell>CNLCU-H</cell><cell>92.42 ?0.21</cell><cell>91.60 ?0.19</cell><cell>92.60 ?0.18</cell><cell>82.69 ?0.43</cell><cell>91.70 ?0.18</cell><cell>87.70 ?0.69</cell><cell>92.33 ?0.26</cell><cell>90.22 ?0.71</cell><cell>91.50 ?0.21</cell><cell>88.79 ?1.22</cell></row></table><note>Test accuracy (%) on MNIST over the last ten epochs. The best two results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Test accuracy on F-MNIST over the last ten epochs. The best two results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>from both small-loss and large-loss examples, we conduct experiments to analyze each part of our methods. Also, as mentioned, we compare robust mean estimation with non-robust mean estimation when learning with noisy labels. More details are provided in Appendix B.4.</figDesc><table><row><cell>Noise type</cell><cell cols="2">Sym.</cell><cell cols="2">Asym.</cell><cell cols="2">Pair.</cell><cell cols="2">Trid.</cell><cell cols="2">Ins.</cell></row><row><cell>Method/Noise ratio</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell></row><row><cell>S2E</cell><cell>80.78 ?0.88</cell><cell>69.72 ?3.94</cell><cell>84.03 ?1.01</cell><cell>75.04 ?1.24</cell><cell>81.72 ?0.93</cell><cell>61.50 ?4.63</cell><cell>81.44 ?0.59</cell><cell>64.39 ?2.82</cell><cell>79.89 ?0.26</cell><cell>62.42 ?3.11</cell></row><row><cell>MentorNet</cell><cell>80.92 ?0.48</cell><cell>74.67 ?1.17</cell><cell>80.37 ?0.26</cell><cell>71.69 ?1.06</cell><cell>77.98 ?0.31</cell><cell>69.39 ?1.73</cell><cell>78.02 ?0.29</cell><cell>71.56 ?0.93</cell><cell>77.02 ?0.71</cell><cell>68.17 ?2.52</cell></row><row><cell>Co-teaching</cell><cell>82.35 ?0.16</cell><cell>77.96 ?0.39</cell><cell>83.87 ?0.24</cell><cell>73.43 ?0.62</cell><cell>80.94 ?0.46</cell><cell>72.81 ?0.92</cell><cell>81.17 ?0.60</cell><cell>74.37 ?0.64</cell><cell>79.92 ?0.57</cell><cell>73.29 ?1.62</cell></row><row><cell>SIGUA</cell><cell>78.19 ?0.22</cell><cell>77.67 ?0.41</cell><cell>75.14 ?0.36</cell><cell>52.76 ?0.68</cell><cell>74.41 ?0.81</cell><cell>61.91 ?5.27</cell><cell>75.75 ?0.53</cell><cell>74.05 ?0.41</cell><cell>74.34 ?0.39</cell><cell>67.98 ?1.34</cell></row><row><cell>JoCor</cell><cell>80.96 ?0.25</cell><cell>76.65 ?0.43</cell><cell>81.39 ?0.74</cell><cell>69.92 ?1.63</cell><cell>80.33 ?0.20</cell><cell>71.62 ?1.05</cell><cell>79.03 ?0.13</cell><cell>74.33 ?1.09</cell><cell>78.21 ?0.34</cell><cell>71.46 ?1.27</cell></row><row><cell>CNLCU-S</cell><cell>83.03 ?0.21</cell><cell>78.25 ?0.70</cell><cell>85.06 ?0.17</cell><cell>75.34 ?0.32</cell><cell>83.16 ?0.25</cell><cell>73.19 ?1.25</cell><cell>82.77 ?0.32</cell><cell>74.37 ?1.37</cell><cell>82.03 ?0.37</cell><cell>73.67 ?1.09</cell></row><row><cell>CNLCU-H</cell><cell>83.03 ?0.47</cell><cell>78.33 ?0.50</cell><cell>84.95 ?0.27</cell><cell>75.29 ?0.80</cell><cell>83.39 ?0.68</cell><cell>73.40 ?1.53</cell><cell>82.52 ?0.71</cell><cell>74.79 ?1.13</cell><cell>81.93 ?0.25</cell><cell>73.58 ?1.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Selected ratio (%) on IM-MNIST and IM-F-MNIST. The best two results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Test accuracy (%) on Clothing1M. The best two results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>TL was supported by Australian Research Council ProjectDE-190101473. BH was supported by the RGC Early Career Scheme No. 22200720 and NSFC Young Scientists Fund No. 62006202. JY was supported by USTC Research Funds of the Double First-Class Initiative (YD2350002001). GN and MS were supported by JST AIP Acceleration Research Grant Number JPMJCR20U3, Japan. MS was also supported by the Institute for AI and Beyond, UTokyo.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy (%) on MNIST over the last ten epochs.</figDesc><table><row><cell>Noise type</cell><cell cols="2">Sym.</cell><cell cols="2">Asym.</cell><cell cols="2">Pair.</cell><cell cols="2">Trid.</cell><cell cols="2">Ins.</cell></row><row><cell cols="2">Method/Noise ratio 20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell></row><row><cell>APL</cell><cell>91.73 ?0.20</cell><cell>89.06 ?0.41</cell><cell>90.13 ?0.17</cell><cell>80.34 ?0.63</cell><cell>90.22 ?0.80</cell><cell>78.54 ?4.33</cell><cell>90.84 ?0.22</cell><cell>86.53 ?0.76</cell><cell>90.96 ?0.77</cell><cell>85.55 ?2.86</cell></row><row><cell>CDR</cell><cell>85.62 ?0.96</cell><cell>71.83 ?1.37</cell><cell>89.78 ?0.41</cell><cell>79.05 ?1.39</cell><cell>85.72 ?0.65</cell><cell>69.07 ?2.31</cell><cell>86.75 ?1.19</cell><cell>73.63 ?2.82</cell><cell>85.92 ?1.43</cell><cell>73.14 ?3.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy (%) on CIFAR-100 over the last ten epochs. The best two results are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Test accuracy (%) on MNIST over last ten epochs.</figDesc><table><row><cell>Noise type</cell><cell cols="2">Sym.</cell><cell cols="2">Asym.</cell><cell cols="2">Pair.</cell><cell cols="2">Trid.</cell><cell cols="2">Ins.</cell></row><row><cell cols="2">Method/Noise ratio 20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell></row><row><cell>CNLCU-S</cell><cell>92.37 ?0.15</cell><cell>91.45 ?0.28</cell><cell>92.57 ?0.15</cell><cell>83.14 ?1.77</cell><cell>92.04 ?0.26</cell><cell>88.20 ?0.44</cell><cell>92.24 ?0.17</cell><cell>90.08 ?0.34</cell><cell>91.69 ?0.10</cell><cell>89.02 ?1.02</cell></row><row><cell>CNLCU-S w/o cl</cell><cell>91.77 ?0.35</cell><cell>89.40 ?0.26</cell><cell>91.25 ?0.30</cell><cell>72.93 ?2.63</cell><cell>91.53 ?0.17</cell><cell>87.31 ?0.59</cell><cell>91.31 ?0.52</cell><cell>89.50 ?0.32</cell><cell>91.09 ?0.13</cell><cell>88.45 ?0.57</cell></row><row><cell>CNLCU-S w/o cs</cell><cell>91.85 ?0.33</cell><cell>90.76 ?0.28</cell><cell>91.94 ?0.09</cell><cell>80.99 ?2.74</cell><cell>91.28 ?0.20</cell><cell>87.31 ?0.72</cell><cell>91.39 ?0.07</cell><cell>89.29 ?0.51</cell><cell>90.98 ?0.43</cell><cell>88.73 ?0.62</cell></row><row><cell>CNLCU-H</cell><cell>92.42 ?0.21</cell><cell>91.60 ?0.19</cell><cell>92.60 ?0.18</cell><cell>82.69 ?0.43</cell><cell>91.70 ?0.18</cell><cell>87.70 ?0.69</cell><cell>92.33 ?0.26</cell><cell>90.22 ?0.71</cell><cell>91.50 ?0.21</cell><cell>88.79 ?1.22</cell></row><row><cell>CNLCU-H w/o cl</cell><cell>91.70 ?0.04</cell><cell>90.05 ?0.31</cell><cell>91.08 ?0.06</cell><cell>71.35 ?2.30</cell><cell>91.03 ?0.29</cell><cell>87.22 ?0.72</cell><cell>91.59 ?0.07</cell><cell>90.01 ?0.24</cell><cell>90.80 ?0.27</cell><cell>88.31 ?1.09</cell></row><row><cell>CNLCU-H w/o cs</cell><cell>91.82 ?0.13</cell><cell>90.92 ?0.42</cell><cell>92.45 ?0.25</cell><cell>80.73 ?1.63</cell><cell>91.21 ?0.17</cell><cell>87.49 ?0.32</cell><cell>92.08 ?0.13</cell><cell>89.72 ?0.24</cell><cell>91.21 ?0.38</cell><cell>88.62 ?0.73</cell></row><row><cell>Co-teaching-M</cell><cell>91.33 ?0.18</cell><cell>89.05 ?0.73</cell><cell>91.14 ?0.90</cell><cell>71.03 ?3.73</cell><cell>90.85 ?0.61</cell><cell>86.95 ?0.19</cell><cell>91.50 ?0.46</cell><cell>89.18 ?0.44</cell><cell>90.74 ?1.06</cell><cell>88.25 ?0.92</cell></row><row><cell>Co-teaching</cell><cell>91.48 ?0.10</cell><cell>88.80 ?0.29</cell><cell>91.03 ?0.14</cell><cell>68.07 ?4.58</cell><cell>90.77 ?0.23</cell><cell>86.91 ?0.71</cell><cell>91.24 ?0.11</cell><cell>89.18 ?0.36</cell><cell>90.60 ?0.12</cell><cell>87.90 ?0.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>CNN on MNIST, F-MNIST, and CIFAR-10.</figDesc><table><row><cell>CNN on MNIST</cell><cell>CNN on F-MNIST</cell><cell>CNN on CIFAR-10</cell></row><row><cell cols="3">28?28 Gray Image 28?28 Gray Image 32?32 RGB Image</cell></row><row><cell></cell><cell>3?3 conv, 128 LReLU</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 128 LReLU</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 128 LReLU</cell><cell></cell></row><row><cell></cell><cell>2?2 max-pool</cell><cell></cell></row><row><cell></cell><cell>dropout, p = 0.25</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 256 LReLU</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 256 LReLU</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 256 LReLU</cell><cell></cell></row><row><cell></cell><cell>2?2 max-pool</cell><cell></cell></row><row><cell></cell><cell>dropout, p = 0.25</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 512 LReLU</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 256 LReLU</cell><cell></cell></row><row><cell></cell><cell>3?3 conv, 128 LReLU</cell><cell></cell></row><row><cell></cell><cell>avg-pool</cell><cell></cell></row><row><cell>dense 128?10</cell><cell>dense 128?10</cell><cell>dense 128?10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>CNN on CIFAR-100.</figDesc><table><row><cell>CNN on CIFAR-100</cell></row><row><cell>32?32 RGB Image</cell></row><row><cell>3?3 conv, 64 ReLU</cell></row><row><cell>3?3 conv, 64 ReLU</cell></row><row><cell>2?2 max-pool</cell></row><row><cell>3?3 conv, 128 ReLU</cell></row><row><cell>3?3 conv, 128 ReLU</cell></row><row><cell>2?2 max-pool</cell></row><row><cell>3?3 conv, 196 ReLU</cell></row><row><cell>3?3 conv, 196 ReLU</cell></row><row><cell>2?2 max-pool</cell></row><row><cell>dense 256?100</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using con dence bounds for exploitation-exploration trade-o s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Concentration inequalities: A nonasymptotic theory of independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Challenging the empirical mean and empirical variance: a deviation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Catoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;IHP Probabilit?s et statistiques</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1148" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding heavy tails in a bounded world or, is a truncated heavy tail heavy or not? Stochastic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gennady</forename><surname>Samorodnitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="109" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A generalized catoni&apos;s m-estimator under nite ?-th moment assumption with ? ? (1, 2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihu</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05008</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05911</idno>
		<title level="m">Recent advances in algorithmic high-dimensional robust statistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Outlier robust mean estimation with subgaussian rates via stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pensia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15618</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exponential and moment inequalities for u-statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evarist</forename><surname>Gin?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Lata?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Zinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Dimensional Probability II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="13" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5836" to="5846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sigua: Forgetting may make learning with noisy labels more robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4006" to="4016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving generalization by controlling label-noise information in neural network weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4071" to="4081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4804" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupling representation and classi er for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust inference via generative classi ers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3763" to="3772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Provably end-to-end label-noise learning without anchor points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Use of k-nearest neighbor classi er for intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V Rao</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; security</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="439" to="448" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">High dimensional robust m-estimation: Arbitrary corruption and heavy tails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Earlylearning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classi cation with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3361" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6543" to="6553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from binary labels with instance-dependent noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1561" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07314</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Coresets for robust training of neural networks against noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Foundations of Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On the shoulders of giants: New approaches to numeracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uncertainty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="95" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self: Learning to lter noisy labels with selfensembling</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Augmentation strategies for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>H?llerer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02130</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">What you see may not be what you get: Ucb bandit algorithms robust to -contamination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Niss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Concentration inequalities for markov chains by marton couplings and spectral methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Paulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Probability</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">General state space markov chains and mcmc algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rey S Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="20" to="71" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Faithful couplings of markov chains: now equals forever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="381" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Meta transition adaptation for robust deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05697</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A novel anomaly detection scheme based on principal component classi er</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Ling</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Ching</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanoksri</forename><surname>Sarinnapakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robustness of conditional gans to noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10271" to="10282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning with group noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Co-mining: Deep face recognition with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9358" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13726" to="13735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A topological lter for learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Class2simi: A noise reduction perspective on learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6835" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Part-dependent label noise: Towards instancedependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Empirical evaluation of recti ed activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Searching to exploit memorization e ect in learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Tin-Yau</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10789" to="10798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Dual t: Reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">How does disagreement bene t co-teaching?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An e cient and provable approach for mixture proportion estimation using linear independence assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning noise transition matrix from only noisy labels via total variation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yivan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pyod: A python toolbox for scalable outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zain</forename><surname>Nasrullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">96</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Error-bounded correction of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11447" to="11457" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
