<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All in One: Exploring Unified Video-Language Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Wu</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tencent PCG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">All in One: Exploring Unified Video-Language Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mainstream Video-Language Pre-training models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref> consist of three parts, a video encoder, a text encoder, and a video-text fusion Transformer. They pursue better performance via utilizing heavier unimodal encoders or multimodal fusion Transformers, resulting in increased parameters with lower efficiency in downstream tasks. In this work, we for the first time introduce an end-to-end video-language model, namely all-in-one Transformer, that embeds raw video and textual signals into joint representations using a unified backbone architecture. We argue that the unique temporal information of video data turns out to be a key barrier hindering the design of a modality-agnostic Transformer. To overcome the challenge, we introduce a novel and effective token rolling operation to encode temporal representations from video clips in a non-parametric manner. The careful design enables the representation learning of both video-text multimodal inputs and unimodal inputs using a unified backbone model. Our pre-trained all-in-one Transformer is transferred to various downstream video-text tasks after fine-tuning, including text-video retrieval, video-question answering, multiple choice and visual commonsense reasoning. State-of-the-art performances with the minimal model FLOPs on nine datasets demonstrate the superiority of our method compared to the competitive counterparts. The code and pretrained model have been released in https://github.com/showlab/all-in-one.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The pre-train-and-then-fine-tune scheme has become a standard paradigm to learn transferable videolanguage representations for a wide range of downstream video-text tasks, for example, text-video retrieval <ref type="bibr">[7,</ref><ref type="bibr" target="#b41">42]</ref>, video-question answering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref>, multiple choice <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref> and visual commonsense reasoning <ref type="bibr" target="#b47">[48]</ref>. In recent years, there has been tremendous progress in the development of videolanguage pre-training (VLP) models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref>, where joint representations are generally produced with a multimodal fusion Transformer network after extracting the visual and language features through unimodal encoders.</p><p>Mainstream VLP methods attempt to boost the pre-training in two ways: i. adopting more expensive video/text encoders to obtain more powerful unimodal features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> ii. designing heavier fusion networks to enhance the associations between modalities <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. Despite their advanced performance, they suffer from the increasing parameters, leading to significant computational inefficiency in downstream tasks.</p><p>To this end, we aim to design the simplest and most lightweight video-language model that gathers all capabilities in one, that is, learning video-language representations from their raw inputs in an  <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref> use deep features from separate encoders before fusion. The fusion layer can be light <ref type="bibr" target="#b3">[4]</ref> or heavy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref>. <ref type="bibr">(b)</ref>. Ours all-in-one Transformer learns video and text joint representations end-to-end from their raw inputs. We also support fast retrieval by feeding unimodal inputs during inference. (c). Comparison of FLOPs and retrieval performance on MSRVTT <ref type="bibr" target="#b41">[42]</ref>. Our All-in-one brings excellent results with modest computational cost. end-to-end manner. In this way, we do not need any extra unimodal encoders (e.g., object detector in <ref type="bibr" target="#b49">[50]</ref> or ResNet visual encoder in <ref type="bibr" target="#b20">[21]</ref>), and embed visual and text signals in a shared and unified model, termed as All-in-one Transformer in our paper. A recent study, ViLT <ref type="bibr" target="#b16">[17]</ref>, accomplishes such end-to-end joint learning in image-text pre-training under the presumption that the Transformer can process images in the same way as it processes text. However, we observe that it is non-trivial to embed videos using a unified Transformer that is also applied to process textual signals due to the unique challenge of modeling temporal information in video.</p><p>Existing works encode temporal representations in video-language pre-training via designing temporal attention layers <ref type="bibr" target="#b3">[4]</ref> or using temporal-aware visual encoders (e.g., 3D convnets in <ref type="bibr" target="#b49">[50]</ref> or video Transformer in <ref type="bibr" target="#b10">[11]</ref>), which are all infeasible to be applied in our All-in-one Transformer as they are modality-dependent. To tackle the challenge, we introduce a novel, effective and flexible method, the temporal token rolling operation, to properly and gradually capture temporal representations in a non-parametric manner. Specifically, a proportion of the visual tokens in each frame of a sparsely sampled video clip are cyclic scrolling from frame to frame. Thus, visual tokens of a certain frame and its corresponding text tokens can "view" temporal dynamics from the rolling tokens of other frames through self-attention layers that naturally occur in the Transformer architecture. A sub-optimal solution to this issue is to aggregate all frames' visual tokens for the self-attention layer, which is inflexible as it increases the time complexity by a factor of k (given k frames per video clip) compared to our method.</p><p>Our All-in-one Transformer is pre-trained towards the objectives of video-text matching and masked language modeling, following the common practice of Image-Language models <ref type="bibr" target="#b16">[17]</ref>. The pre-trained model is then fine-tuned to perform downstream video-text tasks. To further reap the modalityagnostic benefit of our All-in-one Transformer, we claim that our pre-trained model can not only encode the joint representations of video-language multimodal inputs, but also embed the unimodal features by feeding only video or text data into the Transformer. By fine-tuning our pre-trained model with a contrastive loss between video and text features, our All-in-one Transformer can play the role of an ordinary dual-stream framework <ref type="bibr" target="#b3">[4]</ref> on the downstream text-video retrieval tasks, realizing fast retrieval.</p><p>Our contributions are summarized as three-fold. (1) We introduce the simplest, most lightweight, and most efficient video-language model for pre-training, namely All-in-one Transformer, which is the first to capture video-language representations from the raw visual and textual signals end-to-end in a unified backbone architecture. <ref type="bibr" target="#b1">(2)</ref> We elucidate and tackle the difficulties of applying a unified and shared backbone for multimodal video and text data, that is, how to properly process the unique temporal information of videos. A novel temporal token rolling operation is proposed to capture the temporal representations of sparsely sampled frames without any extra parameters or increasing time complexity. (3) Comprehensive experiments on four downstream video-text tasks of nine datasets fully demonstrate the superiority of our pre-trained All-in-one Transformer on both effectiveness and efficiency compared to recent mainstream methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. Moreover, benefiting from the modality-agnostic characteristic of our model, our pre-trained Transformer can be treated as a dual-stream framework to encode separate video and text features for highly efficient retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video-Language Pre-training. Pre-training on large-scale video-text pairs and fine-tuning on specific downstream tasks gradually becomes the standard paradigm in the video-language domain. Pre-trained models show strong transfer ability in a series of popular downstream video-language tasks including Text-to-Video Retrieval <ref type="bibr">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, Video Question Answering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref>, and Visual Storytelling <ref type="bibr" target="#b47">[48]</ref>. Previous approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> leverage offline video and text features extracted from off-the-shelf visual and language backbones. Some recent methods including ClipBERT <ref type="bibr" target="#b20">[21]</ref> and Frozen <ref type="bibr" target="#b3">[4]</ref> have attempted to train models in an end-to-end fashion but still rely on well-trained visual encoders for feature extraction. In addition, these works mainly pre-train models on image-text dataset, like Google Conceptual Captions <ref type="bibr" target="#b30">[31]</ref> and Visual genome <ref type="bibr" target="#b18">[19]</ref>, and finetune the pre-trained models for downstream video-language tasks. In this work, we try to challenge this paradigm and focus on exploring effective strategies for pre-training on pure large-scale video-text benchmarks with only one network, and adapt our approach to various video-language downstream tasks.</p><p>Temporal Modeling in Video Understanding. Temporal modeling is a fundamental yet challenging topic in video representation learning. Several classic ideas including sparse sampling <ref type="bibr" target="#b36">[37]</ref>, 3Dtype operations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> are proposed for temporal modeling in both convolution and Transformer architectures. 3D-type temporal modeling like Timesformer <ref type="bibr" target="#b4">[5]</ref> is extremely time-consuming because of the increasing number of sampled frames, which can be disastrous for large-scale pre-training techniques. Sparse sampling along the temporal dimension, a type of data augmentation proposed in TSN <ref type="bibr" target="#b36">[37]</ref>, has been widely adopted to train video backbones. Based on this, more related works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref> try to shift channels among different frames for temporal modeling in action recognition. Inspired by these works, we try to roll video tokens for better alignment between modalities. This work focuses on parameter-free temporal modeling based on sparsely sampled frames without heavy 3D-type operation.</p><p>Unified Architecture Design for Multimodal Data. Recently the unified model, which is capable of processing either unimodal inputs or multimodal inputs with a shared encoder, has attracted a lot of attention. VATT <ref type="bibr" target="#b0">[1]</ref> trains a shared transformer with unimodal inputs to process Video, Audio and Text via multimodal contrastive learning and improves the performance of action recognition. Omnivore <ref type="bibr" target="#b12">[13]</ref> converts image, video, and single-view 3D modalities into embeddings that are fed into a Transformer model and trains the model with multi-task learning, which focuses on image/video/scene classification. In image-text pre-training, the early work Unimo <ref type="bibr" target="#b21">[22]</ref> solves both understanding and generation tasks with cross-modal contrastive learning. More recently, UFO <ref type="bibr" target="#b35">[36]</ref> also uses contrastive learning and employs a momentum teacher to guide the pre-training of a image-text shared encoder, which incurs large computational costs. Based on cross-modal contrastive learning, our work can also process unimodal inputs and perform retrieval tasks in a dual stream manner, which is very efficient. To the best of our knowledge, All-in-one Transformer is the first unified network for video-language pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose All-in-one Transformer, a generic framework that enables end-to-end learning on video and language data, by learning joint representations directly from raw video pixels and raw text tokens, instead of the deeper feature from separate deep embedder. All-in-one has a succinct architecture as a Video-Language Pre-training model with parameter-free temporal modeling layer. In model design we making the pipeline as simple as possible so that the model can be used almost out of the box. <ref type="figure">Fig.2</ref> gives an overview of All-in-one framework. It adopts a sparse sampling strategy using only S segments (one frame in each segment) at each training step, instead of full-length videos. Formally, we denote a video-text pair as v ? R S?C?H?W (for video) and t ? R P ?|V | (for text sequence), where C is the number of channels, (H, W ) is the resolution of each raw frame, P is the length of input sentence and |V | is the length of the word dictionary.  <ref type="figure">Figure 2</ref>: Model overview. The overall framework is based on ViT <ref type="bibr" target="#b8">[9]</ref> and only the light text tokenizer and the task head adds extra parameters. The temporal token rolling layer is introduced before each self-attention block to model temporal information. For simplicity, the normalization layers are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unified Video-language Transformer</head><p>The Transformer uses constant latent vector size D through all of its layers, so we map both video and text to D dimensions. Specifically, the input video v is sliced into patches and flatten to v ? R N ?(P 2 C) , where (P, P ) is the patch resolution and N = SHW/P 2 . Followed by linear projection V ? R (P 2 C)?D for video patches. And text t with a learned word embedding matrix T ? R |V |?D In this way, v is embedded intov ? R N ?D and t is embedded intot ? R P ?D .</p><p>Learnable Spatio-temporal Position Embeddings and Modality Type Embeddings are added to each modality to retain both positional and modality information. Position embedding V pos ? R (N +1)?D and text positional embedding is T pos ? R (P +1)?D . The text and video embedding are further summed with their corresponding modal-type embedding vectors t type , v type ? R D . Formally,</p><formula xml:id="formula_0">t = [t class ; t 1 T ; . . . ; t P T ] + T pos + t typ? v = [v class ; v 1 V ; . . . ; v N V ] + V pos + v type<label>(1)</label></formula><p>These text tokens are connected in series with vision tokens of each frame and the joint input is recorded as z 0 = [t;v]. Then z 0 is fed into L stacked blocks and each block consists of a temporal Token Rolling layer, a multi-head self-attention layer and a multilayer perceptron (MLP). We initialize the weights of both self-attention and MLP layers from pre-trained ViT <ref type="bibr" target="#b8">[9]</ref> or DeiT <ref type="bibr" target="#b33">[34]</ref>. The visual features of each sampled frame are independently encoded using a visual backbone model to extract the relationship between the frame and its associated textual representation. Formally,</p><formula xml:id="formula_1">z d?1 = T T R(z d?1 ), d = 1...L z d = M LP (M SA(z d?1 )), d = 1...L<label>(2)</label></formula><p>where M SA means multiheaded self-attention, M LP is multilayer perceptron and T T R is short for Temporal Token Rolling Module. Independent predictions from all the sampled frames are fused together to derive a consensus at the video level. Formally, p = 1</p><formula xml:id="formula_2">S S i=1 z L i .</formula><p>For pre-training, objectives are calculated based on this consensus to learn model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Token Rolling</head><p>Motivation. In VLP, the common usage for temporal modeling is to add additional time attention layers <ref type="bibr" target="#b3">[4]</ref> in vision encoder or use the feature from deep off-the-shelf video encoder <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50]</ref>. However, these techniques are particularly designed for video and thus can not be applied to process text signal, as well as bringing a large amount of parameters. For example, by simply adding a temporal attention layer to each block of the Transformer, the model becomes a normal Timesformer <ref type="bibr" target="#b4">[5]</ref> with parameters increased from 86M to 121.7M (an increase of 42%). Thus, these techniques cannot be used in our unified framework and we turn to find new ways to learn temporal information with modest parameters.  <ref type="figure">Figure 4</ref>: The text to video attention weight distribution over tokens. With the Temporal Token Rolling layer, the text token pay more attention to rolled tokens, in contrast to previous centric attention.</p><p>Approach. A straightforward approach, denoted as "Flatten", is to concatenate video and text tokens together and flatten into one tensor which will be fed into the self-attention blocks. Given text token with length m and video token with length S ? n, we show the flatten version in <ref type="figure" target="#fig_2">Fig.3</ref>. However, as the self-attention layer has quadratic complexity, the computation cost will be O((m + Sn) 2 ), about S 2 times more than 1-frame All-in-one 2 . To overcome such limitation, we try to exchange information for different time segments in the token level. The proposed Token Rolling module is described in <ref type="figure">Fig. 2 (b)</ref>. The tokens at different time stamps are denoted as different colors in each row. Along the temporal dimension, we roll parts of the token by 1, leaving the rest unchanged. Then the self-attention is computed in each m + n tokens and treat each token in the same way. In this way, we reduce the computation complex to O(S(m + n) 2 ), around 1 S of the Flatten version. Taking advantage of Token Rolling, longer dependencies between texts and videos are gradually modeled in deeper layers, which helps to learn better video-text alignment. We try to visualize the cross-modality attention weight density among text and video tokens in <ref type="figure">Fig. 4</ref>. For each text token, we compute the similarity by dot product to reveal its corresponding high-weight video tokens (more details are given in appendix). The baseline is All-in-one without rolling layers. We observe a severe inductive bias in the baseline, i.e., text tokens pay more attention to the centric tokens. By introducing the Temporal Token Rolling, these rolled tokens contribute more to the cross-modality interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objectives</head><p>We train All-in-one with two objectives commonly used to train VLP models: video-text matching (VTM), and masked language modeling (MLM). In addition, in order to overcome the disadvantage of low retrieval efficiency of one-stream, we introduce the video-text contrastive loss (VTC).</p><p>Video-text Matching. Given a paired video-text input, we randomly replace the paired video with a different video with the probability of 0.5 and ask the model to distinguish them. For the cls token of the last block, a single linear layer VTM head projects tokens to logits over binary class. We compute negative log-likelihood loss as our VTM loss.</p><p>Masked Language Modeling. MLM <ref type="bibr" target="#b7">[8]</ref> aims to predict the ground truth labels of masked text tokens from the other text and video tokens. Following the common practices <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, we randomly mask text tokens with the probability of 0.15 and model it as a classification task.</p><p>Video-text Contrastive. Inspired by the recent success of contrastive learning in visual-language pretraining <ref type="bibr" target="#b3">[4]</ref>, we also introduce this loss to our unified frameworks when fine-tuning for downstream video-text retrieval task. Specifically, for video-text pairs, we input video and text independently to the shared encoder to obtain high-level features. We then feed these features into a modality-specific projection head to project them into the shared embedding space. Following common practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref>, we use a symmetric (both text-to-video and video-to-text) contrastive loss based on these features. When doing retrieval tasks, we only need to extract unimodal features once.  <ref type="table">Table 1</ref>: Variants of our All-in-one architecture. The throughput is measured for videos at a resolution of 224?224. We use All-in-one-B as default without specific explanation. The ImNet is short for ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To explore model scalability, we use large-scale Webvid-2.5M <ref type="bibr" target="#b3">[4]</ref>, HowTo100M <ref type="bibr" target="#b28">[29]</ref> and YT-Temporal 180M <ref type="bibr" target="#b47">[48]</ref> for Pre-training. We evaluate All-in-one on four popular downstream videolanguage tasks: text-to-video retrieval, video question answering, multiple-choice and visual commonsense reasoning across 9 different datasets. We also provide extensive ablation studies to analyze the key factors that contribute to All-in-one's success, with insights and qualitative results. More tasks and datasets are reported in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Model Variants.</head><p>When considering the generality of All-in-one, we consider using three configurations based on ViT <ref type="bibr" target="#b8">[9]</ref> and DeiT <ref type="bibr" target="#b33">[34]</ref>, as summarized in Tab. 1. To simplify, we use brief notation to indicate the model size: for instance, All-in-one-B/16 means the "Base" variant with 16 ? 16 input patch size. Following ViLT <ref type="bibr" target="#b16">[17]</ref>, we use the bert-base-uncased tokenizer <ref type="bibr" target="#b7">[8]</ref> to tokenize text inputs. We random sample 3 frames and resize each frame to 224 ? 224. Then the patch projection of All-in-one yields 14 ? 14 = 196 patches for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Pre-training &amp; Fine-tuning.</head><p>Considering YT-Temporal 180M <ref type="bibr" target="#b47">[48]</ref> partially overlaps with HowTo100M <ref type="bibr" target="#b28">[29]</ref>, we pretrain on WebVid2.5M+Howto100M as default. If the model is trained on all three datasets, we named it as All-in-one *. Due to the storage limitation, we use the first half of the YT-Temporal 180M. We train all models using AdamW <ref type="bibr" target="#b26">[27]</ref> optimizer with a base learning rate of 10 ?4 and weight decay of 10 ?2 .</p><p>The learning rate was warmed up for 10% of the total training steps and was decayed linearly to zero for the rest of the training.</p><p>For pre-training, we train All-in-one-S and All-in-one-B for 200K steps on NVIDIA A100 GPUs with a batch size of 16 per GPU 3 . For All-in-one-Ti, we pre-train for 100K steps with a batch size of 32 per GPU, as we found it converges very fast. We adopt mixed precision technique <ref type="bibr" target="#b27">[28]</ref> to speed up the training process. As the domain gap between pre-train dataset and downstream visual commonsense reasoning dataset is large, we use batch size 512 and train for 100 epochs for this task. For the other downstream tasks, we train for 20 epochs with a batch size of 256. Note that downstream performance may be further improved if we customize the hyperparameters to each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Video-question Answering.</head><p>Datasets: In this work, we explore TGIF-QA <ref type="bibr" target="#b14">[15]</ref>, MSRVTT-QA <ref type="bibr" target="#b39">[40]</ref> and MSVD-QA <ref type="bibr" target="#b39">[40]</ref>. We For open-ended VQA, the answers are originally in free-form natural language, but it is a common practice to convert the task to a classification task by representing the answer with a class label. Following this practice, we add a two-layer MLP with hidden size 768 on the cls token. For multiple choice VQA (both the questions and candidates are sentences), we concatenate the question and candidates together, with</p><p>[SEP] to distinguish them. We select the candidate with maximum output logit of VTM head as prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Text-video Retrieval.</head><p>Datasets: MSRVTT <ref type="bibr" target="#b41">[42]</ref>, DiDeMo <ref type="bibr" target="#b2">[3]</ref>, ActivityNet Captions <ref type="bibr" target="#b17">[18]</ref> are utilized for this task. Evaluation. We initialize the similarity score head from the pre-trained ITM head, particularly the part that computes the true-pair logits. We train this task with both VTC and VTM since we find these two objectives boost each other. During inference, we simply feed each modality input independently and match pairs according to the cosine similarity of the output feature. In this way, we take advantage of high efficiency of dual-stream frameworks in retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Multiple-choice.</head><p>Datasets: In this task, we adopt MSRVTT multiple-choice test set <ref type="bibr" target="#b39">[40]</ref> and LSMDC multiple-choice test set <ref type="bibr" target="#b32">[33]</ref>. Evaluation: Given a video query and 5 candidate captions, the task is to find the one that fits the query out of 5 possible candidates. The correct answer is the ground-truth (GT) caption, and four other negatives are chosen from other captions that have different activity-phrase labels from the correct answer. We initialize the VTM head from the pretrained model on-top of the CLS token.</p><p>During the train, we simply concat each candidate with the given video together as input and only the correct answer is positive pair while the others negative pairs. We tune the model with cross-entropy loss to maximize the scores on positive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Visual Commonsense Reasoning.</head><p>VCR <ref type="bibr" target="#b46">[47]</ref> is a task and dataset where models must answer commonsense visual questions about images. This task test our model's ability to transfer its video-level understanding to single image. To solve this challenge task, VCR provides additional information to models (in the form of bounding boxes around entities), and explicit groundings between those entities and references in questions. Following previous efforts <ref type="bibr" target="#b47">[48]</ref>, we incorporate the location and identity information by drawing mask around the referenced entity. In this experiment, we compare three variations of our All-in-one to state-of-the-art methods from the literature. For multiple-choice VQA, we evaluate our All-in-one on two sub splits of TGIF-QA and report the result in Tab. 2. We find All-in-one especially good at this type of VQA. With ouly 1 frame input, our All-in-one-B outperforms previous VIOLET <ref type="bibr" target="#b10">[11]</ref> about 5.8% on the Action subset. Interestingly, we find more frames not benefit to Action and Transition but FrameQA. We also report the result of All-in-one on the three open-ended datasets. Surprisingly, even though Just-Ask <ref type="bibr" target="#b43">[44]</ref> is specifically designed for VQA and pretrined on large scale HowToVQA69M, our method still achieves a similar even better result than Just-Ask on MSVD-QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Retrieval Tasks.</head><p>In this experiment, we fine-tune All-in-one on MSRVTT, ActivityNet Caption and DiDeMo datasets. Tab. 3 summarizes results on text-to-video retrieval. In Tab. 3(a), All-in-one achieves significant performance gain over existing methods on MSRVTT retrieval in both 9K and 7K train setting. Compre with these related works, we only use one Cross-modality Encoder and the parameter is half of the Frozen <ref type="bibr" target="#b3">[4]</ref>. All-in-one even leads to 2.1% relative improvement on R@1 when compare with OA-Trans <ref type="bibr" target="#b34">[35]</ref>, which use additional offline object feature and only focus on retrieval. When adopt on LSMDC and DiDeMo dataset, our method also show competitive result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Multiple-choice.</head><p>Tab. 4 shows that All-in-one improves ClipBERT model by 3.2% on accuracy, on MSRVTT multiple choice test task. We also report the zero-shot performance for comparison, we find zero-shot accuracy already close to JSFusion <ref type="bibr" target="#b45">[46]</ref> in MSRVTT multiple choice with only three frames as input.   <ref type="bibr" target="#b47">[48]</ref>. <ref type="figure">Figure 5</ref>: Some examples from VCR <ref type="bibr" target="#b46">[47]</ref> dataset. We add different colors around the identities to be consistent with the identities in the Q&amp;A text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Visual Commonsense Reasoning.</head><p>After pre-training, we use a visual reasoning task to test the generality ability of our model. Our results on the VCR dataset, in comparison to other models at the same ("base") scale, are given in Tab. 5. Moreover, to utilize identity information, we also mask the different identity with different color (as shown in <ref type="figure">Fig. 5</ref>). We observe our model outperforms MERLOT clearly in the same setting with different sources of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Temporal Token Rolling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">The Variations of Temporal Modeling.</head><p>To better study Token Rolling, we also train our All-in-one in four different settings: Single Frame: Pre-training and inference with 1 frame. (a) The retrieval performance on MSR-VTT 9K and 7K training split. For Nets, "O" is object extractor. HowTo is short for HowTo100M <ref type="bibr" target="#b28">[29]</ref>. Notice that COCO <ref type="bibr" target="#b23">[24]</ref>, CC (short for Conceptual Captions <ref type="bibr" target="#b30">[31]</ref>) and VG (short for Visual Genome <ref type="bibr" target="#b18">[19]</ref>) are all image-text datasets, which are not suitable for temporal modeling during pre-training.  All-in-one-B     <ref type="table">Table 7</ref>: The text-to-video retrieval (T2V) and video-to-text retrieval (V2T) results with different objectives on finetuning for retrieval.  In order to understand how many tokens are needed to roll during pre-training, we conduct an ablation study as indicated in Tab. <ref type="bibr">8 (b)</ref>. We follow the All-in-one protocol in the pre-training setup except for a smaller 1024 batch size and 100K steps. Compared to the temporal average baseline (ratio equals to 0), we observe an amazing 5.69% improvement. The benefits come from more effective temporal modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">The Variations of Initialization.</head><p>To answer the question if initialization is crucial for large-scale VLP. We initialize All-in-one with three versions: Scratch, ImageNet and ImageNet-21K. We report the results in Tab. 6 with different train iterations and make following observations: i. Train from scratch convergence slower than train from ImageNet pretrained model. ii. The combination of Webvid2.5M and Howto100M is large enough to train the model from scratch. When training our All-in-one for 800K steps, we find the train from scratch is close to the ImageNet-21K initialized version in both Pre-training and downstream evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Objectives of Retrieval</head><p>To study the effect of objectives during fine-tuning, we experiment with three different combination of objectives. As shown in Tab. 7, the VTC loss have high R@1 and VTM is more effective in R@5 and R@10. With the combination of three objectives, All-in-one can achieve best performance in all measurement. Notice that these methods are trained in a dual-stream way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization</head><p>To better understand the pre-trained All-in-one, we analyze its internal representations. Specifically, given paired ground truth text and raw video, we mask some keywords (both verb and nouns) and ask the model to predict these masked words and further find out which video patch has strong correlations with the masked words. We use optimal transports <ref type="bibr" target="#b38">[39]</ref> to calculate the correlation between video and text. We only show the attention weight that is larger than the given threshold and give some examples of cross-modal alignment in <ref type="figure">Fig. 6</ref>. We find the model can predict correct nouns and verbs in most cases. Sometimes, it predicts the wrong word but with a similar meaning to the correct word. e.g. "guy" and "man". Benefiting from temporal modeling, we also find that the model attends to the motion regions for verbs like "waving" and "walking".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6:</head><p>Cloze evaluation: Given a video and its paired masked text, the model is asked to fill the masked words and show its corresponding high attention patch for this masked word. These samples are randomly sampled from the validation set of Webvid dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present the first unified end-to-end Video-Language Pre-training architecture with raw video and text as input, All-in-one Transformer. By learning only one fusion network, All-in-one is able to complete with a large number of counterparts equipped with additional heavy off-the-shelf video visual embedding networks and holds promise for the future. We hope that the VLP community will focus more on lightweight end-to-end modal interactions within Transformer modules, rather than only on heavier single-modality embedders or larger fusion models. While these initial results are encouraging, this new design of unified video-language interaction also brings new challenges, in particular fine-grained word region alignment. Furthermore, the temporal modeling is still not fully explored and we also encourage future work to use All-in-one for single-modality tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation Study</head><p>In this work Masked Language Modeling (MLM) and Video-text Matching (VTM) are modeled as binary and multiple classes classification tasks, correspondingly. So we also report top-1 classification accuracy (%) for these two pre-training objectives as reference in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 The Variant of Temporal Modeling</head><p>In addition to the parameter-free time token rolling operation proposed in this work, we also try different ways for temporal modeling: i. TimeSformer: For each self-attention block in All-in-one, we add a additional divided space attention and time attention before multi-head self-attention layer. As shown in the middle of <ref type="figure" target="#fig_7">Fig. 8</ref>. ii. Decouple Attn: For text modality and visual modality, we conduct self-attention independently first and then concatenate them again for cross-modality attention. As shown in the right of <ref type="figure" target="#fig_7">Fig. 8</ref>. Pre-training on Webvid2.5M + HowTo100M, we report both the pretrain and downstream evaluation performance in Tab. 9. With more parameters in visual processing, we find that TimeSformer and Decouple Attn are particularly good at Video-text Matching, but not good at Masked Language Modeling. However, we find that these methods are difficult to train, cost about 2-3 times more expensive than All-in-one-B, and show worse results on downstream zero-shot tasks.</p><p>B.2 Do we need to sample more frames?</p><p>To further understand the relation between the number of frames and the quality of learned representation, we conduct pretrain and finetune experiments by varying the sampled frames for train.  <ref type="table">Table 9</ref>: The variations of temporal modeling in the unified All-in-one. PT is short for pre-training and we report the zero-shot multiple-choice result. Considering the memory consumption, we vary the frames from 1 to 16. The difference between the default setting is that we pretrain on Webvid2.5M. We report both the pretrain VTM and MLM accuracy and downstream zero-shot multiple-choice accuracy on both LSMDC and MSRVTT in <ref type="figure" target="#fig_8">Fig.  9</ref>. We find that more frames leads to better results in general and All-in-one is already close to the best results when frames equals to 3. To balance the computation cost and performance, we use 3 frames as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Position Embedding and Modality Type Embedding</head><p>In this experiment, we explore the effect of Position Embedding and Modality Type Embedding. The results are given in Tab. 10, we observe spatio-temporal position embedding help the VTM and modality type embedding helps the MLM pretext. The combination of these two embedding leads to better results on both downstream zero-shot multiple choice result with limited parameters.  In this experiment, we explore where to add our temporal rolling layer. Specifically, All-in-one-B contains 12 self-attention blocks and we add the Token Rolling Layers from the beginning, the 3rd block and the 6th block. The results are report in the left of Tab. 8. We observe more temporal token rolling layers leads to better representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 The Sampling Strategy of Rolled Token</head><p>In this experiment, we explore the sampling strategy of sampling token. We try three versions: i. (b) Different ways of token selection. <ref type="table">Table 11</ref>: The ablation study on zero-shot multiple-choice.</p><p>the right of Tab. 11, selecting the 25% tokens leads to best result. We guess this is due to the multiple perceptron is position sensitive and the random selection or varying layers will lost this information.</p><p>In this work, we adopt block selection as default.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Transferability Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Action Recognition via Linear Probe</head><p>To evaluate the transfer ability of our model on single-modality task. We transfer the learned representation to downstream linear probe result on K400 and HMDB51 dataset. Specifically, we frozen the overall unified model and only learns linear layers based on the cls token of the last layer.</p><p>By pre-training model on these two datasets, we compare the base model with Time Average and previous best method Frozen <ref type="bibr" target="#b3">[4]</ref>.</p><p>The linear probe results are given in Tab. 12. We observe the number of frames have large impact on this task. When adopt same 8 frames, our All-in-one-B clearly outperforms Frozen <ref type="bibr" target="#b3">[4]</ref> especially on large-scale K400 dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Extension to Egocentric Video</head><p>Ego-4d <ref type="bibr" target="#b13">[14]</ref> is a egocentric dataset that has large domain gap with our third-view video from Youtube. We show some examples from Ego-4d in <ref type="figure" target="#fig_0">Fig. 10</ref> and test multiple-choice (5 choices) task on this dataset. We report both the zero-shot result and fine-tune result in Tab. 13. Compared to other multiple-choice benchmarks such as LSMDC and MSR-VTT, zero-shot accuracy is lower, but our All-in-one still outperforms Frozen <ref type="bibr" target="#b3">[4]</ref> clearly by half the parameters in this challenge benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Complexity Analysis of Retrieval</head><p>Due to the specifiy design for contrastive loss, Our All-in-one has a very fast inference running time even for retrieval on million-scale datasets. We use the popular similarity search/ranking library FAISS-GPU open-source library on a server with 8 A100 GPUs and 88 Kernel Intel(R) Xeon(R)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visualization (Cont'd)</head><p>In addition to the person-centric videos, we also visualize some samples about outdoor scene and objects in <ref type="figure" target="#fig_0">Fig. 11</ref>. For find our model can make correct prediction of masked words. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Compare to mainstream video-language pre-training methods. (a). Conventional methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The token rolling vs. flatten. By simply rolling tokens, the computation complex for Self-attention is around one third of Flatten. Not only learns correspondence cross-modality but also inter-frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>experiment with 3</head><label>3</label><figDesc>TGIF-QA tasks: Repeating Action and State Transition for multiple-choice QA, and Frame QA for open-ended QA. Both MSRVTT-QA and MSVD-QA are open-ended VQA. Evaluation: VQA requires answering questions according to the context of the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 3</head><label>3</label><figDesc>Comparing to State-of-the-art 4.3.1 Video-question Answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>All-in-one-B (zero-shot) 3 56.3 (b) LSMDC multiple-choice test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>The parameters &amp; performance over ten downstream datasets with the varying of model size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>TimeSformer and Decouple Attn for temporal modeling. We mainly show the selfattention block of Transformer for simply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Both pretrain and finetune performance with the varying of input frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Some egocentric videos in Ego4d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Cloze evaluation on Outdoor Scene and Animal examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CE" is cross-modality encoder. 384 means the resolution is 384 ? 384 for each frame while the default is 224 ? 224.</figDesc><table><row><cell>Method</cell><cell>Nets</cell><cell cols="4">Params PT Samples Frames</cell><cell cols="2">Action</cell><cell>Transition</cell><cell>FrameQA</cell></row><row><cell cols="2">Heterogeneous [10] T+V+LSTM</cell><cell>-</cell><cell></cell><cell>-</cell><cell>35</cell><cell></cell><cell>73.9</cell><cell>77.8</cell><cell>53.8</cell></row><row><cell>HCRN [20]</cell><cell>T+V+LSTM</cell><cell>-</cell><cell></cell><cell>-</cell><cell>16</cell><cell></cell><cell>75.0</cell><cell>81.4</cell><cell>55.9</cell></row><row><cell>QueST [16]</cell><cell>T+V+LSTM</cell><cell>-</cell><cell></cell><cell>-</cell><cell>16</cell><cell></cell><cell>75.9</cell><cell>81.0</cell><cell>59.7</cell></row><row><cell>ClipBERT [21]</cell><cell>T+V+CE</cell><cell>137M</cell><cell></cell><cell>5.6M</cell><cell>1 ? 1</cell><cell></cell><cell>82.9</cell><cell>87.5</cell><cell>59.4</cell></row><row><cell>VIOLET [11]</cell><cell>T+V+CE</cell><cell>198M</cell><cell></cell><cell>5.5M</cell><cell>16</cell><cell></cell><cell>87.1</cell><cell>93.6</cell><cell>-</cell></row><row><cell>All-in-one-Ti</cell><cell>CE</cell><cell>12M</cell><cell></cell><cell>3.72M</cell><cell>3</cell><cell></cell><cell>80.6</cell><cell>83.5</cell><cell>53.9</cell></row><row><cell>All-in-one-S</cell><cell>CE</cell><cell>33M</cell><cell></cell><cell>3.72M</cell><cell>3</cell><cell></cell><cell>91.2</cell><cell>92.7</cell><cell>64.0</cell></row><row><cell>All-in-one-B</cell><cell>CE</cell><cell>110M</cell><cell></cell><cell>3.72M</cell><cell>1</cell><cell cols="3">92.9 (5.8?) 94.2 (0.6?) 62.5 (3.1?)</cell></row><row><cell>All-in-one-B</cell><cell>CE</cell><cell>110M</cell><cell></cell><cell>3.72M</cell><cell>3</cell><cell cols="3">92.7 (5.6?) 94.3 (0.7?) 64.2 (4.8?)</cell></row><row><cell>All-in-one-B [384]</cell><cell>CE</cell><cell>110M</cell><cell></cell><cell>3.72M</cell><cell>3</cell><cell></cell><cell>94.7</cell><cell>95.1</cell><cell>65.4</cell></row><row><cell>All-in-one-B *</cell><cell>CE</cell><cell>110M</cell><cell></cell><cell>9.72M</cell><cell>3</cell><cell></cell><cell>95.5</cell><cell>94.7</cell><cell>66.3</cell></row><row><cell cols="9">(a) Three sub-tasks on TGIF-QA test set (the first row are methods w/o. pre-training). "T" refers to</cell></row><row><cell cols="4">text encoder, "V" is video encoder and "Method Frames</cell><cell>Accuracy</cell><cell>Method</cell><cell></cell><cell>Frames</cell><cell>Accuracy</cell></row><row><cell>AMU [41]</cell><cell></cell><cell>16</cell><cell></cell><cell>32.5</cell><cell>QueST [16]</cell><cell></cell><cell>10</cell><cell>36.1</cell></row><row><cell cols="2">Heterogeneous [10]</cell><cell>35</cell><cell></cell><cell>33.0</cell><cell>HCRN [20]</cell><cell></cell><cell>16</cell><cell>36.1</cell></row><row><cell cols="2">HCRN [20]</cell><cell>16</cell><cell></cell><cell>35.6</cell><cell>SSML [2]</cell><cell></cell><cell>16</cell><cell>35.1</cell></row><row><cell cols="2">ClipBERT [21]</cell><cell>4 ? 2</cell><cell></cell><cell>37.4</cell><cell cols="2">CoMVT [30]</cell><cell>30</cell><cell>42.6</cell></row><row><cell cols="2">VIOLET [11]</cell><cell>16</cell><cell></cell><cell>43.1</cell><cell cols="2">Just-Ask  ? [44]</cell><cell>32</cell><cell>46.3</cell></row><row><cell cols="2">All-in-one-S</cell><cell>3</cell><cell></cell><cell>39.5</cell><cell>All-in-one-S</cell><cell></cell><cell>3</cell><cell>41.7</cell></row><row><cell cols="2">All-in-one-B</cell><cell>3</cell><cell cols="2">42.9 (0.2?)</cell><cell cols="2">All-in-one-B</cell><cell>3</cell><cell>46.5 (0.2?)</cell></row><row><cell cols="2">All-in-one-B</cell><cell>3 ? 3</cell><cell cols="2">44.3 (1.2?)</cell><cell cols="2">All-in-one-B</cell><cell>3 ? 3</cell><cell>47.9 (1.6?)</cell></row><row><cell cols="2">All-in-one-B *</cell><cell>3</cell><cell></cell><cell>46.8</cell><cell cols="2">All-in-one-B *</cell><cell>3</cell><cell>48.3</cell></row><row><cell cols="5">(b) MRSVTT-QA test set.</cell><cell></cell><cell></cell><cell></cell></row></table><note>(c) MSVD-QA test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison with state-of-the-art methods on VQA. The columns with gray color are open- ended VQA and the others are multiple-choice VQA. ? means use additional large-scale VQA dataset HowToVQA60M [44] for pretraining.? means pre-training with additional YT-Temporal 180M</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Time Average: Pre-training with 1 frame but inference with 3 frames. Time Average ?: Pre-training and inference with 3 frames. Channel Shift: we replace each</figDesc><table><row><cell>Method</cell><cell>Nets</cell><cell>PT Data</cell><cell cols="2">Params Frames</cell><cell></cell><cell>9K Train</cell><cell></cell><cell></cell><cell>7K Train</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>ActBERT [50]</cell><cell>T+O+V+CE</cell><cell>HowTo</cell><cell>275M</cell><cell>32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16.3</cell><cell>42.8</cell><cell>56.9</cell></row><row><cell>ClipBERT [21]</cell><cell>T+V+CE</cell><cell>COCO+VG</cell><cell>137M</cell><cell>8 ? 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>22.0</cell><cell>46.8</cell><cell>59.9</cell></row><row><cell>TACo [45]</cell><cell>T+V+CE</cell><cell>HowTo</cell><cell>212M</cell><cell>48</cell><cell>28.4</cell><cell>57.8</cell><cell>71.2</cell><cell>24.8</cell><cell>52.1</cell><cell>64.0</cell></row><row><cell>VIOLET [11]</cell><cell>T+V+CE</cell><cell>CC+WebVid</cell><cell>198M</cell><cell>16</cell><cell>34.5</cell><cell>63.0</cell><cell>73.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Frozen [4]</cell><cell>T+V</cell><cell>CC+WebVid</cell><cell>232M</cell><cell>8</cell><cell>31.0</cell><cell>59.5</cell><cell>70.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OA-Trans [35]</cell><cell>T+O+V</cell><cell>CC+WebVid</cell><cell>232M</cell><cell>8</cell><cell>35.8</cell><cell>63.4</cell><cell>76.5</cell><cell>32.1</cell><cell>61.0</cell><cell>72.9</cell></row><row><cell>All-in-one-B</cell><cell>CE</cell><cell>HowTo</cell><cell>110M</cell><cell>3</cell><cell>29.5</cell><cell>63.3</cell><cell>71.9</cell><cell>26.5</cell><cell>59.4</cell><cell>69.8</cell></row><row><cell>All-in-one-B</cell><cell>CE</cell><cell>HowTo+WebVid</cell><cell>110M</cell><cell>3</cell><cell>37.1</cell><cell>66.7</cell><cell>75.9</cell><cell>33.8</cell><cell>64.2</cell><cell>74.3</cell></row><row><cell>All-in-one-B</cell><cell>CE</cell><cell>HowTo+WebVid</cell><cell>110M</cell><cell>3 ? 3</cell><cell>37.9</cell><cell>68.1</cell><cell>77.1</cell><cell>34.4</cell><cell>65.4</cell><cell>75.8</cell></row></table><note>Token Rolling layer with channel shift operation. Flatten: As presented in Sec. 3.2. We observe: i. Pre-training with multiple frames are essential for SSL tasks. e.g, from 35.16 to 47.33 on LSMDC. ii. The Token Rolling boosted an amazing 5.42% improvement over time average baseline. Compared with channel shift [23], the rolling on tokens also show superior performance. We guess VLP require learn alignment between patches and the channel operation will erase this boundary. iii. Even the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Frames</cell><cell>Accuracy</cell><cell>Method</cell><cell>Frames</cell><cell>Accuracy</cell></row><row><cell>JSFusion [46]</cell><cell>40</cell><cell>83.4</cell><cell>JSFusion [46]</cell><cell>40</cell><cell>73.5</cell></row><row><cell>ActBERT [50]</cell><cell>32</cell><cell>85.7</cell><cell>MERLOT [48]</cell><cell>8</cell><cell>81.7</cell></row><row><cell>ClipBERT [21]</cell><cell>8 ? 2</cell><cell>88.2</cell><cell>VIOLET [11]</cell><cell>16</cell><cell>82.9</cell></row><row><cell>All-in-one-B</cell><cell>3</cell><cell>91.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All-in-one-B</cell><cell>3 ? 3</cell><cell>92.0 (3.8?)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All-in-one-B *</cell><cell>3</cell><cell>92.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All-in-one-B (zero-shot)</cell><cell>3</cell><cell>80.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(a) MRSVTT multiple-choice test.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison with state-of-the-art methods on text-to-video retrieval. We gray out dual-stream networks that only do retrieval tasks. Notice that OA-Trans [35] uses additional offline object features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods on multiple-choice task.computation complex of Flatten is three times as Token Rolling, the performance of Token Rolling is slightly better than Flatten in both setting. As we discussed in Sec. 3.2, the benefits might come from the Token Rolling is a natural extension of self-attention among patches.</figDesc><table><row><cell>Method</cell><cell>PT Data</cell><cell>Mask</cell><cell>Accuracy</cell></row><row><cell cols="2">MERLOT [48] CC3M+COCO</cell><cell>?</cell><cell>58.9</cell></row><row><cell>MERLOT [48]</cell><cell>HowTo100M</cell><cell>?</cell><cell>66.3</cell></row><row><cell>All-in-one-B</cell><cell>CC3M+COCO</cell><cell>?</cell><cell>60.5 (1.6?)</cell></row><row><cell>All-in-one-B</cell><cell>HowTo100M</cell><cell></cell><cell>65.2</cell></row><row><cell>All-in-one-B</cell><cell>HowTo100M</cell><cell>?</cell><cell>68.4 (2.1?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The visual commonsense reasoning result with different source of pre-training data.</figDesc><table><row><cell>Initialization</cell><cell cols="4">Steps MLM ITM LSMDC</cell></row><row><cell>-</cell><cell>200K</cell><cell>58.7</cell><cell>89.8</cell><cell>51.4</cell></row><row><cell>-</cell><cell>800K</cell><cell>62.3</cell><cell>91.5</cell><cell>55.5</cell></row><row><cell>ImageNet</cell><cell>200K</cell><cell>60.3</cell><cell>90.5</cell><cell>52.2</cell></row><row><cell cols="2">ImageNet-21K 200K</cell><cell>60.5</cell><cell>90.4</cell><cell>52.5</cell></row><row><cell cols="2">ImageNet-21K 800K</cell><cell>62.2</cell><cell>92.1</cell><cell>55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">VTM VTC</cell><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DiDeMo</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>T2V</cell><cell></cell><cell></cell><cell>V2T</cell><cell></cell><cell></cell><cell>T2V</cell><cell></cell><cell></cell><cell>V2T</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="12">R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell></cell><cell>?</cell><cell>36.7</cell><cell>62.6</cell><cell>72.2</cell><cell>36.4</cell><cell>62.9</cell><cell>71.9</cell><cell>29.6</cell><cell>47.3</cell><cell>56.67</cell><cell>29.6</cell><cell>48.0</cell><cell>57.2</cell></row><row><cell>?</cell><cell></cell><cell>33.3</cell><cell>67.0</cell><cell>76.9</cell><cell>34.4</cell><cell>66.3</cell><cell>76.6</cell><cell>30.2</cell><cell>57.5</cell><cell>68.7</cell><cell>27.7</cell><cell>59.6</cell><cell>69.5</cell></row><row><cell>?</cell><cell>?</cell><cell>37.1</cell><cell>66.7</cell><cell>75.9</cell><cell>37.5</cell><cell>66.1</cell><cell>77.4</cell><cell>31.2</cell><cell>59.5</cell><cell>72.1</cell><cell>30.4</cell><cell>58.3</cell><cell>69.4</cell></row></table><note>Comparison with different ini- tialization for All-in-one-B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>The ablation study on zero-shot mutiple-choice. PF is short for pre-training frames and DF is short for downstream frames.</figDesc><table /><note>4.4.2 Ablation on the Rolling Token Ratio.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>The input analysis of All-in-one-B. SPE is short for Spatio-temporal Position Embedding and MTE is short for Modality Type Embedding.</figDesc><table><row><cell>C In-depth Analysis of Token Rolling</cell></row></table><note>C.1 The Position of Temporal Token Rolling Layers</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>The linear probe results on action recognition benchmarks with three frames over kinetics 400 and hmdb51 datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>The multiple-choice result on first-view ego-4d benchmark.Platinum 8255C CPU @ 2.50GHz. Given a new query,Table 14below shows the time needed for visual encoding, textual encoding, and similarity ranking (1st row for thousand-scale and 2nd row for million-scale). Given a new query, the total search time on HowTo100M is (12.05 + 143.25 = 155.3) ms for text-to-video retrieval and (33.16 + 143.25 = 176.41) ms for video-to-text retrieval, which is acceptable in practice.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Samples Visual Embedding Text Embedding Similarity Ranking</cell></row><row><cell>MSR</cell><cell>1K</cell><cell>33.16 ms</cell><cell>12.05ms</cell><cell>1.51ms</cell></row><row><cell cols="2">HowTo100M 128.94M</cell><cell>33.16ms</cell><cell>12.05ms</cell><cell>143.25ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Running time analysis for All-in-one during retrieval/inference. Numbers are averaged over 1000 runs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The length of text tokens m much smaller than video tokens n in general.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">32 GPUs take 7 days total, 128 GPUs take less than 2 days total.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This project is supported by the National Research Foundation, Singapore under its NRFF award NRF-NRFF13-2021-0008. We would like to thank David Junhao Zhang for his kindly help on Transformer training.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we first evaluate the model scalability and provide more ablation studies about All-in-one. Then we transfer All-in-one to more downstream tasks and datasets. At last, we provide retrieval efficiency and more visualization analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Scalability</head><p>In this experiment, we evaluate the model from All-in-one-Ti to All-in-one-L using three assessment tasks: Fine-tune, Zero-shot and Linear Probe. Zero-shot means we directly test the pretrained model on downstream task without fine-tining, Linear Probe means we frozen the overall model and only the last linear layer is learned on downstream tasks. We varying the model size from 13M to 320M and do evaluation on 10 different datasets. For fair comparison, the pre-training and fine-tuning settings are consistent for models of different scales. The results are presented in <ref type="figure">Fig. 7</ref> and we make the following observations:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI. vol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1999" to="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Violet: End-to-end videolanguage transformers with masked visual-token modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12681</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bridgeformer: Bridging video-text retrieval with multiple choice questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08377</idno>
		<title level="m">Omnivore: A single model for many visual modalities</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07058</idno>
		<title level="m">Ego4d: Around the world in 3,000 hours of egocentric video</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Divide and conquer: Question-guided spatiotemporal contextual attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11101" to="11108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9972" to="9981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Mixed precision training</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Look before you speak: Visually contextualized utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16877" to="16887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object-aware video-language pre-training for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ufo: A unified transformer for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10023</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<title level="m">Actionclip: A new paradigm for video action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A fast proximal point method for computing exact wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in artificial intelligence</title>
		<imprint>
			<biblScope unit="page" from="433" to="453" />
			<date type="published" when="2020" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01194</idno>
		<title level="m">Video-text pre-training with learned regions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Just ask: Learning to answer questions from millions of narrated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1686" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Taco: Token-aware cascade contrastive learning for video-text alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11562" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Merlot: Multimodal neural script knowledge models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">For Zero-shot and Linear Probe task, we observe large model leads to better result in general. ii. However, we find sometimes All-in-one-L on the Fine-tune task leads to worse result than All-inone-B in several benchmarks (circled). We show the train curve of MSRVTT-QA in the right of Fig</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">For more large dataset like TVQA and TGIF-QA (ten times larger than MSRVTT-QA), All-in-one-L still lead to better results. We conclude that simply pursuing larger models is not suitable for all cases, especially for fine-tuning on small scale, and All-in-one-B is a better choice in most cases</title>
	</analytic>
	<monogr>
		<title level="m">Since MSRVTT-QA only contains 10K video-text pairs, we find that the model severely overfits the dataset with few iterations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
