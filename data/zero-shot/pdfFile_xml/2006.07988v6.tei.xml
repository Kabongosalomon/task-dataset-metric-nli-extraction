<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
							<email>ichien3@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
							<email>jianhao2@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
							<email>panli@purdue.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
							<email>milenkov@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-ofthe-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data. Our implementation is available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-centered machine learning has received significant interest in recent years due to the ubiquity of graph-structured data and its importance in solving numerous real-world problems such as semisupervised node classification and graph classification <ref type="bibr" target="#b44">(Zhu, 2005;</ref><ref type="bibr" target="#b33">Shervashidze et al., 2011;</ref><ref type="bibr" target="#b25">L? &amp; Zhou, 2011)</ref>. Usually, the data at hand contains two sources of information: Node features and graph topology. As an example, in social networks, nodes represent users that have different combinations of interests and properties captured by their corresponding feature vectors; edges on the other hand document observable friendship and collaboration relations that may or may not depend on the node features. Hence, learning methods that are able to simultaneously and adaptively exploit node features and the graph topology are highly desirable as they make use of their latent connections and thereby improve learning on graphs.</p><p>Graph neural networks (GNN) leverage their representational power to provide state-of-the-art performance when addressing the above described application domains. Many GNNs use message passing <ref type="bibr" target="#b13">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b3">Battaglia et al., 2018)</ref> to manipulate node features and graph topology. They are constructed by stacking (graph) neural network layers which essentially propagate and transform node features over the given graph topology. Different types of layers have been proposed and used in practice, including graph convolutional layers (GCN) <ref type="bibr" target="#b7">(Bruna et al., 2014;</ref><ref type="bibr" target="#b19">Kipf &amp; Welling, 2017)</ref>, graph attention layers (GAT) <ref type="bibr" target="#b36">(Velickovic et al., 2018)</ref> and many others <ref type="bibr" target="#b15">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b38">Wijesinghe &amp; Wang, 2019;</ref><ref type="bibr" target="#b42">Zeng et al., 2020;</ref><ref type="bibr" target="#b2">Abu-El-Haija et al., 2019)</ref>.</p><p>However, most of the existing GNN architectures have two fundamental weaknesses which restrict their learning ability on general graph-structured data. First, most of them seem to be tailor-made to work on homophilic (associative) graphs. The homophily principle <ref type="bibr" target="#b27">(McPherson et al., 2001)</ref> in the context of node classification asserts that nodes from the same class tend to form edges. Homophily is also a common assumption in graph clustering <ref type="bibr" target="#b37">(Von Luxburg, 2007;</ref><ref type="bibr" target="#b35">Tsourakakis, 2015;</ref><ref type="bibr" target="#b9">Dau &amp; Milenkovic, 2017)</ref> and in many GNNs design <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>. Methods developed for homophilic graphs are nonuniversal in so far that they fail to properly solve learning problems on heterophilic (disassortative) graphs <ref type="bibr" target="#b29">(Pei et al., 2019;</ref><ref type="bibr" target="#b5">Bojchevski et al., 2019;</ref>. In heterophilic graphs, nodes with distinct labels are more likely to link together (For example, many people tend to preferentially connect with people of the opposite sex in dating graphs, different classes of amino acids are more likely to connect within many protein structures <ref type="bibr" target="#b43">(Zhu et al., 2020)</ref> etc). GNNs model the homophily principle by aggregating node features within graph neighborhoods. For this purpose, they use different mechanisms such as averaging in each network layer. Neighborhood aggregation is problematic and significantly more difficult for heterophilic graphs <ref type="bibr" target="#b17">(Jia &amp; Benson, 2020)</ref>.</p><p>Second, most of the existing GNNs fail to be "deep enough". Although in principle an arbitrary number of layers may be stacked, practical models are usually shallow (including 2-4 layers) as these architectures are known to achieve better empirical performance than deep networks. A widely accepted explanation for the performance degradation of GNNs with increasing depth is feature-oversmoothing, which may be intuitively explained as follows. The process of GNN feature propagating represents a form of random walks on "feature graphs," and under proper conditions, such random walks converge with exponential rate to their stationary points. This essentially levels the expressive power of the features and renders them nondiscriminative. This intuitive reasoning was first described for linear settings in  and has been recently studied in <ref type="bibr" target="#b28">Oono &amp; Suzuki (2020)</ref> for a setting involving nonlinear rectifiers. We address these two described weaknesses by combining GNNs with Generalized PageRank techniques (GPR) within a new model termed GPR-GNN. The GPR-GNN architecture is designed to first learn the hidden features and then to propagate them via GPR techniques. The focal component of the network is the GPR procedure that associates each step of feature propagation with a learnable weight. The weights depend on the contributions of different steps during the information propagation procedure, and they can be both positive and negative. This departures from common nonnegativity assumptions <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> allows for the signs of the weights to adapt to the homophily/heterophily structure of the underlying graphs. The amplitudes of the weights trade-off the degree of smoothing of node features and the aggregation power of topological features. These traits do not change with the choice of the initialization procedure and elucidate the process used to combine node features and the graph structure so as to achieve (near)-optimal predictions. In summary, the GPR-GNN method can simultaneously learn the node label patterns of disparate classes of graphs and prevent feature over-smoothing.</p><p>The excellent performance of GPR-GNN is demonstrated empirically, on real world datasets, and further supported through a number of theoretical findings. In the latter setting, we show that the GPR procedure relates to general polynomial graph filtering, which can naturally deal with both high and low frequency parts of the graph signals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights <ref type="bibr" target="#b39">(Wu et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al., 2018;</ref> inevitably act as low-pass filters. Thus, they fail to learn the labels of heterophilic graphs. We also establish that GPR-GNN can provably mitigate the feature-over-smoothing issue in an adaptive manner even after large-step propagation (i.e., after a large number of propagation steps). Hence, the method is able to make use of informative large-step propagation.</p><p>To test the performance of GPR-GNN on homophilic and heterophilic node label patterns and determine the trade-off between node and topological feature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type="bibr" target="#b11">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controlling the "informativeness ratio" between node features and graph topology, The learnt GPR weights of the GPR-GNN on real world datasets. Cora is homophilic while Texas is heterophilic (Here, H stands for the level of homophily defined below). An interesting trend may be observed: For the heterophilic case the weights alternate from positive to negative with dampening amplitudes (more examples are provided in Section 5). The shaded region corresponds to a 95% confidence interval.</p><p>where the graph can vary from being highly homophilic to highly heterophilic. We show that GPR-GNN outperforms all other baseline methods for the task of semi-supervised node classification on the cSBM consistently from strong homophily to strong heterophily. We then proceed to show that GPR-GNN offers state-of-the-art performance on node-classification benchmark real-world datasets which contain both homophilic and heterophilic graphs. Due to the space limit, we put all proofs, formal theorem statements, and the conclusion section in the Supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Let G = (V, E) be an undirected graph with nodes V and edges E. Let n denote the number of nodes, assumed to belong to one of C ? 2 classes. The nodes are associated with the node feature matrix X ? R n?f , where f denotes the number of features per node. Throughout the paper, we use X i: to indicate the i th row and X :j to indicate the j th column of the matrix X, respectively. The symbol ? ij is reserved for the Kronecker delta function. The graph G is described by the adjacency matrix A, while? stands for the adjacency matrix for a graph with added self-loops. We letD be the diagonal degree matrix of? and? sym =D ?1/2?D?1/2 denote the symmetric normalized adjacency matrix with self-loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GPR-GNNS: MOTIVATION AND CONTRIBUTIONS</head><p>Generalized PageRanks. Generalized PageRank (GPR) methods were first used in the context of unsupervised graph clustering where they showed significant performance improvements over <ref type="bibr">Personalized PageRank (Kloumann et al., 2017;</ref><ref type="bibr" target="#b23">Li et al., 2019)</ref>. The operational principles of GPRs can be succinctly described as follows. Given a seed node s ? V in some cluster of the graph, a one-dimensional feature vector</p><formula xml:id="formula_0">H (0) ? R n?1 is initialized according to H (0) v: = ? vs . The GPR score is defined as ? k=0 ? k? k sym H (0) = ? k=0 ? k H (k)</formula><p>, where the parameters ? k ? R, k = 0, 1, 2, . . ., are referred to as the GPR weights. Clustering of the graph is performed locally by thresholding the GPR score. Certain PangRank methods, such as Personalized PageRank or heat-kernel PageRank <ref type="bibr" target="#b8">(Chung, 2007)</ref>, are associated with specific choices of GPR weights <ref type="bibr" target="#b23">(Li et al., 2019)</ref>. For an excellent in-depth discussion of PageRank methods, the interested reader is referred to <ref type="bibr" target="#b14">(Gleich, 2015)</ref>. The work in <ref type="bibr" target="#b23">Li et al. (2019)</ref> recently introduced and theoretically analyzed a special form of GPR termed Inverse PR (IPR) and showed that long random walk paths are more beneficial for clustering then previously assumed, provided that the GPR weights are properly selected (Note that IPR was developed for homophilic graphs and optimal GPR weights for heterophilic graphs are not currently known).</p><p>Equivalence of the GPR method and polynomial graph filtering. If we truncate the infinite sum in the definition of GPR at some natural number K, K k=0 ? k? k sym corresponds to a polynomial graph filter of order K. Thus, learning the optimal GPR weights is equivalent to learning the optimal polynomial graph filter. Note that one can approximate any graph filter using a polynomial graph filter <ref type="bibr" target="#b34">(Shuman et al., 2013)</ref> and hence the GPR method is able to deal with a large range of different node label patterns. Also, increasing K allows one to better approximate the underlying optimal graph filter. This once again shows that large-step propagation is beneficial.</p><p>Universality with respect to node label patterns: Homophily versus heterophily. In their recent work, <ref type="bibr" target="#b29">Pei et al. (2019)</ref> proposed an index to measure the level of homophily of nodes in a graph H(G) = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|V | v?V</head><p>Number of neighbors of v ? V that have the same label as v Number of neighbors of v . Note that H(G) ? 1 corresponds to strong homophily while H(G) ? 0 indicates strong heterophily. <ref type="figure">Figures 1 (b)</ref> and (c) plot the GPR weights learnt by our GPR-GNN method on a homophilic (Cora) and heterophilic (Texas) dataset. The learnt GPR weights from Cora match the behavior of IPR <ref type="bibr" target="#b23">(Li et al., 2019)</ref>, which verifies that large-step propagation is indeed of great importance for homophilic graphs. The GPR weights learnt from Texas behave significantly differently from all known PR variants, taking a number of negative values. These differences in weight patterns are observed under random initialization, demonstrating that the weights are actually learned by the network and not forced by specific initialization. Furthermore, the large difference in the GPR weights for these two graph models illustrates the learning power of GPR-GNN and their universal adaptability.</p><p>The over-smoothing problem. One of the key components in most GNN models is the graph convolutional layer, described by</p><formula xml:id="formula_1">H (k) GCN = ReLU ? sym H (k?1) GCN W (k) ,P GCN = softmax ? sym H (K?1) GCN W (k) ,</formula><p>where H (0) GCN = X and W (k) represents the trainable weight matrix for the k th layer. The key issue that limits stacking multiple layers is the over-smoothing phenomenon: If one were to remove ReLU in the above expression, lim k??? k sym H (0) = H (?) , where each row of H (?) only depends on the degree of the corresponding node, provided that the graph is irreducible and aperiodic. This shows that the model looses discriminative information provided by the node features as the number of layers increases.</p><p>Mitigating graph heterophily and over-smoothing issues with the GPR-GNN model. GPR-GNN first extracts hidden state features for each node and then uses GPR to propagate them. The GPR-GNN process can be mathematically described as:</p><formula xml:id="formula_2">P = softmax(Z), Z = K k=0 ? k H (k) , H (k) =? sym H (k?1) , H (0) i: = f ? (X i: ),<label>(1)</label></formula><p>where f ? (.) represents a neural network with parameter set {?} that generates the hidden state features H (0) . The GPR weights ? k are trained together with {?} in an end-to-end fashion. The GPR-GNN model is easy to interpret: As already pointed out, GPR-GNN has the ability to adaptively control the contribution of each propagation step and adjust it to the node label pattern. Examining the learnt GPR weights also helps with elucidating the properties of the topological information of a graph (i.e., determining the optimal polynomial graph filter), as illustrated in <ref type="figure">Figure 1</ref>  Placing GPR-GNNs in the context of related prior work. Among the methods that differ from repeated stacking of GCN layers, APPNP <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> represents one of the state-of-theart GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC <ref type="bibr" target="#b39">(Wu et al., 2019)</ref> are special cases of our model since APPNP fixes ? k = ?(1??) k , ? K = (1? ?) K , while SGC removes all nonlinearities with ? k = ? kK , respectively. These two weight choices correspond to Personalized PageRank (PPR) <ref type="bibr" target="#b16">(Jeh &amp; Widom, 2003)</ref>, which is known to be suboptimal compared to the IPR framework when applied to homophilic node classification <ref type="bibr" target="#b23">(Li et al., 2019)</ref>. Fixing the GPR weights makes the model unable to adaptively learn the optimal propagation rules which is of crucial importance: As we will show in Section 4, the fixed PPR weights corresponds to low-pass graph filters which makes them inadequate for learning on heterophilic graphs. The recent work <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> showed that fixed PPR weights (APPNP) can also provably resolve the over-smoothing problem. However, the way APPNP prevents over-smoothing is independent on the node label information. In contrast, the escape of GPR-GNN from over-smoothing is guided by the node label information (Theorem 4.2). A detailed discussion of this phenomena along with illustrative examples is delegated to the Supplement.</p><p>Among the GCN-like models, JK-Net <ref type="bibr" target="#b40">(Xu et al., 2018)</ref> exhibits some similarities with GPR-GNN. It also aggregates the outputs of different GCN layers to arrive at the final output. On the other hand, the GCN-Cheby method <ref type="bibr" target="#b10">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b19">Kipf &amp; Welling, 2017</ref>) is related to polynomial graph filtering, where each convolutional layer propagates multiple steps and the graph filter is related to Chebyshev polynomials. In both cases, the depth of the models is limited in practice <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> and they are not easy to interpret as our GPR-GNN method. Some prior work also emphasizes adaptively learning the importance of different steps <ref type="bibr" target="#b1">(Abu-El-Haija et al., 2018;</ref><ref type="bibr" target="#b4">Berberidis et al., 2018)</ref>. Nevertheless, none of the above works is applicable for semisupervised learning with GNNs and considers heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL PROPERTIES OF GPR-GNNS</head><p>Graph filtering aspects of GPR-GNNs. As mentioned in Section 3, the GPR component of the network may be viewed as a polynomial graph filter. Let? sym = U?U T be the eigenvalue decomposition of? sym . Then, the corresponding polynomial graph filter equals</p><formula xml:id="formula_3">K k=0 ? k? k sym = Ug ?,K (?)U T , where g ?,K (?) is applied element-wise and g ?,K (?) = K k=0 ? k ? k . We estab- lished the following result. Theorem 4.1 (Informal). Assume that the graph G is connected. If ? k ? 0 ?k ? {0, 1, ..., K}, K k=0 ? k = 1 and ?k &gt; 0 such that ? k &gt; 0, then g ?,K (?) is a low-pass graph filter. Also, if ? k = (??) k , ? ? (0, 1) and K is large enough, then g ?,K (?) is a high-pass graph filter.</formula><p>By Theorem 4.1 and from our discussion in Section 3, we know that both APPNP and SGC will invariably suppress the high frequency components. Thus, they are inadequate for use on heterophilic graphs. In contrast, if one allows ? k to be negative and learned adaptively the graph filter will pass relevant high frequencies. This is what allows GPR-GNN to perform exceptionally well on heterophilic graphs (see <ref type="figure">Figures 1)</ref>.</p><p>GPR-GNN can escape from over-smoothing. As already emphasized, one crucial innovation of the GPR-GNN method is to make the GPR weights adaptively learnable, which allows GPR-GNN to avoid over-smoothing and trade node and topology feature informativeness. Intuitively, when largestep propagation is not beneficial, it increases the training loss. Hence, the corresponding GPR weights should decay in magnitude. This observation is captured by the following result, whose more formal statement and proof are delegated to the Supplement due to space limitations. Theorem 4.2 (Informal). Assume the graph G is connected and the training set contains nodes from each of the classes. Also assume that k is large enough so that the over-smoothing effect occurs for H (k) , ?k ? k which dominate the contribution to the final output Z. Then, the gradients of ? k and ? k are identical in sign for all k ? k .</p><p>Theorem 4.2 shows that as long as over-smoothing happens, |? k | will approach 0 for all k ? k when we use an optimizer such as stochastic gradient descent (SGD) which has a suitable learning rate decay. This reduces the contribution of the corresponding steps H (k) in the final output Z. When the weights |? k | are small enough so that H (k) no longer dominates the value of the final output Z, the over-smoothing effect is eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS FOR NEW CSBM SYNTHETIC AND REAL-WORLD DATASETS</head><p>Synthetic data. In order to test the ability of label learning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type="bibr" target="#b11">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consider the case with two equal-size classes. In cSBMs, the node features are Gaussian random vectors, where the mean of the Gaussian depends on the community assignment. The difference of the means is controlled by a parameter ?, while the difference of the edge densities in the communities and between the communities is controlled by a parameter ?. Hence ? and ? capture the "relative informativeness" of node features and the graph topology, respectively. Moreover, positive ? s correspond to homophilic graphs while negative ? s correspond to heterophilic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type="bibr" target="#b11">Deshpande et al. (2018)</ref>. The results show that, asymptotically, one needs ? 2 + ? 2 /? &gt; 1 to ensure a vanishing ratio of the misclassified nodes and the total number of nodes, where ? = n/f and f as before denotes the dimension of the node feature vector.</p><p>Note that given a tolerance value &gt; 0, ? 2 + ? 2 /? = 1 + is an arc of an ellipsoid for which ? ? 0 and ? ? 0. To fairly and continuously control the extent of information carried by the node features and graph topology, we introduce a parameter ? = arctan( ? ? ? ? ) ? 2 ? . The setting ? = 0 indicates that only node features are informative, while |?| = 1 indicates that only the graph topology is informative. Moreover, ? = 1 corresponds to strongly homophilic graphs while ? = ?1 corresponds to strongly heterophilic graphs. Note that the values ? and ?? convey the same amount of information regarding graph topology. This is due to the fact that ? 2 = (??) 2 . Ideally, GNNs that are able to optimally learn on both homophilic and heterophilic graph should have similar performances for ? and ??. Due to space limitation we refer the interested reader to <ref type="bibr" target="#b11">(Deshpande et al., 2018)</ref> for a review of all formal theoretical results and only outline the cSBM properties needed for our analysis. Additional information is also available in the Supplement.</p><p>Our experimental setup examines the semi-supervised node classification task in the transductive setting. We consider two different choices for the random split into training/validation/test samples, which we call sparse splitting (2.5%/2.5%/95%) and dense splitting (60%/20%/20%), respectively. The sparse splittnig is more similar to the original semi-supervised setting considered in <ref type="bibr" target="#b19">Kipf &amp; Welling (2017)</ref> while the dense setting is considered in <ref type="bibr" target="#b29">Pei et al. (2019)</ref> for studying heterophilic graphs. We run each experiment 100 times with multiple random splits and different initializations.</p><p>Methods used for comparisons. We compare GPR-GNN with 6 baseline models: MLP, GCN (Kipf &amp; Welling, 2017), GAT <ref type="bibr" target="#b36">(Velickovic et al., 2018)</ref>, JK-Net <ref type="bibr" target="#b40">(Xu et al., 2018)</ref>, GCN-Cheby <ref type="bibr" target="#b10">(Defferrard et al., 2016)</ref>, APPNP <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>, SGC <ref type="bibr" target="#b39">(Wu et al., 2019)</ref>, SAGE <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref> and Geom-GCN <ref type="bibr" target="#b29">(Pei et al., 2019)</ref>. For all architectures, we use the corresponding Pytorch Geometric library implementations <ref type="bibr" target="#b12">(Fey &amp; Lenssen, 2019)</ref>. For Geom-GCN, we directly use the code provided by the authors 2 . We could not test Geom-GCN on cSBM and other datasets not originally tested in the paper due to a preprocessing subroutine that is not publicly available <ref type="bibr" target="#b29">(Pei et al., 2019)</ref>.</p><p>The GPR-GNN model setup and hyperparameter tuning. We choose random walk path lengths with K = 10 and use a 2-layer (MLP) with 64 hidden units for the NN component. For the GPR weights, we use different initializations including PPR with ? ? {0.1, 0.2, 0.5, 0.9}, ? k = ? 0k or ? Kk and the default random initialization in pytorch. Similarly, for APPNP we search the optimal ? within {0.1, 0.2, 0.5, 0.9}. For other hyperparameter tuning, we optimize the learning rate over {0.002, 0.01, 0.05} and weight decay {0.0, 0.0005} for all models. For Geom-GCN, we use the best variants in the original paper for each dataset. Finally, we use GPR-GNN(rand) to describe the results obtained with random initialization of the GPR weights. Further experimental settings are discussed in the Supplement.</p><p>Results. We examine the robustness of all baseline methods and GPR-GNN using cSBM-generated data with ? ? {?1, ?0.75, ?0.5, ..., 1}, which includes graphs across the heterophily/homophily spectrum. The results are summarized in <ref type="figure">Figure 2</ref>. For both the sparse and dense setting, GPR-GNN significantly outperforms all other baseline models whenever ? &lt; 0 (heterophilic graphs). On the other hand, all baseline GNNs can be worse then simple MLP when the graph information is weak (? = 0, ?0.25). This shows that existing GNNs cannot apply to arbitrary graphs, while GPR-GNN is clearly more robust. APPNP methods have the worst performance on strongly heterophilic graphs. This is in agreement with the result of Theorem 4.1 which asserts that APPNP intrinsically acts a low-pass filter and is thus inadequate for strong heterophily settings. JKNet, GCN-Cheby and SAGE are the only three baseline models that are able to learn strongly heterophilic graphs under dense splitting. This is also to be expected since JKNet is the only baseline model that combines results from different steps at the last layer, which is similar to what is done in GPR-GNN. GCN-Cheby uses multiple steps in each layers which allows it to partially adapt to heterophilic settings as each layer is related to a polynomial graph filter of higher order compared to that of GCN. SAGE treats ego-embeddings and embeddings from neighboring nodes differently and does not simply average them out. This allows SAGE to adapt to the heterophilic case since the ego-embeddings prevent nodes from being overwhelmed by information from their neighbors. Nevertheless, JKNet, GCN-Cheby and SAGE are not deep in practice. Moreover, JKNet fails to learn under the sparse splitting model while GCN-Cheby and SAGE fail to learn well when the graph information is strong (|?| ? 0.5), again under the sparse splitting model.</p><formula xml:id="formula_4">(a) ? = 0.25, (H(G) = 0.673) (b) ? = 0.75, (H(G) = 0.928) (c) ? = ?0.25, (H(G) = 0.328) (d) ? = ?0.75, (H(G) = 0.073)</formula><p>Also, we observe that random initialization of our GPR weights only results in slight performance drops under dense splitting. The drop is more evident for sparse splitting setting but our method still outperforms baseline models by a large margin for strongly heterophilic graphs. This is also to be expected as we have less label information in the sparse splitting setting where the implicit bias provided by good GPR initialization is helpful. The implicit bias becomes irrelevant for the dense splitting setting, since the label information is sufficiently rich.</p><p>Besides the strong performance of GPR-GNN, the other benefit is its interpretability. In <ref type="figure">Figure 3</ref>, we demonstrate the learnt GPR weights by our GPR-GNN on cSBM with random initialization. When the graph is weak homophilic (? = 0.25), the learnt GPR weights are decreasing. This is similar to the PPR weights used in APPNP, despite that the decaying speed is different. When the graph is strong homophilic (? = 0.75), the learnt GPR weights are increasing which is significantly different from the PPR weights. This result matches the recent finding in <ref type="bibr" target="#b23">Li et al. (2019)</ref> and behave similar to IPR proposed by the authors. On the other hand, the learnt GPR weights have zig-zag shape when the graph is heterophilic. This again validates Theorem 4.1 as GPR weights with alternating signs correspond to a high-pass filter. Interestingly, when ? = ?0.25 the magnitude of learnt GPR weight is decreasing. This is because the graph information is weak and the node feature information is more important in this case. It makes sense that the learnt GPR weight focus on the first few steps. Hence, we have validated the interpretablity of GPR-GNN. In practice, one can use the learnt GPR weights to better understand the graph structured data at hand. We showcase this benefit in the results of real world benchmark datasets.</p><p>Real world benchmark datasets. We use 5 homophilic benchmark datasets available from the Pytorch Geometric library, including the citation graphs Cora, CiteSeer, PubMed <ref type="bibr" target="#b31">(Sen et al., 2008;</ref><ref type="bibr" target="#b41">Yang et al., 2016)</ref> and the Amazon co-purchase graphs Computers and Photo <ref type="bibr" target="#b26">(McAuley et al., 2015;</ref><ref type="bibr" target="#b32">Shchur et al., 2018)</ref>. We also use 5 heterophilic benchmark datasets tested in <ref type="bibr" target="#b29">Pei et al. (2019)</ref>, including Wikipedia graphs Chameleon and Squirrel <ref type="bibr">(Rozemberczki et al., 2021)</ref>, the Actor cooccurrence graph, and webpage graphs Texas and Cornell from WebKB 3 . We summarize the dataset statistics in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Results on real-world datasets. We use accuracy (the micro-F1 score) as the evaluation metric along with a 95% confidence interval. The relevant results are summarized in <ref type="table" target="#tab_1">Table 2</ref>. For homophilic datasets, we provide results for sparse splitting which is more aligned with the original setting used  <ref type="table" target="#tab_0">Classes  7  6  5  10  8  5  5  5  5  5  Features 1433  3703  500  767  745  2325  2089  932  1703  1703  Nodes 2708  3327  19717  13752  7650  2277  5201  7600  183  183  Edges 5278  4552  44324  245861  119081  31371  198353 26659</ref>    <ref type="formula" target="#formula_2">(2017)</ref>; <ref type="bibr" target="#b32">Shchur et al. (2018)</ref>. For the heterophilic datasets, we adopt dense splitting which is used in <ref type="bibr" target="#b29">Pei et al. (2019)</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows that, in general, GPR-GNN outperforms all tested methods. On homophilic datasets, GPR-GNN achieves the state-of-the-art performance. On heterophilic datasets, GPR-GNN significantly outperforms all the other baseline models. It is important to point out that there are two different patterns to be observed among the heterophilic datasets. On Chameleon and Squirrel, MLP and APPNP perform worse then other baseline methods such as GCN and JKNet. In contrast, MLP and APPNP outperform the other baseline methods on Actor, Texas and Cornell. We conjecture that this is due to the fact that the graph topology information is strong and weak, respectively. Note that these two patterns match the results of the cSBM experiments for ? close to ?1 and 0, respectively ( <ref type="figure">Figure 2</ref>). Furthermore, the homophily measure H(G) proposed by <ref type="bibr" target="#b29">Pei et al. (2019)</ref> cannot characterize such differences in heterophilic datasets. We relegate the more detailed discussion of this topic along with illustrative examples to the Supplement.For fairness, we also repeated the experiment involving GeomGCN on homophilic datasets using a dense split -the observed performance pattern tends to be similar which can be found in Supplement.</p><p>We also examined the learned GPR weights on real datasets in <ref type="figure" target="#fig_5">Figure 4</ref>. Due to space limitations, a more comprehensive GPR weight analysis for other datasets is deferred to the Supplement. We can see that learned GPR weights are all positive for homophilic datasets (PubMed and Photo). In contrast, some GPR weights learned from heterophilic datasets (Actor and Squirrel) are negative. These results agree with the patterns observed on cSBMs. Interestingly, the learned weight ? 0 has the largest magnitude for the Actor dataset. This indicates that most of the information is contained in node features. From <ref type="table" target="#tab_1">Table 2</ref> we can also see that MLPs indeed outperforms most baseline GNNs (this is similar to the case of cSBM(? = ?0.25)). On the other hand, GPR weights learned from Squirrel have a zig-zag pattern. This implies that graph topology is more informative for Squirrel compared to Actor. From <ref type="table" target="#tab_1">Table 2</ref> we also see that baseline GNNs also outperform MLPs on Squirrel.</p><p>Escaping from over-smoothing and dynamics of learning GPR weights. To demonstrate the ability of GPR-GNNs to escape from over-smoothing, we choose the initial GPR weights to be ? k = ? kK . This ensures that over-smoothing effects are present with high probability at the very beginning of the learning process. On cSBM(? = ?1) with dense splitting, we find that for 96 out of 100 runs, GPR-GNN predicts the same labels for all nodes at epoch 0, which implies that over-smoothing indeed occurs immediately. The final prediction is 98.79% accurate which is much larger than the initial accuracy of 50.07% at epoch 0. Similar results can be observed for other datasets and this verifies our theoretical findings. We plot the dynamics of the learned GPR weights in <ref type="figure" target="#fig_5">Figure 4</ref>(e)-(h), which shows that the peak at last step is indeed reduced while the GPR weights for other steps   are significantly increased in magnitude. More results on the dynamics of learning GPR weights may be found in the Supplement.</p><p>Efficiency analysis. We also examine the computational complexity of GPR-GNNs compared to other baseline models. We report the empirical training time in <ref type="table" target="#tab_2">Table 3</ref>. Compared to APPNP, we only need to learn K+1 additional GPR weights for GPR-GNN, and usually K ? 20 (i.e. we choose K = 10 in our experiments). This additional computations are dominated by the computations performed by the neural network module f ? . We can observe from <ref type="table" target="#tab_2">Table 3</ref> that indeed GPR-GNN has a running time similar to that of APPNP. It is nevertheless worth pointing out that the authors of <ref type="bibr" target="#b6">Bojchevski et al. (2020)</ref> successfully scaled APPNP to operate on large graphs. Whether the same techniques may be used to scale GPR-GNNs is an interesting open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We addressed two fundamental weaknesses of existing GNNs: Failing to act as universal learners by not generalizing to heterophilic graphs and making use of large number of propagation steps. We developed a novel GPR-GNN architecture which combines adaptive generalized PageRank (GPR) scheme with GNNs. We theoretically showed that our method does not only mitigates feature oversmoothing but also works on highly diverse node label patterns. We also tested GPR-GNNs on both homophilic and heterophilic node label patterns, and proposed a novel synthetic benchmark datasets generated by the contextual stochastic block model. Our experiments on real-world benchmark datasets showed clear performance gains of GPR-GNN over the state-of-the-art methods. Moreover, we showed that GPR-GNN has desirable interpretability properties which is of independent interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 DETAILED DISCUSSION ON PREVENTING OVER-SMOOTHING.</p><p>As mentioned in Section 4, another method -APPNP -can also provably prevents oversmoothing <ref type="bibr" target="#b20">Klicpera et al. (2018)</ref>. The authors of this study use the fact that the PPR propagation will converge to ? ppr H (0) , where ? ppr = ?(I n ? (1 ? ?)? sym ) ?1 is independent on the node label information provided in the training data. Each row of ? ppr H (0) still depends on H (0) and thus APPNP will not suffer from the over-smoothing effect. However, since ? ppr is independent of the label information, it can cause undesired consequences that we discuss in what follows.    <ref type="figure" target="#fig_8">(Figure 5 (a)</ref>). Consider two different node label assignments shown in <ref type="figure" target="#fig_8">Figure 5</ref> (b) and <ref type="figure" target="#fig_8">Figure 5 (c)</ref>. Obviously, the graph topologies depicted in <ref type="figure" target="#fig_8">Figure 5 (b)</ref> and (c) are identical and the only difference is the class label assignment. In <ref type="figure" target="#fig_8">Figure 5 (b)</ref>, the graph is homophilic and hence the optimal graph filter should emphasize the low-frequency part of the graph signal. In contrast, in <ref type="figure" target="#fig_8">Figure 5 (c)</ref>, the graph is heterophilic as the graph is bipartite with respect to the labels. Hence, the optimal graph filter should emphasize the high-frequency part of the graph signal. This example illustrates that the optimal graph filter should depend on both the graph topology and the node label information. Recall that the equivalent graph filter that APPNP uses in the asymptotic regime is ? ppr which is independent on the node label information. Also, Theorem 4.1 established that APPNP intrinsically utilizes a low-pass filter. In contrast, GPR-GNN learns the GPR weights guided by the node label information which allows it to account for both cases (homophilic and heterophilic) shown.  In case 1, blue and green nodes link to all orange and purple nodes. In case 2, blue nodes only link to orange nodes and green nodes only link to purple nodes. From the definition of H(G) one can see that both cases have H(G) = 0, since in both cases nodes do not link to other nodes of the same label. However, it is obvious that the graph topology carries more node label information in case 2 compared to case 1. In fact, for case 1 it is impossible to distinguish blue and green nodes merely from the graph topology (and the same is true of orange and purple nodes). One possible alternative for the homophily measure is the Chernoff-Hellinger divergence Abbe (2017) of the empirical edge probability matrix B; here B ij is the empirical probability of an edge with one end node labeled i and the other labeled j. The intuition behind our suggestion lies in the fact that the Chernoff-Hellinger divergence characterizes the fundamental limit of SBMs. However, as many practical graph generative processes may significantly differ from SBMs, investigating alternative homophily/heterophily measures is another interesting open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DISCUSSION ON THE INSUFFICIENCY OF HOMOPHILY MEASURE H(G)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PROOF OF THEOREM 4.1</head><p>We first state the formal version of Theorem 4.1. Theorem A.1 (Formal version of Theorem 4.1). Assume the graph G is connected. Let ? 1 ? ? 2 ? ... ? ? n be the eigenvalues of? sym . If ? k ? 0 ?k ? {0, 1, ..., K}, K k=0 ? k = 1 and ?k &gt; 0 such that ? k &gt; 0, then |g ?,</p><formula xml:id="formula_5">K (? i )/g ?,K (? 1 )| &lt; 1 ?i ? 2. Also, if ? k = (??) k , ? ? (0, 1) and K ? ?, then | lim K?? g ?,K (? i )/ lim K?? g ?,K (? 1 )| &gt; 1 ?i ? 2.</formula><p>Note that |g ?,K (? i )/g ?,K (? 1 )| &lt; 1 ?i ? 2 implies that after applying the graph filter g ?,K , the lowest frequency component (correspond to ? 1 ) further dominates. Recall that in the unfiltered case, we do not multiply with? sym . It can also be viewed as multiplying the identity matrix I, where the eigenvalue ratio is |?i| 0 |?1| 0 = 1. Hence g ?,K acts like a low pass filter in this case. In contrast, | lim K?? g ?,K (? i )/ lim K?? g ?,K (? 1 )| &gt; 1 ?i ? 2 implies that after applying the graph filter, the lowest frequency component (correspond to ? 1 ) no longer dominates. This correspond to the high pass filter case.</p><p>Proof. We start with the low pass filter result. From basic spectral analysis <ref type="bibr" target="#b37">(Von Luxburg, 2007)</ref> we know that ? 1 = 1 and |? i | &lt; 1, ?i ? 2. One can also find the analysis in the proof of our Lemma A.2 in the Supplement. Then by assumption we know that</p><formula xml:id="formula_6">g ?,K (? 1 ) = K k=0 ? k = 1.</formula><p>Hence, proving Theorem A.1 is equivalent to show</p><formula xml:id="formula_7">|g ?,K (? i )| &lt; 1 ?i ? 2.</formula><p>This is obvious since g ?,K (?) = K k=0 ? k ? k is a polynomial of order K with nonnegative coefficients. It is easy to check that ?k ? 1, |?| k &lt; 1, ?|?| &lt; 1. Combine with the fact that all ? k 's are nonnegative we have</p><formula xml:id="formula_8">|g ?,K (? i )| ? K k=0 ? k |? k | = K k=0 ? k |?| k (a) ? K k=0 ? k = 1.</formula><p>Finally, note that the only possibility that the equality in (a) holds is ? k = ? 0,k since ?k ? 1, |?| k &lt; 1, ?|?| &lt; 1. However, by assumption K k=0 ? k = 1 and ?k &gt; 0 such that ? k &gt; 0 we know that this is impossible. Hence (a) is a strict inequality &lt;. Together we complete the proof for low pass filtering part.</p><p>For the high pass filter result, it is not hard to see that</p><formula xml:id="formula_9">lim K?? g ?,K (?) = lim K?? K k=0 ? k ? k = lim K?? K k=0 (???) k = 1 1 + ?? ,</formula><p>where the last step is due to the fact that ? ? (0, 1) and thus lim K?? (???) K = 0, ?|?| ? 1. Thus we have</p><formula xml:id="formula_10">lim K?? g ?,K (? i ) lim K?? g ?,K (? 1 ) = 1 + ? 1 + ?? i (b) &gt; 1 ?i ? 2.</formula><p>The strict inequalities (b) is from the fact that |? i | &lt; 1, ?i ? 2. Notably, sup ??[1,?1) 1 1+?? happens at the boundary ? = ?1, which corresponds the the bipartite graph. It further shows that the graph filter with respect to the choice ? k = (??) k emphasizes high frequency components and thus it is indeed acting as a high pass filter.</p><p>A.4 PROOF OF THEOREM 4.2</p><p>We start by introducing some additional notation, lemmas and definition before we proceed to the formal statement of Theorem 4.2. The label matrix is denoted by Y ? R n?C , where each row is a one-hot vector. We use 1[?] ? R C to denote the argmax of the vector ? ? R C : we have 1[?] i = 1 if and only if ? i = max(?) (ties are broken evenly), and 1[?] i = 0 otherwise. Let us replace the softmax(?) with softmax ? (?), where we let softmax ? (?) i = e ??i /( j e ??j ) stand for the softmax with a smooth parameter ? &gt; 0. Note that for ? = 1 we recover the standard softmax. With a slight abuse of notation, for the vector ? we write exp(?) to denote element-wise exponentiation. We use ?, ? to denote the standard Euclidean inner product. Also we use L for the cross entropy loss where</p><formula xml:id="formula_11">L = i?V ? log( P i: , Y i: ).</formula><p>Lemma A.2. Assume that the nodes in an undirected and connected graph G have one of C labels. Then, for k large enough, we have</p><formula xml:id="formula_12">H (k) :j = ? j ? + o k (1) ?j ? [C], where ? i = D ii v?VD vv and ? T = ? T H (0) .<label>(2)</label></formula><p>For any H (0) and large enough k ? K, if the label prediction is dominated by H (k) , all nodes will have a representation proportional to ? k ?. Hence, we will arrive at the same label for all nodes. This is what we refer to as the over-smoothing phenomenon.</p><p>Definition A.3 (The over-smoothing phenomenon). First, recall that Z = k ? k H (k) . If oversmoothing occurs in the GPR-GNN for K sufficiently large, we have Z :j = c 0 ? j ?, ?j ? [C] for some c 0 &gt; 0 if ? k &gt; 0 and Z :j = ?c 0 ? j ?, ?j ? [C] for some c 0 &gt; 0 if ? k &lt; 0.</p><p>Lemma A.4. Let L = i?T L i = i?T ?log( P i: , Y i: ) be the cross entropy loss and let T be the training set. Under the same assumption as given in Lemma A.2, the gradient of ? k for k large enough is ?L ?? k = i?T ?? i P i: ? Y i: , ? + o k (1). Lemma A.5. For any real vector ? ? R C and ? &gt; 0 large enough, we have softmax ? (?) = 1[?] + o ? (1). Now we are ready to state the formal version of Theorem 4.2. Theorem A.6 (Formal version of Theorem 4.2). Under the same assumptions as those listed in Lemma A.2, if the training set contains nodes from each class, then the GPR-GNN method can always avoid over-smoothing. More specifically, for k, ? large enough we have</p><formula xml:id="formula_13">?L ?? k = i?T ?? i max j?[C] ? j ? ? 1[Yi:] + o k (1) + o ? (1), when ? k &gt; 0.<label>(3)</label></formula><formula xml:id="formula_14">?L ?? k = i?T ?? i min j?[C] ? j ? ? 1[Yi:] + o k (1) + o ? (1), when ? k &lt; 0.<label>(4)</label></formula><p>Note that when ? k &gt; 0, (3) ? 0 when ignoring the o(1) term. The equality is achieved if and only if</p><formula xml:id="formula_15">max j?[C] ? j = ? 1[Yi:</formula><p>] . This means that over-smoothing results in a prediction that perfectly aligns with the ground truth label in the training set. However, if our training set contains at least one node from each class then the equality can never be attained. Thus, the gradient of ? k will always be positive when ? k &gt; 0. Similarly when ? k &lt; 0, (4) ? 0 when ignoring the o(1) term. The equality is achieved if and only if min j?[C] ? j = ? 1[Yi:] . By the same reason we know that under the assumption on training set the equality can never be attained. Thus, the gradient of ? k will always be negative when ? k &lt; 0. Finally, it is not hard to check that the gradient is bounded in magnitude.</p><p>Together we have shown that the gradient of ? k and ? k are of the same sign. This directly implies that |? k | will approach to 0 until we escape from over-smoothing when we use a decreasing learning rate for the optimizer (i.e. SGD).</p><p>Proof. First, let us assume the over-smoothing takes place and the ? k &gt; 0 for the dominate term. By Definition A.3, we know that Z :j = c 0 ? j ?, ?j ? [C] for some c 0 &gt; 0 and K sufficiently large. By Lemma A.4 we have</p><formula xml:id="formula_16">?L ?? k = i?T ?? i e ?Zi: j?[C] e ?Zij ? Y i: , ? + o k (1) (5) = i?T ?? i e ?c0?i? j?[C] e ?c0?i?j ? Y i: , ? + o k (1),<label>(6)</label></formula><p>where the last step follows from Definition A.3. Next, by Lemma A.5, we may approximate the softmax ? by the true argmax for ? &gt; 0 large enough according to</p><formula xml:id="formula_17">i?T ?? i 1[c 0 ? i ?] ? Y i: , ? + o k (1) + o ? (1) (7) = i?T ?? i 1[?] ? Y i: , ? + o k (1) + o ? (1) (8) = i?T ?? i max j?[C] ? j ? ? 1[Yi:] + o k (1) + o ? (1).<label>(9)</label></formula><p>The first equality is due to the fact that c 0 &gt; 0 and ? i &gt; 0. Recall that by Lemma A.2, ? i = ?D ii v?VD vv . Since we have a self-loop for each node,D ii &gt; 0 and thus ? i &gt; 0. For the case ? k &lt; 0, the same analysis still valid until (7). Hence we have</p><formula xml:id="formula_18">i?T ?? i 1[?c 0 ? i ?] ? Y i: , ? + o k (1) + o ? (1) (10) = i?T ?? i 1[??] ? Y i: , ? + o k (1) + o ? (1) (11) = i?T ?? i min j?[C] ? j ? ? 1[Yi:] + o k (1) + o ? (1).<label>(12)</label></formula><p>Together we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 CSBM DETAILS</head><p>The cSBM adds Gaussian random vectors as node features on top of the classical SBM. For simplicity, we assume C = 2 equally sized communities with node labels v i in {+1, ?1}. Each node i is associate with a f dimensional Gaussian vector b i = ? n v i u + Zi ? f where n is the number of nodes, u ? N (0, I/f ) and Z i ? R f has independent standard normal entries. The (undirected) graph in cSBM is described by the adjacency matrix A defined as</p><formula xml:id="formula_19">P (A ij = 1) = d+? ? d n if v i v j &gt; 0 d?? ? d n otherwise .</formula><p>Similar to the classical SBM, given the node labels the edges are independent. The symbol d stands for the average degree of the graph. Also, recall that ? and ? control the information strength carried by the node features and the graph structure respectively.</p><p>One reason for using the cSBM to generate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type="bibr" target="#b11">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type="bibr" target="#b11">Deshpande et al. (2018)</ref>). Assume that n, f ? ?, n f ? ? and d ? ?. Then there exists an estimatorv such that lim inf n??</p><formula xml:id="formula_20">| v,v | n</formula><p>is bounded away from 0 if and only if ? 2 + ? 2 ? &gt; 1.</p><p>In our experiment, we set n = 5000, f = 2000 and thus have ? = 2.5. We vary ? and ? along the arc ? 2 + ? 2 /? = 1 + for some &gt; 0 to ensure that we are in the achievable parameter regime. We also choose = 3.25 for all our experiment.</p><p>A.6 PROOF OF LEMMA A.2</p><p>Note that the proof of Lemma A.2 reduces to a standard analysis of random walks on graph. We include it for completeness and refer the interested readers to the tutorial Von Luxburg <ref type="formula" target="#formula_12">(2007)</ref>.</p><p>We start by showing that the symmetric graph Laplacia?</p><formula xml:id="formula_21">L sym = I ?D ?1/2?D?1/2 = I ?? sym<label>(13)</label></formula><p>is positive semi-definite. Let u be any real vector of unit norm and f =D ?1/2 u, then we have</p><formula xml:id="formula_22">u TL sym u = u T u ? u TD?1/2?D?1/2 u = n i=1 u 2 i ? n i,j=1 f i f j?ij (14) = n i=1D ii f 2 i ? n i,j=1 f i f j?ij = 1 2 ( n i=1D ii f 2 i ? 2 n i,j=1 f i f j?ij + n j=1D jj f 2 j ) (15) = 1 2 n i,j=1? ij (f i ? f j ) 2 ,<label>(16)</label></formula><p>where the last step follows from the definition of the degree.</p><p>Next we show that 0 is indeed an eigenvalue ofL sym associated with the unit eigenvector ? where</p><formula xml:id="formula_23">? = ?D ii ? vD vv .</formula><p>Let 1 be the all one vector. Then, a direct calculation reveals that</p><formula xml:id="formula_24">L sym ? = ? ?D ?1/2?D?1/2 ? = ? ?D ?1/2?D?1/2D1/2 1 ? 1 vD vv (17) = ? ?D ?1/2? 1 ? 1 vD vv = ? ?D ?1/2D 1 ? 1 vD vv (18) = ? ?D 1/2 1 ? 1 vD vv = ? ? ? = 0.<label>(19)</label></formula><p>Combining this result with the positive semi-definite property of the Laplacian shows that 0 is indeed the smallest eigenvalue ofL sym associated with the eigenvector ?. Moreover, from (16) and the assumption that the graph is connected, it is not hard to see that the multiplicity of the eigenvalue 0 is exactly 1 (See Proposition 2 and 4 in Von Luxburg (2007) for more detail). Finally, from (13) it is obvious that the the largest eigenvalue of? sym is 1, which correspond to the eigenvector ?. Hence all other eigenvalues of? sym 1 &gt; ? 2 ? ... ? ? n .</p><p>Next, we prove that |? n | &lt; 1. This can also be shown directly from <ref type="bibr">(16)</ref>. Note that</p><formula xml:id="formula_25">u TL sym u = 1 2 n i,j=1? ij (f i ? f j ) 2 (20) ? n i,j=1? ij (f 2 i + f 2 j ) = 2 n i,j=1? ij f 2 i = 2 n i,j=1? ij u 2 ? D ii (21) = 2 n i=1 u 2 ? D ii n j=1? ij = 2 n i=1 u 2 ? D iiD ii = 2 n i=1 u 2 i = 2.<label>(22)</label></formula><p>The inequality follows from an application of the Cauchy-Schwartz inequality. Consequently, the largest eigenvalue ofL sym is bounded by 2 which means that |? n | ? 1. Note that equality holds if and only if the underlying graph is bipartite. However, this is impossible in our setting since we have added a self loop to each node. Hence |? n | &lt; 1. This means</p><formula xml:id="formula_26">lim k??? k sym = ?? T .<label>(23)</label></formula><p>Hence, for any H (0) we have</p><formula xml:id="formula_27">lim k??? k sym H (0) = ?? T H (0) = ?? T .<label>(24)</label></formula><p>Note that this can also be written with the o k (1) term as </p><formula xml:id="formula_28">A k sym H (0) = ?? T + o k (1).<label>(25)</label></formula><p>Then by taking the partial derivative of the loss function with respect to ? k we have</p><formula xml:id="formula_30">?L ?? k = ? ?? k i?T (log( C m=1 e ?Zim ) ? ?Z i: , Y i: ).<label>(27)</label></formula><p>Next, recall that for GPR-GNN we also have Z = K k=0 ? k H <ref type="bibr">(k)</ref> . Plugging this expression into the previous formula and applying the chain rule we obtain</p><formula xml:id="formula_31">? ?? k i?T (log( C m=1 e ?Zim ) ? ?Z i: , Y i: ) = i?T ( C m=1 e ?Zim ??Zim ?? k C m=1 e Zim ? ?H (k ) i: , Y i: ) (28) = i?T ( C m=1 e ?Zim ?H (k ) im C m=1 e ?Zim ? ?H (k ) i: , Y i: )<label>(29)</label></formula><p>Settin k = k for large enough k, it follows from Lemma A.2 that</p><formula xml:id="formula_32">?L ?? k = i?T ?( C m=1 e ?Zim H (k) im C m=1 e ?Zim ? H (k) i: , Y i: ) (30) = i?T ?( C m=1 e ?Zim (? i ? m + o k (1)) C m=1 e ?Zim ? ? i ? + o k (1), Y i: ) (31) = i?T ? i ?( C m=1 e ?Zim ? m C m=1 e ?Zim ? ?, Y i: ) + o k (1)<label>(32)</label></formula><formula xml:id="formula_33">= i?T ? i ?( C m=1P im ? m ? ?, Y i: ) + o k (1) = i?T ?? i P i: ? Y i: , ? + o k (1).<label>(33)</label></formula><p>Note that in <ref type="formula" target="#formula_12">(32)</ref> and <ref type="formula" target="#formula_13">(33)</ref> we used the definition of the soft predictionP = softmax ? (Z). This completes the proof.</p><p>A.8 PROOF OF LEMMA A.5 Let? = max(?). Then by the definition of softmax ? for ? &gt; 0 we have</p><formula xml:id="formula_34">softmax ? (?) = e ?? C m=1 e ??m = e ??(???) C m=1 e ??(???m) .<label>(34)</label></formula><p>Note that? ? ? m &gt; 0 when ? m =? and? ? ? m = 0 when ? m =?. Without loss of generality we assume that there are p maxima in ?, where 1 ? p ? C, and let P denote the set of indices of those maxima. Then, taking the limit ? ? ? we have</p><formula xml:id="formula_35">lim ??? softmax ? (?) j = lim ??? e ??(???j ) m / ?P e ??(???m) + p = 0, if ? j =? 1 p , otherwise.<label>(35)</label></formula><p>This implies that for ? &gt; 0 large enough one has</p><formula xml:id="formula_36">softmax ? (?) = 1[?] + o ? (1).<label>(36)</label></formula><p>The above result completes the proof.</p><p>A.9 ADDITIONAL EXPERIMENTAL DETAILS For all baseline models, we directly use the implementation available in the Pytorch Geometric library <ref type="bibr" target="#b12">Fey &amp; Lenssen (2019)</ref>.We use early stopping 200 and a maximum number of epochs equal to 1000 for both real benchmark dataset and our cSBM synthetic datasets. All models use the Adam optimizer <ref type="bibr" target="#b18">Kingma &amp; Ba (2014)</ref>. Note that the early stopping criteria is exactly the same as in Pytorch Geometric -when the epoch is greater than half of the maximum epoch, we check if the current validation loss is lower than the average over the past 200 epochs. If it is not lower, we stop the training process.</p><p>For GCN, we use 2 GCN layers with 64 hidden units. For GAT, we use 2 GAT convolutional layers, where the first layer has 8 attention heads and each head has 8 hidden units; the second layer has 1 attention head and 64 hidden units. For GCN-Cheby, we use 2 steps propagation for each layer with 32 hidden units. Note that the number of equivalent hidden units for each layer is64 for this case. For JK-Net, we use the GCN-based model with 2 layers and 16 hidden units in each layer. As for the layer aggregation part, we use a LSTM with 16 channels and 4 layers. For the MLP, we choose a 2-layer fully connected network with 64 hidden units. For APPNP we use the same 2-layer MLP with 10 steps of propagation. Besides the GPR-GNN, we fix the dropout rate for the NN part to be 0.5 as APPNP and optimize the dropout rate for the GPR part among {0, 0.5, 0.7}. For Geom-GCN, we choose the datasets already tested in the paper were the method was first described <ref type="bibr" target="#b29">(Pei et al., 2019)</ref>. For SGC, we use the default K = 2 layers after test among {2, 3}. For SAGE, we use 2 SAGE convolutional layers with 64 hidden units.</p><p>The heterophilic datasets used in <ref type="bibr" target="#b29">(Pei et al., 2019)</ref>. The graphs Chameleon, Actor, Squirrel, Texas and Cornell in their original form are directed graphs (see the github repository of <ref type="bibr" target="#b29">(Pei et al., 2019)</ref>).</p><p>Since the usual setting for semi-supervised node classifications involves undirected graph, we transformed the graphs into undirected to test them on all previously described benchmark methods. We keep the input graph directed for Geom-GCN as the method uses a fixed preprocessing scheme that was unfortunately not made public by the authors. Our homophily measure values H(G) in <ref type="table" target="#tab_0">Table 1</ref> are all based on undirected graphs and hence the numbers are different from those reported in <ref type="bibr" target="#b29">(Pei et al., 2019)</ref>.</p><p>A.10 ADDITIONAL EXPERIMENTAL RESULTS       : Additional experiments illustrating that GPR-GNN escapes over-smoothing. We initialize the GPR weights ? k = ? kK as described in Section 5. We report the mean accuracy at Epoch 0 and after training (Final epoch). The over-smoothing ratio indicates how many time out of the 100 runs that GPR-GNN started with lead to the same label for all nodes. For an illustration of how GPR weights change over different epochs, please check <ref type="figure">Figure 9</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>of our proposed GPR-GNN model. (b) Cora, H(G) = 0.825 (c) Texas, H(G) = 0.057 Figure 1: (a) Hidden state feature extraction is performed by a neural networks using individual node features propagated via GPR. Note that both the GPR weights ? k and parameter set {?} of the neural network are learned simultaneously in an end-to-end fashion (as indicated in red). (b)-(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 2 :</head><label>32</label><figDesc>Figure (a)-(d)shows the learnt GPR weights by GPR-GNN with random initialization on cSBM, dense split. The shaded region indicates 95% confidence interval. Accuracy of tested models on cSBM. Error bars indicate 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figures (a)-(d)show the learned GPR weights of our GPR-GNN method with random initialization on various datasets, for dense splitting. Figures (e)-(f) show the learned weights of our GPR-GNN method with initialization ? kK on cSBM(? = ?1), for dense splitting. The shaded region indicates a 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>A simple example demonstrating how GPR-GNN escapes over-smoothing. Let us consider a simple example shown in Figure 5 involving a connected and undirected graph G = (V, E)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( a )</head><label>a</label><figDesc>Case 1. (b) Case 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>A simple example for explaining the insufficiency of homophily measure H(G). As mentioned in Section 5, the homophily measure H(G) is inadequate for characterizing whether a heterophilic graph topology is informative or not. Consider two simple examples depicted in Fig-ure 6, where the color of the nodes indicates their label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 :</head><label>7</label><figDesc>Figures (a)-(i) show the learned GPR weights by GPR-GNN with random initialization on cSBM, dense splitting. The shaded region indicates a 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Benchmark dataset properties and statistics.</figDesc><table /><note>Dataset Cora Citeseer PubMed Computers Photo Chameleon Squirrel Actor Texas Cornell</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on real world benchmark datasets: Mean accuracy (%) ? 95% confidence interval. Boldface letters are used to mark the best results while underlined boldface letters indicate results within the given confidence interval of the best result. 51?0.36 67.63?0.38 85.07?0.09 82.90?0.37 91.93?0.26 67.48?0.40 39.30?0.27 49.93?0.53 92.92?0.61 91.36?0.70 APPNP 79.41?0.38 68.59?0.30 85.02?0.09 81.99?0.26 91.11?0.26 51.91?0.56 38.86?0.24 34.77?0.34 91.18?0.70 91.80?0.63 MLP 50.34?0.48 52.88?0.51 80.57?0.12 70.48?0.28 78.69?0.30 46.72?0.46 38.58?0.25 31.28?0.27 92.26?0.71 91.36?0.70 SGC 70.81?0.67 58.98?0.47 82.09?0.11 76.27?0.36 83.80?0.46 63.02?0.43 29.39?0.20 43.14?0.28 55.18?1.17 47.80?1.50 GCN 75.21?0.38 67.30?0.35 84.27?0.01 82.52?0.32 90.54?0.21 60.96?0.78 30.59?0.23 45.66?0.39 75.16?0.96 66.72?1.37 GAT 76.70?0.42 67.20?0.46 83.28?0.12 81.95?0.38 90.09?0.27 63.9?0.46 35.98?0.23 42.72?0.33 78.87?0.86 76.00?1.01 SAGE 70.89?0.54 61.52?0.44 81.30?0.10 83.11?0.23 90.51?0.25 62.15?0.42 36.37?0.21 41.26?0.26 79.03?1.20 71.41?1.24 JKNet 73.22?0.64 60.85?0.76 82.91?0.11 77.80?0.97 87.70?0.70 62.92?0.49 33.41?0.25 44.72?0.48 75.53?1.16 66.73?1.73 GCN-Cheby 71.39?0.51 65.67?0.38 83.83?0.12 82.41?0.28 90.09?0.28 59.96?0.51 38.02?0.23 40.67?0.31 86.08?0.96 85.33?1.04 GeomGCN 20.37?1.13 20.30?0.90 58.20?1.23 NA NA 61.06?0.49 31.81?0.24 38.28?0.27 58.56?1.77 55.59?1.59</figDesc><table><row><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed Computers</cell><cell>Photo</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell><cell>Cornell</cell></row><row><cell>GPRGNN 79.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>in Kipf &amp; Welling</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>.75s 45.76ms / 12.02s 218.82ms / 96.58s 89.41ms / 18.06s 43.94ms / 8.88s 440.55ms / 88.99s 12.34ms / 3.08s</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Pubmed</cell><cell>Computers</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell cols="8">GPRGNN 17.62ms / 3.74s 20.19ms / 5.53s 39.93ms / 11.40s 16.74ms / 3.40s 19.31ms / 4.49s 25.28ms / 5.12s 17.56ms / 3.55s</cell></row><row><cell>APPNP</cell><cell cols="7">17.16ms / 4.00s 18.47ms / 6.29s 39.59ms / 20.00s 17.01ms / 3.44s 16.32ms / 4.04s 22.93ms / 4.63s 15.96ms / 3.24s</cell></row><row><cell>MLP</cell><cell cols="2">4.14ms / 0.92s 5.43ms / 2.86s</cell><cell>5.33ms / 2.77s</cell><cell cols="2">3.41ms / 0.69s 4.84ms / 0.98s</cell><cell>5.19ms / 1.05s</cell><cell>3.81ms / 1.04s</cell></row><row><cell>SGC</cell><cell cols="2">3.31ms / 3.31s 3.81ms / 3.81s</cell><cell>4.36ms / 4.36s</cell><cell cols="2">3.13ms / 3.13s 3.98ms / 1.00s</cell><cell>4.79ms / 4.79s</cell><cell>2.86ms / 2.09s</cell></row><row><cell>GCN</cell><cell cols="7">9.25ms / 1.97s 14.11ms / 4.17s 32.45ms / 16.29s 13.83ms / 2.79s 12.39ms / 2.50s 27.11ms / 5.56s 10.22ms / 2.06s</cell></row><row><cell>GAT</cell><cell cols="7">14.78ms / 3.42s 21.52ms / 6.70s 61.45ms / 24.28s 16.63ms / 3.63s 18.91ms / 3.86s 47.46ms / 10.05s 15.50ms / 3.13s</cell></row><row><cell>SAGE</cell><cell cols="7">12.06ms / 2.44s 28.82ms / 6.32s 171.36ms / 71.94s 64.43ms / 13.02s 27.95ms / 5.65s 343.47ms / 69.38s 6.08ms / 1.28s</cell></row><row><cell>JKNet</cell><cell cols="7">18.97ms / 4.41s 24.48ms / 6.61s 35.02ms / 14.96s 20.03ms / 5.15s 23.52ms / 4.75s 29.89ms / 6.67s 19.67ms / 4.01s</cell></row><row><cell cols="2">GCN-cheby 22.96ms / 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Efficiency on selected real world benchmark datasets: Average running time per epoch(ms)/average total running time(s). Note that Geom-GCN requires a preprocessing proce- dure so we do not include it in the table. Complete efficiency table for all benchmark datasets is in Supplementary due to space limit.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The values of the homophily measure for cSBM datasets. Linux Machine with 48 cores, 376GB of RAM, and a NVIDIA Tesla P100 GPU with 12GB of GPU memory. For the training set, we ensure that number of nodes from each class is approximately the same an keep the total number of training nodes close to 2.5%/60%. For the validation set, we randomly sample 2.5%/20% of the nodes and place the remaining ones into the test set.</figDesc><table><row><cell>?</cell><cell>?1</cell><cell cols="3">?0.75 ?0.5 ?0.25</cell><cell>0</cell><cell>0.25</cell><cell>0.5</cell><cell>0.75</cell><cell>1</cell></row><row><cell cols="2">H(G) 0.039</cell><cell>0.073</cell><cell>0.170</cell><cell>0.328</cell><cell cols="4">0.500 0.673 0.829 0.928 0.960</cell></row><row><cell cols="4">All experiments are performed on a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results for cSBM, sparse splitting. Bold values indicate the best obtained result and while bold, underlined values indicate results within a 95% confidence interval with respect to the best result.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results for cSBM, dense splitting. Bold values indicate the best results found while bold, underlined values indicate results within a 95% confidence interval with respect to the best result.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results on homophilic real-world benchmark datasets tested in<ref type="bibr" target="#b29">(Pei et al., 2019)</ref>, dense splitting: Mean accuracy (%) ? 95% confidence interval. Boldface values indicate the best results found while boldface, underlined values indicates results within the confidence interval with respect to the best result.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell></row><row><cell>GPRGNN</cell><cell>88.65?0.28</cell><cell>80.01?0.28</cell><cell>89.18?0.15</cell></row><row><cell>APPNP</cell><cell>88.1?0.23</cell><cell>80.5?0.26</cell><cell>89.15?0.13</cell></row><row><cell>MLP</cell><cell>76.44?0.30</cell><cell>76.25?0.28</cell><cell>86.43?0.13</cell></row><row><cell>SGC</cell><cell>86.58?0.26</cell><cell>76.23?0.29</cell><cell>83.52?0.10</cell></row><row><cell>GCN</cell><cell>86.87?0.25</cell><cell>79.28?0.25</cell><cell>86.97?0.12</cell></row><row><cell>GAT</cell><cell>87.52?0.24</cell><cell>80.56?0.31</cell><cell>86.64?0.11</cell></row><row><cell>SAGE</cell><cell>86.58?0.26</cell><cell>78.24?0.30</cell><cell>86.85?0.11</cell></row><row><cell>JKNet</cell><cell>86.97?0.27</cell><cell>77.69?0.35</cell><cell>87.38?0.13</cell></row><row><cell>GCN-Cheby</cell><cell>86.46?0.26</cell><cell>78.66?0.26</cell><cell>88.2?0.09</cell></row><row><cell>GeomGCN</cell><cell>85.4?0.26</cell><cell>76.42?0.37</cell><cell>88.51?0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/graphdml-uiuc-jlu/geom-gcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported in part by the NSF Emerging Frontiers of Science of Information Grant 0939370 and the NSF CIF 1618366 Grant. The authors thank Mohamad Bairakdar for helpful comments on an earlier version of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 10</ref><p>: The dynamics of learning GPR weights with random initialization on various benchmark datasets, dense splitting. The shaded region indicates a 95% confidence interval. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9180" to="9190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive diffusions for scalable learning over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Berberidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Nikolakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining and Learning with Graphs Workshop @ ACM KDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is pagerank all you need for scalable graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD, MLG Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR2014)</title>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="http" to="openreview" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The heat kernel as the pagerank of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="19735" to="19740" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent network features and overlapping community discovery via boolean intersection representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3219" to="3234" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contextual stochastic block models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8581" to="8593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pagerank beyond the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="363" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
		<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Outcome correlation in graph neural network regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08274</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Block models and personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Isabel M Kloumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing generalized pagerank methods for seedexpansion community detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11705" to="11716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-Scale Attributed Node Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">77</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Provably fast inference of latent features from networks: With applications to learning social circles and multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charalampos</forename><surname>Tsourakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1111" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dfnets: Spectral cnns for graphs with feedbacklooped filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suranga</forename><surname>Wok Asiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wijesinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6007" to="6018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph-SAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11468</idno>
		<title level="m">Generalizing graph neural networks beyond homophily</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">825) (b) Citeseer, (H(G) = 0.718) (c) PubMed, (H(G) = 0.792) (d) Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cora</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>H(G) = 0.802) (e) Photo, (H(G) = 0.849) (f) Chameleon, (H(G) = 0.247</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">H(G) = 0.215) (h) Squirrel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Actor</surname></persName>
		</author>
		<idno>0.217</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename></persName>
		</author>
		<idno>0.301</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Figures (a)-(j) show the learned GPR weights by GPR-GNN with random initialization on various benchmark datasets, dense splitting. The shaded region indicates a 95% confidence interval. Note that the learned GPR weights are all positive for every homophilic dataset. There is at least one negative learned GPR weight for every heterophilic dataset</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
