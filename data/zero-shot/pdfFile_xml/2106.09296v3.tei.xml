<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Voice2Series: Reprogramming Acoustic Models for Time Series Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Han</forename><surname>Huck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Yun</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Voice2Series: Reprogramming Acoustic Models for Time Series Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to classify time series with limited data is a practical yet challenging problem. Current methods are primarily based on hand-designed feature extraction rules or domain-specific data augmentation. Motivated by the advances in deep speech processing models and the fact that voice data are univariate temporal signals, in this paper, we propose Voice2Series (V2S), a novel end-toend approach that reprograms acoustic models for time series classification, through input transformation learning and output label mapping. Leveraging the representation learning power of a large-scale pre-trained speech processing model, on 30 different time series tasks we show that V2S performs competitive results on 19 time series classification tasks. We further provide a theoretical justification of V2S by proving its population risk is upper bounded by the source risk and a Wasserstein distance accounting for feature alignment via reprogramming. Our results offer new and effective means to time series classification. Our code of is available at https://github.com/huckiyang/ Voice2Series-Reprogramming.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning for time series data has rich applications in a variety of domains, ranging from medical diagnosis (e.g., physiological signals such as electrocardiogram (ECG) <ref type="bibr" target="#b25">(Kampouraki et al., 2008)</ref>), finance/weather forecasting, to industrial measurements (e.g., sensors and Internet of Things (IoT)). It is worth noting that one common practical challenge that prevents time series learning tasks from using modern large-scale deep learning models is data scarcity. While many efforts <ref type="bibr" target="#b10">(Fawaz et al., 2018;</ref><ref type="bibr" target="#b56">Ye &amp; Dai, 2018;</ref><ref type="bibr" target="#b27">Kashiparekh et al., 2019)</ref>   learning and model adaptation for time series classification, a principled approach is lacking and its performance may not be comparable to conventional statistical learning benchmarks <ref type="bibr" target="#b32">(Langkvist et al., 2014)</ref>.</p><p>To bridge this gap, we propose a novel approach, named voice to series (V2S), for time series classification by reprogramming a pre-trained acoustic model (AM), such as a spoken-terms recognition model. Unlike general time series tasks, modern AMs are trained on massive human voice datasets and are considered as a mature technology widely deployed in intelligent electronic devices. The rationale of V2S lies in the fact that voice data can be viewed as univariate temporal signals, and therefore a well-trained AM is likely to be reprogrammed as a powerful feature extractor for solving time series classification tasks. <ref type="figure" target="#fig_0">Figure 1</ref> shows a schematic illustration of the proposed V2S framework, including (a) a trainable reprogram layer, (b) a pre-trained AM, and (c) a specified label mapping function between source (human voice) and target (time series) labels.</p><p>Model reprogramming was firstly introduced in <ref type="bibr" target="#b9">(Elsayed et al., 2019)</ref>. The authors show that one can learn a universal input transformation function to reprogram a pre-trained ImageNet model (without changing the model weights) for solving MNIST/CIFAR-10 image classification and simple vision-based counting tasks with high accuracy. It can be viewed as an efficient approach for transfer learning with limited data, and it has achieved state-of-the-art (SOTA) results on biomedical image classification tasks <ref type="bibr" target="#b45">(Tsai et al., 2020)</ref>. However, despite the empirical success, little is known on how and why reprogramming can be successful.</p><p>Different from existing works, this paper aims to address the following three fundamental questions: (i) Can acoustic models be reprogrammed for time series classification? (ii) Can V2S outperform SOTA time-series classification arXiv:2106.09296v3 <ref type="bibr">[cs.</ref>LG] 14 Jan 2022</p><p>Voice2Series: Reprogramming Acoustic Models for Time Series Classification results? (iii) Is there any theoretical justification on why reprogramming works?</p><p>Our main contributions in this paper provide affirmative answers to the aforementioned fundamental questions, which are summarized as follows.</p><p>1. We propose V2S, a novel and unified approach to reprogram large-scale pre-trained acoustic models for different time series classification tasks. To the best of our knowledge, V2S is the first framework that enables reprogramming for time series tasks.</p><p>2. Tested on a standard UCR time series classification benchmark <ref type="bibr" target="#b6">(Dau et al., 2019)</ref>, V2S performs competitively on 19 out of 30 datasets, suggesting that V2S is a potentially effective approach for time series classification.</p><p>3. In Section 4, we develop a theoretical risk analysis to characterize the performance of reprogramming on the target task via source risk and representation alignment loss. In Section 5, we also show how our theoretical results can be used to assess the performance of reprogramming. Moreover, we provide interpretation on V2S through auditory neural saliency map and embedding visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Time Series Classification</head><p>Learning to classify time series is a standard research topic in machine learning and signal processing. A major research branch uses designed features followed by conventional classifiers, such as digital filter design in together with support vector machine (SVM) <ref type="bibr" target="#b25">(Kampouraki et al., 2008)</ref>, decisiontrees <ref type="bibr" target="#b15">(Geurts, 2001)</ref>, or kernel based methods <ref type="bibr" target="#b57">(Zhang et al., 2010;</ref><ref type="bibr" target="#b34">Lines et al., 2012)</ref>. Recently, deep learning models have been utilized in time series <ref type="bibr" target="#b11">(Fawaz et al., 2019)</ref> and demonstrated improved performance. The methods range from a completely end-to-end classifier <ref type="bibr" target="#b57">(Zhang et al., 2010;</ref><ref type="bibr" target="#b49">Wang et al., 2017</ref>) to a mixture model <ref type="bibr">(Hong et al., 2019)</ref> that combines feature engineering and deep learning. Notably, feature engineering methods <ref type="bibr" target="#b49">(Wang et al., 2017)</ref> can still attain competitive results on some time series classification tasks, especially when the number of training data is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Reprogramming</head><p>Although in the original paper <ref type="bibr" target="#b9">(Elsayed et al., 2019</ref>) model reprogramming was phrased as "adversarial" reprogramming, it is not limited to the adversarial setting, nor does it involve any adversarial training. <ref type="bibr" target="#b9">Elsayed et al. (2019)</ref> showed that the pre-trained ImageNet models can be reprogrammed for classifying other image datasets and for solving visionbased counting tasks. <ref type="bibr" target="#b45">Tsai et al. (2020)</ref> demonstrated the advantage of reprogramming on label-limited data such as biomedical image classification, and used zeroth order optimization <ref type="bibr" target="#b36">(Liu et al., 2020)</ref> to enable reprogramming of black-box machine learning systems.</p><p>Beyond image data, model reprogramming has been used in natural language processing (NLP) <ref type="bibr" target="#b38">(Neekhara et al., 2019;</ref><ref type="bibr" target="#b16">Hambardzumyan et al., 2020)</ref>, such as machine translation and sentiment classification. <ref type="bibr" target="#b48">Vinod et al. (2020)</ref> further showed that NLP models can be reprogrammed for molecule learning tasks in biochemistry.</p><p>For reprogramming with time series, it remains unclear what source domain and pre-trained models to be used. Current research works on reprogramming do not extend to time series because image and text data have fundamentally different feature characteristics than time series. One of our major contributions is the proposal of reprogramming pretrained acoustic models on abundant human voice data for time series classification. To our best knowledge, our V2S is the first framework on reprogramming for time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Acoustic Modeling</head><p>Recent deep learning models have shown impressive results on predicting label(s) from acoustic information. The central idea is to use a large number of spectral features (e.g., Mel-spectrogram or log-power spectrum) as training inputs to capture the important features. Some of the latent features learned from AM are interpretable based on physiological auditory experiments (e.g., cortex responses <ref type="bibr" target="#b28">(Kaya &amp; Elhilali, 2017)</ref>) or neural saliency methods. Several efforts have been put into the design of a large and deep neural network to extract features from human voice datasets. Among them, residual neural network (ResNet) <ref type="bibr" target="#b17">(He et al., 2016;</ref><ref type="bibr" target="#b43">Saon et al., 2017)</ref> and VGGish <ref type="bibr" target="#b19">(Hershey et al., 2017)</ref> models are popular backbones for AM tasks, such as spoken-term recognition <ref type="bibr" target="#b54">(Yang et al., 2021a;</ref><ref type="bibr" target="#b8">de Andrade et al., 2018)</ref> and speech enhancement <ref type="bibr" target="#b52">(Xu et al., 2014;</ref><ref type="bibr" target="#b53">Yang et al., 2020)</ref>. It is worth noting that standard transfer learning via parameter finetuning is not ideal for time series tasks with limited data, as acoustic data and time-series data are often quite diverse in scale. Our V2S addresses this issue via learning an input transformation function while fixing the pre-trained AM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Voice2Series (V2S)</head><p>3.1. Mathematical Notation <ref type="table" target="#tab_1">Table 1</ref> summarizes the major mathematical notations used in this paper for V2S reprogramming. Throughout this paper, we will denote a K-way acoustic classification model pre-trained on voice data as a source model, and use the term target data to denote the univariate time-series data to </p><formula xml:id="formula_0">S / T source/target domain X S / X T the space of source/target data samples Y S / Y T the space of source/target data labels D S ? X S ? Y S / D T ? X T ? Y T source/target data distribution (x, y) ? D</formula><p>data sample x and one-hot coded label y drawn from D K number of source labels</p><formula xml:id="formula_1">f S : R d ? [0, 1] K pre-trained K-way source classification model ? : R K ? [0, 1] K softmax function in neural network, and K k=1 [?(?)] k = 1 z(?) ? R K logit (pre-softmax) representation, and f (x) = ?(z(x)) (x, y) f (x) ? y 2 risk function of (x, y) based on classifier f E D [ (x, y)] E (x,y) ?D [ (x, y)] = E D f (x) ? y 2 population risk based on classifier f ?, ?</formula><p>additive input transformation on target data, parameterized by ? be reprogrammed. The notation P is reserved for denoting a probability function. The remaining notations will be introduced when applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">V2S Reprogramming on Data Inputs</head><p>Here we formulate the problem of V2S reprogramming on data inputs. Let x t ? X T ? R d T denote a univariate time series input from the target domain with d T temporal features. Our V2S aims to find a trainable input transformation function H that is universal to all target data inputs, which serves the purpose of reprogramming x t into the source data space X S ? R d S , where d T &lt; d S . Specifically, the reprogrammed sample x t is formulated as:</p><formula xml:id="formula_2">x t = H(x t ; ?) := Pad(x t ) + M ? ?<label>(1)</label></formula><p>where Pad(x t ) is a zero padding function that outputs a zeropadded time series of dimension d S . The location of the segment x t to be placed in x t is a design parameter and we defer the discussion to Section 5.2. The term M ? {0, 1} d S is a binary mask that indicates the location of x t in its zeropadded input Pad(x t ), where the i-th entry of M is 0 if x t is present (indicating the entry is non-reprogrammable), and it is 1 otherwise (indicating the entry is not occupied and thus reprogrammable). The operator denotes element-wise product. Finally, ? ? R d S is a set of trainable parameters for aligning source and target domain data distributions. One can consider a more complex function W (?) in our reprogramming function. But in practice we do not observe notable gains when compared to the simple function ?. In what follows, we will use the term ? M ? to denote the trainable additive input transformation for V2S reprogramming. Moreover, for ease of representation we will omit the padding notation and simply use x t + ? to denote the reprogrammed target data, by treating the "+" operation as a zero-padded broadcasting function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">V2S Reprogramming on Acoustic Models (AMs)</head><p>We select a pre-trained deep acoustic classification model as the source model (f S ) for model reprogramming. We assume the source model has softmax as the final layer and outputs nonnegative confidence score (prediction probability) for each source label. With the transformed data inputs H(x t ; ?) described in (1), one can obtain the class prediction of the source model f S on an reprogrammed target data sample x t , denoted by P (y s |f S (H(x t ; ?))), for all y s ? Y S</p><p>Next, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, we assign a (many-to-one) label mapping function h to map source labels to target labels. For a target label y t ? Y T , its class prediction will be the averaged class predictions over the set of source labels assigned to it. We use the term P (h(Y S )|f S (H(x t ; ?))) to denote the prediction probability of the target task on the associated ground-truth target label y t = h(Y S ). Finally, we learn the optimal parameters ? * for data input reprogramming by optimizing the following objective:</p><formula xml:id="formula_4">? * = arg min ? ? log P (h(Y S )|f S (H(x t ; ?)) V2S loss L ; (3) where h (Y S ) = y t</formula><p>The optimization will be implemented by minimizing the empirical loss (V2S loss L) evaluated on all target-domain training data pairs {x t , y t } for solving ? * .</p><p>In practice, we find that many-to-one label mapping can improve the reprogramming accuracy when compared to one-to-one label mapping, similar to the findings in <ref type="bibr" target="#b45">(Tsai et al., 2020)</ref>. Below we make a concrete example on how many-to-one label mapping is used for V2S reprogramming. Consider the case of reprogramming spoken-term AM for ECG classification. One can choose to map multiple (but non-overlapping) classes from the source task (e.g., 'yes', 'no', 'up', 'down' in AM classes) to every class from the target task (e.g., 'Normal' or 'Ischemia' in ECG classes), leading to a specified mapping function h. Let B ? Y S denote the set of source labels mapping to the target label y t ? Y T . Then, the class prediction of y t based on V2S reprogramming is the aggregated prediction over the assigned source labels, which is defined as</p><formula xml:id="formula_5">P (y t |f S (H(x t ; ?)) = 1 |B| ys?B P (y s |f S (H(x t ; ?)) (4)</formula><p>where |B| denotes the number of labels in B. In our implementation we use random (but non-overlapping) many-toone mapping between source and target labels. Each target label is assigned with the same number of source labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">V2S Algorithm</head><p>Algorithm 1 summarizes the training procedure of our proposed V2S reprogramming algorithm. The algorithm uses the ADAM optimizer <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015)</ref> to find the optimal reprogramming parameters ? * that minimize the V2S loss L as defined in <ref type="formula">(3)</ref>, which is evaluated over all targetdomain training data. In our implementation of Algorithm 1 we use stochastic optimization with minibatches.</p><p>Algorithm 1 Voice to Series (V2S) Reprogramming</p><formula xml:id="formula_6">1: Inputs: Pre-trained acoustic model f S , V2S loss L in (3), target domain training data {x (i) t , y (i) t } n i=1</formula><p>, mask function M , multi-label mapping function h(?), maximum number of iterations T , initial learning rate ? 2: Output: Optimal reprogramming parameters ? * 3: Initialize ? randomly; set t = 0 4: #Generate reprogrammed data input</p><formula xml:id="formula_7">H(x (i) t ; ?) = Pad(x (i) t ) + M ?, ? i = {1, 2, . . . , n} 5: #Compute V2S loss L from equation (3) L(?) = ? 1 n n i=1 log P (y (i) t |f S (H(x (i) t ); ?)) 6: #Solve reprogramming parameters</formula><p>Use ADAM optimizer to solve for ? * based on L(?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Population Risk via Reprogramming</head><p>To provide theoretical justification on the effectiveness of V2S, in what follows we establish a formal population risk analysis and prove that based on V2S, the population risk of the target task is upper bounded by the sum of the source population risk and the Wasserstein-1 distance between the logit representations of the source data and the reprogrammed target data. Our analysis matches the intuition that a high-accuracy (low population risk) source model with a better source-target data alignment (small Wasserstein-1 distance) should exhibit better reprogramming performance. In Section 5.4, we show that our derived population risk bound can be used to assess the reprogramming performance of V2S for different source models and target tasks. We also note that our theoretical analysis is not limited to V2S reprogramming. It applies to generic classification tasks.</p><p>Using the mathematical notation summarized in <ref type="table" target="#tab_1">Table 1</ref>, the source model is a pre-trained K-way neural network classifier f S (?) = ?(z S (?)) with a softmax layer ?(?) as the final model output. We omit the notation of the model parameters in our analysis because reprogramming does not change the pre-trained model parameters. The notation (x, y) is used to describe a data sample x and its one-hot coded label y. We will use the subscript s/t to denote source/target data when applicable. For the purpose of analysis, given a neural network classifier f , we consider the root mean squared error (RMSE) denoted by f (x) ? y 2 .</p><p>To put forth our analysis, we make the following assumptions based on the framework of reprogramming:</p><formula xml:id="formula_8">1. The source risk is S , that is, E D S [ (x s , y s )] = S .</formula><p>2. The source-target label space has a specified surjective one-to-one label mapping function h t for every target</p><formula xml:id="formula_9">label t, such that ?y t ? Y T , y t = h t (Y S ) y s ? Y S , and h t = h t if t = t .</formula><p>3. Based on reprogramming, the target loss function T with an additive input transformation function ? can be represented as</p><formula xml:id="formula_10">T (x t + ?, y t ) (a) = T (x t + ?, y s ) (b) = S (x t + ?, y s ), where (a)</formula><p>is induced by label mapping (Assumption 2) and (b) is induced by reprogramming the source loss with target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The learned input transformation function for reprogramming is denoted by</head><formula xml:id="formula_11">? * arg min ? E D T [ S (x t + ?, y s )]</formula><p>, which is the minimizer of the target population risk with the reprogramming loss objective.</p><p>5. Domain-independent drawing of source and target data: Let ? S (?) and ? T (?) denote the probability density function of source data and target data distributions over X S and X T , respectively. The joint probability density function is the product of their marginals, i.e.,</p><formula xml:id="formula_12">? S,T (x s , x t ) = ? S (x s ) ? ? T (x t ).</formula><p>For a given neural network classifier, the following lemma associates the expected RMSE of model predictions on two different domains with the Wasserstein-1 distance between their corresponding probability measures on the logit representations, which will play a key role in characterizing the population risk for reprogramming. Wasserstein distance is a statistical distance between two probability measures ? and ? , and it has been widely used for studying optimal  transport problems <ref type="bibr" target="#b39">(Peyr? &amp; Cuturi, 2018)</ref>. Specifically, for any p ? 1, the Wasserstein-p distance is defined as</p><formula xml:id="formula_13">W p (?, ? ) = inf ???(?,? ) x ? x p d?(x, x ) 1/p ,</formula><p>where ?(?, ? ) denotes all joint distributions ? that have marginals ? and ? .</p><p>Lemma 1: Given a K-way neural network classifier f (?) = ?(z(?)). Let ? z and ? z be the probability measures of the logit representations {z(x)} and {z(x )} from two data domains D and D , where x ? D and x ? D . Assume independent draws for x and x , i.e.,</p><formula xml:id="formula_14">? D,D (x, x ) = ? D (x) ? ? D (x ). Then E x?D, x ?D f (x) ? f (x ) 2 ? 2 ? K ? W 1 (? z , ? z ),</formula><p>where W 1 (? z , ? z ) is the Wasserstein-1 distance between ? z and ? z . Proof: Please see Appendix A.</p><p>With Lemma 1, we now state the main theorem regarding an upper bound on population risk for reprogramming.</p><p>Theorem 1: Let ? * denote the learned additive input transformation for reprogramming (Assumption 4). The population risk for the target task via reprogramming a K-way Theorem 1 shows that the target population risk via reprogramming is upper bounded by the summation of two terms: (i) the source population risk S , and (ii) the representation alignment loss in the logit layer between the source data z S (x s ) and the reprogrammed target data z S (x t + ? * ) based on the same source neural network classifier f S (?) = ?(z S (?)), measured by their Wasserstein-1 distance. The results suggest that reprogramming can attain better performance (lower risk) when the source model has a lower source loss and a smaller representation alignment loss.</p><formula xml:id="formula_15">source neural network classifier f S (?) = ?(z S (?)), denoted by E D T [ T (x t + ? * , y t )], is upper bounded by E D T [ T (x t + ? * , y t )] ? S source risk + 2 ? K ? W 1 (?(z S (x t + ? * )), ?(z S (x s ))) xt?D T , xs?D S</formula><p>In the extreme case, if the source and target representations can be fully aligned, the Wasserstein-1 distance will become 0 and thus the target task via reprogramming can perform as well as the source task. On the other hand, if the representation alignment loss is large, then it may dominate the source risk and hinder the performance on the target task. In the next section we will investigate how the representation alignment loss can inform the reprogramming performance for V2S. We would also like to make a final remark that our risk analysis can be extended beyond the additive input transformation setting, by considering a more complex function input transformation function g(x t ) (e.g., an affine transformation). However, in practice, we observe little gain of doing so in V2S and therefore focus on the additive input transformation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Acoustic Models (AMs) and Source Datasets</head><p>We start by introducing the source models and the source datasets they trained on. The pre-trained source models will be used in our V2S experiments.</p><p>Limited-vocabulary Voice Commends Dataset: To create a large-scale (?100k training samples) pre-trained acoustic model for our experiments, we select the Google Speech Commands V2 (Warden, 2018) (denoted as GSCv2) dataset, which contains 105,829 utterances of 35 words from 2,618 recorded speakers with a sampling rate of 16 kHz. We also provide some discussion on models trained on other acoustic datasets (e.g., AudioSet ) in Appendix D. In general, using the same network archi-tecture we find that AMs trained on the voice commands dataset show better V2S performance than other datasets, which could be attributed to its similarity to short-length (one-second or less) time series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention based AMs:</head><p>For training the source model, we use a popular transformer based single-head attention architecture <ref type="bibr" target="#b8">(de Andrade et al., 2018)</ref> for V2S reprogramming, denoted as V2S a <ref type="figure" target="#fig_1">(Figure 2 (a)</ref>). We also train a similar architecture with U-Net <ref type="bibr" target="#b37">(Long et al., 2015)</ref>, denoted as V2S u <ref type="figure" target="#fig_1">(Figure 2 (b)</ref>), which is designed to enhance feature extraction in acoustic tasks <ref type="bibr" target="#b54">(Yang et al., 2021a)</ref>. Both pretrained V2S a and V2S u models have a comparable number (?0.2M/0.3M) of model parameters and test accuracy (96.90%/96.92%). Their data input dimension is d ?16k and thus the reprogramming function ? has ?16k trainable parameters. The details of the attention based models and comparisons to other popular neural network architectures are given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">V2S Implementation and Baseline</head><p>V2S Implementation: We use Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016</ref>) (v2.2) to implement our V2S framework following Algorithm 1. To enable end-to-end V2S training, we use the Kapre toolkit <ref type="bibr" target="#b4">(Choi et al., 2017)</ref> to incorporate an on-GPU audio preprocessing layer, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For the V2S parameters in Algorithm 1, we use ? = 0.05 and a mini-batch size of 32 with T = 100 training epochs. We use maximal many-to-one random label mapping, which assigns |Y S | |Y T | non-overlapping source labels to every target label, where |Y| is the size of the label set Y and z is the floor function that gives the largest integer not exceeding z.</p><p>To stabilize the training process, we add weight decay as a regularization term to the V2S loss and set the regularization coefficient to be 0.04. Our implementation of V2S reprogramming layer is available at https://github.com/ huckiyang/Voice2Series-Reprogramming.</p><p>For model tuning, we use dropout during training on the reprogramming parameters ?. Moreover, during input reprogramming we also replicate the target signal x t into m segments and place them starting from the beginning of the reprogrammed input with an identical interval (see <ref type="figure" target="#fig_5">Figure  4</ref> (a) as an example with m = 3). For each task, we report the best result of V2S among a set of hyperparmeters with dropout rate ? {0, 0.1, 0.2, 0.3, 0.4} and the number of target signal replication m ? {1, 2, . . . , 10}. We use 10-fold splitting on training data to select the best performed model based on the validation loss and report the accuracy on test data with an average of 10 runs, which follows a similar experimental setting used in <ref type="bibr" target="#b2">(Cabello et al., 2020)</ref>.</p><p>Transfer Learning Baseline (TF a ): To demonstrate the effectivness of reprogramming, we also provide a transfer learning baseline using the same V2S a pre-trained model. Different from V2S, this baseline (named TF a ) does not use input reprogramming but instead allows fine-tuning the pretrained model parameters using the zero-padded target data. An additional dense layer for task-dependent classification is also included for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">UCR Time Series Classification Benchmark</head><p>UCR Archive <ref type="bibr" target="#b6">(Dau et al., 2019</ref>) is a prominent benchmark that contains a large collection of time series classification datasets with default training and testing data splitting. The current state-of-the-art (SOTA) results on test accuracy are obtained from the following methods: (i) deep neural networks including fully convolutional networks (FCN) and deep residual neural networks as reported in <ref type="bibr" target="#b49">(Wang et al., 2017)</ref>; (ii) bag-of-features framework <ref type="bibr" target="#b44">(Sch?fer, 2015;</ref><ref type="bibr" target="#b2">Cabello et al., 2020)</ref>; (iii) ensemble-based framework <ref type="bibr" target="#b20">(Hills et al., 2014;</ref><ref type="bibr" target="#b1">Bagnall et al., 2015;</ref><ref type="bibr" target="#b35">Lines et al., 2018)</ref>; (iv) time warping framework <ref type="bibr" target="#b41">(Ratanamahatana &amp; Keogh, 2005)</ref>.</p><p>To ensure each target label is at least assigned with 3 unique source labels, we select 30 time series datasets in UCR Archive with the number of target labels ? 10 for our V2S experiments. We note that for each dataset, the algorithm that achieves the current result can vary, and therefore the comparison can be unfair to V2S.</p><p>In addition to comparing the standard test accuracy of each dataset as well as the mean and median accuracy over all datasets, we also report the mean per-class error (MPCE) proposed in <ref type="bibr" target="#b49">(Wang et al., 2017)</ref>, which is a single metric for performance evaluation over multiple datasets. MPCE is the sum of per-class error (PCE) over J datasets, definded as MPCE = j?[J] PCE j = ej cj , which comprises of the error rate (e j ) and the number of classes (c j ) for each dataset.</p><p>Reprogramming Performance: <ref type="table" target="#tab_3">Table 2</ref> summarizes the performance of each method on 30 datasets. Notably, our reprogrammed V2S a model attains either better or equivalent results on 19 out of 30 time series datasets, suggesting that V2S as a single method is a competitive and promising approach for time series classification. The transfer learning baseline TF a has poor performance, which can be attributed to limited training data. V2S a has higher mean/median accuracy (accuracy increases by 1.84/2.63%) and lower MPCE (relative error decreases by about 2.87%) than that of current results, demonstrating the effectiveness of V2S. For most datasets, V2S a has better performance than V2S u , which can be explained by Theorem 1 through a lower empirical target risk upper bound (see Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Representation Alignment Loss</head><p>According to Theorem 1, the target risk is upper bounded by the sum of a fixed source risk and a representation align- ment loss between the source and reprogrammed target data. The latter is measured by the Wasserstein-1 distance of their logit representations. We use the following experiments to empirically verify the representation alignment loss during V2S training, and motivate its use for reprogramming performance assessment. Specifically, for computational efficiency we use the sliced Wasserstein-2 distance (SWD) <ref type="bibr" target="#b31">(Kolouri et al., 2018)</ref> to approximate the Wasserstein-1 distance in Theorem 1. SWD uses one-dimensional (1D) random projection (we use 1,000 runs) to compute the sliced Wasserstein-2 distance by invoking 1D-optimal transport (OT), which possesses computational efficiency when compared to higher-dimensional OT problems <ref type="bibr" target="#b39">(Peyr? &amp; Cuturi, 2018)</ref>. Moreover, the Wasserstein-1 distance is upper bounded by the Wasserstein-2 distance <ref type="bibr" target="#b39">(Peyr? &amp; Cuturi, 2018)</ref>, and therefore the SWD will serve as a good approxi-mation of the exact representation alignment loss.</p><p>Wasserstein Distance during Training: Using the Distal-PhalanxTW <ref type="bibr" target="#b7">(Davis, 2013)</ref> dataset and V2S a in <ref type="table" target="#tab_3">Table 2</ref>, <ref type="figure" target="#fig_3">Figure 3</ref> shows the validation (test) accuracy, validation (test) loss, and SWD during V2S training. One can observe a similar trend between test loss and SWD, suggesting that V2S indeed learns to reprogram the target data representations by gradually making them closer to the source data distribution, as indicated by Theorem 1.</p><p>Model Selection: Based on Theorem 1, one can leverage our derived risk bound for V2S model selection. Comparing V2S a and V2S u , <ref type="table" target="#tab_4">Table 3</ref> shows the validation loss of the source task (GSCv2 voice dataset <ref type="bibr" target="#b50">(Warden, 2018)</ref>) and the mean/median SWD over all 30 training sets of the target tasks in <ref type="table" target="#tab_3">Table 2</ref>. We find that V2S a indeed has a lower sum  <ref type="bibr" target="#b7">(Davis, 2013)</ref>. All values are averaged over the test set. The rows are (a) validation (test) accuracy, (b) validation loss, and (c) sliced Wasserstein distance (SWD) <ref type="bibr" target="#b31">(Kolouri et al., 2018)</ref>. of the source loss and SWD than V2S u , which explains its improved performance in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Additional Analysis on V2S Interpretation</head><p>To gain further insights on V2S, we study its acoustic saliency map and embedding visualization.</p><p>Attention and Class Activation Mapping: To interpret the prediction made by V2S, we provide neural saliency analysis over the spectrogram of the reprogrammed features by class activation mapping (CAM) <ref type="bibr" target="#b58">(Zhou et al., 2016)</ref> using the Worms dataset <ref type="bibr" target="#b1">(Bagnall et al., 2015)</ref> and V2S a . Activation and attention mapping methods <ref type="bibr" target="#b51">(Wu &amp; Lee, 2019)</ref> have been used in auditory analysis <ref type="bibr" target="#b12">(Fritz et al., 2007)</ref> and investigated on its relationship between audio signal and brain cortex activation by neural physiology studies <ref type="bibr" target="#b28">(Kaya &amp; Elhilali, 2017;</ref><ref type="bibr" target="#b47">Veale et al., 2017)</ref>. Interestingly, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>, the corresponding attention head (b) of pre-trained AM (non-trainable during V2S process) could still recognize the original temporal patterns from the reprogrammed input signal in (a). We also show the Mel-spectrogram of reprogrammed input in (c), indicating the activated spatialtemporal acoustic features corresponding to weighted output prediction. Furthermore, in reference to the V2S architecture introduced in <ref type="figure" target="#fig_1">Figure 2</ref>, we select the first and the second convolution layer for CAM visualization in (d) and (e).  From the analysis, we observe different functions of these two layers. The first convolution layer tends to focus on the target signal segments themselves as well as their lowfrequency acoustic features of the reprogrammed input's Mel-spectrogram in (c), whereas the second convolution layer tends to put more focus on the high-frequency components in the reprogrammed input.</p><p>Embedding Visualization: We use t-distributed stochas-Voice2Series: Reprogramming Acoustic Models for Time Series Classification <ref type="figure">Figure 5</ref>. tSNE plots of the logit representations using the Strawberry training set <ref type="bibr" target="#b21">(Holland et al., 1998)</ref> and V2Sa, for the cases of before and after V2S reprogramming, and fine-tuned transfer learning (TFa).</p><p>tic neighbor embedding (tSNE) (Van der Maaten &amp; Hinton, 2008) to visualize the logit representations of the Strawberry training set <ref type="bibr" target="#b21">(Holland et al., 1998)</ref> for the cases of before and after reprogramming, and the transfer learning baseline (TF a ). As shown in <ref type="figure">Figure 5</ref>, after reprogramming tSNE results show a clear separation between the embeddings from different target classes, suggesting that V2S indeed learns meaningful and discriminative data representations to reprogram the pre-trained acoustic model for time series classification. On the other hand, the embedding visualization of transfer learning shows low class-wise separability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Additional Discussion</head><p>In what follows, we provide additional discussion and insights for V2S reprogramming.</p><p>Many-to-one Label Mapping: The many-to-one mapping can be viewed as ensemble averaging of single-class outputs, a practical technique to improve classification for V2S reprogramming. Our current results show that many-to-one label mapping performs better results in control experiments.</p><p>Significance Testing: In Section 5, our results suggest that V2S as a single method can achieve or exceed competitive performance on 19 out of 30 datasets obtained from different methods. We further run significant testing on two levels using their accuracies based on the 30 datasets from Future Works: Our future works include a wider range of performance evaluations on different acoustic and speech models (e.g., those associated with lexical information) for V2S reprogramming, model reprogramming for lowresource speech processing, and extension to multivariate time series tasks. The proposed theory would also provide insights on analyzing the success of adversarial reprogramming in vision and language processing domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed V2S, a novel approach to reprogram a pre-trained acoustic model for time series classification. We also developed a theoretical risk analysis to characterize the reprogramming performance. Experimental results on UCR benchmark showed superior performance of V2S, by achieving new (or equal) state-of-the-art accuracy on 19 out of 30 datasets. We also provided in-depth studies on the success of V2S through representation alignment, acoustic saliency map, and embedding visualization. The V2S could further incorporate with different advanced data augmentation techniques for future studies.</p><p>Note that by Assumption 1,</p><formula xml:id="formula_16">E D S B = E D S [ (x s , y s )] = S . Next, we proceed to bound E D S ,D T A E xs?D S ,xt?D T A.</formula><p>Using Lemma 1, we have</p><formula xml:id="formula_17">E D S ,D T A ? 2 ? K ? W 1 (?(z S (x t + ? * )), ?(z S (x s ))) xt?D T , xs?D S<label>(19)</label></formula><p>Finally, take E D S ,D T on both sides of equation <ref type="formula" target="#formula_2">(18)</ref> completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pre-Trained Model Studies</head><p>We provide advanced studies over different pre-trained acoustic architectures and the associated time series classification performance. In particular, we select the models below, which have attained competitive performance tested on Google Speech Commands version 2 dataset <ref type="bibr" target="#b50">(Warden, 2018)</ref> or shown cutting-edge performance (ResNet <ref type="bibr" target="#b17">(He et al., 2016)</ref>) in the acoustic scene (VGGish <ref type="bibr" target="#b19">(Hershey et al., 2017)</ref>) and time series classification (TCN <ref type="bibr" target="#b33">(Lea et al., 2016)</ref>). These models will be compared with V2S a (recurrent Attention <ref type="bibr" target="#b8">(de Andrade et al., 2018)</ref>) and V2S u (V2S a enhanced by U-Net <ref type="bibr" target="#b54">(Yang et al., 2021a)</ref>) used in the main paper.</p><p>ResNet: Deep residual network <ref type="bibr" target="#b17">(He et al., 2016)</ref> (ResNet) is a popular deep architecture to resolve the gradient vanish issues by passing latent features with a residual connection, and it has been widely used in acoustic modeling tasks. We select a 34-layer ResNet model training from the scratch for V2S (denoted as V2S r ), which follows the identical parameter settings in <ref type="bibr" target="#b23">(Hu et al., 2020;</ref><ref type="bibr" target="#b24">2021)</ref> for reproducible studies.</p><p>VGGish: VGGish <ref type="bibr" target="#b19">(Hershey et al., 2017)</ref> is a deep and wide neural network architecture with multi-channel convolution layers, which has been proposed for speech and acoustic modeling. VGGish is also well-known for the large-scale acoustic embedding studies with Audio-Set  from 2 million Youtube audios. We use the same architecture and train two models: (i) training from scratch (denoted as V2S v ) and (ii) selecting an Audio-Set pretrained VGGish and then fine-tuning (denoted as V2S p ) on the Google Speech commands dataset for V2S.</p><p>Temporal Convolution Network (TCN): TCN <ref type="bibr" target="#b33">(Lea et al., 2016)</ref> is an efficient architecture using temporal convolution with causal kernel for sequence classification tasks. We select the TCN architecture and train it from scratch as a baseline for V2S (denoted as V2S t ).</p><p>OpenL3: OpenL3 <ref type="bibr" target="#b5">(Cramer et al., 2019)</ref> is a much recent embedding method with a deep fusion layer for acoustic modeling. We use pretrained OpenL3 embeddings and a dense layer for classification as another baseline for V2S (denoted as V2S o ). <ref type="table" target="#tab_6">Table 4</ref> shows different neural architectures for acoustic modeling to be used with the proposed V2S method, where acoustic models are pre-trained with the Google Speech Commands dataset <ref type="bibr" target="#b50">(Warden, 2018)</ref> version two with 32 commands (denoted as GSCv2.) From the first three rows of <ref type="table" target="#tab_6">Table 4</ref>, we observe the recurrent attention models and TCN perform better in mean prediction accuracy, validation loss of the source task. For the target task (same as <ref type="table" target="#tab_3">Table 2</ref>), recurrent attention models (V2S a and V2S u ) attain the best performance. Both VGGish based architectures (V2S v and V2S p ) show degraded performance on the target tasks prediction, which can be explained by the recent findings <ref type="bibr" target="#b30">(Kloberdanz, 2020)</ref> on the degraded performance of using VGG <ref type="bibr">(Simonyan &amp; Zisserman, 2015)</ref> and MobileNet-V2 <ref type="bibr" target="#b42">(Sandler et al., 2018)</ref> based "wide" and deep convolutional neural architectures for reprogramming visual models. These findings could be also explained in the sense that the wide neural architectures fail to adapt the source domain distribution according to the sliced Wasserstein distance (SWD) results (fourth row in <ref type="table" target="#tab_6">Table 4</ref>), as indicated by Theorem 1. We observe that using the pretrained models associated with higher source accuracy does not always guarantee higher target accuracy (e.g, V2S t and V2S u )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. V2S Performance and Sliced Wasserstein Distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Ablation Studies</head><p>Based on the discussion in Section C, we further select three efficient V2S models with different model capacity (0.2M/1M/4.7M), including V2S a , V2S r , and V2S t , to study the mean target accuracy in different training settings and to provide some insights into effective design of V2S models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Pretrained Models from Different Dataset</head><p>In the previous model reprogramming studies <ref type="bibr" target="#b45">(Tsai et al., 2020;</ref><ref type="bibr" target="#b9">Elsayed et al., 2019)</ref>, little has been discussed regarding the effectiveness of using different datasets to train the pretrained models. We study three other public acoustic classification benchmark datasets (source tasks), (1) TAU Urban Acoustic Scenes 2020 Mobile <ref type="bibr" target="#b18">(Heittola et al., 2020)</ref> (denoted as TAU-UAC), from the annual IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE), (2) AudioSet using in <ref type="bibr" target="#b19">(Hershey et al., 2017)</ref>, and (3) ESC-50 <ref type="bibr" target="#b40">(Piczak, 2025)</ref>, a dataset for environmental sound classification, for providing a preliminary V2S study. We extract acoustic features by Mel-spectrogram using Kapre audio layer following the same resolution and frequency setup in <ref type="bibr" target="#b55">2021b)</ref> for a fair and reproducible comparison.</p><p>As shown in <ref type="table" target="#tab_7">Table 5</ref>, the V2S models pretrained from GSCv2 show a higher mean prediction accuracy and lower SWD than the other models, which could be due to the shorter sample length (less than one second) of its source acoustic inputs. In <ref type="bibr" target="#b45">(Tsai et al., 2020)</ref>, frequency mapping techniques show improved performance for black-box (e.g., zeroth order gradient estimation <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b36">Liu et al., 2020)</ref> based) adversarial reprogramming models for image classification. We also follow the setup in <ref type="bibr" target="#b45">(Tsai et al., 2020)</ref> to compare the many-to-one frequency mapping and the many-to-one random mapping for time series classification. However, the frequency mapping based V2S results show equal or slightly worse (-0.013%) mean prediction accuracy and WSD (+0.0028) performance with 100 runs, which may be owing to the differences of the dimensions and scales between the tasks of image and time series reprogramming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. More tSNE Visualization</head><p>We provide more tSNE visualization over different test datasets to better understand the embedding results of V2S models discussed in Section 5.5. In <ref type="figure" target="#fig_6">Figure 6</ref> (a) to (e), the reprogrammed representations (rightmost side) show better disentangled results in both 2D and 3D tSNE plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Hardware Setup and Energy Cost Discussion</head><p>We use Nvidia GPUs (2080-Ti and V100) for our experiments with Compute Unified Device Architecture (CUDA) version 10.1. To conduct the results shown in <ref type="table" target="#tab_3">Table 2</ref>, it takes around 40 min to run 100 epochs (maximum) with a batch size 32 for each time series prediction dataset considering the hyper-parameters tuning (e.g., dropout rate) described in Section 5.2 of the main paper. In total, the experiments presented (30 datasets and its ablation studies) in this paper took around 120 computing hours with a 300W power supplier. As another advantage, the V2S reprogramming techniques freeze pretrained neural models and only used a reprogramming layer for training new tasks. The proposed method could potentially recycle well-trained models for an additional task to alleviate extra energy costs toward deploying responsible ML systems.</p><p>Before <ref type="formula" target="#formula_3">V2S</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Schematic illustration of the proposed Voice2Series (V2S) framework: (a) trainable reprogram layer; (b) pre-trained acoustic model (AM); (c) source-target label mapping function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>V2S architectures: (a) V2Sa (de Andrade et al., 2018) and (b) V2Su (Yang et al., 2021a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>representation alignment loss via reprogrammingProof: Please see Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Training-time reprogramming analysis using V2Sa and DistalPhalanxTW dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Targeted (blue) and reprogrammed (black) time series (b) Attention weight of reprogrammed input (c) Mel-spectrogram of reprogrammed input (d) Class activation mapping of (c) from 1st conv-layer (e) Class activation mapping of (c) from 2nd conv-layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of a data sample in the Worms dataset<ref type="bibr" target="#b1">(Bagnall et al., 2015)</ref> using V2Sa. The rows are (a) the original target time series and its reprogrammed pattern as illustrated inFigure 1, (b)the associated attention-head predicted by V2Sa, (c) Mel-spectrogram of the reprogrammed input, and (d)/(e) its neural saliency maps via class activation mapping<ref type="bibr" target="#b58">(Zhou et al., 2016)</ref> from the first/second convolution layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>More tSNE visualization. Numbers in the legend are class label indices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>have been made to advance transfer 1 Georgia Institute of Technology 2 Columbia University 3 IBM Research. Correspondence to: Chao-Han Huck Yang &lt;huckiyang@gatech.edu&gt;, Pin-Yu Chen &lt;pinyu.chen@ibm.com&gt;. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</figDesc><table><row><cell>target</cell><cell cols="2">reprogrammed</cell><cell cols="2">source output</cell><cell cols="2">target output</cell></row><row><cell></cell><cell>Reprogram Layer</cell><cell cols="2">Pretrained AM</cell><cell></cell><cell>Label Mapping</cell><cell></cell></row><row><cell>xt</cell><cell>(a)</cell><cell>xt'</cell><cell>(b)</cell><cell>ys</cell><cell>(c)</cell><cell>yt</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Mathematical notation for reprogrammingSymbolMeaning</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Voice2Series: Reprogramming Acoustic Models for Time Series Classification</figDesc><table><row><cell>Trainable</cell><cell cols="2">Pretrained (Frozen)</cell><cell>Non-Trainable</cell><cell cols="2">Label Mapping</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Reprogram</cell><cell>Audio Layer</cell><cell>Conv2D 1st</cell><cell>Conv2D 2nd</cell><cell>RNNs</cell><cell>Attention</cell><cell>Dense</cell><cell>Output</cell></row><row><cell></cell><cell>Reprogram</cell><cell>Audio Layer</cell><cell>U-Net</cell><cell>Conv2D 1st +2nd</cell><cell>RNNs</cell><cell>Attention</cell><cell>Dense</cell><cell>Output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison of test accuracy (%) on 30 UCR time series classification datasets<ref type="bibr" target="#b6">(Dau et al., 2019)</ref>. Our proposed V2Sa outperforms or ties with the current prediction results (discussed in Section 5.3) on 19 out of 30 datasets. Dataset Type Input size Train. Data Class SOTA V2S a V2S u TF a</figDesc><table><row><cell>Coffee</cell><cell>SPECTRO</cell><cell>286</cell><cell>28</cell><cell>2</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>53.57</cell></row><row><cell>DistalPhalanxTW</cell><cell>IMAGE</cell><cell>80</cell><cell>400</cell><cell>6</cell><cell cols="4">79.28 79.14 75.34 70.21</cell></row><row><cell>ECG 200</cell><cell>ECG</cell><cell>96</cell><cell>100</cell><cell>2</cell><cell>90.9</cell><cell>87</cell><cell>87.40</cell><cell>81</cell></row><row><cell>ECG 5000</cell><cell>ECG</cell><cell>140</cell><cell>500</cell><cell>5</cell><cell cols="4">94.62 93.96 93.11 58.37</cell></row><row><cell>Earthquakes</cell><cell>SENSOR</cell><cell>512</cell><cell>322</cell><cell>2</cell><cell cols="4">76.91 78.42 76.45 74.82</cell></row><row><cell>FordA</cell><cell>SENSOR</cell><cell>500</cell><cell>2500</cell><cell>2</cell><cell>96.44</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>FordB</cell><cell>SENSOR</cell><cell>500</cell><cell>3636</cell><cell>2</cell><cell>92.86</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>GunPoint</cell><cell>MOTION</cell><cell>150</cell><cell>50</cell><cell>2</cell><cell>100</cell><cell cols="3">96.67 93.33 49.33</cell></row><row><cell>HAM</cell><cell>SPECTROM</cell><cell>431</cell><cell>109</cell><cell>2</cell><cell>83.6</cell><cell cols="3">78.1 71.43 51.42</cell></row><row><cell>HandOutlines</cell><cell>IMAGE</cell><cell>2709</cell><cell>1000</cell><cell>2</cell><cell cols="4">93.24 93.24 91.08 64.05</cell></row><row><cell>Haptics</cell><cell>MOTION</cell><cell>1092</cell><cell>155</cell><cell>5</cell><cell cols="4">51.95 52.27 50.32 21.75</cell></row><row><cell>Herring</cell><cell>IMAGE</cell><cell>512</cell><cell>64</cell><cell>2</cell><cell cols="4">68.75 68.75 64.06 59.37</cell></row><row><cell>ItalyPowerDemand</cell><cell>SENSOR</cell><cell>24</cell><cell>67</cell><cell>2</cell><cell cols="3">97.06 97.08 96.31</cell><cell>97</cell></row><row><cell>Lightning2</cell><cell>SENSOR</cell><cell>637</cell><cell>60</cell><cell>2</cell><cell>86.89</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>MiddlePhalanxOutlineCorrect</cell><cell>IMAGE</cell><cell>80</cell><cell>600</cell><cell>2</cell><cell cols="4">72.23 83.51 81.79 57.04</cell></row><row><cell>MiddlePhalanxTW</cell><cell>IMAGE</cell><cell>80</cell><cell>399</cell><cell>6</cell><cell cols="4">58.69 65.58 63.64 27.27</cell></row><row><cell>Plane</cell><cell>SENSOR</cell><cell>144</cell><cell>105</cell><cell>7</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>9.52</cell></row><row><cell>ProximalPhalanxOutlineAgeGroup</cell><cell>IMAGE</cell><cell>80</cell><cell>400</cell><cell>3</cell><cell cols="4">88.09 88.78 87.8 48.78</cell></row><row><cell>ProximalPhalanxOutlineCorrect</cell><cell>IMAGE</cell><cell>80</cell><cell>600</cell><cell>2</cell><cell>92.1</cell><cell cols="3">91.07 90.03 68.38</cell></row><row><cell>ProximalPhalanxTW</cell><cell>IMAGE</cell><cell>80</cell><cell>400</cell><cell>6</cell><cell cols="4">81.86 84.88 83.41 35.12</cell></row><row><cell>SmallKitchenAppliances</cell><cell>DEVICE</cell><cell>720</cell><cell>375</cell><cell>3</cell><cell cols="4">85.33 83.47 74.93 33.33</cell></row><row><cell>SonyAIBORobotSurface</cell><cell>SENSOR</cell><cell>70</cell><cell>20</cell><cell>2</cell><cell cols="4">96.02 96.02 91.71 34.23</cell></row><row><cell>Strawberry</cell><cell>SPECTRO</cell><cell>235</cell><cell>613</cell><cell>2</cell><cell>98.1</cell><cell cols="3">97.57 91.89 64.32</cell></row><row><cell>SyntheticControl</cell><cell>SIMULATED</cell><cell>60</cell><cell>300</cell><cell>6</cell><cell>100</cell><cell>98</cell><cell>99</cell><cell>49.33</cell></row><row><cell>Trace</cell><cell>SENSOR</cell><cell>271</cell><cell>100</cell><cell>4</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>18.99</cell></row><row><cell>TwoLeadECG</cell><cell>ECG</cell><cell>82</cell><cell>23</cell><cell>2</cell><cell>100</cell><cell cols="3">96.66 97.81 49.95</cell></row><row><cell>Wafer</cell><cell>SENSOR</cell><cell>152</cell><cell>1000</cell><cell>2</cell><cell>99.98</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>WormsTwoClass</cell><cell>MOTION</cell><cell>900</cell><cell>181</cell><cell>2</cell><cell>83.12</cell><cell cols="3">98.7 90.91 57.14</cell></row><row><cell>Worms</cell><cell>MOTION</cell><cell>900</cell><cell>181</cell><cell>5</cell><cell cols="4">80.17 83.12 80.34 42.85</cell></row><row><cell>Wine</cell><cell>SPECTRO</cell><cell>234</cell><cell>57</cell><cell>2</cell><cell cols="3">92.61 90.74 90.74</cell><cell>50</cell></row><row><cell>Mean accuracy (?)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">88.02 89.37 87.36 57.57</cell></row><row><cell>Median accuracy (?)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">92.36 93.60 91.00 55.30</cell></row><row><cell>MPCE (mean per class error) (?)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.09</cell><cell>2.04</cell><cell cols="2">2.13 48.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Validation</figDesc><table><row><cell></cell><cell cols="3">loss (LossS ) of the source task (GSCv2 voice</cell></row><row><cell cols="4">dataset (Warden, 2018)) and mean/median Sliced Wasserstein Dis-</cell></row><row><cell cols="3">tance (SWD) of all training sets in Table 2.</cell><cell></cell></row><row><cell cols="4">Model Loss S (?) Mean SWD (?) Median SWD (?)</cell></row><row><cell>V2S a</cell><cell>0.1709</cell><cell>1.829</cell><cell>1.943</cell></row><row><cell>V2S u</cell><cell>0.1734</cell><cell>1.873</cell><cell>1.977</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>V2S ablation studies with different pre-trained acoustic models.</figDesc><table><row><cell>Model</cell><cell>V2S a</cell><cell>V2S u</cell><cell>V2S r</cell><cell>V2S v</cell><cell>V2S p</cell><cell>V2S t</cell><cell>V2S o</cell></row><row><cell>Parameters (?)</cell><cell>0.2M</cell><cell>0.3M</cell><cell>1M</cell><cell>62M</cell><cell>62M</cell><cell>1M</cell><cell>4.7M</cell></row><row><cell>Source Acc. (?)</cell><cell>96.90</cell><cell>96.92</cell><cell>96.40</cell><cell>95.40</cell><cell>95.19</cell><cell>96.93</cell><cell>92.34</cell></row><row><cell>Source Loss (?)</cell><cell cols="7">0.1709 0.1734 0.1786 0.1947 0.1983 0.1756 0.2145</cell></row><row><cell>Mean SWD (?)</cell><cell>1.829</cell><cell>1.873</cell><cell>1.894</cell><cell>4.716</cell><cell>4.956</cell><cell>1.901</cell><cell>5.305</cell></row><row><cell cols="2">Mean Target Acc. (?) 89.37</cell><cell>87.36</cell><cell>87.01</cell><cell>67.00</cell><cell>62.83</cell><cell>86.21</cell><cell>60.34</cell></row><row><cell>Target MPCE (?)</cell><cell>2.04</cell><cell>2.13</cell><cell>2.26</cell><cell>34.1</cell><cell>39.1</cell><cell>2.36</cell><cell>41.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>V2S performance (mean prediction accuracy) on time series classification (same asTable 2) with different pretrained neural acoustic models and the mean sliced Wasserstein distance for each dataset.</figDesc><table><row><cell>Pretrained Dataset</cell><cell cols="2">GSCv2 TAU</cell><cell cols="2">AudioSet ESC</cell></row><row><cell># Training Samples</cell><cell>105k</cell><cell>13.9k</cell><cell>2.08M</cell><cell>2k</cell></row><row><cell># Output Classes</cell><cell>35</cell><cell>10</cell><cell>527</cell><cell>50</cell></row><row><cell>Audio length per sample clips</cell><cell>1 sec.</cell><cell cols="2">10 sec. 10 sec.</cell><cell>5 sec.</cell></row><row><cell cols="2">Mean Target Acc. w/ V2S a (?) 89.37</cell><cell>82.61</cell><cell>80.68</cell><cell>84.48</cell></row><row><cell cols="2">Mean Target Acc. w/ V2S r (?) 87.36</cell><cell>83.57</cell><cell>79.96</cell><cell>83.05</cell></row><row><cell cols="2">Mean Target Acc. w/ V2S t (?) 86.31</cell><cell>80.1</cell><cell>81.81</cell><cell>84.58</cell></row><row><cell>Mean SWD per dataset (?)</cell><cell>1.882</cell><cell>2.267</cell><cell>2.481</cell><cell>2.162</cell></row><row><cell>D.2. Different V2S Mapping Settings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the comments and discussion from anonymous reviewers during the double-blind review process.</p><p>Voice2Series: Reprogramming Acoustic Models for Time Series Classification</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Lemma 1</head><p>For brevity we use [K] to denote the integer set {1, 2, . . . , K}. We have</p><p>(a) follows the neural network model, (b) follows the definition of expectation, (c) follows the assumption of independent data drawing, and (d) follows that</p><p>We further make the following three notes:</p><p>where g Lip is defined as sup x,x |g(x) ? g(x )|/ x ? x 2 , and we use the fact that [?(z)] k is 1-Lipschitz for any k ? [K] <ref type="bibr" target="#b13">(Gao &amp; Pavel, 2017)</ref> </p><p>Finally, (f) follows the Kantorovich-Rubinstein theorem <ref type="bibr" target="#b26">(Kantorovich &amp; Rubinstein, 1958)</ref> of the dual representation of the Wasserstein-1 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 1</head><p>First, we decompose the target risk function as</p><p>(a) is based on Assumption 3, (b) is based on the definition of risk function, (c) is by subtracting and adding the same term f S (x s ), and (d) is based on the triangle inequality.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Time-series classification with cote: the collective of transformationbased ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2522" to="2535" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast and accurate time series classification through supervised interval search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Naghizade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kulik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Workshop on Artificial Intelligence and Security</title>
		<imprint>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kapre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Look, listen, and learn more: Design choices for deep audio embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3852" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The ucr time series archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gharghabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1293" to="1305" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Predictive modelling of bone ageing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of East Anglia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L D S</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial reprogramming of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on big data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Auditory attention-focusing the searchlight on sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="455" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the properties of the softmax function with application in game theory and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pattern extraction for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on principles of data mining and knowledge discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="115" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00121</idno>
		<title level="m">Word-level adversarial reprogramming</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Acoustic scene classification in dcase 2020 challenge: generalization across devices and low complexity solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14623</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ieee international conference on acoustics, speech and signal processing (icassp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Classification of time series by shapelet transformation. Data mining and knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baranauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="851" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Use of fourier transform infrared spectroscopy and partial least squares regression for the detection of adulteration of strawberry purees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kemsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Science of Food and Agriculture</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11333</idno>
		<title level="m">Voice2Series: Reprogramming Acoustic Models for Time Series Classification Hong</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Multilevel knowledge-guided attention for modeling electrocardiography signals</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Devicerobust acoustic scene classification based on two-stage categorization and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08389</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A two-stage approach to device-robust acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="845" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heartbeat time series classification with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kampouraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nikou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="512" to="518" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On a space of completely additive functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kantorovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vestnik Leningradskogo Universiteta</title>
		<imprint>
			<date type="published" when="1958" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convtimenet: A pre-trained deep convolutional neural network for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashiparekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Narwariya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modelling auditory attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">20160101</biblScope>
			<date type="published" when="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<title level="m">A method for stochastic optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reprogramming of neural networks: A new and improved machine learning technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kloberdanz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ISU Master Thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sliced wasserstein distance for learning gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3427" to="3436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A review of unsupervised feature learning and deep learning for timeseries modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Langkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A shapelet transform for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Time series classification with hive-cote: The hierarchical vote collective of transformation-based ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A primer on zeroth-order optimization in signal processing and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial reprogramming of text classification neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neekhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5219" to="5228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Computational optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00567</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv e-prints. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esc</surname></persName>
		</author>
		<idno type="DOI">10.1145/2733373.2806390</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=2733373.2806390" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2025" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Three myths about dynamic time warping data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 SIAM international conference on data mining</title>
		<meeting>the 2005 SIAM international conference on data mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="506" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-L</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The boss is concerned with time series classification in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sch?fer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Voice2Series: Reprogramming Acoustic Models for Time Series Classification Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1505" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9614" to="9624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">How is visual salience computed in the brain? insights from behaviour, neurobiology and modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Hafed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">20160113</biblScope>
			<date type="published" when="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reprogramming language models for molecular representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03460</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Enhancing sound texture in cnn-based acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="815" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="7" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Characterizing speech adversarial examples using selfattention u-net enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3107" to="3111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Decentralizing feature extraction with quantum convolutional neural network for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6523" to="6527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pateaae: Incorporating adversarial autoencoder into private aggregation of teacher ensembles for spoken command classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01271</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A novel transfer learning framework for time series forecasting. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="74" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Time series classification using support vector machine with gaussian elastic metric kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="29" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
	<note>Voice2Series: Reprogramming Acoustic Models for Time Series Classification</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
