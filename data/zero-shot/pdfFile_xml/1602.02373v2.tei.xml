<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RIEJOHNSON@GMAIL.COM RJ Research Consulting</orgName>
								<address>
									<region>Tarrytown, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">TONGZHANG@BAIDU.COM Big Data Lab, Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson &amp; Zhang, 2015a;b). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of 'text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text categorization is the task of assigning labels to documents written in a natural language, and it has numerous real-world applications including sentiment analysis as well as traditional topic assignment tasks. The state-of-the art methods for text categorization had long been linear predictors (e.g., SVM with a linear kernel) with either bag-ofword or bag-of-n-gram vectors (hereafter bow) as input, e.g., <ref type="bibr" target="#b7">(Joachims, 1998;</ref><ref type="bibr" target="#b14">Lewis et al., 2004)</ref>. This, however, A convolutional neural network (CNN) <ref type="bibr" target="#b13">(LeCun et al., 1986</ref>) is a feedforward neural network with convolution layers interleaved with pooling layers, originally developed for image processing. In its convolution layer, a small region of data (e.g., a small square of image) at every location is converted to a low-dimensional vector with information relevant to the task being preserved, which we loosely term 'embedding'. The embedding function is shared among all the locations, so that useful features can be detected irrespective of their locations. In its simplest form, onehot CNN works as follows. A document is represented as a sequence of one-hot vectors (each of which indicates a word by the position of a 1); a convolution layer converts small regions of the document (e.g., "I love it") to low-dimensional vectors at every location (embedding of text regions); a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average; and the top layer classifies a document vector with a linear model <ref type="figure">(Figure 1)</ref>. The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods.</p><p>In this work, we consider a more general framework (subsuming one-hot CNN) which jointly trains a feature generator and a linear model, where the feature generator consists of 'region embedding + pooling'. The specific region embedding function of one-hot CNN takes the simple form v(x ) = max(0, Wx + b) ,</p><p>where x is a concatenation of one-hot vectors (therefore, 'one-hot' in the name) of the words in the -th region (of a fixed size), and the weight matrix W and the bias vector b need to be trained. It is simple and fast to compute, and considering its simplicity, the method works surprisingly well if the region size is appropriately set. However, there are also potential shortcomings. The region size must be fixed, which may not be optimal as the size of relevant regions may vary. Practically, the region size cannot be very large as the number of parameters to be learned (components of W) depends on it. JZ15 proposed variations to alleviate these issues. For example, a bow-input variation allows x above to be a bow vector of the region. This enables a larger region, but at the expense of losing word order in the region and so its use may be limited.</p><p>In this work, we build on the general framework of 'region embedding + pooling' and explore a more sophisticated region embedding via Long Short-Term Memory (LSTM), seeking to overcome the shortcomings above, in the supervised and semi-supervised settings. LSTM <ref type="bibr" target="#b6">(Hochreiter &amp; Schmidhuder, 1997</ref>) is a recurrent neural network. In its typical applications to text, an LSTM takes words in a sequence one by one; i.e., at time t, it takes as input the t-th word and the output from time t ? 1. Therefore, the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far (or a relevant part of it). It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks. That is, an LSTM can be used to embed text regions of variable (and possibly large) sizes.</p><p>We pursue the best use of LSTM for our purpose, and then compare the resulting model with the previous best methods including one-hot CNN and previous LSTM. Our strategy is to simplify the model as much as possible, including elimination of a word embedding layer routinely used to produce input to LSTM. Our findings are threefold. First, in the supervised setting, our simplification strategy leads to higher accuracy and faster training than previous LSTM. Second, accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input. Third, both our LSTM models and one-hot CNN strongly outperform other methods including previous LSTM. The best results are obtained by combining the two types of region embeddings (LSTM embed-dings and CNN embeddings) trained on unlabeled data, indicating that their strengths are complementary. Overall, our results show that for text categorization, embeddings of text regions, which can convey higher-level concepts than single words in isolation, are useful, and that useful region embeddings can be learned without going through word embedding learning. We report performances exceeding the previous best results on four benchmark datasets. Our code and experimental details are available at http://riejohnson.com/cnn download.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Preliminary</head><p>On text, LSTM has been used for labeling or generating words. It has been also used for representing short sentences mostly for sentiment analysis, and some of them rely on syntactic parse trees; see e.g., <ref type="bibr" target="#b23">(Zhu et al., 2015;</ref><ref type="bibr" target="#b18">Tang et al., 2015;</ref><ref type="bibr" target="#b17">Tai et al., 2015;</ref><ref type="bibr" target="#b11">Le &amp; Zuidema, 2015)</ref>. Unlike these studies, this work as well as JZ15 focuses on classifying general full-length documents without any special linguistic knowledge. Similarly, DL15 <ref type="bibr" target="#b3">(Dai &amp; Le, 2015)</ref> applied LSTM to categorizing general full-length documents. Therefore, our empirical comparisons will focus on DL15 and JZ15, both of which reported new state of the art results. Let us first introduce the general LSTM formulation, and then briefly describe DL15's model as it illustrates the challenges in using LSTMs for this task.</p><p>LSTM While several variations exist, we base our work on the following LSTM formulation, which was used in, e.g., <ref type="bibr" target="#b21">(Zaremba &amp; Sutskever, 2014)</ref> </p><formula xml:id="formula_1">i t = ?(W (i) x t + U (i) h t?1 + b (i) ) , o t = ?(W (o) x t + U (o) h t?1 + b (o) ) , f t = ?(W (f ) x t + U (f ) h t?1 + b (f ) ) , u t = tanh(W (u) x t + U (u) h t?1 + b (u) ) , c t = i t u t + f t c t?1 , h t = o t tanh(c t ) ,</formula><p>where denotes element-wise multiplication and ? is an element-wise squash function to make the gating values in [0, 1]. We fix ? to sigmoid. x t ? R d is the input from the lower layer at time step t, where d would be, for example, size of vocabulary if the input was a one-hot vector representing a word, or the dimensionality of word vector if the lower layer was a word embedding layer. With q LSTM units, the dimensionality of the weight matrices and bias vectors, which need to be trained, are W (?) ? ? R q?d , U (?) ? R q?q , and b (?) ? R q for all types <ref type="bibr">(i, o, f, u)</ref>. The centerpiece of LSTM is the memory cells c t , designed to counteract the risk of vanishing/exploding gradients, thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks. The forget gate f t <ref type="bibr" target="#b4">(Gers et al., 2000)</ref> is for resetting the memory cells. The input gate i t and output gate o t control the input and output of the memory cells.</p><p>Word-vector LSTM (wv-LSTM) [DL15] DL15's application of LSTM to text categorization is straightforward. As illustrated in <ref type="figure">Figure 2</ref>, for each document, the output of the LSTM layer is the output of the last time step (corresponding to the last word of the document), which represents the whole document (document embedding). Like many other studies of LSTM on text, words are first converted to low-dimensional dense word vectors via a word embedding layer; therefore, we call it word-vector LSTM or wv-LSTM. DL15 observed that wv-LSTM underperformed linear predictors and its training was unstable. This was attributed to the fact that documents are long.</p><p>In addition, we found that training and testing of wv-LSTM is time/resource consuming. To put it into perspective, using a GPU, one epoch of wv-LSTM training takes nearly 20 times longer than that of one-hot CNN training even though it achieves poorer accuracy (the first two rows of <ref type="table">Table 1</ref>). This is due to the sequential nature of LSTM, i.e., computation at time t requires the output of time t ? 1, whereas modern computation depends on parallelization for speedup. Documents in a mini-batch can be processed in parallel, but the variability of document lengths reduces the degree of parallelization 1 .</p><p>It was shown in DL15 that training becomes stable and accuracy improves drastically when LSTM and the word embedding layer are jointly pre-trained with either the language model learning objective (predicting the next word) or autoencoder objective (memorizing the document).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Supervised LSTM for text categorization</head><p>Within the framework of 'region embedding + pooling' for text categorization, we seek effective and efficient use of LSTM as an alternative region embedding method. This section focuses on an end-to-end supervised setting so that there is no additional data (e.g., unlabeled data) or additional algorithm (e.g., for learning a word embedding). Our general strategy is to simplify the model as much as possible. We start with elimination of the word embedding layer so that one-hot vectors are directly fed to LSTM, which we call one-hot LSTM in short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Elimination of the word embedding layer</head><p>Facts: A word embedding is a linear operation that can be written as Vx t with x t being a one-hot vector and columns of V being word vectors. Therefore, by replacing the LSTM weights W (?) with W (?) V and removing the word embedding layer, a word-vector LSTM can be turned into a one-hot LSTM without changing the model behavior. Thus, word-vector LSTM is not more expressive than one-hot LSTM; rather, a merit, if any, of training with a word embedding layer would be through imposing restrictions (e.g., a low-rank V makes a less expressive model) to achieve good prior/regularization effects.</p><p>In the end-to-end supervised setting, a word embedding matrix V would need to be initialized randomly and trained as part of the model. In the preliminary experiments under our framework, we were unable to improve accuracy over one-hot LSTM by inclusion of such a randomly initialized word embedding layer; i.e., random vectors failed to provide good prior effects. Instead, demerits were evident -more meta-parameters to tune, poor accuracy with lowdimensional word vectors, and slow training/testing with high-dimensional word vectors as they are dense.</p><p>If a word embedding is appropriately pre-trained with unlabeled data, its inclusion is a form of semi-supervised learning and could be useful. We will show later, however, that this type of approach falls behind our approach of learning region embeddings through training one-hot LSTM on unlabeled data. Altogether, elimination of the word embedding layer was found to be useful; thus, we base our work on one-hot LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">More simplifications</head><p>We introduce four more useful modifications to wv-LSTM that lead to higher accuracy or faster training.</p><p>Pooling: simplifying sub-problems Our framework of 'region embedding + pooling' has a simplification effect as follows. In wv-LSTM, the sub-problem that LSTM needs to solve is to represent the entire document by one vector (document embedding). We make this easy by changing it to detecting regions of text (of arbitrary sizes) that are relevant to the task and representing them by vectors (region embedding). As illustrated in <ref type="figure">Figure 3</ref>, we let the LSTM layer emit vectors h t at each time step, and let pooling aggregate them into a document vector. With wv-LSTM, LSTM has to remember relevant information until it gets to the end of the document even if relevant information was observed 10K words away. The task of our LSTM is easier as it is allowed to forget old things via the forget gate and can focus on representing the concepts conveyed by smaller segments such as phrases or sentences.</p><p>A related architecture appears in the Deep Learning Tutorials 2 though it uses a word embedding. Another related work is <ref type="bibr" target="#b10">(Lai et al., 2015)</ref>, which combined pooling with non-LSTM recurrent networks and a word embedding.</p><p>Chopping for speeding up training In addition to simplifying the sub-problem, pooling has the merit of enabling faster training via chopping. Since we set the goal of LSTM to embedding text regions instead of documents, it is no longer crucial to go through the document from the beginning to the end sequentially. At the time of training, we can chop each document into segments of a fixed length that is sufficiently long (e.g., 50 or 100) and process all the segments in a mini batch in parallel as if these segments were individual documents. (Note that this is done only in the LSTM layer and pooling is done over the entire document.) We perform testing without chopping. That is, we train LSTM with approximations of sequences for speed up and test with real sequences for better accuracy. There is a risk of chopping important phrases (e.g., "don't | like it"), and this can be easily avoided by having segments slightly overlap. However, we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping.</p><p>Removing the input/output gates We found that when LSTM is followed by pooling, the presence of input and output gates typically does not improve accuracy, while removing them nearly halves the time and memory required for training and testing. It is intuitive, in particular, that pooling can make the output gate unnecessary; the role of the output gate is to prevent undesirable information from entering the output h t , and such irrelevant information can be filtered out by max-pooling. Without the input and output gates, the LSTM formulation can be simplified to:</p><formula xml:id="formula_2">f t = ?(W (f ) x t + U (f ) h t?1 + b (f ) ) ,</formula><p>(2)</p><formula xml:id="formula_3">u t = tanh(W (u) x t + U (u) h t?1 + b (u) ) , (3) c t = u t + f t c t?1 , h t = tanh(c t ) .</formula><p>This is equivalent to fixing i t and o t to all ones. It is in spirit similar to Gated Recurrent Units <ref type="bibr" target="#b2">(Cho et al., 2014)</ref> but simpler, having fewer gates.</p><p>Bidirectional LSTM for better accuracy The changes from wv-LSTM above substantially reduce the time and 2 http://deeplearning.net/tutorial/lstm.html One-hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, we concatenate the output of a forward LSTM (left to right) and a backward LSTM (right to left), which is referred to as bidirectional LSTM in the literature. The resulting model is a one-hot bidirectional LSTM with pooling, and we abbreviate it to oh-2LSTMp. <ref type="table">Table 1</ref> shows how much accuracy and/or training speed can be improved by elimination of the word embedding layer, pooling, chopping, removing the input/output gates, and adding the backward LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Experiments (supervised)</head><p>We used four datasets: IMDB, Elec, RCV1 (second-level topics), and 20-newsgroup (20NG) 3 , to facilitate direct comparison with JZ15 and DL15. The first three were used in JZ15. IMDB and 20NG were used in DL15. The datasets are summarized in <ref type="table">Table 2</ref>.</p><p>The data was converted to lower-case letters. In the neural network experiments, vocabulary was reduced to the most frequent 30K words of the training data to reduce computational burden; square loss was minimized with dropout  applied to the input to the top layer; weights were initialized by the <ref type="table" target="#tab_2">Gaussian distribution with  #train  #test  avg max #class  IMDB 25,000 25,000 265  3K  2  Elec  25,000 25,000 124  6K  2  RCV1 15,564 49,838 249 12K  55  20NG 11,293 7,528 267 12K  20   Table 2</ref>. Data. "avg"/"max": the average/maximum length of documents (#words) of the training/test data. IMDB and Elec are for sentiment classification (positive vs. negative) of movie reviews and Amazon electronics product reviews, respectively. RCV1 (second-level topics only) and 20NG are for topic categorization of Reuters news articles and newsgroup messages, respectively.</p><p>zero mean and standard deviation 0.01. Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop <ref type="bibr" target="#b19">(Tieleman &amp; Hinton, 2012)</ref> for acceleration.</p><p>Hyper parameters such as learning rates were chosen based on the performance on the development data, which was a held-out portion of the training data, and training was redone using all the training data with the chosen parameters.</p><p>We used the same pooling method as used in JZ15, which parameterizes the number of pooling regions so that pooling is done for k non-overlapping regions of equal size, and the resulting k vectors are concatenated to make one vector per document. The pooling settings chosen based on the performance on the development data are the same as JZ15a, which are max-pooling with k=1 on IMDB and Elec and average-pooling with k=10 on RCV1; on 20NG, max-pooling with k=10 was chosen.   Comparing the two types of LSTM in <ref type="table" target="#tab_2">Table 3</ref>, we see that our one-hot bidirectional LSTM with pooling (oh-2LSTMp) outperforms word-vector LSTM (wv-LSTM) on all the datasets, confirming the effectiveness of our approach.</p><p>Now we review the non-LSTM baseline methods. The last row of <ref type="table" target="#tab_2">Table 3</ref> shows the best one-hot CNN results within the constraints above. They were obtained by bow-CNN (whose input to the embedding function <ref type="formula" target="#formula_0">(1)</ref> is a bow vector of the region) with region size 20 on RCV1, and seq-CNN (with the regular concatenation input) with region size 3 on the others. In <ref type="table" target="#tab_2">Table 3</ref>, on three out of the four datasets, oh-2LSTMp outperforms SVM and the CNN. However, on RCV1, it underperforms both. We conjecture that this is because strict word order is not very useful on RCV1. This point can also be observed in the SVM and CNN performances. Only on RCV1, n-gram SVM is no better than bag-of-word SVM, and only on RCV1, bow-CNN outperforms seq-CNN. That is, on RCV1, bags of words in a window of 20 at every location are more useful than words in strict order. This is presumably because the former can more easily cover variability of expressions indicative of topics. Thus, LSTM, which does not have an ability to put words into bags, loses to bow-CNN. More on one-hot CNN vs. one-hot LSTM LSTM can embed regions of variable (and possibly large) sizes whereas CNN requires the region size to be fixed. We attribute to this fact the small improvements of oh-2LSTMp over oh-CNN in <ref type="table" target="#tab_2">Table 3</ref>. However, this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes. We show in the table above that one-hot CNNs with two layers (of 1000 feature maps each) with two different region sizes 4 rival oh-2LSTMp. Although these models are larger than those in <ref type="table" target="#tab_2">Table 3</ref>, training/testing is still faster than the LSTM models due to simplicity of the region embeddings. By comparison, the strength of LSTM to embed larger regions appears not to be a big contributor here. This may be because the amount of training data is not sufficient enough to learn the relevance of longer word sequences. Overall, one-hot CNN works surprising well considering its simplicity, and this observation motivates the idea of combining the two types of region embeddings, discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the previous best results on 20NG</head><p>The previous best performance on 20NG is 15.3 (not shown in the table) of DL15, obtained by pre-training wv-LSTM of 1024 units with labeled training data. Our oh-2LSTMp achieved 13.32, which is 2% better. The previous best results on the other datasets use unlabeled data, and we will review them with our semi-supervised results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-supervised LSTM</head><p>To exploit unlabeled data as an additional resource, we use a non-linear extension of two-view feature learning, whose linear version appeared in our earlier work <ref type="bibr" target="#b0">(Ando &amp; Zhang, 2005;</ref><ref type="bibr" target="#b1">2007)</ref>. This was used in JZ15b to learn from unlabeled data a region embedding embodied by a convolution layer. In this work we use it to learn a region embedding embodied by a one-hot LSTM. Let us start with a brief review of non-linear two-view feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-view embedding (tv-embedding) [JZ15b]</head><p>A rough sketch is as follows. Consider two views of the input. An embedding is called a tv-embedding if the embedded view is as good as the original view for the purpose of predicting the other view. If the two views and the labels (classification targets) are related to one another only through some hidden states, then the tv-embedded view is as good as the original view for the purpose of classification. Such an embedding is useful provided that its dimensionality is much lower than the original view.</p><p>JZ15b applied this idea by regarding text regions embedded by the convolution layer as one view and their surrounding context as the other view and training a tv-embedding (embodied by a convolution layer) on unlabeled data. The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one-hot CNN, resulting in higher accuracy.   we consider the following two views: the words we have already seen in the document (view-1), and the next few words (view-2). The task of tv-embedding learning is to predict view-2 based on view-1. We train one-hot LSTMs in both directions, as in <ref type="figure" target="#fig_2">Figure 5</ref>, on unlabeled data. For this purpose, we use the input and output gates as well as the forget gate as we found them to be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning LSTM tv-embeddings</head><p>The theory of tv-embedding says that the region embeddings obtained in this way are useful for the task of interest if the two views are related to each other through the concepts relevant to the task. To reduce undesirable relations between the views such as syntactic relations, JZ15b performed vocabulary control to remove function words from (and only from) the vocabulary of the target view, which we found useful also for LSTM.</p><p>We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing <ref type="formula">(2)</ref> and (3) by the following:</p><formula xml:id="formula_4">f t = ?(W (f ) x t + j?S W (j,f ) x j t + U (f ) h t?1 + b (f ) ) , u t = ?(W (u) x t + j?S W (j,u) x j t + U (u) h t?1 + b (u)</formula><p>) .</p><p>x j t is the output of a tv-embedding (an LSTM trained with unlabeled data) indexed by j at time step t, and S is a set of tv-embeddings which contains the two LSTMs going forward and backward as in <ref type="figure" target="#fig_2">Figure 5</ref>. Although it is possible to fine-tune the tv-embeddings with labeled data, for simplicity and faster training, we fixed them in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Combining LSTM tv-embeddings and CNN tv-embeddings</head><p>It is easy to see that the set S above can be expanded with any tv-embeddings, not only those in the form of LSTM (LSTM tv-embeddings) but also with the tv-embeddings in the form of convolution layers (CNN tv-embeddings) such as those obtained in JZ15b. Similarly, it is possible to use LSTM tv-embeddings to produce additional input to CNN. While both LSTM tv-embeddings and CNN tv-embeddings are region embeddings, their formulations are very different from each other; therefore, we expect that they complement each other and bring further performance improvements when combined. We will empirically confirm these conjectures in the experiments below. Note that being able to naturally combine several tv-embeddings is a strength of 2?100-dim LSTM tv-embed. 6.66 6.08 9.24 5 oh-CNN <ref type="bibr">[JZ15b]</ref> 1?200-dim CNN tv-embed. 6.81 6.57 7.97 our framework, which uses unlabeled data to produce additional input to LSTM instead of pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semi-supervised experiments</head><p>We used IMDB, Elec, and RCV1 for our semi-supervised experiments; 20NG was excluded due to the absence of standard unlabeled data. <ref type="table" target="#tab_6">Table 4</ref> summarizes the unlabeled data. To experiment with LSTM tv-embeddings, we trained two LSTMs (forward and backward) with 100 units each on unlabeled data. The training objective was to predict the next k words where k was set to 20 for RCV1 and 5 for others. Similar to JZ15b, we minimized weighted square</p><formula xml:id="formula_5">loss i,j ? i,j (z i [j] ? p i [j]</formula><p>) 2 where i goes through the time steps, z represents the next k words by a bow vector, and p is the model output; ? i,j were set to achieve negative sampling effect for speed-up; vocabulary control was performed for reducing undesirable relations between views, which sets the vocabulary of the target (i.e., the k words) to the 30K most frequent words excluding function words (or stop words on RCV1). Other details followed the supervised experiments.</p><p>Our semi-supervised one-hot bidirectional LSTM with pooling (oh-2LSTMp) in row#4 of <ref type="table" target="#tab_8">Table 5</ref> used the two LSTM tv-embeddings trained on unlabeled data as described above, to produce additional input to one-hot LSTMs in two directions (500 units each). Compared with the supervised oh-2LSTMp <ref type="table" target="#tab_2">(Table 3)</ref>, clear performance improvements were obtained on all the datasets, thus, confirming the effectiveness of our approach.</p><p>We review the semi-supervised performance of wv-LSTMs <ref type="table" target="#tab_8">(Table 5 row#1</ref>). In DL15 the model consisted of a word embedding layer of 512 dimensions, an LSTM layer with 1024 units, and 30 hidden units on top of the LSTM layer; the word embedding layer and the LSTM were pre-trained with unlabeled data and were fine-tuned with labeled data; pre-training used either the language model objective or autoencoder objective. The error rate on IMDB is from DL15, and those on Elec and RCV1 are our best effort to perform pre-training with the language model objective. We used the same configuration on Elec as DL15; however, on RCV1, which has 55 classes, 30 hidden units turned out to be too few and we changed it to 1000. Although the pre-trained wv-LSTM clearly outperformed the supervised wv-LSTM <ref type="table" target="#tab_2">(Table 3)</ref>, it underperformed the models with region tv-embeddings <ref type="table" target="#tab_6">(Table 5 row#4</ref> <ref type="bibr">,5)</ref>.</p><p>Previous studies on LSTM for text often convert words into pre-trained word vectors, and word2vec <ref type="bibr" target="#b15">(Mikolov et al., 2013)</ref> is a popular choice for this purpose. Therefore, we tested wv-2LSTMp (word-vector bidirectional LSTM with pooling), whose only difference from oh-2LSTMp is that the input to the LSTM layers is the pre-trained word vectors. The word vectors were optionally updated (finetuned) during training. Two types of word vectors were tested. The Google News word vectors were trained by word2vec on a huge (100 billion-word) news corpus and are provided publicly. On our tasks, wv-2LSTMp using the Google News vectors ( <ref type="table" target="#tab_8">Table 5</ref> row#2) performed relatively poorly. When word2vec was trained with the domain unlabeled data, better results were observed after we scaled word vectors appropriately <ref type="table" target="#tab_2">(Table 5 row#3</ref>). Still, it underperformed the models with region tv-embeddings (row #4,5), which used the same domain unlabeled data. We attribute the superiority of the models with tv-embeddings to the fact that they learn, from unlabeled data, embeddings of text regions, which can convey higher-level concepts than single words in isolation. Now we review the performance of one-hot CNN with one 200-dim CNN tv-embedding <ref type="table" target="#tab_8">(Table 5</ref> row#5), which is comparable with our LSTM with two 100-dim LSTM tv-embeddings (row#4) in terms of the dimensionality of tv-embeddings. The LSTM (row#4) rivals or outperforms the CNN (row#5) on IMDB/Elec but underperforms it on RCV1. Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1, we obtain 8.62, but it still does not reach 7.97 of the CNN. As discussed earlier, we attribute the superiority of one-hot CNN on RCV1 to its unique way of representing parts of documents via bow input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Experiments combining CNN tv-embeddings and LSTM tv-embeddings</head><p>In Section 3.3 we noted that LSTM tv-embeddings and CNN tv-embeddings can be naturally combined. We experimented with this idea in the following two settings. In one setting, oh-2LSTMp takes additional input from five embeddings: two LSTM tv-embeddings used in <ref type="table" target="#tab_8">Table 5</ref> and three CNN tv-embeddings from JZ15b obtained by three distinct combinations of training objectives and input representations, which are publicly provided. These CNN tv-embeddings were trained to be applied to text regions of size k at every location taking bow input, where k is 5 on IMDB/Elec and 20 on RCV1. We connect each of the CNN tv-embeddings to an LSTM by aligning the centers of the regions of the former with the LSTM time steps; e.g., the CNN tv-embedding result on the first five words is passed to the LSTM at the time step on the third word. In the second setting, we trained one-hot CNN with these five types of tv-embeddings by replacing (1) max(0, Wx + b) by max(0, Wx + j W (j) x j + b) where x j is the output of the j-th tv-embedding with the same alignment as above.</p><p>Rows 3-4 of <ref type="table" target="#tab_9">Table 6</ref> show the results of these two types of models. For comparison, we also show the results of the LSTM with LSTM tv-embeddings only (row#1) and the CNN with CNN tv-embeddings only (row#2). To see the effects of combination, compare row#3 with row#1, and compare row#4 with row#2. For example, adding the CNN tv-embeddings to the LSTM of row#1, the error rate on IMDB improved from 6.66 to 5.94, and adding the LSTM tv-embeddings to the CNN of row#2, the error rate on RCV1 improved from 7.71 to 7.15. The results indicate that, as expected, LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Comparison with the previous best results</head><p>The previous best results in the literature are shown in Table 7. More results of previous semi-supervised models can be found in JZ15b, all of which clearly underperform the semi-supervised one-hot CNN of <ref type="table">Table 7</ref>. The best supervised results on IMDB/Elec of JZ15a are in the first row, obtained by integrating a document embedding layer into one-hot CNN. Many more of the previous results on IMDB can be found in <ref type="bibr" target="#b12">(Le &amp; Mikolov, 2014)</ref>, all of which are over 10% except for 8.78 by bi-gram NBSVM <ref type="bibr" target="#b20">(Wang &amp; Manning, 2012)</ref>. 7.42 by paragraph vectors <ref type="bibr" target="#b12">(Le &amp; Mikolov, 2014</ref>) and 6.51 by JZ15b were considered to be large improvements. As shown in the last row of <ref type="table">Table 7</ref>, our new model further improved it to 5.94; also on Elec and RCV1, our best models exceeded the previous best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Within the general framework of 'region embedding + pooling' for text categorization, we explored region embeddings via one-hot LSTM. The region embedding of onehot LSTM rivaled or outperformed that of the state-of-the art one-hot CNN, proving its effectiveness. We also found that the models with either one of these two types of region embedding strongly outperformed other methods including previous LSTM. The best results were obtained by combining the two types of region embedding trained on unlabeled data, suggesting that their strengths are complementary. As a result, we reported substantial improvements over the previous best results on benchmark datasets.</p><p>At a high level, our results indicate the following. First, on this task, embeddings of text regions, which can convey higher-level concepts, are more useful than embeddings of single words in isolation. Second, useful region embeddings can be learned by working with one-hot vectors directly, either on labeled data or unlabeled data. Finally, a promising future direction might be to seek, under this framework, new region embedding methods with complementary benefits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Word vector LSTM (wv-LSTM) as in [DL15]. One-hot LSTM with pooling (oh-LSTMp).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>oh-2LSTMp: our one-hot bidirectional LSTM with pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Training LSTM tv-embeddings on unlabeled data In this work we obtain a tv-embedding in the form of LSTM from unlabeled data as follows. At each time step,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Error rates (%). Supervised results without any pretraining. SVM and oh-CNN results on all but 20NG are from JZ15a and JZ15b, respectively; wv-LSTM results on IMDB and 20NG are from DL15; all others are new experimental results of this work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>-dimensional vector at each time step. An exception is wv-LSTM, equipped with 512 LSTM units (smaller than 2?500) and a word embedding layer of 512 dimensions; DL15 states that without pre-training, addition of more LSTM units broke down training. A more complex and larger one-hot CNN will be reviewed later.</figDesc><table><row><cell>shows the error rates obtained without any addi-</cell></row><row><cell>tional unlabeled data or pre-training of any sort. For mean-</cell></row><row><cell>ingful comparison, this table shows neural networks with</cell></row><row><cell>comparable dimensionality of embeddings, which are one-</cell></row><row><cell>hot CNN with one convolution layer with 1000 feature</cell></row><row><cell>maps and bidirectional LSTMs of 500 units each. In other</cell></row><row><cell>words, the convolution layer produces a 1000-dimensional</cell></row><row><cell>vector at each location, and the LSTM in each direction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Unlabeled data. See JZ15b for more details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Semi-supervised error rates (%). The wv-LSTM result on IMDB is from [DL15]; the oh-CNN results are from [JZ15b]; all others are the results of our new experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Error rates (%) obtained by combining CNN tv-embed. and LSTM tv-embed. (rows 3-4). LSTM tv-embed. were 100-dim each on IMDB and Elec, and 300-dim on RCV1. To see the combination effects, compare row#3 with #1, and compare row#4 with #2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unlabeled data usage</cell><cell>IMDB Elec RCV1</cell></row><row><cell cols="3">1 oh-2LSTMp</cell><cell cols="2">two LSTM tv-embed.</cell><cell>6.66</cell><cell>6.08</cell><cell>8.62</cell></row><row><cell cols="5">2 oh-CNN [JZ15b] 3?100-dim CNN tv-embed.</cell><cell>6.51</cell><cell>6.27</cell><cell>7.71</cell></row><row><cell cols="3">3 oh-2LSTMp</cell><cell cols="2">3?100-dim CNN tv-embed.</cell><cell>5.94</cell><cell>5.55</cell><cell>8.52</cell></row><row><cell cols="2">4 oh-CNN</cell><cell></cell><cell></cell><cell>+ two LSTM tv-embed.</cell><cell>6.05</cell><cell>5.87</cell><cell>7.15</cell></row><row><cell></cell><cell cols="2">U IMDB</cell><cell>Elec</cell><cell>RCV1</cell></row><row><cell>oh-CNN+doc. [JZ15a]</cell><cell>N</cell><cell>7.67</cell><cell>7.14</cell><cell>-</cell></row><row><cell cols="5">Co-tr. optimized [JZ15b] Y (8.06) (7.63) (8.73)</cell></row><row><cell>Para.vector [LM14]</cell><cell>Y</cell><cell>7.42</cell><cell>-</cell><cell>-</cell></row><row><cell>wv-LSTM [DL15]</cell><cell>Y</cell><cell>7.24</cell><cell>-</cell><cell>-</cell></row><row><cell>oh-CNN(semi.) [JZ15b]</cell><cell>Y</cell><cell>6.51</cell><cell>6.27</cell><cell>7.71</cell></row><row><cell>Our best model</cell><cell>Y</cell><cell>5.94</cell><cell>5.55</cell><cell>7.15</cell></row><row><cell cols="5">Table 7. Comparison with previous best results. Error rates (%).</cell></row><row><cell cols="5">"U": Was unlabeled data used? "Co-tr. optimized": co-training</cell></row><row><cell cols="5">using oh-CNN as a base learner with parameters (e.g., when to</cell></row><row><cell cols="5">stop) optimized on the test data; it demonstrates the difficulty of</cell></row><row><cell cols="3">exploiting unlabeled data on these tasks.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> suggested making each mini-batch consist of sequences of similar lengths, but we found that on our tasks this strategy slows down convergence presumably by hampering the stochastic nature of SGD.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://ana.cachopo.org/datasets-for-single-label-textcategorization</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Region sizes were 2 and 3 for IMDB, 3 and 4 for Elec, and 3 and 20 (bow input) for 20NG.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank anonymous reviewers for valuable feedback. This research was partially supported by NSF IIS-1250985, NSF IIS-1407939, and NIH R01AI116744.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-view feature generation model for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compositional distributional semantics with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Fourth Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marchine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural netowkrs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved semantic representations from treestructured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijman</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iiya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Characterlevel convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hongyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
