<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjun</forename><surname>Liu</surname></persName>
							<email>shuliu@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Sehgal</surname></persName>
							<email>seh-gal.n@husky.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ostadabbas</surname></persName>
							<email>ostadabbas@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sehgal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ostadabbas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) 1 The ScanAva+ dataset is available at: Augmented Cognition Lab Webpage.. The AHuP code also can be found at GitHub AdaptedHu-manPose.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D human pose estimation ? domain shift ? semantic aware adaptation ? synthetic human datasets</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ultimate goal for an inference model is to be robust and functional in real life applications. However, training vs. test data domain gaps often negatively affect model performance. This issue is especially critical for the monocular 3D human pose estimation problem, in which 3D human data is often collected in a controlled lab setting. In this paper, we focus on alleviating the negative effect of domain shift in both appearance and pose space for 3D human pose estimation by presenting our adapted human pose (AHuP) approach. AHuP is built upon two key components: (1) semantically aware adaptation (SAA) for the cross-domain feature space adaptation, and (2) skeletal pose adaptation (SPA) for the pose space adaptation which takes only limited information from the target domain. By using zero real 3D human pose data, one of our adapted synthetic models shows comparable performance with the SOTA pose estimation models trained with large scale real 3D human datasets. The proposed SPA can be also employed independently as a light-weighted head to improve existing SOTA models in a novel context. A new 3D scan-based synthetic human dataset called ScanAva+ is also going to be publicly released with this work. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Because of the great endeavors of the computer vision community, significant advancements have been achieved for 3D human pose estimation with ever improving performance on well recognized benchmarks <ref type="bibr" target="#b18">[19]</ref>. Existing approaches come from versatile genres such as end-to-end learning <ref type="bibr" target="#b24">[25]</ref>, direct 2D-to-3D lifting <ref type="bibr" target="#b35">[35]</ref> and even unsupervised methods <ref type="bibr" target="#b7">[8]</ref>. Although improved performance has been reported with decreased information dependence on the training sets <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b7">8]</ref>, the majority of these studies are conducted via a training/testing data split from the same benchmark datasets, which share very similar contexts. In essence, these 3D human pose benchmarks could be quite different from the reallife applications, and the domain gap between the source data and target applications could lead to potential performance drops. And, while the domain shift issue has been extensively investigated for the classification tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">58]</ref>, it has rarely been addressed in the pose regression problems.</p><p>In this paper, we investigate how domain shift influences the 3D pose estimation model performance and, more importantly, how to counter its adverse effects. Our approach incorporates a semantically aware adaptation (SAA) technique as well as a skeletal pose adaptation (SPA) method towards developing a robust 3D human pose estimation model. Our approach is not only capable of training a monocular 3D human pose estimation model using zero real 3D human pose data, but also can be added to the existing state-of-the-art (SOTA) models as a light-weighted head for adaptation when tested under a novel context or dataset.</p><p>Unfortunately, performance evaluation of the 3D pose estimation models under real world applications is not widely conducted, since acquiring labeled 3D human pose data often requires professional motion capture systems <ref type="bibr" target="#b18">[19]</ref>, and a few available datasets are collected under lab settings with very limited samples in the wild <ref type="bibr" target="#b37">[37]</ref>. This is-arXiv:2105.10837v2 [cs.CV] 22 Jan 2022 sue could inherently result in the similarity of the environments, the camera settings (in both extrinsic and intrinsic parameters), and also human pose distributions between training and evaluation data splits. As a consequence, training and testing on the same benchmark disproportionately take advantage from these context similarities, which do not always hold in practical applications. For a 2D-to-3D lifting approach <ref type="bibr" target="#b35">[35]</ref>, such mapping is conducted inherently under a fixed camera setting. Although the model in <ref type="bibr" target="#b7">[8]</ref> employs no 3D pose data directly, its 2D pose data is a direct projection of the 3D data from the same benchmark, which holds identical pose semantics and distributions of their target domains. It is also evident in their reported performance that using pose data without adaptation leads to a significant performance drop. Even for template-based pose estimation approaches such as <ref type="bibr" target="#b3">[4]</ref>, the template is explicitly adapted to the poses from the target domain using the motion and shape (Mosh) capture approach from sparse markers <ref type="bibr" target="#b30">[31]</ref>. Please note that in these approaches, to adapt the pose, the real 3D human benchmark data has to be employed in Mosh approach <ref type="bibr" target="#b30">[31]</ref>. In the recent work <ref type="bibr" target="#b25">[26]</ref>, few 3D human training data can be extensively augmented for training and an impressive result has been achieved. Yet the focus is on augmentation, where the real 3D human data are still needed.</p><p>Admittedly, it is a good practice to employ as much information as we can get for an improved performance. Yet, in a real life application, we may have access to no pose or image data in the target domain, or, at best, only limited number of measurements can be collected from the target domain a priori.</p><p>Here, we introduce our adapted human pose (AHuP) approach, which allows us to study the monocular 3D human pose estimation problem under a highly unaware context, assuming zero or a very limited amount of information from the target domain. We conducted our study on a more challenging case, when only the synthetic human data is employed for 3D information. With synthetic appearances, simplified skeletons, and different pose distributions, learning from synthetic humans and testing on real 3D human poses becomes a typical manifestation of the potential domain shift. Introducing synthetic data for learning is not a new idea, yet competitive performance is only reported by further incorporating real 3D human data into the current models <ref type="bibr" target="#b61">[61]</ref>. Furthermore, there has been no specific discussion on how these differences affect the pose estimation performance and the proper way to address the challenges associated with the domain gap caused by training vs. testing data distributions. From our study, on one hand, with only synthetic human data for 3D part, AHuP shows comparable performance with SOTA models despite the benefit from training on real 3D human benchmark data. As synthetic human simulation is an efficient and easy-to-produce source pose data generator, this could potentially lead to a more economic solution for the 3D pose estimation problem. On the other hand, for the existing SOTA 3D human pose estimation models, AHuP can be also added as a light-weighted adaptation head for performance improvement when a novel context or dataset is used during the test phase. This improvement is observed on several SOTA models and on all training and testing combinations in our study.</p><p>In short, our work makes the following contributions:</p><p>-Presents an adapted human pose (AHuP) approach (see <ref type="figure">Fig. 1</ref>) that improves the performance of the state-of-theart 3D human pose estimation models for both syntheticto-real and real-to-real cases when training and test data come from different contexts. By using zero real 3D human pose data, one of our adapted synthetic models shows comparable performance with the SOTA pose estimation models trained based on a large mount of real 3D human pose data. -Proposes a semantically aware adaptation (SAA) method for the cross-domain feature space adaptation, which shows noticeable pose estimation performance improvement over conventional domain adaptation techniques. -Introduces a skeletal pose adaptation (SPA) approach, which takes only a limited amount of information from the target domain to adapt the pose, instead of requiring the whole 3D pose data from the target dataset for adaptation purposes, and which can be employed independently as a light-weighted head on top of existing SOTA models for improvement under a novel context. -Introduces and publicly releases a new 3D scan-based synthetic human dataset, called ScanAva+, which is an extended version of our previous ScanAVA dataset (which only had 15 scans), by adding 26 new synthetic human objects with higher texture and size variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section, we provide an overview of the state-of-the-art in the field of 3D human pose estimation and related works that leverage synthetic human models to deal with the 3D data scarcity issues. We also give a summary of the works that employ domain adaptation techniques to close the gap between their training and the test datasets, especially when synthetic data are employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Problem of Monocular 3D Human Pose Estimation</head><p>For monocular 3D human pose estimation, early attempts followed the 2D pose paradigm and conducted straightforward end-to-end training directly on available 3D human pose datasets such as Human 3.6M <ref type="bibr" target="#b18">[19]</ref> and HumanEva Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data 3 <ref type="figure">Fig. 1</ref>: The proposed adapted human pose (AHuP) framework, where SAA stands for the semantically aware adaptation approach, SPA stands for the skeletal pose adaptation. G stands for the feature extractor, T for task head, D for discriminator and D i stands for the i-th channel of D.</p><p>[53]. However, the resultant models are often not very generalizable, especially when applied on real world ("in the wild") images. This is mainly due to the fact that most 3D human datasets are collected under controlled lab settings that lack meaningful variations. To improve the generalization ability in the wild, another line of work takes a twostage strategy, where it first trains a known 2D pose estimation model <ref type="bibr" target="#b42">[42]</ref>, then recovers the 3D poses on top of that <ref type="bibr" target="#b69">[69]</ref>. Some recent works further mix 2D and 3D pose data together to solve the pose estimation problem.</p><p>In <ref type="bibr" target="#b46">[46]</ref>, the authors considered 3D pose a multi-task learning process using 2D pose and depth data. <ref type="bibr" target="#b68">[68]</ref> further incorporated a weakly-supervised loss in order to integrate 2D and 3D data. The 3D pose estimation is also solved in a 2D-to-3D lifting manner without learning from the corresponding image or even operating in an unsupervised manner <ref type="bibr" target="#b7">[8]</ref>. A template-based approach has also been employed by fitting the projected silhouette with prior constraints <ref type="bibr" target="#b3">[4]</ref>. However, 2D-to-3D lifting or silhouette-based approaches inherently abandoned the image features, which may not always be optimal and which will be illustrated in our experiments. In recent studies, kinematic constraints have also been introduced where human motion sequence data is required <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b43">43]</ref> Another approach in 3D pose estimation is to employ synthetic human pose data, summarized in the following section. In short, no matter what exact methodology is used, methods with competitive performance in the 3D human pose estimation field usually cannot avoid employing some real 3D human pose data in their training processes, which makes them expensive to train. In this work, we would like to explore this exact challenging case by using not a single real 3D human dataset in the process. Even though the very recent works in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b19">20]</ref> successfully estimate 3D poses without any explicit 3D labels, they use a multi-view setting, in which 3D coordinates under that can be estimated from the 2D pose via the multi-view geometry. However, a calibrated multi-view setting will not be always available in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Synthetic Human Pose Data</head><p>Employing synthetic data to solve real world problems is not a new concept in the computer vision field. For low level vision tasks, synthetic images have been employed for stereo vision <ref type="bibr" target="#b45">[45]</ref> and optical flow estimation <ref type="bibr" target="#b4">[5]</ref>. For higher level tasks, computer-aided design (CAD) models have also been extensively used for object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b33">34]</ref> or segmentation <ref type="bibr" target="#b17">[18]</ref>.</p><p>Synthetic human figures have been extensively used for learning purposes, such as silhouette-based action recognition tasks <ref type="bibr" target="#b48">[48]</ref>, and crowd counting <ref type="bibr" target="#b63">[63]</ref>. Towards the human pose estimation, <ref type="bibr" target="#b61">[61]</ref>, SURREAL provides 145 subjects and over 6.5 million frames with detailed pose and segmentation labels based on a morphable human template. Templates from the shape completion and animation of people (SCAPE) <ref type="bibr" target="#b2">[3]</ref> have also been employed for large-scale 3D human pose dataset forming <ref type="bibr" target="#b8">[9]</ref>. Another branch is synthesizing human pose data directly via a human body scanning process, which could be limiting in terms of the number of scans, yet garment geometry details are better preserved, such as our recently developed Scanned Avatar (ScanAva+) dataset that contains 3D scans of 41 human subjects <ref type="bibr" target="#b28">[29]</ref>. Nonetheless, no matter how realistic the simulated data look and despite applying conventional domain adaptation methods on them, models trained on the synthetic data alone perform noticeably worse than models trained on real human pose data <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr" target="#b3">4</ref> Shuangjun Liu et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Synthetic vs. Real Data Domain Adaptation</head><p>Domain adaptation is a long-standing topic in the computer vision field, yet synthetic-to-real data domain adaptation for task transfer learning still remains a challenge. Well-known algorithms mainly address the domain gap issue at the feature level by aligning extracted features from both domains and subsequently minimizing certain distance measures between them, such as maximum mean discrepancy <ref type="bibr" target="#b29">[30]</ref>, correlation distance <ref type="bibr" target="#b54">[54]</ref>, or adversarial discrimination loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">58]</ref>. In recent years, domain adaptation has also been introduced for segmentation <ref type="bibr">[17,?,?]</ref> or object detection tasks [?,?,?,?], but rarely for human pose regression. However, such measures are usually based on the overall image features, which do not guarantee semantic consistency. Though this issue has been addressed recently <ref type="bibr" target="#b31">[32]</ref>, it usually remains a classification problem.</p><p>In the image synthesis approach described in <ref type="bibr" target="#b62">[62]</ref>, the authors enforce semantic consistency with an L 1 loss. The loss is computed per-layer between features extracted from a synthetic image and a real sampled image using a pretrained VGG model, with an adaptive weight that decreases for deeper layers. In <ref type="bibr" target="#b5">[6]</ref>, for unsupervised domain adaptation, the authors use a two-stream convolutional neural network (CNN), one for the source domain and one for the target domain, with shared weights. When training a classifier, they assign pseudo-labels and then use an adaptive centroid alignment to offset the negative influence from false pseudo labels and enforce cross-domain class consistency. Similarly, in the segmentation task of <ref type="bibr" target="#b31">[32]</ref>, the authors propose a category-level adversarial network, where two classifiers identify classes whose features are distributed differently between the source and target domains and proportionally increase an adversarial loss to enforce semantic alignment across domains.</p><p>Despite being an important concept, semantic consistency is commonly used for classification tasks, such as in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">41]</ref>, or for image synthesis purposes <ref type="bibr" target="#b62">[62]</ref> instead of human pose. Though adversarial learning is introduced in human pose estimation before <ref type="bibr" target="#b65">[65]</ref>, it is mainly used for pose regularization purposes, where the training portion of the real 3D human pose benchmark is required.</p><p>In this work, we explore how 3D pose estimation performance will be negatively affected by the domain shift between source and target domains through the use of synthetic 3D human pose data as our training source. More importantly, we present the AHuP approach, which incorporates our counter action adaptation techniques in both feature space and pose space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Introducing Adapted Human Pose (AHuP)</head><p>When a pre-trained 3D human pose estimation model is employed in a real world application, many context elements are different from the benchmark that the model was trained on: the person will no longer wear tight clothes to facilitate the motion capture process, no tracker bead is attached, the background will be more versatile compared to a lab environment, and a different pose semantic definition may be used. As in most cases, human pose is estimated based on extracted image patches <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b39">39]</ref>, so we can assume the differences mainly come from the appearance and pose semantics.</p><p>To investigate this problem under a more challenging scenario, we conducted our study on synthetic human since it has noticeable differences in both appearance and pose distributions compared to the common 3D pose benchmarks. Synthetic human usually holds visible unrealistic appearances with simplified skeletons, and their simulated poses, although similar, will never be the same as that of real humans. We proposed both a semantic aware adaptation approach (SAA) on the image feature space and also a skeletal pose adaptation (SPA) approach for skeleton space to align the domain gap as shown in <ref type="figure">Fig. 1</ref>. In this section, we will focus on introducing the SAA and SPA design, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Aware Adaptation (SAA)</head><p>Domain shift is an common issue when a model learned from domain A is applied in domain B for a similar task. There are consistent efforts in the computer vision community to address such an issue. For discriminative tasks <ref type="bibr" target="#b59">[59]</ref>, despite variations in the specifics of the model components <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">58]</ref>, a commonly employed structure is to adapt two datasets A and B for a common task network T , by forming a feature extractor network G (or two for an asymmetric mapping case). This will map two datasets into a common feature space by minimizing a distance measure, such as the maximum mean discrepancy <ref type="bibr" target="#b47">[47]</ref>, or by confusing the discriminator with a generative adversarial network (GAN) structure <ref type="bibr" target="#b59">[59]</ref>. Such distance measures are usually based on the statistics of overall feature maps <ref type="bibr" target="#b59">[59]</ref> or local patches <ref type="bibr" target="#b20">[21]</ref>, uniformly. Although related studies are mainly conducted for classification tasks, there are also recent works that begin to address the segmentation problems with cycle consistency <ref type="bibr" target="#b16">[17]</ref>.</p><p>However, one problem often overlooked is the semantic meaning consistency between two datasets, which so far has been limited to the classification problems <ref type="bibr" target="#b31">[32]</ref>. If the same semantic entity demonstrates varying patterns in different domains, it is very possible that in an image/feature space, the nearest neighbors from two domains do not hold the same semantic meaning. A common solution to shorten Classes in Dataset A Classes in Dataset B Nearest Neighbor Alignment Semantically Aware Adaptation</p><formula xml:id="formula_0">+ (1 d(x))log (1 D i (G(x))[c]))</formula><p>re, c stands for the coordinate in the feature space, N is total joint numbers, D i (?)[c] stands for the i-th channel of t coordinate c, stride stands for the downsampling scale e backbone, jt(i) is the i-th joint location, and [?] is the d operation. We train the G and T networks together inimize the regression error and confusing the D at the e time. We employ the cross entropy loss between the D iction and a uniform distribution as:</p><formula xml:id="formula_1">nf = N X i=1 1(c = [jt(i)/stride])(1/2 log D i (G(x))[c])</formula><p>(2) he total loss during G and T phase is given as:</p><formula xml:id="formula_2">L = L 1 (?, T (G(y))) + L conf (3)</formula><p>re is the coefficient for confusing loss,? stands for the ground truth. What is L1? what happened to L SAA . also ground truth should not be as?.</p><p>letal Pose Adaptation (SPA) e adaptation has been extensively employed in many of 3D human pose estimation works, especially when data of the benchmark domains are introduced. The main idea align the introduced pose or its 2D projection ((Ganin Lempitsky 2014)) with the target domain either by direct ping <ref type="bibr" target="#b3">(Bogo et al. 2016)</ref> or via a discriminator to detect poses ). The underlying assumption is that there is a extensively collected target pose space e aligned to. In a real world application, however it is kely to have access to a large volume of 3D pose data to facilitate the alignment. Instead, it is more reasonable to assume that some countable low dimensional and interpretable parameters from the target domain could be gathered, such as the tailor measurements of the body limbs. In order to achieve skeletal pose adaptation (SPA), we use the normalized limb length vector s(y) as the skeletal descriptor, where the shoulder width is normalized to 1. Due to the different pose semantic definition and the subjects' physiques, this descriptor could vary among different pose datasets. The aligned pose is given as:</p><formula xml:id="formula_3">y SP A = y + f(y) (4)</formula><p>where y is the output of the pose estimation network as y = T (G(x)), f is a mapping function where we employ a multi-layer regression network. To keep the pose semantic consistency, we employ a dual direction pivoting strategy by pushing the mapped pose to both source and target results in different representations at the same time, as shown in <ref type="figure" target="#fig_8">Fig. 3</ref>. By pivoting the resultant pose back to its original, we assume the mapped pose should not be far away from the original pose to keep the pose semantic meaning. By pushing the resultant pose skeleton descriptor to target to make the resultant skeleton similar to target. This paragraph is poorly written: Another tip we found is that clipping the gradient is helpful during training. Without a gradient cap, we usually see a bumping up effect at the early epochs, where the mapped pose keeps on drifting away from the initial position. Though it will finally converge, yet it ere, c stands for the coordinate in the feature space, N is total joint numbers, D i (?)[c] stands for the i-th channel of at coordinate c, stride stands for the downsampling scale the backbone, jt(i) is the i-th joint location, and [?] is the nd operation. We train the G and T networks together minimize the regression error and confusing the D at the e time. We employ the cross entropy loss between the D diction and a uniform distribution as:</p><formula xml:id="formula_4">onf = N X i=1 1(c = [jt(i)/stride])(1/2 log D i (G(x))[c]) (2)</formula><p>The total loss during G and T phase is given as:</p><formula xml:id="formula_5">L = L 1 (?, T (G(y))) + L conf (3)</formula><p>ere is the coefficient for confusing loss,? stands for the e ground truth. What is L1? what happened to L SAA . also se ground truth should not be as?.</p><p>eletal Pose Adaptation (SPA) se adaptation has been extensively employed in many of 3D human pose estimation works, especially when data of the benchmark domains are introduced. The main idea o align the introduced pose or its 2D projection ((Ganin Lempitsky 2014)) with the target domain either by direct pping <ref type="bibr" target="#b3">(Bogo et al. 2016)</ref> or via a discriminator to detect e poses ). The underlying assumption e is that there is a extensively collected target pose space be aligned to. In a real world application, however it is likely to have access to a large volume of 3D pose data to facilitate the alignment. Instead, it is more reasonable to assume that some countable low dimensional and interpretable parameters from the target domain could be gathered, such as the tailor measurements of the body limbs. In order to achieve skeletal pose adaptation (SPA), we use the normalized limb length vector s(y) as the skeletal descriptor, where the shoulder width is normalized to 1. Due to the different pose semantic definition and the subjects' physiques, this descriptor could vary among different pose datasets. The aligned pose is given as:</p><formula xml:id="formula_6">y SP A = y + f(y) (4)</formula><p>where y is the output of the pose estimation network as y = T (G(x)), f is a mapping function where we employ a multi-layer regression network. To keep the pose semantic consistency, we employ a dual direction pivoting strategy by pushing the mapped pose to both source and target results in different representations at the same time, as shown in <ref type="figure" target="#fig_8">Fig. 3</ref>. By pivoting the resultant pose back to its original, we assume the mapped pose should not be far away from the original pose to keep the pose semantic meaning. By pushing the resultant pose skeleton descriptor to target to make the resultant skeleton similar to target. This paragraph is poorly written: Another tip we found is that clipping the gradient is helpful during training. Without a gradient cap, we usually see a bumping up effect at the early epochs, where the mapped pose keeps on drifting away from the initial position. Though it will finally converge, yet it</p><formula xml:id="formula_7">(x))log (1 D i (G(x))[c]))</formula><p>oordinate in the feature space, N is D i (?)[c] stands for the i-th channel of e stands for the downsampling scale s the i-th joint location, and [?] is the ain the G and T networks together ion error and confusing the D at the he cross entropy loss between the D distribution as:</p><formula xml:id="formula_8">jt(i)/stride])(1/2 log D i (G(x))[c])</formula><p>(2) G and T phase is given as:</p><formula xml:id="formula_9">, T (G(y))) + L conf<label>(3)</label></formula><p>t for confusing loss,? stands for the is L1? what happened to L SAA . also d not be as?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tion (SPA)</head><p>n extensively employed in many of mation works, especially when data mains are introduced. The main idea d pose or its 2D projection ((Ganin ith the target domain either by direct 016) or via a discriminator to detect 2018). The underlying assumption tensively collected target pose space eal world application, however it is to a large volume of 3D pose data to facilitate the alignment. Instead, it is more reasonable to assume that some countable low dimensional and interpretable parameters from the target domain could be gathered, such as the tailor measurements of the body limbs. In order to achieve skeletal pose adaptation (SPA), we use the normalized limb length vector s(y) as the skeletal descriptor, where the shoulder width is normalized to 1. Due to the different pose semantic definition and the subjects' physiques, this descriptor could vary among different pose datasets. The aligned pose is given as:</p><formula xml:id="formula_10">y SP A = y + f(y) (4)</formula><p>where y is the output of the pose estimation network as y = T (G(x)), f is a mapping function where we employ a multi-layer regression network. To keep the pose semantic consistency, we employ a dual direction pivoting strategy by pushing the mapped pose to both source and target results in different representations at the same time, as shown in <ref type="figure" target="#fig_8">Fig. 3</ref>. By pivoting the resultant pose back to its original, we assume the mapped pose should not be far away from the original pose to keep the pose semantic meaning. By pushing the resultant pose skeleton descriptor to target to make the resultant skeleton similar to target. This paragraph is poorly written: Another tip we found is that clipping the gradient is helpful during training. Without a gradient cap, we usually see a bumping up effect at the early epochs, where the mapped pose keeps on drifting away from the initial position. Though it will finally converge, yet it</p><formula xml:id="formula_11">(a) (b)</formula><p>actor and T stands for the task head for 3D . As the extracted features from the backbone ose to be highly generalized into a low resoluy patchGAN , we design the to be feature-wise by setting the convolution ne with logistic regression. During training h channel, only the corresponding feature will sed on the joint location. We avoid using the benchmark <ref type="bibr" target="#b18">(Ionescu et al. 2014</ref>) by learning ell available 2D human datasets. he adversarial learning strategy by training the tworks, iteratively in an adversarial manner. mage x, we add a domain indicator d(x) with cate if that is real or synthetic, respectively. , we optimize D by minimizing the SAA loss</p><formula xml:id="formula_12">= N X i=1 1(c = [jt(i)/stride]) (1) ? (d(x)log Di(G(x))[c] + (1 d(x))log (1 Di(G(x))[c]))</formula><p>for the coordinate in the feature space, N is umbers, Di(?)[c] stands for the i-th channel of e c, stride stands for the downsampling scale e, jt(i) is the i-th joint location, and [?] is the n. We train the G and T networks together e regression error and confusing the D at the employ the cross entropy loss between the D a uniform distribution as:</p><formula xml:id="formula_13">1(c = [jt(i)/stride])(1/2 log Di(G(x))[c])</formula><p>(2) s during G and T phase is given as:</p><formula xml:id="formula_14">= |ygt T (G(y))| + Lconf<label>(3)</label></formula><p>coefficient for confusing loss, ygt stands for d truth, and we use norm 1 for regression Pose Adaptation (SPA) n has been extensively employed in many of pose estimation works, especially when data mark domains are introduced. The main idea introduced pose or its 2D projection ((Chen th the target domain either by direct mapping 6) or via a discriminator to detect fake poses 18). The underlying assumption here is that sively collected target pose data to be aligned a real world application, it is unlikely to a large volume of 3D pose data to facilitate Instead, it is more reasonable to assume that low dimensional and interpretable parameters domain could be gathered, such as the tailor of the body limbs.</p><p>the normalized limb length vector s(y) as the skeletal descriptor, where the shoulder width is normalized to 1. Due to the different pose semantic definition and subjects' physiques, this descriptor could vary among different pose datasets. The aligned pose is given as:</p><formula xml:id="formula_15">ySPA = y + f (y) (4)</formula><p>where y is the output of the pose estimation network as y = T (G(x)), f is a mapping function where we employ a multi-layer regression network. To keep the pose semantic consistency, we employ a dual direction pivoting strategy by pushing the mapped pose to both source and target results in different representations at the same time, as shown in <ref type="figure">Fig. 2b</ref>. By pivoting the resultant pose back to its original, we assume the mapped pose should not be far away from the original pose to keep the pose semantic meaning. By pushing the resultant pose skeleton descriptor to target to make the resultant skeleton similar to target. The loss for SPA is given as: LSPA = ||f (y)||1 + ||s(ySPA) star||2 (5) where, star stands for the target skeletal measure. We employ norm 2 loss for skeleton loss suppress far biased joint and use a norm 1 for initial pose pivoting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Datasets</head><p>To evaluate the AHuP model performance, we employed well-known datasets in the human pose estimation field that have been extensively studied. For real 3D human pose data, we chose the Human3.6M <ref type="bibr">(Ionescu et</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>To provide a comprehensive view of our evaluation we employ extensively used metrics from both real human benchmarks to report our performance, including mean per joint position error (MPJPE) for Human3.6M  SAA loss as:</p><formula xml:id="formula_16">LSAA = N X i=1 1(c = Jhm(i)) (1) ? (d(x)log Di(G(x))[c] + (1 d(x))log (1 Di(G(x))[c]))</formula><p>where, c stands for the coordinate in the feature space, N is the total joint numbers, Di(?)[c] stands for the i-th channel of D at coordinate c, stride stands for the downsampling scale of the backbone, Jhm(i) is the i-th joint location in heatmap space. Comparing to conventional adaptation approaches <ref type="bibr">[40, ?]</ref> where the features adaptation is over the whole image region, our approach specify the semantic meaning via multi-channel discriminator. We train the G and T networks together by minimize the regression error and confusing the D at the same time. We employ the cross entropy loss between the D prediction and a uniform distribution for confusing purpose as:</p><formula xml:id="formula_17">Lconf = N X i=1 1(c = Jhm(i)])( 1 2 log Di(G(x))[c]) (2)</formula><p>The total loss during G and T phase is given as:</p><formula xml:id="formula_18">L = |ygt T (G(y))| + Lconf<label>(3)</label></formula><p>where is the coefficient for confusion loss, ygt is the pose ground truth, and we use norm 1 for regression supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Skeletal Pose Adaptation (SPA)</head><p>Pose adaptation has been extensively employed in many of the 3D human pose estimation works, especially when data out of the benchmark domains are introduced. The main idea is to align the introduced pose or its 2D projection ( <ref type="bibr" target="#b4">[5]</ref>) with the target domain either by direct mapping <ref type="bibr" target="#b2">[3]</ref> or via a discriminator to detect fake poses <ref type="bibr" target="#b48">[48]</ref>. The underlying assumption here is that there is an extensively collected target pose data to be aligned to. However in a real world application, it is unlikely to have access to a large volume of 3D pose data to facilitate the alignment. Instead, it is more reasonable to assume that some countable low dimensional and interpretable parameters from the target domain could be gathered, such as the tailor measurements of the body limbs.</p><p>In order to achieve skeletal pose adaptation (SPA), we use the normalized limb length vector s(y) as the skeletal descriptor with the should width as the normalization factor. Due to the different pose semantic definition and subjects' physiques, this descriptor could vary among different pose datasets. The aligned pose is given as ySPA</p><formula xml:id="formula_19">= y + f (y),</formula><p>where y is the output of the pose estimation network as y = T (G(x)), f is a mapping function to learn where we employ a multi-layer regression network. To keep the pose semantic consistency, we employ a dual direction pivoting strategy by pushing the mapped pose to both source and target results in different representations at the same time, as shown in <ref type="figure">Fig. 2b</ref>. By pivoting the resultant pose back to its original pose estimation y, we assume the mapped pose should not be far away from the original pose to keep the pose semantic meaning. By pushing the resultant pose skeleton descriptor s(ySPA) to target sy tar to make the resultant skeleton similar to target. As the pose descriptor s(?) is differentiable, our network can be effectively updated to enforce the skeleton similarity during the model training process. Similar to s(ySPA) the s(ytar) inherently comes from the target pose data ytar. But as it is not always available in practice, we can measure sy tar directly from the exact target or other subjects from the same dataset as s(ytar) is low dimensional which can be tailor measured.</p><p>The loss for SPA is given as:</p><formula xml:id="formula_20">LSPA = ||f (y)||1 + ||s(ySPA) star||2<label>(4)</label></formula><p>where, star stands for the target skeletal measures. We employ norm 2 loss for skeletal similarity pivoting and use norm 1 loss for initial pose pivoting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance Evaluation</head><p>Our implementation employs a ResNet <ref type="bibr" target="#b12">[13]</ref> and an integral human pose head <ref type="bibr" target="#b42">[42]</ref> for G and T networks, respectively. Implementation details are provided in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Datasets</head><p>To evaluate the AHuP model performance, we employed well-known datasets in the human pose estimation field that have been extensively studied. For real 3D human pose data, we chose the Human3.6M <ref type="bibr" target="#b14">[15]</ref> and MuPoTs <ref type="bibr" target="#b27">[28]</ref> datasets. For real 2D pose data, we chose the MSCOCO <ref type="bibr" target="#b19">[20]</ref> and MPII <ref type="bibr" target="#b1">[2]</ref> datasets. For synthetic human data, one branch comes from the deformable human template, in which we chose SURREAL dataset <ref type="bibr" target="#b46">[46]</ref>. In SURREAL, we used the released train split to extract sample images. We kept 0.05 portion of the whole section for the validation/test purpose. The other branch comes from the construction of virtual avatars through the direct 3D scanning of humans, in which we chose the ScanAva dataset <ref type="bibr" target="#b20">[21]</ref> and use their toolkit to collect additional scans to <ref type="bibr" target="#b40">40</ref>  ss as:</p><formula xml:id="formula_21">L SAA = N X i=1 1(c = J hm (i)) (1) ? (d(x)log D i (G(x))[c] + (1 d(x))log (1 D i (G(x))[c]))</formula><p>c stands for the coordinate in the feature space, N otal joint numbers, D i (?)[c] stands for the i-th chan-D at coordinate c, stride stands for the downsamcale of the backbone, J hm (i) is the i-th joint locaheatmap space. Comparing to conventional adapapproaches <ref type="bibr">[40, ?]</ref> where the features adaptation is e whole image region, our approach specify the semeaning via multi-channel discriminator. We train nd T networks together by minimize the regression nd confusing the D at the same time. We employ the ntropy loss between the D prediction and a uniform tion for confusing purpose as:</p><formula xml:id="formula_22">f = N X i=1 1(c = J hm (i)])( 1 2 log D i (G(x))[c]) (2)</formula><p>total loss during G and T phase is given as:</p><formula xml:id="formula_23">L = |y gt T (G(y))| + L conf<label>(3)</label></formula><p>is the coefficient for confusion loss, y gt is the pose truth, and we use norm 1 for regression supervision.</p><p>eletal Pose Adaptation (SPA) e adaptation has been extensively employed in many 3D human pose estimation works, especially when t of the benchmark domains are introduced. The ea is to align the introduced pose or its 2D projection ith the target domain either by direct mapping <ref type="bibr" target="#b2">[3]</ref> or iscriminator to detect fake poses <ref type="bibr" target="#b48">[48]</ref>. The underlyumption here is that there is an extensively collected pose data to be aligned to. However in a real world tion, it is unlikely to have access to a large volume of e data to facilitate the alignment. Instead, it is more able to assume that some countable low dimensional erpretable parameters from the target domain could ered, such as the tailor measurements of the body rder to achieve skeletal pose adaptation (SPA), we normalized limb length vector s(y) as the skeletal tor with the should width as the normalization factor. the different pose semantic definition and subjects' ues, this descriptor could vary among different pose s. The aligned pose is given as</p><formula xml:id="formula_24">y SP A = y + f(y),</formula><p>where y is the output of the pose estimation network as y = T (G(x)), f is a mapping function to learn where we employ a multi-layer regression network. To keep the pose semantic consistency, we employ a dual direction pivoting strategy by pushing the mapped pose to both source and target results in different representations at the same time, as shown in <ref type="figure">Fig. 2b</ref>. By pivoting the resultant pose back to its original pose estimation y, we assume the mapped pose should not be far away from the original pose to keep the pose semantic meaning. By pushing the resultant pose skeleton descriptor s(y SP A ) to target s ytar to make the resultant skeleton similar to target. As the pose descriptor s(?) is differentiable, our network can be effectively updated to enforce the skeleton similarity during the model training process. Similar to s(y SP A ) the s(y tar ) inherently comes from the target pose data y tar . But as it is not always available in practice, we can measure s ytar directly from the exact target or other subjects from the same dataset as s(y tar ) is low dimensional which can be tailor measured.</p><p>The loss for SPA is given as:</p><formula xml:id="formula_25">L SP A = ||f(y)|| 1 + ||s(y SP A ) s tar || 2<label>(4)</label></formula><p>where, s tar stands for the target skeletal measures. We employ norm 2 loss for skeletal similarity pivoting and use norm 1 loss for initial pose pivoting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance Evaluation</head><p>Our implementation employs a ResNet <ref type="bibr" target="#b12">[13]</ref> and an integral human pose head <ref type="bibr" target="#b42">[42]</ref> for G and T networks, respectively. Implementation details are provided in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Datasets</head><p>To evaluate the AHuP model performance, we employed well-known datasets in the human pose estimation field that have been extensively studied. For real 3D human pose data, we chose the Human3.6M <ref type="bibr" target="#b14">[15]</ref> and MuPoTs <ref type="bibr" target="#b27">[28]</ref> datasets. For real 2D pose data, we chose the MSCOCO <ref type="bibr" target="#b19">[20]</ref> and MPII <ref type="bibr" target="#b1">[2]</ref> datasets. For synthetic human data, one branch comes from the deformable human template, in which we chose SURREAL dataset <ref type="bibr" target="#b46">[46]</ref>. In SURREAL, we used the released train split to extract sample images. We kept 0.05 portion of the whole section for the validation/test purpose. The other branch comes from the construction of virtual avatars through the direct 3D scanning of humans, in which we chose the ScanAva dataset <ref type="bibr" target="#b20">[21]</ref> and use their toolkit to collect additional scans to 40 with 35 for training. If not specifically indicated, we employed a basic setting with pose data from SYN + MPII + MSCOCO for all synthetic union cases, where SYN stands for either ScanAva or SURREAL. the overall distance between domains is aligning the nearest neighbored patterns together to blur their domain identities. However, what if the well-aligned pattern comes out to hold different semantic meanings as <ref type="figure">Fig. 2a</ref> displays? Such adaptation will possibly turn out to make the result even more misleading, especially for the regression tasks such as human pose estimation, where body parts are usually more similar than distinct categories in a classification task.</p><p>We argue that the adaptation process should emphasize the semantic awareness in distance measures to achieve a semantically aware adaptation (SAA).</p><p>In AHuP, an intuitive way to achieve SAA is employing an individual discriminator for each recognizable body part. For efficiency, we use a multi-channel structure for AHuP to indicate different body parts as shown in <ref type="figure">Fig. 1</ref>, where D stands for the discriminator, D i stands for the i-th channel of the discriminator for corresponding joint, G stands for the feature extractor and T stands for the task head as the 3D pose regression. As the extracted features from the backbone network G suppose to be highly generalized into a low resolution, inspired by patchGAN <ref type="bibr" target="#b20">[21]</ref>, we design the discriminator D to be feature-wise by setting the convolution kernel size to one with sigmoid. During the training process, for each channel, only the corresponding feature will be activated based on the joint location. We avoid using any 3D human pose benchmark in this process by learning this information from the available 2D human datasets.</p><p>We employ the adversarial learning strategy by training D, G, and T networks iteratively in an adversarial manner. For each input image x, we add a domain indicator d(x) as 0 or 1 to indicate if it is real or synthetic, respectively. During D phase, we optimize D by minimizing the SAA D loss as:</p><formula xml:id="formula_26">L SAA D = ? N i=1 1(c = J hm (i))(d(x)log D i (G(x))[c] (1) + (1 ? d(x))log (1 ? D i (G(x))[c])),</formula><p>where, N is the total joint numbers, c stands for the coordinate in the feature space, D i (?)[c] stands for the i-th channel of D at coordinate c, J hm (i) is the i-th joint location in heatmap space. Compared to the conventional adaptation approaches <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b59">59]</ref>, where the feature adaptation is over the whole image region, our approach specifies the semantic meaning via multi-channel discriminator.</p><p>We train the G and T networks together by minimize the regression error and confusing the D at the same time. We employ the cross entropy loss between the D prediction and a uniform distribution for confusing purpose as:</p><formula xml:id="formula_27">L conf = ? N i=1 1(c = J hm (i)])( 1 2 log D i (G(x))[c]). (2)</formula><p>The total loss during G and T phase is given as:</p><formula xml:id="formula_28">L SAA GT = ||y gt ? T (G(y))|| 1 + ?L conf ,<label>(3)</label></formula><p>where ? is the coefficient for confusion loss, y gt is the pose ground truth, and we use norm L 1 for regression supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skeletal Pose Adaptation (SPA)</head><p>Pose adaptation has been extensively employed in many of the 3D human pose estimation works, especially when data outside of the benchmark domains are introduced. The main idea is to align the introduced pose or its 2D projection ( <ref type="bibr" target="#b7">[8]</ref>) with the target domain either by direct mapping <ref type="bibr" target="#b3">[4]</ref> or via a discriminator to detect fake poses <ref type="bibr" target="#b65">[65]</ref>. The underlying assumption here is that there is extensively collected target pose data to be aligned to, which is not usually the case in the real applications. Instead, it is more reasonable to assume that only some countable low dimensional and interpretable parameters from the target domain could be gathered, such as the tailor measurements of the body's limbs. In order to achieve the skeletal pose adaptation (SPA), we use the normalized limb length vector s(y) as the skeletal descriptor with the shoulder width as its normalization factor. Due to the different pose semantic definition and subjects' physiques, this descriptor could vary among different datasets. The aligned pose is given as y SP A = y + f (y), where y is the output of the pose estimation network as y = T (G(x)) in <ref type="figure">Fig. 1</ref>, f is a mapping function which is a multi-layer regression network with a two linear residue block in between as shown in <ref type="figure">Fig. 1</ref>. What we want from this mapping is a pose holding similar semantic meaning with the source domain and at the same time a skeleton similar to the target domain. So, we employ a dual direction pivoting strategy in both pose and skeleton spaces by pushing the mapped pose to the source pose and at the same time pushing the mapped skeleton to the target skeleton, as shown in <ref type="figure">Fig. 2b</ref>.</p><p>To be specific, in original pose representation, by pivoting the resultant pose back to its original pose estimation y, we assume the mapped pose should not be far away from the source, namely the original pose prediction, to keep the pose semantic meaning. In the skeletal representation, this is achieved by pushing the resultant pose skeleton descriptor s(y SP A ) to target s(y tar ) to make the resultant skeleton similar to target. As the skeletal descriptor s(?) is differentiable, our network can effectively be updated to enforce the skeleton similarity during the model training process. Similar to s(y SP A ), s(y tar ) inherently comes from the target pose data y tar . But as target pose data is not always available in practice, given s(y tar ) is low dimensional, we can tailor measure s(y tar ) directly from the exact target or other subjects from the same dataset.</p><p>The loss for SPA is given as:</p><formula xml:id="formula_29">L SP A = ||f (y)|| 1 + ||s(y SP A ) ? s(y tar )|| 2 ,<label>(4)</label></formula><p>where, s(y tar ) stands for the target skeletal measures. We employ norm L 2 loss for skeletal similarity pivoting and use norm L 1 loss for initial pose pivoting. In our design, SPA is trained after SAA, which acts as an additional component to the SAA network.</p><p>We have to point out that though human geometric has been employed for human pose estimation <ref type="bibr" target="#b68">[68]</ref>, it is given as an additional constraint in an end-to-end training process where the target 3D human pose is available. However, SPA can be added as a light-weighted head on top of an existing pose estimation network for adaptation purposes and the training process is done without any target pose data but only with a series of low dimensional tailor measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Evaluation</head><p>To implement the AHuP approach for monocular 3D human pose estimation, we configured the architecture in <ref type="figure">Fig. 1</ref> by employing a ResNet <ref type="bibr" target="#b15">[16]</ref> and an integral human pose head <ref type="bibr" target="#b56">[56]</ref> for G and T networks, respectively. D is similar to <ref type="bibr" target="#b20">[21]</ref> in its a feature-wise manner with kernel size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AHuP Implementation Details</head><p>Our work is implemented via the PyTorch framework and each configuration is trained with a Nvidia v100 GPU. For the backbone feature extractor network G, although training from scratch converges, initialization from pre-trained weights via ImageNet can accelerate this process. Therefore, our backbone network is initialized with the pre-trained weights from ImageNet <ref type="bibr" target="#b9">[10]</ref>. All other networks are initialized via Xavier <ref type="bibr" target="#b12">[13]</ref>. During training and testing of the SAA network (as introduced in main text Sec. 3.1), we chose batch size to be 120.</p><p>For feature-wise D shown in <ref type="figure">Fig. 1</ref>, we chose a 3-layer configuration with kernel size 3 stride 1. Networks G and T were jointly trained in an adversarial learning procedure, with D serving as the counterpart. Learning rate is set at 1e-3 with a decreasing rate of 0.1 at epoch 11 and 13 with a total of 15. All input images are human-centered, cropped and resized to be 256?256. The output heatmap is set as 64?64?64. During training, we initially train the 2D part at first 5 epochs to facilitate SAA process and 3D supervision is added after.</p><p>To facilitate the training and testing across different datasets, we chose the shared or similar joints to match the 17 joint configuration of the Human3.6M dataset by reordering and renaming. For data feeding, we followed a pivoted matching principle that whenever the leading feeder gives a batch, all subordinates will feed equivalent data to match. Since our study is focused on 3D human pose estimation, we always put the 3D pose dataset as the pivot feeder. For a fair comparison among varying size datasets, we fixed the iteration per epoch at 2500 by repeating the exhausted data loader, if any.</p><p>For the SPA model, we decoupled it from the SAA net training process by learning the mapping directly from the ground truth data. To respect the convention of avoiding using any information from the test data, we used the mean skeletal descriptor s(y tar ) of the training split with the assumption that it is similar to the test split within the same dataset. For f (y), the hidden neuron for each linear layer is set as 1024. The initial learning rate is set to be 1e-4 with a decreasing rate of 0.95 at each epoch with a total 70 epoch and batch size 256. During the training, the SUR-REAL data is downsampled with the rate of 90 to become balanced with the ScanAva+ dataset. Human3.6M is also downsampled with the rate 5 for training and 64 for testing, as commonly done in related studies <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56]</ref>. Our augmentation includes rotation, scaling, color jittering, and synthetic occlusion <ref type="bibr" target="#b67">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Datasets</head><p>For AHuP performance evaluation and comparison with the SOTA, we employed several publicly-available datasets in the human pose estimation field that have been used extensively. For real 3D human pose data, we chose the Hu-man3.6M <ref type="bibr" target="#b18">[19]</ref> and MuPoTs <ref type="bibr" target="#b37">[37]</ref> datasets to represent lab and outdoor environments, respectively. For real 2D human pose data, we chose the MSCOCO <ref type="bibr" target="#b27">[28]</ref> and MPII <ref type="bibr" target="#b1">[2]</ref> datasets. As for synthetic human pose data, one branch comes from the deformable human template, in which we chose SURREAL dataset <ref type="bibr" target="#b61">[61]</ref>. In SURREAL, we used the released train split to extract sample images. We kept a 0.05 portion of the whole section for validation and test purposes. The other synthetic branch comes from the construction of virtual avatars through the direct 3D scanning of humans, in which we chose the ScanAva+ dataset developed in our lab <ref type="bibr" target="#b28">[29]</ref>. The original ScanAva has only 15 scans, which is fewer than its counterpart, SURREAL dataset. To balance the comparison in our study, we used the toolkit employed in the original ScanAva in order to collect additional human scans to augment the dataset to contain 41 full body scans and we formed the ScanAva+ dataset, in which 36 scans are used for the training. If not specifically indicated, we employed a basic setting with pose data from SYN + MPII + MSCOCO collectively for all synthetic union cases, where SYN stands for either ScanAva+ or SURREAL datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>To provide a comprehensive view in our evaluation, we employ extensively-used metrics from real human pose benchmarks to report our performance, including mean per joint position error (MPJPE) for Human3.6M <ref type="bibr" target="#b18">[19]</ref>, 3D percentage of correct key-points (3DPCK), and the area under curve (AUC) for MuPoTS <ref type="bibr" target="#b37">[37]</ref>. For MPJPE, we also reported the Procrustes analysis (PA MPJPE) version <ref type="bibr" target="#b13">[14]</ref>, which is more reliable and fair, especially for cross-set evaluation due to varying camera parameters, joint definition, and body shape distributions.</p><p>For 3DPCK, we follow the official configuration of <ref type="bibr" target="#b37">[37]</ref> with a 15cm tolerance for joint location estimation accuracy. We assume every human is correctly detected and compare all cases with pelvis rooted error. For Human3.6M, we also follow 2nd protocol during evaluation <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b39">39]</ref>, where subjects 9 and 11 are used for testing <ref type="bibr" target="#b3">[4]</ref>. Due to the joint definition differences, we use Human3.6M as a template and map the similar joints and interpolate missing ones for other datasets. Please note that in the original protocols used in the majority of the 3D pose estimation works, the training split of the Human3.6M is employed during training, which we do not use at all. This makes our task a more challenging case (and at the same time more realistic in nature) than the original protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To evaluate how the proposed AHuP framework can enhance the 3D pose estimation with only 3D synthetic data, we added each component one-by-one to form the following settings: (1) pure 3D synthetic data based learning either with SURREAL or ScanAva+, (2) learning with conventional adaptation approach by aligning the whole feature space directly similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">54]</ref>, named with suffix 'C', (3) semantic aware adaptation with suffix 'SAA', (4) further adding 2D pose task from additional 2D human pose dataset (MSCOCO and MPII datasets) with suffix 'Jo2D', (5) further adding skeletal pose adaptation with suffix 'SPA'.</p><p>From the results shown in Tab. 1, we can see that conventional adaptation by aligning the whole features <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b58">58]</ref> does improve the performance on both synthetic datasets across all real benchmarks. By employing the SAA strategy, the improvement is significant on ScanAva+, and slightly but still noticeable on SURREAL. Additional 2D pose tasks from the real 2D human dataset shows further improvement, which agrees with the existing studies <ref type="bibr" target="#b68">[68]</ref>. SPA shows noticeable improvement for both ScanAva+ and SURREAL on Human3.6M but not much difference on MuPoTS. Although MuCo is the training split for MuPoTS <ref type="bibr" target="#b37">[37]</ref>, the two datasets are in fact captured in different environments separately, which could result in differences in the skeletal descriptors. We still notice that in the SURREAL case, although the improvement is not obvious on PA MPJPE, the 3DPCK metric is improved significantly. It shows that by adding SPA , many of the pose errors fall back into the tolerance range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Study in Pose and Feature Spaces</head><p>Tab. 1 shows that applying the AHuP approach leads to pose estimation performance improvement on both real 3D pose benchmarks; however, there are much stronger improvements when ScanAva+ dataset is used for training compared to the SURREAL. To figure out the underlying reason, we further investigated the characteristics of the datasets themselves. We randomly extracted 5000 3D human pose samples from all four 3D datasets, including real (Hu-man3.6M and MuPoTS) and synthetic (ScanAva+ and SUR-REAL) pose datasets, and visualized them via a t-distributed stochastic neighbor embedding (t-SNE) approach <ref type="bibr" target="#b32">[33]</ref> in <ref type="figure" target="#fig_8">Fig. 3</ref>. This plot is purely based on the raw pelvis rooted 3D pose data without filtering after matching the joint order across datasets, in order to reflect the essential pose difference among these sets. From the plot, surprisingly we found out a higher agreement between all real datasets and ScanAva, yet a clear boundary around SURREAL. It seems SURREAL does not hold a well-overlapping pose manifold with the others. The causes could be multi-fold, including the camera setting and joint definition in their rendering process, which all possibly affect the final pose distribution. When SURREAL is the only 3D source, the model can hardly learn any more than its pose coverage. This presumes to be the cause of limited improvements in the case when only SURREAL data is used.</p><p>It is also interesting to investigate how these approaches influence the feature space. To better illustrate this, we visualized the output features of G from all four datasets under different model configurations as shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. Due to computational intensity and also to prevent a cluttered <ref type="table">Table 1</ref>: Ablation study of AHuP when tested on real 3D human pose datasets as Human3.6M Protocol#2 and MuPoTs. C stands for a conventional adaptation from <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">54]</ref>, SAA for semantic aware adaptation, Jo2D for adding 2D pose estimation tasks from additional 2D human pose dataset (MSCOCO and MPII datasets), SPA for adding skeletal pose adaptation.  visualization, we evenly sampled 100 images from each dataset, with the G network's output features downsampled to 1024 ? 4 ? 4. <ref type="figure" target="#fig_0">Fig. 4</ref> shows how differently each dataset is in the "eye" of these models. In <ref type="figure" target="#fig_0">Fig. 4a</ref>, all datasets show clear clustering effect. This is reasonable as the model based on only synthetic data can hardly generalize well to capture the shared features for both of real and synthetic data. With the adaptor introduction in <ref type="figure" target="#fig_0">Fig. 4b</ref>, the clustering effect has been eliminated much, but we notice that the synthetic features are more located on the right hand side for both SUR-REAL and ScanAva+. With SAA as shown in <ref type="figure" target="#fig_0">Fig. 4c</ref>, there is no obvious improvement over the C version, but the synthetic features are more evenly distributed in the space. With additional 2D task from real 2D human pose in <ref type="figure" target="#fig_0">Fig. 4d</ref>, the distribution becomes flattened. One interesting observation is that SURREAL shows a more obvious clustering effect than the adaptation-only version. Instead of losing the generalization ability, we believe this clustering effect on the contrary indicates an improved ability of the network to recognize different poses. As SURREAL does not hold a similar pose distribution to the others, it supposes to show a different pattern in a well recognized pose space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Adaptation of SOTA Models using AHuP</head><p>Although our proposed AHuP approach shows improvement over the models trained on the synthetic human data, one immediate question is why bother to use AHuP given that well-performed 3D human pose models already exist? Here, we examine how AHuP is also capable of improving the performance of the existing SOTA 3D pose estimation models when tested under a different context or dataset from their training set.</p><p>In order to conduct a fair comparison, we designed a cross-set evaluation experiment that all candidate models are trained and tested on different datasets to mimic the effect when they are employed in applications under a novel context. We chose two SOTA pose estimation models introduced by Sun et al. in <ref type="bibr" target="#b56">[56]</ref> and by Zhou et al. in <ref type="bibr" target="#b68">[68]</ref> in this experimental analysis. To cover more possible scenarios, for top performer <ref type="bibr" target="#b56">[56]</ref>, we trained two networks with Human3.6M + MPII + MSCOCO and MuCo + MPII + MSCOCO, respectively. For <ref type="bibr" target="#b68">[68]</ref>, we employed their official release of the pre-trained model on Human3.6M + MPII. We tested both model on a 3D human pose dataset that was novel for them. As they already learned from the real domains with limited shift in the appearance, we only evaluated the effect of our SPA on these models. Their model performance with or without SPA is reported in Tab. 2. The results illustrate that adding AHuP adaptation in the form of SPA to these models leads to consistent performance improvement over the versions without SPA. These improvements are seen in both models <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b68">68]</ref>,in all of the training set combinations, and for different types of test sets. ply switching the SPA head without retraining the network itself. The benefits of this SPA adaptation strategy include:</p><p>-Time efficient adaptation: To evaluate a model on different benchmarks, it is common to train a specific model for each of the new benchmarks, as suggested by <ref type="bibr" target="#b39">[39]</ref>.</p><p>However, retraining the model in <ref type="bibr" target="#b56">[56]</ref> on a new dataset takes two days <ref type="bibr" target="#b39">[39]</ref>. In contrast, training a SPA head takes less than 20 minutes. -Memory efficient storage: Suppose we train a new model for each new dataset/context; the storage cost will be proportional to the number of the datasets scaled by the size of the network. However, in SPA adaptation, we only need one copy of the model (e.g., PoseNet) with different SPA adaptation heads. An example of the storage comparison for models in <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b68">68]</ref> with and without SPA strategy is shown in <ref type="figure">Fig. 5</ref>, where the more potential datasets there are to work on, the more memory saving our SPA adaptation it will lead to. -Efficient computational cost: Adding SPA in inference processes will inevitably increase the calculation cost, but the cost increase is actually negligible compared to the computation of the original network. A comparison of computational cost of the two models in <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b68">68]</ref>    <ref type="table">Table 3</ref>: Computational cost of the models in <ref type="bibr" target="#b56">[56]</ref> and <ref type="bibr" target="#b68">[68]</ref> with and without SPA in Gigaflops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Original +SPA Computational cost increase (%) Sun et al. <ref type="bibr" target="#b56">[56]</ref> 13.097 13.101 0.031% Zhou et al. <ref type="bibr" target="#b68">[68]</ref> 12.003 12.007 0.033%  Methods S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 . . . Avg AUC Rogez <ref type="bibr">[</ref> using the training and test split from the same benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">37]</ref>, with no domain shift to overcome at all <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b39">39]</ref>. Following the convention in <ref type="bibr" target="#b39">[39]</ref>, we reported our performance under Human3.6M Protocol#2 as shown in Tab. 4. As 3D pose estimation is a scale-uncertain process, different datasets have different camera poses and parameters, which will directly affect the regression results. So we also included the rigid Procrustes analysis (PA) alignment <ref type="bibr" target="#b13">[14]</ref> result for a more just comparison. Though it cannot match the best performing models, we demonstrate that our model can already rival some approaches that learn directly from real 3D human pose data <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b6">7]</ref>. Furthermore, the difference in pose definition could introduce additional estimation errors. For example, in ScanAva+ the "head" joint lies at the top point of the head, yet in Human3.6M, this joint is biased towards the head center. The "ankle joint" in Human 3.6M lies at the back of the heel, yet ScanAva+ and SURREAL place them closer to the center of the ankle above the foot.</p><p>One question is whether or not we really benefit from the learned features that are extracted from the synthetic data with the SAA, or does it only constitute a 2D-to-3D lifting. To investigate this question, we specifically trained a 2D-to-3D lifting SOTA model <ref type="bibr" target="#b35">[35]</ref> under the same setting of AHuP with synthetic ScanAva+ data. Its performance is reported as Martinez (ScanAva+) in the Tab. 4, which shows that AHuP still performs noticeably better than 2D-to-3D lifting version when employed under the same setting.</p><p>Another set of real 3D pose data evaluations is conducted on MuPoTS <ref type="bibr" target="#b37">[37]</ref> with results shown in Tab. 5. It is quite surprising that despite the fact that we do not use a single frame of real 3D human data, AHuP shows competitive performance among SOTA models that are trained on the real 3D human pose data. This mainly stems from the fact that MuPoTS is collected in the wild with multiple people, so the images are in a more natural setting. In comparison, Human3.6M is collected in a studio environment with a limited number of subjects. For a fixed lab setting such as Human3.6M, many context factors, such as camera pose and background, can be inherently well-studied from its corresponding training set, but it is not the case for MuPoTS, since its data seems to have higher variations in these factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Qualitative Comparison</head><p>We also visualized the recovered 3D pose results when AHuP trained on ScanAva+ is used (see <ref type="figure">Fig. 6</ref>). Despite the lower performance compared to the top rank 3D pose estimation models, the recovered skeletons via our method agree well with human perception. In fact, when recovered joints are within a tolerable error threshold, from a human perspective, our prediction is "semantically" correct. We also tested AHuP on synthetic data in the last two columns of <ref type="figure">Fig. 6</ref>, which performs equivalently well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human3.6M</head><p>MuPoTS ScanAva SURREAL <ref type="figure">Fig. 6</ref>: Qualitative recovery results of AHuP trained on ScanAva+ tested across domains and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Evaluation in 3D Multi-Person Pose Estimation</head><p>Our approach also shows compatibility with other works.</p><p>Combining with the proposed approach in <ref type="bibr" target="#b39">[39]</ref> by employing Mask-RCNN <ref type="bibr" target="#b14">[15]</ref> as detect-net (for human detection), our model can be well integrated as a pose-net (for 3D human pose estimation) for multi-person pose estimation. As multi-person performance is also affected by root localization, we only report qualitative results, as shown in <ref type="figure" target="#fig_10">Fig. 7</ref> without taking credits from <ref type="bibr" target="#b39">[39]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The ultimate goal of training an inference model is to make the model ready to perform in some real world applications.</p><p>Training and testing under different contexts potentially introduces the domain gap and influences the model performance negatively. This issue is especially magnified in the 3D human pose problem, where the majority of the 3D human pose benchmarks are collected under controlled lab settings. To mitigate this effect, we presented our adapted human pose (AHuP) approach that incorporates a semantic awareness adaptation (SAA) technique as well as a skeletal pose adaptation (SPA) algorithm and illustrated how AHuP improves 3D pose estimation model performance both quantitatively and qualitatively. For a better illustration of an application with a significant context shift, we chose the synthetic human data to train our inference model without using any real 3D human pose data. We then tested AHuP on the well-known 3D human pose benchmarks, in which it showed comparable performance with many of the state-ofthe-art (SOTA) models, which have full access to the real 3D pose data. For existing SOTA models, our approach can also be added as a light-weighted adaptation head which showed consistent improvement for all the candidate models, over all training and testing combinations in our study. Admittedly, without having access to the target real 3D human data, AHuP has a challenge in beating the best performers. However, in a real-life problem, the solution we care most about is often not what we can achieve under an ideal condition (such as a controlled lab setting) but how well we can get a solution when given limited access to target data under the practical constraints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>L1 loss between input pose and the mapped pose with and without gradient clipping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>L1 loss between input pose and the mapped pose with and without gradient clipping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>L1 loss between input pose and the mapped pose with and without gradient clipping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Our implementation employs a ResNet (He et al. 2016) and an integral human pose head (Sun et al. 2018) for G and T networks, respectively. Implementation details are provided in the Supplementary Materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(Ionescu et al. 2014), 3D percentage of correct key-points (3DPCK), and the area under curve (AUC) for MuPoTS (Mehta et al. 2018a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>#****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 Fig. 2 :</head><label>42</label><figDesc>(a) Semantically aware adaptation (SAA) vs. nearest neighbor alignment between datasets A and B, assumed to be from two different domains. (b) Skeletal pose adaptation (SPA) based on dual direction pivoting to both source and target in different representation. Black arrow indicates the forward mapping. Red arrow indicates the pivoting direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Adapted</head><label></label><figDesc>Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 :</head><label>3</label><figDesc>t-SNE plot of pelvis rooted 3D pose data across 5000 data samples randomly extracted from Human3.6M, MuPoTS, ScanAva+ and SURREAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4. 7 Fig. 4 :Fig. 5 :</head><label>745</label><figDesc>Practical Values of AHuP SPA can be employed as a switchable head on top of an existing network such as ResNet to act as a light-weighted adaptation strategy. The adaptation can be achieved by sim-t-SNE plot of network G's output features applied on Human3.6M, MuPoTS, ScanAva+, and SURREAL datasets under model configurations of (a) no adaptation, (b) with a conventional adaptor C, (c) with SAA, (d) with SAA + Jo2D. Memory usage based on the number of parameters in million (M) in the SOTA models from<ref type="bibr" target="#b56">[56]</ref> and<ref type="bibr" target="#b68">[68]</ref>, when customized for varying numbers of datasets with and without SPA: (a) in full scale, (b) zoomed in to show the slope differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative recovery results of ScanAva-AHuP on MSCOCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>al. 2014) and MuPoTs (Mehta et al. 2018a) datasets. For real 2D pose data, we chose the MSCOCO (Lin et al. 2014) and MPII (Andriluka et al. 2014) datasets. For synthetic human data, one branch comes from the deformable human template, in which we chose SURREAL dataset (Varol et al. 2017). In SURREAL, we used the released trained session to extract sample images. We kept 0.05 portion of the whole section for the validation/test purpose. The other branch comes from the construction of virtual avatars through the direct 3D scanning of humans, in which we chose the ScanAva dataset (Liu and Ostadabbas 2018). If not specifically indicated, we employed a basic setting with pose data from SYN + MPII + MSCOCO for all synthetic union cases, where SYN stands for either ScanAva or SURREAL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with 35 for training. If not specifically indicated, we employed a basic setting with pose data from SYN + MPII + MSCOCO for all synthetic union cases, where SYN stands for either ScanAva or SURREAL.</figDesc><table><row><cell></cell><cell>379</cell></row><row><cell></cell><cell>380</cell></row><row><cell></cell><cell>381</cell></row><row><cell></cell><cell>382</cell></row><row><cell></cell><cell>383</cell></row><row><cell></cell><cell>384</cell></row><row><cell></cell><cell>385</cell></row><row><cell></cell><cell>386</cell></row><row><cell></cell><cell>387</cell></row><row><cell></cell><cell>388</cell></row><row><cell></cell><cell>389</cell></row><row><cell></cell><cell>390</cell></row><row><cell></cell><cell>391</cell></row><row><cell></cell><cell>392</cell></row><row><cell></cell><cell>393</cell></row><row><cell></cell><cell>394</cell></row><row><cell></cell><cell>395</cell></row><row><cell></cell><cell>396</cell></row><row><cell></cell><cell>397</cell></row><row><cell></cell><cell>398</cell></row><row><cell></cell><cell>399</cell></row><row><cell></cell><cell>400</cell></row><row><cell></cell><cell>401</cell></row><row><cell></cell><cell>402</cell></row><row><cell></cell><cell>403</cell></row><row><cell></cell><cell>404</cell></row><row><cell></cell><cell>405</cell></row><row><cell></cell><cell>406</cell></row><row><cell></cell><cell>407</cell></row><row><cell></cell><cell>408</cell></row><row><cell></cell><cell>409</cell></row><row><cell></cell><cell>410</cell></row><row><cell></cell><cell>411</cell></row><row><cell></cell><cell>412</cell></row><row><cell></cell><cell>413</cell></row><row><cell></cell><cell>414</cell></row><row><cell></cell><cell>415</cell></row><row><cell></cell><cell>416</cell></row><row><cell></cell><cell>417</cell></row><row><cell></cell><cell>418</cell></row><row><cell></cell><cell>419</cell></row><row><cell></cell><cell>420</cell></row><row><cell></cell><cell>421</cell></row><row><cell></cell><cell>422</cell></row><row><cell></cell><cell>423</cell></row><row><cell></cell><cell>424</cell></row><row><cell></cell><cell>425</cell></row><row><cell></cell><cell>426</cell></row><row><cell></cell><cell>427</cell></row><row><cell></cell><cell>428</cell></row><row><cell>4</cell><cell>429 430</cell></row><row><cell></cell><cell>431</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Cross benchmark evaluation of the 3D pose estimation models in<ref type="bibr" target="#b56">[56]</ref> and<ref type="bibr" target="#b68">[68]</ref>, when trained and evaluated on different 3D pose datasets, with and without SPA. MuCo is the official training portion of the MuPoTS dataset<ref type="bibr" target="#b37">[37]</ref>.</figDesc><table><row><cell cols="4">AUC Sun et al. [56] trained on MuCo + MSCOCO + MPII Benchmarks SPA MPJPE PA 3DPCK</cell></row><row><cell>Human36M ScanAva+ SURREAL</cell><cell>82.1 77.9 92.6 91.7 154.8 154.2</cell><cell>75.8 92.7 87.7 87.7 63.0 63.2</cell><cell>49.0 51.5 43.7 44.2 26.6 27.0</cell></row><row><cell cols="4">Sun et al. [56] trained on Human3.6M + MSCOCO + MPII MuPoTS 111.1 84.1 41.0 105.9 84.1 41.1 ScanAva+ 111.8 77.5 38.1 106.9 79.2 40.8 SURREAL 171.8 54.7 22.3 170.1 55.7 23.2</cell></row><row><cell cols="4">Zhou et al. [68] trained on Human3.6M + MPII</cell></row><row><cell>MuPoTS ScanAva+ SURREAL</cell><cell>111.8 106.9 91.6 89.1 156.3 153.6</cell><cell>77.5 79.2 87.5 87.8 62.7 63.6</cell><cell>38.1 40.8 44.0 45.5 26.4 28.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art based on the MPJPE metric tested on Human3.6M dataset using Protocol#2. Please note that unlike AHuP, all of these models have used some real 3D human pose data in their training process. For our method, PA stands for when the PA rigid alignment is applied. The last two rows stands for Martinez approach<ref type="bibr" target="#b35">[35]</ref> trained on ScanAva+ dataset.</figDesc><table><row><cell>Methods</cell><cell>Dir</cell><cell>Dis</cell><cell>Eat</cell><cell>Gre</cell><cell>Phon.</cell><cell>Pose</cell><cell>Pur.</cell><cell>Sit</cell><cell>SitD</cell><cell>Smo.</cell><cell>Phot.</cell><cell>Wait</cell><cell cols="3">Walk WalkD. WalkP.</cell><cell>Avg</cell></row><row><cell>Akhter &amp; Black [1] Ramakrishna [49] Zhou [70] SMPLify [4] Chen[7] Tome [57] Moreno [40] Zhou [71] Jahangiri [22] Mehta [36] Martinez [35] Fang [11] Sun [55] Sun [11] Moon [39]</cell><cell cols="13">199.2 177.6 161.8 197.8 176.2 186.5 195.4 167.3 160.7 173.7 177.8 181.9 176.2 137.4 149.3 141.6 154.3 157.7 158.9 141.8 158.1 168.6 175.6 160.4 161.7 150.0 99.7 95.8 87.9 116.8 108.3 107.3 93.5 95.3 109.1 137.5 106.0 102.2 106.5 62.0 60.2 67.8 76.5 92.1 77.0 73.0 75.3 100.3 137.3 83.4 77.3 79.7 89.9 97.6 90.0 107.9 107.3 93.6 136.1 133.1 240.1 106.7 139.2 106.2 87.0 65.0 73.5 76.8 86.4 86.3 68.9 74.8 110.2 173.9 85.0 110.7 85.8 71.4 69.5 80.2 78.2 87.0 100.8 76.0 69.7 104.7 113.9 89.7 102.7 98.5 79.2 68.7 74.8 67.8 76.4 76.3 84.0 70.2 88.0 113.8 78.0 98.4 90.1 62.6 74.4 66.7 67.9 75.2 77.3 70.6 64.5 95.6 127.3 79.6 79.1 73.4 67.4 57.5 68.6 59.6 67.3 78.1 56.9 69.1 98.0 117.5 69.5 82.4 68.0 55.3 51.8 56.2 58.1 59.0 69.5 55.2 58.1 74.0 94.6 62.3 78.4 59.1 49.5 50.1 54.3 57.0 57.1 66.6 53.4 55.7 72.8 88.6 60.3 73.3 57.7 47.5 52.8 54.8 54.2 54.3 61.8 53.1 53.6 71.7 86.7 61.5 67.2 53.4 47.1 47.5 47.7 49.5 50.2 51.4 43.8 46.4 58.9 65.7 49.4 55.8 47.8 38.9 50.5 55.7 50.1 51.7 53.9 46.8 50.0 61.9 68.0 52.5 55.9 49.9 41.8</cell><cell>198.6 174.8 110.4 86.8 114.1 86.3 82.4 75.1 71.8 76.5 65.1 62.7 61.6 49.0 56.1</cell><cell>192.7 150.2 115.2 81.7 90.6 73.1 77.2 73.6 72.8 61.4 52.4 50.6 63.4 43.8 46.9</cell><cell>181.1 157.3 106.7 82.3 114.2 88.4 87.3 79.9 77.6 72.9 62.9 60.4 59.1 49.6 53.3</cell></row><row><cell>ScanAva-AHuP ScanAva-AHuP PA</cell><cell cols="13">135.9 137.2 104.0 137.1 139.4 133.6 140.8 133.7 163.3 129.5 137.9 139.5 123.2 75.2 79.1 68.0 79.1 91.7 75.8 82.3 100.9 128.0 87.4 83.3 81.8 76.8</cell><cell>135.1 82.2</cell><cell>130.2 78.3</cell><cell>134.5 85.1</cell></row><row><cell>Martinez (ScanAva+) [35] Martinez (ScanAva+) [35] PA</cell><cell cols="13">153.2 152.6 129.7 153.8 151.9 149.9 144.0 159.8 191.0 146.2 147.9 158.7 148.5 97.8 97.9 93.1 99.5 105.7 91.3 91.0 121.4 139.7 104.6 96.6 99.5 105.2</cell><cell>140.8 98.2</cell><cell>139.8 98.0</cell><cell>151.5 103.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>3DPCK comparison with the state-of-the-art tested on the MuPoTS dataset. 16 out of 20 subjects are listed due to space limitation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The ScanAva+ dataset is available at: Augmented Cognition Lab Webpage.. The AHuP code also can be found at GitHub AdaptedHu-manPose.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Shuangjun Liu et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Shuangjun Liu et al.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with semantic consistency for cross-domain image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">https:/doi-org.ezproxy.neu.edu/10.1145/3357384.3357918</idno>
		<ptr target="https://doi-org.ezproxy.neu.edu/10.1145/3357384.33579184" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu-Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Shann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H L</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Hsiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00285</idno>
		<title level="m">Virtual-to-real: Learning to control in visual semantic segmentation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02330</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">3d human pose estimation from monocular images with deep convolutional neural network. In: Asian Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-view object class detection with a 3d geometric model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A semi-supervised data augmentation approach using 3d graphical engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ostadabbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning appearance in virtual scenarios for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ger?nimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data 13 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single-shot multiperson 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson1,6" />
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Camera distance-aware topdown approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantically consistent regularization for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semisupervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards a simulation driven stereo vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohkawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)</title>
		<meeting>the 21st International Conference on Pattern Recognition (ICPR2012)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Covariate shift and local learning by distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qui?onero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vihasi: virtual human action silhouette data for the performance evaluation of silhouette-based action recognition methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ragheb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second ACM/IEEE International Conference on Distributed Smart Cameras</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lcr-net: Localizationclassification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Example-guided style-consistent image synthesis from semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning from synthetic data for crowd counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjun</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
