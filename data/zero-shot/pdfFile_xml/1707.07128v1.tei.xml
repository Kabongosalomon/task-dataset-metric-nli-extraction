<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SINGLE IMAGE SUPER-RESOLUTION WITH DILATED CONVOLUTION BASED MULTI-SCALE INFORMATION LEARNING INCEPTION MODULE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuzhen</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SINGLE IMAGE SUPER-RESOLUTION WITH DILATED CONVOLUTION BASED MULTI-SCALE INFORMATION LEARNING INCEPTION MODULE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image super-resolution</term>
					<term>convolutional neural network</term>
					<term>multi-scale information</term>
					<term>dilated convolution</term>
					<term>inception module</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional works have shown that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Make full use of these multi-scale information can improve the image restoration performance. However, the current proposed deep learning based restoration methods do not take the multi-scale information into account. In this paper, we propose a dilated convolution based inception module to learn multi-scale information and design a deep network for single image super-resolution. Different dilated convolution learns different scale feature, then the inception module concatenates all these features to fuse multi-scale information. In order to increase the reception field of our network to catch more contextual information, we cascade multiple inception modules to constitute a deep network to conduct single image super-resolution. With the novel dilated convolution based inception module, the proposed end-to-end single image superresolution network can take advantage of multi-scale information to improve image super-resolution performance. Experimental results show that our proposed method outperforms many state-of-the-art single image super-resolution methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper, we focus on single image super-resolution (SR), which aims at recovering a high-resolution (HR) image from a given low-resolution (LR) one. It is always the research emphasis because of the requirement of higher definition video displaying, such as the new generation of Ultra High Definition (UHD) TVs. However, most video content would not at sufficiently high resolution for UHD TVs. As a result, we have to develop efficient SR algorithms to generate UHD content from lower resolutions <ref type="bibr" target="#b1">[2]</ref>.</p><p>Traditional single image SR methods always try to find a new image prior or propose a new way to use the existing image priors. A lot of image prior information have been explored in the image restoration literature, such as local smooth, non-local self-similarity and image sparsity. Based on the assumption that low and high resolution images have the same sparse representation, Yang et al. <ref type="bibr" target="#b2">[3]</ref> use two coupled dictionaries to learn a nonlinear mapping between the LR and the HR images. In <ref type="bibr" target="#b3">[4]</ref>, Glasner et al. begin to use the image multi-scale information for single image SR and obtain state-of-the-art results.</p><p>Recently, due to the availability of much larger training dataset and the powerful GPU implementation, deep learning based methods achieve great success in many fields, including both high level and low level computer vision problems. Look through the literature, most state-of-the-art single image SR methods are based on deep learning. The pioneering SR method is SRCNN proposed by Dong et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. They establish the relationship between the traditional sparse-coding based SR methods and their network structure and demonstrate that a convolutional neural network (CNN) can learn a mapping from low resolution image to high resolution one in an end-to-end manner. Dong et al. successfully expand SRCNN for compression artifacts reduction by introducing a feature enhancement layer <ref type="bibr" target="#b6">[7]</ref>. Soon after, they proposed a fast SRCNN method, which directly maps the original lowresolution image to the high-resolution one <ref type="bibr" target="#b7">[8]</ref>. Different from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, some works try to learn image high frequency details instead of the undegraded image. In <ref type="bibr" target="#b8">[9]</ref>, Kim et al. cascade many convolutional layers to form a very deep network to learn image residual.</p><p>Investigating an effective way to use multi-scale information is also important. The degraded image can be successful recovered is mainly based on the assumption that patches in a natural image tend to redundantly recur many times inside the image. However, it is not only exist in the same scale but also across different scales. It has been demonstrated that make full use of multi-scale information can improve the restoration result in traditional methods <ref type="bibr" target="#b3">[4]</ref>. However, the multiscale information has been little investigated in deep learning methods. In <ref type="bibr" target="#b8">[9]</ref>, Kim et al. try to train a multi-scale model for different magnification SR. It is a very rough tactics to exploit the scale information since they just put different scale image as input for training. Its success can be attribute to the powerful learning ability of CNN instead of the multi-scale information being considered in the network structure.</p><p>In this paper, we propose a dilated convolution based inception module to learn multi-scale information and design a deep network for single image SR. <ref type="figure" target="#fig_0">Fig. 1</ref> makes a comparison between the proposed dilated convolution based inception module and the original GoogLeNet inception module. Our proposed new inception module contains multiple different scale dilated convolution that makes it can learn multiscale image information. Furthermore, we cascade multiple dilated convolution based inception modules to constitute a deep network for single image SR. In short, the contributions of this work are mainly in three aspects: 1) we proposed a dilated convolution based inception module, which can learn multi-scale information with only single scale image input; 2) we design a novel deep network with the proposed dilated convolution based inception module for single image SR. 3) experimental results show that our proposed new method outperforms many state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DILATED CONVOLUTION BASED INCEPTION MODULE</head><p>Dilated Convolution: It has been referred to in the past as convolution with a dilated filter <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, Yu et al. call it dilated convolution instead of convolution with a dilated filter to clarify that no dilated filter is constructed or represented. It can be formulated as</p><formula xml:id="formula_0">(F * l k) (p) = s+lt=p F (s) k (t)<label>(1)</label></formula><p>where * l is called as a dilated convolution or an l-dilated convolution, F and k is a discrete function and a discrete filter of size (2r + 1) 2 , respectively. Three dilated convolutions have been shown in the middle part of <ref type="figure" target="#fig_1">Fig. 2</ref>, where k is a 3 ? 3 filter and the kernel dilation factors are 1, 2 and 3, respectively. It shows that filters are dilated by inserting dilated ? 1 zeros between filter elements for a given dilation factor. We refer the reader to <ref type="bibr" target="#b11">[12]</ref> for more information about the dilated convolution.</p><p>Dilated Convolution based Inception Module: To make full use of the image multi-scale information, Szegedy et al. <ref type="bibr" target="#b0">[1]</ref> proposed an inception module for image classification. As shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>, the GoogLeNet inception module contains multiple convolution with different kernel size. It concatenates the outputs of these different size filter to fuse different scale information. With this multi-scale information learning structure, GoogLeNet achieves the new state-ofthe-art performance for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). Inspired by this successful work, we propose a dilated convolution based inception module to learn multi-scale information for improving single image SR performance.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>, our new inception module first use three dilated convolution with kernel size 3 ? 3 and dilated factor 1, 2 and 3, respectively, to operate on the previous layers. Then, we concatenate all these convolution outputs to fuse different scale information.</p><p>Insights: there are a lot of ways to learn the multi-scale information by deep network. For example, the GoogLeNet inception module uses different kernel size of convolution with different receptive field to operate on the previous layer. However, it is still operated on the same image scale space. The other choice is to operate on different scale image input. Farabet et al. <ref type="bibr" target="#b12">[13]</ref> use this way to learn multi-scale feature for scene parsing. For single image SR, the lower solution image always has much more sharp detail information than the interpolation result. Therefore, it can improve the SR result by taking these small scale images into account. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the process of our proposed dilated convolution based inception module how to learn multi-scale information. The blue solid lines indicate that all these dilated convolutions actually operate on the same scale image, while the red dash lines mean these convolutions with different dilated factors learn the corresponding scale image information. Furthermore, the output of the dilated convolution can keep the same size with its input, so we can fuse these different scale information through concatenation operator easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED MULTI-SCALE INFORMATION LEARNING NETWORK STRUCTURE</head><p>The configuration of our proposed single image SR network structure is outlined in <ref type="figure">Fig. 3</ref>, which cascades m + 1 dilated convolution based inception modules and m + 2 common discrete convolutions. It can be explained as three phases, i.e. feature extraction, feature enhancement and image reconstruction. Since residual learning have been proven to be an effective way to accelerate the network convergence in recent works <ref type="bibr" target="#b8">[9]</ref>, we follow them to make our network predict image high frequency details instead of the HR image itself. The predicted image frequency details will be added to the input LR image to get the final desired HR one. As most single image SR works done, we first up-sample the LR image to the desired size using bicubic interpolation. For the ease of presentation, we still call the interpolation result as a "low-resolution" image, although it has the same size as the HR image. For the feature extraction phase, n dilated convolution based inception modules operate on the LR input. The filter kernel size is 3 ? 3 ? c, where c is the number of image channel, for the first inception module layer. The inception module combines different scale feature information through concatenation operator. To further fuse these multiscale information, in the next we add a nonlinear transformation layer after the inception module layer, which contains 3n common convolution with filter kernel size 3 ? 3 ? 3n.</p><p>Both the high level and low level vision works have proven the deeper the network the better the performance. Furthermore, the larger reception field of the network can catch more contextual information that gives more clues for predicting high frequency details. Therefore, we iterate m times the process of dilated convolution based inception modules following with a common convolution operator for the feature enhancement phase. In this phase, the filter kernel size is 3 ? 3 ? 3n as show in <ref type="figure">Fig. 3</ref>. In the image reconstruction phase, we use a single common convolution of size 3 ? 3 ? 3n to predict the high frequency details. Finally, the predicted high frequency details will add to the interpolation output to get the desired HR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training</head><p>There are a lot of perceptually relevant characteristics based loss functions have been proposed in the literature. But for a fair comparison with SRCNN and FSRCNN, we adopt the mean square error (MSE) as the cost function of our network. Our goal is to train an end-to-end mapping F to predict high frequency detail? = F (x), where x is an interpolation result of the LR image and? is the estimated high frequency detail image. Given a training dataset {x i , y i } N i=1 , the optimization objective is represented as</p><formula xml:id="formula_1">min ? 1 2N N i=1 F (x i ; ?) ? y i 2 F (2)</formula><p>where ? is the network parameters needed to be trained, F (x i ; ?) is the estimated high frequency image with respect to the interpolation result of a LR image. In the literature, people suggest to use to recently proposed Parametric Rectified Linear Unit (PReLU) as the activation function instead of the commonly-used Rectified Linear Unit (ReLU). But, in order to reduce parameters, ReLU is used after each convolution layer in our very deep network that has got a satisfactory result for comparison. We use the adaptive moment estimation (Adam) <ref type="bibr" target="#b13">[14]</ref> to optimize all network parameters instead of the commonly used stochastic gradient descent one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>A lot of experiments have been done to show dramatic improvement in performance can be obtained by our proposed method. We give the experimental details and report the quantitative results on three popular dataset in this section. We name the proposed method as Multiple Scale Super-Resolution Network (MSSRNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets for Training and Testing</head><p>It is well known that training dataset is very important for the performance of learning based image restoration methods. A lot of training dataset can be found in the literature. For example, SRCNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> uses a 91 images dataset and VDSR <ref type="bibr" target="#b8">[9]</ref> uses 291 images dataset. However, the 91 images dataset is too small to push a deep model to the best performance. Some image in the 291 image dataset are JPEG format, which are not optimal for the SR task. We follow FSRCNN <ref type="bibr" target="#b7">[8]</ref> to use the General-100 dataset, which contains 100 bmp-format images (with no compression). We set the patch size as 48 ? 48, and use data augmentation (rotation or flip) to prepare training data. Following FSRCNN, SRCNN and SCN, we use three dataset, i.e. Set5 <ref type="bibr" target="#b14">[15]</ref> (5 images), Set14 <ref type="bibr" target="#b15">[16]</ref> (14 images) and BSD200 <ref type="bibr" target="#b16">[17]</ref> (200 images) for testing, which are widely used for benchmark in other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementing Details and Parameters</head><p>For each dilated convolution based inception module layer, we set n = 8. That is, there are 8 inception module per layer. For feature enhancement process, we iterate 5 inception modules to make the network deeper, i.e. m = 5. Filters are initialized using the initializer proposed by He et al. <ref type="bibr" target="#b17">[18]</ref> with <ref type="figure">Fig. 3</ref>. The proposed single image super-resolution network structure with dilated convolution based multi-scale information learning inception module. It uses inception module, which contains different scale dilated convolution, to learn multi-scale information. Multiple inception modules cascade to constitute a deep network to predict high frequency detail information.  <ref type="bibr" target="#b18">[19]</ref> to set the exponential decay rates for the first and second moment estimate to 0.9 and 0.999, respectively. Each model was trained only 100 epochs and each epoch iterates 2000 times with batch size of 64. We set a lager learning rate in the initial training phase to accelerate convergence, then decrease it gradually to make the model more stable. Therefore, the learning rates are 0.001, 0.0001 and 0.00001 for the first 50 epochs, the 51 to 80 epochs and the last 20 epochs, respectively. Our model is implemented by the MatConvNet package <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-Art Methods</head><p>We compare our method with five state-of-the-art learning based SR algorithms that rely on external databases, namely the A+ <ref type="bibr" target="#b19">[20]</ref>, SRF <ref type="bibr" target="#b20">[21]</ref>, SRCNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, FSRCNN <ref type="bibr" target="#b7">[8]</ref> and SCN <ref type="bibr" target="#b21">[22]</ref>. A+ and SRF are two state-of-the-art traditional methods, while SRCNN, FSRCNN and SCN are three newest popular deep learning based single image super-resolution image methods. In <ref type="table" target="#tab_0">Table 1</ref>, we provide a summary of quantitative evaluation on several datasets. The results of other five methods are the same as reported at FSRCNN. Our method outperforms all previous methods in these datasets. Compare with the newest FSRCNN, our method can improve roughly 0.33 dB, 0.22Db and 0.37 dB on average with respect to upsample factor 2, 3 and 4 on Set5 dataset, respectively. Over the three dataset and three up-sample factor, our MSSRNet can improve roughly 2.33 dB, 0.6 dB, 0.46 dB, 0.74 dB, 0.32 dB and 0.25 dB on average, in comparison with Bicubic, A+, SRF, SRCNN, SCN and FSRCNN, respectively. To get better performance, we can increase the network depth (larger m), which is called deeper is better in the literature, and the network width with larger n. In our experiments, we have implemented the fatter network with n = 16 and n = 32, and the deeper network with m = 10 and m = 15. Both the deeper and the fatter networks show PSNR and SSIM gain. The reader can download our test code 1 to get more quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we use deep learning technology to solve the single image super-resolution problem. We first propose a dilated convolution based inception module, which can learn multi-scale information from the single scale input image. We design a deep network, which cascades multiple dilated convolution based inception modules, for single image superresolution. Experimental results show that the proposed method outperforms many state-of-the-art ones. As future work we plan to explore MSSRNet for video processing. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>(a) Dilated convolution based inception module (b) Original GoogLeNet inception module [1] Fig. Comparison between the proposed dilated convolution based inception module and the original GoogLeNet inception module. Our proposed new inception module contains multiple different scale dilated convolution that makes it can learn multi-scale image information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the proposed inception how to learn multi-scale information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This work has been supported in part by the Major State Basic Research Development Program of China (973 Program 2015CB351804), the National Science Foundation of China under Grant No. 61572155.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results of average PSNR (dB) and SSIM on the Set5<ref type="bibr" target="#b14">[15]</ref>, Set14<ref type="bibr" target="#b15">[16]</ref> and BSD200<ref type="bibr" target="#b16">[17]</ref> dataset</figDesc><table><row><cell>Dataset</cell><cell>Scale</cell><cell>Bicubic PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM A+ SRF SRCNN SCN FSRCNN MSSRNet</cell></row><row><cell></cell><cell>?2</cell><cell>33.66/0.9299 36.55/0.9544 36.87/0.9556 36.34/0.9521 36.76/0.9545 37.00/0.9558 37.33/0.9581</cell></row><row><cell>Set5</cell><cell>?3</cell><cell>30.39/0.9299 32.59/0.9088 32.71/0.9098 32.39/0.9033 33.04/0.9136 33.16/0.9140 33.38/0.9178</cell></row><row><cell></cell><cell>?4</cell><cell>28.42/0.8104 30.28/0.8603 30.35/0.8600 30.09/0.8503 30.82/0.8728 30.71/0.8657 31.10/0.8777</cell></row><row><cell></cell><cell>?2</cell><cell>30.23/0.8687 32.28/0.9056 32.51/0.9074 32.18/0.9039 32.48/0.9067 32.63/0.9088 32.89/0.9117</cell></row><row><cell>Set14</cell><cell>?3</cell><cell>27.54/0.7736 29.13/0.8188 29.23/0.8206 29.00/0.8145 29.37/0.8226 29.43/0.8242 29.57/0.8282</cell></row><row><cell></cell><cell>?4</cell><cell>26.00/0.7019 27.32/0.7471 27.41/0.7497 27.20/0.7413 27.62/0.7571 27.59/0.7535 27.83/0.7631</cell></row><row><cell></cell><cell>?2</cell><cell>29.70/0.8625 31.44/0.9031 31.65/0.9053 31.38/0.9287 31.63/0.9048 31.80/0.9074 32.08/0.9118</cell></row><row><cell>BSD200</cell><cell>?3</cell><cell>27.26/0.7638 28.36/0.8078 28.45/0.8095 28.28/0.8038 28.54/0.8119 28.60/0.8137 28.78/0.8188</cell></row><row><cell></cell><cell>?4</cell><cell>25.97/0.6949 26.83/0.7359 26.89/0.7368 26.73/0.7291 27.02/0.7434 26.98/0.7398 27.17/0.7489</cell></row><row><cell>Avg.</cell><cell></cell><cell>28.80/0.8151 30.53/0.8491 30.67/0.8505 30.39/0.8474 30.81/0.8542 30.88/0.8537 31.13/0.8596</cell></row></table><note>values sampled from the Uniform distribution. For the other hyper-parameters of Adam, we follow</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/wzhshi/MSSRNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Superresolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2009 IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><surname>Tchamitchian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
	<note>in Wavelets</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The discrete wavelet transform: wedding the a trous and mallat algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2482" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scene parsing with multiscale feature learning, purity trees, and optimal covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.2160</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-complexity singleimage super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Line Alberi-Morel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference on</title>
		<meeting>Eighth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deeply improved sparse coding for image super-resolution. arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08905</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
