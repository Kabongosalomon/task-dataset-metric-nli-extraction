<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Domain Expansion for Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibin</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Lab of DEKE, Renmin University of China DAYONG DING, Vistel AI Lab, Visionary Intelligence Ltd. Beijing GANG YANG, School of Information, Renmin University of China XIRONG LI ? , Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Domain Expansion for Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Scene understanding</term>
					<term>Object detection</term>
					<term>Transfer learn- ing Additional Key Words and Phrases: Visual Categorization, Domain Expansion, Classifier Generalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expanding visual categorization into a novel domain without the need of extra annotation has been a longterm interest for multimedia intelligence. Previously, this challenge has been approached by unsupervised domain adaptation (UDA). Given labeled data from a source domain and unlabeled data from a target domain, UDA seeks for a deep representation that is both discriminative and domain-invariant. While UDA focuses on the target domain, we argue that the performance on both source and target domains matters, as in practice which domain a test example comes from is unknown. In this paper we extend UDA by proposing a new task called unsupervised domain expansion (UDE), which aims to adapt a deep model for the target domain with its unlabeled data, meanwhile maintaining the model's performance on the source domain. We propose Knowledge Distillation Domain Expansion (KDDE) as a general method for the UDE task. Its domainadaptation module can be instantiated with any existing model. We develop a knowledge distillation based learning mechanism, enabling KDDE to optimize a single objective wherein the source and target domains are equally treated. Extensive experiments on two major benchmarks, i.e., Office-Home and DomainNet, show that KDDE compares favorably against four competitive baselines, i.e., DDC, DANN, DAAN, and CDAN, for both UDA and UDE tasks. Our study also reveals that the current UDA models improve their performance on the target domain at the cost of noticeable performance loss on the source domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It has been recognized early by the multimedia community that visual classifiers trained on a specific domain do not necessarily perform well on a distinct domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref>. Even for the same concept, e.g., helicopter, changing from one domain to another, e.g., clipart ? painting, would result in significant discrepancy in visual appearance, see <ref type="figure" target="#fig_0">Fig. 1</ref>. When such discrepancy is propagated to the feature space wherein classification is performed, performance degeneration occurs. Note that the advance of deep learning does not alleviate the problem. Rather, due to its "super" learning ability, deep representations tend to be dataset biased <ref type="bibr" target="#b44">[45]</ref>.</p><p>In order to improve the generalization ability of a deep visual classifier without the need of extra annotation, deep unsupervised domain adaptation (UDA) has been actively studied <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. Given the availability of labeled data from a source domain and unlabeled data from a target domain, UDA seeks for a deep representation that is both discriminative and domain-invariant. In the seminal work by Tzeng et al. <ref type="bibr" target="#b34">[35]</ref>, a deep convolutional neural network termed Deep Domain Confusion (DDC) is developed to simultaneously minimize the classification loss and a domain discrepancy loss computed in terms of first-order statistics of the deep features from the two domains. Follow-ups improve DDC in varied aspects, including Deep CORAL <ref type="bibr" target="#b30">[31]</ref> that replaces first-order statistics by second-order statistics, JAN <ref type="bibr" target="#b16">[17]</ref> that measures domain discrepancy on multiple task-specific layers, and DANN <ref type="bibr" target="#b6">[7]</ref> and CDAN <ref type="bibr" target="#b15">[16]</ref> that reduce domain discrepancy by adversarial learning, to name just a few.</p><p>While the above efforts have accomplished well for the UDA task, how they perform in the original source domain is mostly unreported, to the best of our knowledge. The absence of performance evaluation on the source domain rises an important question: is a domain-adapted model indeed domain-invariant? A follow-up question is whether the performance gain for the target domain is obtained at the cost of significant performance loss in the source domain? These two questions are so far overlooked, as existing works on UDA typically assume to know which is the target distribution to tackle. In synthetic-to-real UDA <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref>, for instance, one treats the synthetic data as the source domain and thus only the real dataset performance matters in the end. However, we argue that the performance on both source and target domains matters, as in practice which domain a test example comes from can be unknown. In this paper we extend UDA by proposing a new task, which aims to adapt a deep model for the target domain with its unlabeled data, meanwhile maintaining the model's performance on the source domain. As this factually expands the model's applicable domain, we coin the new task unsupervised domain expansion (UDE).</p><p>UDE targets at an expanded domain consisting of test examples from the source and target domains both. So it handles with ease the situation when the domain of a test example is unknown. Such a situation is not uncommon. Consider the medical field for instance. Optical Coherence Tomography (OCT) images, as an important means for ophthalmologists to assess retinal conditions, are actively used for automated retinal screening and referral recommendation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37]</ref>. As a matter of fact, an eye center is often equipped with multiple types of OCT devices made by distinct manufacturers, let alone multiple eye centers. Meanwhile, OCT images taken by distinct devices can show noticeable discrepancy in their visual appearance, see <ref type="figure">Fig. 2</ref>. Even though UDA improves the generalization ability of a model trained on samples collected from one device for another device, domain adaptation per device is economically unaffordable. With the goal to optimize performance for the expanded domain with a single model, UDE is essential for real-world medical image classification. What is more, by simultaneously evaluating on both source and target domains, UDE provides a direct measure of to what extent a resultant model is domain-invariant, which is mostly missing in the rich literature of UDA. Although UDA models have considered the performance of the source domain in their learning process, the two objectives to be optimized, i.e., discriminability and domain-invariance, are not always consistent. Such a property makes the existing models difficult to maximize their performance on the expanded source + target domain. While existing models such as DDC and CDAN have a trade-off parameter to balance the two objectives, tunning such a parameter was among our early yet unsuccessful efforts, see Section 4.6.4. A new method for UDE is thus in demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source domain Zeiss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expanded domain</head><p>A straightforward solution for the UDE task is to train a domain classifier to automatically determine which domain a specific test image belongs to. Accordingly, if the test image is deemed to be from the source (or target) domain, a model trained on the source domain (or another model adapted w.r.t. the target domain) is selected to handle the image, see <ref type="figure">Fig. 3</ref> (a). Such a solution is cumbersome as it needs to deploy three classifiers at the inference stage. Moreover, misclassification by the domain classifier will make the test image assigned to an inappropriate model. By contrast, we aim for a single domain-expanded model that handle test images from both domains in an unbiased manner, as illustrated in <ref type="figure">Fig. 3</ref> (b). To that end, we resort to deep knowledge distillation (KD) <ref type="bibr" target="#b8">[9]</ref>, initially developed for transferring "dark" knowledge in a cumbersome deep model to a smaller model. We exploit the KD technique with a novel motivation of simultaneously transferring knowledge from the source and domain-adapted models into another model to make that specific model effective for the expanded domain. In sum, our contributions are as follows:</p><p>? We propose UDE as a new task. By simultaneously considering the performance on the source and target domains, UDE is more practical yet more challenging than UDA. The new task allows us to directly measure to what extent a model is domain-invariant. Moreover, to the best of our knowledge, this paper is the first to systematically document the performance of the current UDA models on the source domain, empirically revealing that they are essentially domain-specific rather than domain-invariant. ? We propose Knowledge Distillation Domain Expansion (KDDE), a general method for the UDE task. Its domain-adaptation module can be instantiated with any existing model. Moreover, our knowledge distillation based learning mechanism allows KDDE to optimize a single objective wherein the source and target domains are equally treated. Note that the knowledge distillation technique used by this paper is not new by itself. We adopt it as a proof-of-concept solution for UDE, developed based on a teacher-student framework. While built upon existing components, KDDE provides a principled approach to leveraging previous UDA models for domain expansion, even when multi-domain shifts exist. ? Extensive experiments on two major benchmarks, i.e., Office-Home <ref type="bibr" target="#b35">[36]</ref> and DomainNet <ref type="bibr" target="#b22">[23]</ref>,</p><p>show that KDDE outperforms four competitive baselines, i.e., DDC, DANN, DAAN, and CDAN, for the UDA and UDE tasks both. Besides, experiments on cross-device OCT image classification show a high potential of the proposed method for improving the generalization ability of a medical image classification system in a real-world multi-device scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As we have noted in Section 1, prior work on UDE does not exist. Nonetheless, UDE relies on UDA techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. The proposed KDDE model also benefits from progress in knowledge distillation based deep transfer learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>. Therefore, we review briefly recent developments regarding the two topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Unsupervised Domain Adaptation</head><p>Depending on how domain discrepancy is modeled, we categorize deep learning methods for UDA into two categories, i.e., metric-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> and adversarial methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>. As a representative work of the first category, DDC <ref type="bibr" target="#b34">[35]</ref> measures domain discrepancy in terms of the Euclidean distance between mean feature vectors of the source and target domains. By jointly minimizing the classification loss on the labeled source domain and the inter-domain distance, DDC aims to learn discriminative yet domain-invariant feature representations. While DDC considers only the last feature layer of the underlying classification network, Joint Adaptation Network (JAN) <ref type="bibr" target="#b16">[17]</ref> measures domain discrepancy on multiple task-specific layers.</p><p>Deep CORAL <ref type="bibr" target="#b30">[31]</ref> utilizes second-order statistics, learning to reduce the divergence between the covariance matrices of the two domains. Although the above models are end-to-end, their metrics for domain discrepancy have to be empirically predefined, which could be suboptimal.</p><p>To bypass the difficulty in specifying a proper discrepancy metric, several methods that resort to adversarial learning have been developed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>. Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b6">[7]</ref> introduces a domain classifier as a discriminator, while its feature extractor tries to generate domain-invariant features to confuse the discriminator. Dynamic Adversarial Adaptation (DAAN) <ref type="bibr" target="#b41">[42]</ref> improves DANN by introducing multiple concept-specific discriminators to dynamically weigh the importance of marginal and conditional distributions. In order to align both learned features and predicted classes, Conditional Discriminative Adversarial Network (CDAN) <ref type="bibr" target="#b15">[16]</ref> extends DANN by taking multilinear conditioning of feature representations and classification results as the input of its discriminator. Different from CDAN that uses a feed-forward network as its discriminator, Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b26">[27]</ref> builds a CNN network with two classification branches, and exploits the discrepancy between their output to determine whether a given example is from the source or target domain. Joint Adversarial Domain Adaptation (JADA) <ref type="bibr" target="#b12">[13]</ref> extends MCD by adding an additional domain classifier to achieve class-wise and domain-wise alignments. While all the above works concentrate on the target domain, our work provides a generic approach to improving their performance on the expanded domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Knowledge Distillation</head><p>Deep knowledge distillation is originally proposed to transfer "dark" knowledge in a cumbersome deep model or an ensemble to a smaller model <ref type="bibr" target="#b8">[9]</ref>. Compared with the big model, the small model trained by its own typically has a lower accuracy. Knowledge distillation provides a principled mechanism to let the small model learn as a student from the big model as a teacher. In particular, the student mimics the teacher's behavior by minimizing the Kullback-Leibler divergence or the crossentropy loss between the output of the two models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. For its general applicability, knowledge distillation has been widely used in varied tasks such as object detection <ref type="bibr" target="#b2">[3]</ref>, pose regression <ref type="bibr" target="#b27">[28]</ref>, semantic segmentation <ref type="bibr" target="#b13">[14]</ref>, and saliency prediction <ref type="bibr" target="#b43">[44]</ref>. Not surprisingly, the technology has been investigated for domain adaptation in the context of speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> and image recongition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>. As we target at domain expansion, we leverage knowledge distillation in a different manner, both conceptually and technically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formalization</head><p>We consider multi-class visual categorization, where a specific example belongs to one of predefined visual concepts. Ground truth of is indicated by , a -dimensional one-hot vector. A deep visual classification network classifies by first employing a feature extractor to obtain a vectorized feature representation from its raw pixels. Then, is fed into a -way classifier to produce a categorical probability vector?, where the value of its -th dimension is the probability of the example belonging to the -th concept, i.e.,</p><formula xml:id="formula_0">= ( ), = ( ).<label>(1)</label></formula><p>Such a paradigm as expressed in Eq. 1 remains valid to this day, even though and have now been jointly deployed and end-to-end trained by deep learning. As UDE is derived from UDA, we adopt common notations from the latter for the ease of consistent description. For both UDE and UDA, we have access to a set of labeled training examples {( , , , )} =1 from a source domain and a set of unlabeled training examples { , } =1 from a target domain . However, different from UDA that focuses on , UDE treats the expanded domain + as its "target" domain. Therefore, our goal is to train a unified model that can accurately classify novel examples regardless of their original domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UDE by Knowledge Distillation</head><p>We propose a knowledge distillation based method for UDE, which we term KDDE. As illustrated in <ref type="figure">Fig. 4</ref>, KDDE is performed in two steps. In the first step, two domain-specific classifiers, denoted as and ? , are trained for the source and target domains, respectively. Note that we use the notation ? to emphasize that as is unlabeled, any domain-adapted classifier shall departure from . In the second step, we treat and ? as two teacher models and transfer their "dark" knowledge into a student model + via a knowledge distillation process. In particular, by mimicking</p><p>for classifying examples from and ? for , + essentially becomes domain-invariant. We detail the two steps as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Step 1(a): Training a source-only model . Learning a classifier from labeled data is relatively simple. We adopt the cross-entropy loss, a common classification loss used for training a multi-class deep neural network. Given ( , ) as {( , , , )} =1 , the classification loss is written as</p><formula xml:id="formula_1">( , , ) = 1 ?? ( , ) ?( , ) log( ( )).<label>(2)</label></formula><p>By minimizing ( , , ), we obtain a domain-specific model for as</p><formula xml:id="formula_2">= arg min ( , , ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Step 1(b):</head><p>Training a domain-adapted model ? . Recall that KDDE, as a two-stage solution, is agnostic to the implementation of a specific UDA model used in its first stage. Hence, for obtaining ? , any method for unsupervised domain adaptation can, in principle, be adopted. A typical process of model adaptation tries to strike a proper balance between a model's discriminability, as reflected by the classification loss on , and its domain-invariant representation ability, as measured by inter-domain discrepancy between and . More formally, we have</p><formula xml:id="formula_3">? = arg min ( , , ) discriminative + ? ( , , ) domain-invariant ,<label>(4)</label></formula><p>Step  <ref type="bibr" target="#b15">[16]</ref> as a running example). Then, knowledge distillation is performed to transfer "dark" knowledge from the two domain-specific models into a student model + to make it applicable to both domains.</p><p>where is a domain discrepancy based loss, and is a positive hyper-parameter. Algorithm 1 describes the domain adaptation process at a high level. In what follows, we use the state-of-the-art CDAN model <ref type="bibr" target="#b15">[16]</ref> as a running example to instantiate . Compute the inter-domain discrepancy ( , , , , ? ) ; </p><formula xml:id="formula_4">( , , , ) = ?? ? log ( ( ) ??) + ?? ? log(1 ? ( ( ) ??))<label>(5)</label></formula><p>The training process of CDAN is implemented as a two-player minimax game as follows:</p><formula xml:id="formula_5">( ? , ) = arg min max ( , , ) + ? ( , , , ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.3</head><p>Step 2. Training a domain-expanded model + . Given the domain-specific models and ? , we now transfer their capabilities in their own domains into a new model + by knowledge distillation.</p><p>In a standard scenario where one wants to distill the knowledge in a big teacher model into a relatively small student model <ref type="bibr" target="#b8">[9]</ref>, both ground-truth hard labels and soft labels predicted by the teacher model are available for computing the distillation loss. By contrast, in the setting of UDE, the expanded domain has only partial ground-truth labels by definition. More importantly, in order to make + domain-invariant, we shall not only treat and ? equally, but also exploit training examples from and ? in the same manner. To that end, we opt to compute the distillation loss fully based on the soft labels.</p><p>Specifically, we adopt the Kullback-Leibler (KL) divergence, as previously used to quantify how a student's output matches with its teacher <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. Per training example, the soft labels produced by the teacher / student model is essentially a probability distribution over the concepts. The KL divergence provides a natural measure of how the probability distribution produced by the student is different from that of the teacher, making it a popular loss for knowledge distillation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. Indeed, our ablation study in Section 4.6.1 shows that the KL divergence loss is better than other losses such as cross-entroy and 2 . Let ( ( )) be a -dimensional categorical distribution estimated based on soft labels of an example set produced by a specific network . Accordingly, the KL divergence from to each of the two teachers is defined as ( ( ( ))|| ( ( ))) and ( ( ? ( ))|| ( ( ))), respectively. The knowledge distillation loss is defined as ( , , , ? , ) = ( ( ( ))|| ( ( ))) + ( ( ? ( ))|| ( ( ))).</p><p>Note that the terms in Eq. 7 are practically computed by a mini-batch approach. As demonstrated in <ref type="figure">Fig. 4</ref>, in each iteration two mini-batches are independently and randomly sampled from and . They are then fed into and ? to get the soft labels, which are used to approximate ( ( )) and ( ? ( )), respectively. Meanwhile, the two batches are also fed to the student model. Minimizing lets the student model mimic the teachers' behaviors on both and . Consequently, we obtain the domain-invariant model as</p><formula xml:id="formula_7">+ = arg min ( , , , ? , ).<label>(8)</label></formula><p>A high-level description of the training process is given in Algorithm 2. Note that during training, we need to store three models (two teachers and ? and one student + ). We consider such storage overhead affordable as one typically has access to more computational resources in the training stage than in the inference stage. Moreover, as the teachers are only used to product the soft labels, their weights are frozen, meaning the GPU footprint is much less than simultaneously training all the three models. Also note that in the inference stage. + has the same model size and computation overhead as its teachers. Hence, the proposed method is feasible in real-world scenarios. Update + by back propagation based on ; 10 end 11 end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical Analysis</head><p>Comparing Eq. 4 and Eq. 8, we see that UDA essentially tries to simultaneously optimize two distinct and sometimes conflictive objectives, i.e., discriminability and domain-invariance. By contrast, our KDDE optimizes a single objective. We believe such a property improves the cross-domain generalization ability of KDDE. Nonetheless, because + is learned from the source model and the domain-adapted model ? in an unbiased manner, domain expansion is obtained at the cost of certain performance drop in the source domain. In other words, + will be less effective than its teacher on the source domain, but perform better on the expanded domain, which is the goal of this research.</p><p>As for the target domain, + will be better than its teacher ? . According to Ben-David et al. <ref type="bibr" target="#b1">[2]</ref>, the target domain error of a UDA model is bounded mainly by its classification error in the source domain and the divergence between the induced source marginal and the induced target marginal. In theory, a UDA model shall be trained to simultaneously minimize the two terms and reduce accordingly the up bound of the target domain error. In practice, however, the classification error in the source domain is not effectively reduced when compared to the source-only model. This is confirmed by our experiments in Section 4 that a number of present-day UDA models including DDC <ref type="bibr" target="#b34">[35]</ref>, DANN <ref type="bibr" target="#b6">[7]</ref>, DAAN <ref type="bibr" target="#b41">[42]</ref> and CDAN <ref type="bibr" target="#b15">[16]</ref> suffer performance loss in the source domain. With knowledge distillation, + effectively integrates the merits of ? for minimizing the domain divergence and for minimizing the source error, and thus lowers the up bound of the target domain error.</p><p>Through knowledge distillation, KDDE injects the dark knowledge of the source-only model , which performs well on the source domain, and the domain-adapted model ? , which is supposed to perform well on the target domain, into a single model + . The effect of knowledge distillation on + is visualized in <ref type="figure" target="#fig_2">Fig. 5</ref>. By contrast, although CDAN also uses one classifier for both domains, the classifier is essentially ? used in KDDE. Hence, it is less effective than + to handle the expanded domain. Also notice that the purpose of knowledge distillation is not to reduce the difference between the two teacher models, see Eq. 7. Therefore, KDDE is conceptually different from Maximum classifier discrepancy (MCD) <ref type="bibr" target="#b26">[27]</ref>, which is to reduce the discrepancy between two domain-adapted classifiers via a novel adversarial learning mechanism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the effectiveness of the proposed KDDE model on two public benchmarks, i.e., Office-Home <ref type="bibr" target="#b35">[36]</ref> and DomainNet <ref type="bibr" target="#b22">[23]</ref>, and a private dataset OCT-11k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.1</head><p>Office-Home. The Office-Home dataset contains 15,588 images of 65 object categories typically found in office and home settings, e.g., chair, table, and TV. Images were collected from the following four distinct domains, i.e., Art (A), Clipart (C), Product (P) and Real world (R). As the dataset is previously used in the domain adaption setting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39</ref>] that considers only performance on the target domain, no test set is provided for the source domain. So to evaluate UDE, for each domain, we randomly divide its images into two disjoint subsets, one for training and the other for test, at a ratio of 1:1, see <ref type="table" target="#tab_6">Table 1</ref>. An expanded domain of C+P means using Clipart as the (labeled) source domain, which is expanded with Product as the (unlabeled) target domain. Due to such asymmetric nature of the label information, P+C differs from C+P. Pairing the individual domains results in 12 distinct UDE tasks in total.  <ref type="bibr" target="#b15">[16]</ref>: Also based adversarial learning, using multilinear conditioning of deep features and classification results as the input of its discriminator.</p><p>Recall that DDC and CDAN are representatives for metric-based and adversarial methods, respectively. Hence, we instantiate ? using the two models separately, resulting in two variants of KDDE, denoted as KDDE(DDC) and KDDE(CDAN).</p><p>We implement the source model using ResNet-50 that is trained exclusively on the source domain. A model for UDA / UDE shall outperform this baseline on the target / expanded domain. For fair comparison between distinct models, we also use ResNet-50 as their backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Model training.</head><p>We run all experiments with PyTorch <ref type="bibr" target="#b21">[22]</ref>. We start with ResNet-50 pretrained on ImageNet. SGD is used for training, with momentum of 0.9 and weight decay of 0.0005. The initial learning rate of ResNet-50, DDC, DANN, DAAN and CDAN is empirically set to 0.001, and 0.005 for KDDE. For the adversarial methods, i.e., DANN, DAAN and CDAN, we adopt the inverse-decay learning rate strategy from <ref type="bibr" target="#b15">[16]</ref>. As for ResNet-50, DDC and KDDE, the learning rate is decayed by 0.1 every 30 epochs on Office-Home and every 10 epochs on DomainNet, as the latter has much more training examples and thus more iterations per epoch. For the same reason, for each model we train 100 epochs on Office-Home and a less number of 30 epochs on DomainNet.</p><p>For DDC, DANN and DAAN, the trade-off parameter is empirically set to be 10, 1, and 1, respectively. As for CDAN, we follow the original paper <ref type="bibr" target="#b15">[16]</ref> to adjust dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evaluation protocol.</head><p>For overall performance, we report accuracy, i.e., the percentage of test images correctly classified, as commonly used for evaluating multi-class image classification. For an expanded domain, e.g., A+C, its test set is the union of the test sets of the Art and Clipart domains. To cancel out data imbalance, the accuracy of the expanded domain is obtained by averaging over the two individual domains. The performance of a specific concept is measured by F1-score, the harmonic mean of precision and recall. For binary classification on OCT-11k, we additionally report the area under the ROC curve (AUC). <ref type="table" target="#tab_4">Table 2</ref> reports the overall performance, with per-task results detailed in <ref type="table" target="#tab_8">Table 3</ref> and <ref type="table" target="#tab_9">Table 4</ref>. Note that although the performance gap appears to be relatively small, see also the performance reported in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>, the significance of the gap shall not be underestimated due to the challenging nature of the UDA / UDE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Office-Home</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Performance on the target domain.</head><p>All the domain adaption models are found to be better than the source-only ResNet-50. This is consistent with previous works on UDA. Among them, CDAN performs the best, obtaining a relative improvement of 4.36%. KDDE(CDAN) surpasses CDAN, with accuracy increased from 61.85 to 63.90. KDDE(DDC) is better than DDC, with accuracy increased from 60.61 to 61.62. The results confirm that KDDE is beneficial for the UDA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance on the source domain.</head><p>In contrast to their performance on the target domain, the domain adaption models consistently show performance degeneration on the source domain, with their relative loss ranging from 0.20% (DAAN) to 2.03% (CDAN). In particular, CDAN as the best domain adaptation model degenerates the most, suggesting that the gain on the target domain is obtained at the cost of affecting the classification ability on the source domain. The use of the   <ref type="table" target="#tab_8">Table 3</ref> and 4, for 10 out of all the 12 UDE tasks, KDDE(CDAN) has the highest accuracy, followed by KDDE(DDC). To further verify the necessity of KDDE, we compare it with the Model Selection method, previously shown in <ref type="figure">Fig. 3 (a)</ref>. Given the two domain-specific models (ResNet-50 and CDAN) trained, model selection classifies a test example using ResNet-50 if the example is deemed to be from the source domain, and using CDAN otherwise. To that end, another ResNet-50 is trained as a domain classifier. Hence, the model selection method requires three ResNet-50 models per task. In addition, we compare with Model ensemble, another baseline that combines ResNet-50 and CDAN by late average fusion.</p><p>We select two UDE tasks from Office-Home, namely C?A and P?R, considering that the Clipart and Art domains have significant visual difference, while the Product and Real_world domains look similar. Indeed, this is confirmed by the performance of the domain classifier, which can separate Clipart from Art with an accuracy of 96.56 and a lower accuracy of 85.39 for distinguishing the other two domains. As shown in <ref type="table" target="#tab_10">Table 5</ref>, KDDE is better than model selection, which uses three ResNet-50 models. To remove the influence of incorrect domain classification, we also try model selection with ground-truth domain labels, which corresponds to Model selection (Oracle) in <ref type="table" target="#tab_10">Table  5</ref>. Again, KDDE is better. Also note that providing ground-truth domain labels does not necessarily lead to better performance. Model ensemble has accuracy of 66.78 on C?A and 83.25 on P?R. Note that KDDE uses one ResNet50 model while the ensemble requires two ResNet50s. KDDE is better than the ensemble in terms of accuracy, yet uses 50% less resources at runtime. The results further demonstrate the importance of learning domain-invariant models for UDE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on DomainNet</head><p>Overall performance on DomainNet is summarized in <ref type="table" target="#tab_4">Table 2</ref>, with detailed results reported in <ref type="table" target="#tab_11">Table 6 and Table 7</ref>.  4.4.1 Performance on the target domain. Similar to the results on Office-Home, we again observe that the domain adaptation models are effective for improving the performance of ResNet-50 on the target domain. In particular, as <ref type="table" target="#tab_4">Table 2</ref> shows, DDC, CDAN and DANN obtain a relative improvement of 4.71%, 3.72% and 3.04%, respectively. However, different from Office-Home, the classical DDC model now outperforms CDAN and DANN on DomainNet. The result suggests that although domain adaptation by adversarial learning is theoretically more appealing than the metric-based counterpart, much room exists for improving the adversarial methods. The proposed KDDE is again found to be effective, surpassing the best DDC with accuracy increased from 46.20 to 48.04, which accounts for a relative improvement of 3.98%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Performance on the source domain.</head><p>The source-only ResNet-50 model performs expectedly best on the source domain. Compared to DDC, DANN and CDAN have relatively larger loss in performance. The result suggests that the adversarial methods perform domain adaptation more aggressively. With KDDE, the relative loss of DDC is reduced from 2.15% to 0.81%, and that of CDAN is reduced from 4.86% to 1.61%. This result again shows that KDDE better preserves the classification ability for the source domain. Recall that KDDE targets at the expanded domain, with knowledge from the source and target domain models treated equally. It is therefore less effective than ResNet50 exclusively trained on the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.3</head><p>Performance on the expanded domain. The best baseline is DDC, obtaining a relative improvement of 1.28% against ResNet-50. With KDDE, this number increases to 2.87%. Moreover, KDDE consistently outperforms the baselines for all the 12 UDE tasks, see <ref type="table" target="#tab_11">Table 6 and Table 7</ref>. These results clearly justify the effectiveness of the proposed model for UDE. <ref type="figure" target="#fig_4">Fig. 6</ref> shows a concept-based comparison of ResNet-50, CDAN and KDDE(CDAN) on three UDE tasks, i.e., clipart(c) ? painting(p), clipart(c) ? real(r) and clipart(c) ? sketch(s). For the ease of comparison, for each model we have sorted all the 345 concepts in descending order according to their F1-scores. This allows us to measure the performance gap between the -th best-performed concept of the models. As shown in the first column of <ref type="figure" target="#fig_4">Fig. 6</ref>, when tested on the source domain, the 150-th best F1-score of ResNet-50 and KDDE (CDAN) is approximately 0.8, while the corresponding position of CDAN is noticeably lower. The curve of KDDE(CDAN) is between ResNet-50 and CDAN, indicating that the performance of CDAN for the source domain has been recovered to some extent by KDDE. As for the target domain, see the middle column of <ref type="figure" target="#fig_4">Fig. 6, KDDE(CDAN)</ref>is higher than CDAN, followed by ResNet-50, proving that KDDE(CDAN) is also beneficial for the UDA task. As shown in the last column, KDDE(CDAN) scores the best for the majority of the concepts on the expanded domain. <ref type="figure" target="#fig_0">Fig. 1</ref> presents the t-SNE <ref type="bibr" target="#b17">[18]</ref> embedding of deep features learned by ResNet-50, CDAN and KDDE (CDAN) in the setting of clipart(c)?painting(p). For better visualization, we only show test examples of 10 concepts selected at random from the top 30 best-performed concepts of ResNet-50. Across the source, target and expanded domains, intra-class data points tend to stay closer while inter-class data points are more distant in the feature space of KDDE(CDAN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Qualitative analysis.</head><p>In order to better understand the behavior of the three models, we further employ Grad-CAM <ref type="bibr" target="#b28">[29]</ref> to visualize how the decisions are made. <ref type="figure" target="#fig_2">Fig. 5</ref> shows Grad-CAM based heatmaps, where the first three rows and the last three rows are test images selected from the source (clipart) and the target (real) domains, respectively. Note that ResNet-50 clearly differs from CDAN in terms of their salient areas. By contrast, KDDE(CDAN) imitates ResNet-50 on the source domain (first three rows), yet resembles CDAN on the target domain (last three rows). These heatmaps demonstrate how KDDE adaptively learns from the two domain-specific models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on OCT-11k</head><p>Results on OCT-11k measured in terms of confusion matrices, accuracy and AUC are summarized in <ref type="table" target="#tab_13">Table 8</ref>. Similar to our previous experiments on Office-Home and DomainNet, the source-only ResNet-50 performs well on the source domain (Zeiss) but fails to generalize to the target domain (Topcon). Its confusion matrix shows that a large number of 334 true negatives are incorrectly classified as positive. This is in line with its ROC curve in <ref type="figure">Fig. 2c</ref>   ROC curve of CDAN is quite close to that of ResNet-50, its operating point given the same cutoff obtains a smaller FPR. The results suggest that UDA effectively improves the model's insensitivity w.r.t. the value of the cutoff on the target domain. However, this advantage is obtained at the cost of noticeable performance drop on the source domain (0.8233 versus 0.8533 in accuracy and 0.8926 versus 0.9326 in AUC). By contrast, KDDE(CDAN) obtains the best overall performance, with no need of tuning the cutoff, justifying KDDE as a more principled approach to improving medical image classification in a multi-device scenario. We compare the KL divergence loss with two alternatives, i.e., the 2 loss and the cross-entropy loss. As shown in <ref type="table" target="#tab_14">Table 9</ref>, the KL divergence loss performs the best. In order to verify if features obtained by KDDE are more discriminative, we consider cross-domain image retrieval, where each test image from one domain is used as a query example to retrieve images from the other domain. In particular, we conduct cross-domain image retrieval on two UDE tasks, i.e., C?A and P?R, on Office-Home. We compare ResNet-50, DDC and KDDE(DDC). Per model, the dissimilarity between two images and ? is defined as the 2 distance between their 2,048-d features ( ) and ( ? ). As <ref type="table" target="#tab_6">Table 10</ref> shows, using features of KDDE obtains higher precisions, indicating that cross-domain instances of the same class stay more closer in the intermediate feature space. Some qualitative results are presented in <ref type="figure">Fig. 7</ref>, where the top-5 returned items w.r.t. KDDE consistently exhibit domain-invariant visual patterns of sneakers. Both quantitative and qualitative results allow us to conclude that knowledge distillation results in more discriminative and domain-invariant feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">KDDE for multi-domain shifts.</head><p>Existing works on domain adaptation typically consider single-domain shift, where one wants to adapt a model for a single target domain, e.g., A? . We investigate a more challenging scenario of multi-domain shifts, which is to generalize the model simultaneously to multiple target domains, e.g., A? {C, P, R}. To that end, we improve CDAN by modifying its binary domain discriminator to support 4-way classification. We term the variant CDAN+. The performance of CDAN+ and KDDE (CDAN+) is reported in <ref type="table" target="#tab_6">Table 11</ref>. Patterns similar to the previous single-domain shift experiments are observed. That is, CDAN+ outperforms the source-only ResNet-50 on the target domains but is less effective on the original source domain,  <ref type="table" target="#tab_4">Table 12</ref> performance of DDC with the trade-off parameter chosen from {0, 0.01, 0.1, 1, 10, 20, 100}, where = 10 is the choice we have used so far. The peak performance is reached given = 0.01, yet remains lower than KDDE. Moreover, by highlighting the best-performed per domain using light-blue cells, we see that its optimal value is domain-dependent. Compared to simply tunning the trade-off parameter, KDDE is a more principled approach. It is worth mentioning that DDC ( =0) is not equivalent to the source-only ResNet-50. Because mini-batches sampled from the target domain have been used together with source mini-batches to estimate mean and variance of the Batch Normalization (BN) layers. This improves domain-variance of the intermediate features to some extent, and consequently leads to better performance (71.59 versus 70.11 in <ref type="table" target="#tab_4">Table 12</ref>). <ref type="table" target="#tab_6">Table 11</ref>. Performance of multi-domain shifts on Office-Home. A?{C,P,R} means using A as a source domain, which will be expanded to cover C, P and R. CDAN+ is our improved version of CDAN <ref type="bibr" target="#b15">[16]</ref> that performs 4-way classification in its domain discriminator. The source-only ResNet-50 is used as a reference. Performance increases (decreases) over this reference are shown in red (green).  4.6.5 KDDE with MCD. As we have noted in Section 3, the UDA module of the proposed KDDE method can be implemented using any state-of-the-art UDA model. Here we instantiate the module using MCD <ref type="bibr" target="#b26">[27]</ref>. Again, ResNet-50 is used as their backbones. As shown in <ref type="table" target="#tab_6">Table 13</ref>, KDDE(MCD) consistently outperforms MCD.  <ref type="table" target="#tab_4">Table 2</ref> alone needs around 750 GPU hours, when running the related experiments in parallel on four GPU cards (two GTX 2080Ti plus two 1080Ti GPUs). Probably because of this reason, we rarely see reproducibility test in the literature of domain adaptation. Nonetheless, to reduce randomness, we run the experiments three times for all the tasks of Office-Home. As <ref type="table" target="#tab_6">Table 14</ref> shows, our major conclusion, i.e., the proposed KDDE is more effective, is again confirmed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have defined a new task termed unsupervised domain expansion (UDE). Accordingly, two benchmark datasets, i.e., Office-Home and DomainNet, have been re-purposed for the task. Our evaluation about four present-day domain adaptation models, either metric-based or adversarial, shows that their gain on the target domain is obtained at the cost of affecting their classification ability on the source domain. The proposed KDDE model effectively reduces such cost, and is found to be effective for both the new task and the traditional unsupervised domain adaptation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Visualization of deep feature spaces obtained by three models, that is, ResNet-50 fully trained on a labeled source domain, CDAN [16] as a state-of-the-art domain adaptation model and the proposed Knowledge Distillation Domain Expansion (KDDE) that adaptively learns from both ResNet-50 and CDAN. Across the source, target and expanded domains, intra-class data points tend to stay closer while inter-class data points are more distant in the feature space of KDDE. With no need of extra labeled data, KDDE effectively expands the applicable domain of visual classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>10 Update</head><label>10</label><figDesc>reduces domain discrepancy by adversarial learning, where a feed-forward neural network is used as a discriminator to disentangle the source examples from the target examples . Different from previous adversarial learning based methods where only the intermediate features ( ) and ( ) are considered, CDAN uses multilinear conditioning of the features and class prediction ( ) ??as the input of . The output of is the probability of a given example coming from the source domain. Accordingly, we have the discriminator reward as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Grad-CAM heatmaps of ResNet-50, CDAN and KDDE. Heatmaps highlight regions important for a model to make predictions. Test images in the first three rows are from the source domain (clipart), while images in the last three rows are from the target domain (real). Activated regions of KDDE ( + ) match well with those of ResNet-50 ( ) on the source domain and those of CDAN ( ? ) on the target domain, suggesting the ability of KDDE to adaptively learn from the two domain-specific models. Data from DomainNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>that the operating point with default cutoff of 0.5 produces a relatively larger False Positive Rate (FPR). Interestingly, while the 0 50 100 150 200 250 300 350Index of concepts, sorted by F1-score</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>A concept-based comparison of ResNet-50, CDAN and KDDE (CDAN) on DomainNet. For the majority of the 345 concepts, KDDE(CDAN) is either better or comparable to the other two models on the target domain and on the expanded domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. Abnormal OCT image recognition as a showcase of Unsupervised Domain Expansion (UDE).</figDesc><table><row><cell></cell><cell></cell><cell>Classifier performance on the</cell></row><row><cell>Zeiss</cell><cell>+ Topcon</cell><cell>expanded domain</cell></row><row><cell>Abnormal</cell><cell></cell><cell></cell></row><row><cell>Normal</cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row></table><note>OCT images taken by distinct devices, e.g., (a) Zeiss Cirrus OCT and (b) Topcon 2000FA OCT, often differ noticeably in their visual appearance. A domain-adapted classifier, departing from the source domain (Zeiss), emphasizes its performance on the target domain (Topcon). In contrast, a domain-expanded classifier aims for the overall performance of both source and target domains, which is essential for real-world applications. Red circles on (c) ROC curves of the three classifiers indicate their operating points using the default cutoff of 0.5. See Section 4 for detailed experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Training a domain-expanded model + by KDDE Input: , , , ? Compute ( , , , , , ? , + ) using Eq. 7;</figDesc><table><row><cell></cell><cell>Output: +</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">1 Set hyper-parameters:</cell><cell>_</cell><cell>,</cell><cell>_</cell><cell>;</cell></row><row><cell cols="2">2 Compute</cell><cell>_</cell><cell>given</cell><cell cols="2">(| |, | |) and</cell><cell>_</cell><cell>;</cell></row><row><cell cols="6">3 Initialize + with an ImageNet-pretrained model;</cell></row><row><cell cols="2">4 for ? 1,</cell><cell>_</cell><cell>do</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>for ? 1,</cell><cell>_</cell><cell>do</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell cols="3">Sample , from ;</cell><cell></cell><cell></cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>7 Sample, from ;8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Three datasets used in our experiments for UDA and UDE. Note that different from the traditional setting of UDA which uses all examples in the source domain for training, the setting of UDE divides the source examples into two disjoint parts, training and test. This allows us to evaluate both UDA and UDE models on the original source domain, which is fully ignored by the literature of UDA. All the Zeiss images and a subset of 900 Topcon images were labeled by experts into two classes, namely positive and negative. An image was labeled as positive if certain pathological anomaly such as macular edema, macular hole, drusen, and choroidal atrophy is present. Accordingly, we treat Zeiss as the source domain and Topcon as the target domain. To let the expanded domain contain an equal number of test examples from the individual domains, we randomly sample a subset of 900 Zeiss images as the test set of the source domain. The Zeiss test set contains 581 positives and 319 negatives, while the Topcon test set contains 471 positives and 429 negatives. Among the first to obtain domain-invariant deep features by adversarial learning.? DAAN<ref type="bibr" target="#b41">[42]</ref>: An adversarial learning based domain adaptation model, with discriminator networks per concept. For its high demand for GPU memory, we are unable to run DAAN on DomainNet.</figDesc><table><row><cell>Dataset</cell><cell>Domains</cell><cell cols="2">Images total training</cell><cell>test</cell></row><row><cell>Office-Home [36] 15,588 images 65 classes</cell><cell>A: Art C: Clipart P: Product R: Real_world</cell><cell>2,427 4,365 4,439 4,357</cell><cell>1,201 2,165 2,201 2,161</cell><cell>1,226 2,200 2,238 2,196</cell></row><row><cell>Subset of</cell><cell>c: clipart</cell><cell>48,129</cell><cell cols="2">33,525 14,604</cell></row><row><cell>DomainNet [23]</cell><cell>p: painting</cell><cell>72,266</cell><cell cols="2">50,416 21,850</cell></row><row><cell>362,470 images</cell><cell>r: real</cell><cell cols="3">172,947 120,906 52,041</cell></row><row><cell>345 classes</cell><cell>s: sketch</cell><cell>69,128</cell><cell cols="2">48,212 20,916</cell></row><row><cell>OCT-11k (our private dataset)</cell><cell>Z: Zeiss</cell><cell>5,900</cell><cell>5,000</cell><cell>900</cell></row><row><cell>11,800 images, 2 classes</cell><cell>T: Topcon</cell><cell>5,900</cell><cell>5,000</cell><cell>900</cell></row></table><note>4.1.2 DomainNet. DomainNet is a recent large-scale benchmark dataset used in the Visual Domain Adaptation Challenge at ICCV 2019 1 . Compared with Office-Home, DomainNet contains a much larger number of 345 object categories with more intra-class diversity and inter-class ambiguity in visual appearance. As such, it is difficult to obtain high classification accuracy even within a narrow domain. The full set of DomainNet has six domains, i.e., , clipart (c), infograph (i), painting (p), quickdraw (q), real (r) and sketch (s). Saito et al. [26] exclude inforgraph and quickdraw from their study as they find annotations of these two domains are over noisy. We follow their setup, experimenting with the four other domains.4.1.3 OCT-11k. In order to evaluate the effectiveness of our proposed method for medical image classification in a cross-device setting, we build a set of 11,800 OCT B-scan images, half of which was collected by Zeiss Cirrus OCT and the other half from Topcon 2000FA OCT.4.2 Implementation 4.2.1 Baselines. We compare with the following state-of-the-art domain adaptation models. ? DDC [35]: A classical deep domain adaptation model that minimizes domain discrepancy measured in light of first-order statistics of the deep features.? DANN [7]:? CDAN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Overall performance of distinct models on Office-Home and DomainNet. Performance metric is accuracy, shown in percentages. We use ResNet-50 as a reference. Performance increases (decreases) over this reference are shown in red (green). Note that training DAAN on DomainNet is beyond our computational capacity, so its performance on DomainNet is unavailable. Better performance of KDDE against alternatives on the target / expanded domain justifies its effectiveness for UDA / UDE.</figDesc><table><row><cell>Model</cell><cell cols="3">Office-Home Source domain Target domain Expanded domain</cell></row><row><cell>ResNet-50</cell><cell>82.57</cell><cell>57.49</cell><cell>70.03</cell></row><row><cell>DANN</cell><cell>81.42?-1.15</cell><cell>60.89?+3.40</cell><cell>71.16?+1.13</cell></row><row><cell>CDAN</cell><cell>80.54?-2.03</cell><cell>61.85?+4.36</cell><cell>71.20?+1.17</cell></row><row><cell>DDC</cell><cell>82.22?-0.35</cell><cell>60.61?+3.12</cell><cell>71.41?+1.38</cell></row><row><cell>DAAN</cell><cell>82.37?-0.20</cell><cell>60.78?+3.29</cell><cell>71.57?+1.54</cell></row><row><cell>KDDE(DDC)</cell><cell cols="2">82.57?+0.006 61.62?+4.13</cell><cell>72.10 ?+2.07</cell></row><row><cell cols="2">KDDE(CDAN) 81.44?-1.13</cell><cell>63.90?+6.41</cell><cell>72.67?+2.64</cell></row><row><cell>Model</cell><cell cols="3">DomainNet Source domain Target domain Expanded domain</cell></row><row><cell>ResNet-50</cell><cell>74.59</cell><cell>41.49</cell><cell>58.04</cell></row><row><cell>DANN</cell><cell>69.37?-5.22</cell><cell>44.53?+3.04</cell><cell>56.95?-1.09</cell></row><row><cell>CDAN</cell><cell>69.73?-4.86</cell><cell>45.21?+3.72</cell><cell>57.47?-0.57</cell></row><row><cell>DDC</cell><cell>72.44?-2.15</cell><cell>46.20?+4.71</cell><cell>59.32?+1.28</cell></row><row><cell>DAAN</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell></row><row><cell>KDDE(DDC)</cell><cell>73.78?-0.81</cell><cell>48.04?+6.55</cell><cell>60.91?+2.87</cell></row><row><cell cols="2">KDDE(CDAN) 72.98?-1.61</cell><cell>47.65?+6.16</cell><cell>60.31?+2.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 .</head><label>3</label><figDesc>45.23 60.22 75.20 58.45 66.83 75.20 69.35 72.28 DDC 72.51 49.09 60.80 73.65 62.60 68.13 73.90 70.86 72.38 DANN 71.29 49.23 60.26 73.33 60.99 67.16 74.31 70.17 72.24 DAAN 73.98 48.95 61.47 73.98 64.30 69.14 74.39 71.17 72.78 CDAN 70.96 46.73 58.85 71.78 64.61 68.20 72.59 70.63 71.61 KDDE(DDC) 73.08 48.86 60.97 74.39 63.67 69.03 74.96 71.22 73.09 KDDE(CDAN) 70.07 48.77 59.42 72.19 66.71 69.45 73.98 72.40 73.19 47.06 62.99 78.91 57.55 68.23 78.91 59.65 69.28 DDC 80.23 50.90 65.57 79.59 62.60 71.10 78.77 63.57 71.17 DANN 78.27 54.08 66.18 78.86 61.71 70.29 78.91 63.02 70.97 DAAN 78.64 53.34 65.99 79.86 62.29 71.08 79.82 64.12 71.97 CDAN 77.82 53.34 65.58 78.00 66.13 72.07 79.09 63.93 71.51 KDDE(DDC) 80.05 54.57 67.31 80.68 64.97 72.83 80.18 65.16 72.67 KDDE(CDAN) 78.27 57.75 68.01 79.77 68.68 74.23 80.36 66.67 73.52proposed KDDE reduces such cost. In particular, KDDE(CDAN) reduces the loss of CDAN from 2.03% to 1.13%, while KDDE(DDC) is even comparable to the original ResNet-50 model.</figDesc><table><row><cell>Model</cell><cell>A</cell><cell>A?C C</cell><cell>A+C</cell><cell>A</cell><cell>A?P P</cell><cell>A+P</cell><cell>A</cell><cell>A?R R</cell><cell>A+R</cell></row><row><cell cols="2">ResNet-50 75.20 Model C</cell><cell>C?A A</cell><cell>C+A</cell><cell>C</cell><cell>C?P P</cell><cell>C+P</cell><cell>C</cell><cell>C?R R</cell><cell>C+R</cell></row><row><cell>ResNet-50</cell><cell>78.91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Performance on Office-Home. Art (A) and Clipart (C) are used as the source domain, respectively. The notation ? means using Clipart (C) as the target domain and consequently resulting in an expanded domain of A + C. For five out of the six UDA / UDE tasks, KDDE(CDAN) performs the best.4.3.3 Performance on the expanded domain. KDDE(CDAN) performs the best. Moreover, as shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Performance on Office-Home. Product (P) and Real_world (R) are used as the source domain, respectively. For all the six UDA tasks and five out of the fix UDE tasks, KDDE(CDAN) performs the best. 50.33 71.19 92.05 43.86 67.96 92.05 70.31 81.18 DDC 92.09 53.67 72.88 91.55 45.05 68.30 92.49 71.95 82.22 DANN 90.57 53.18 71.88 89.95 47.05 68.50 91.64 72.59 82.12 DAAN 91.82 53.67 72.75 91.51 44.09 67.80 92.45 72.63 82.54 CDAN 91.11 53.67 72.39 88.83 49.36 69.10 91.20 73.82 82.51 KDDE(DDC) 91.91 54.73 73.32 91.60 46.27 68.94 92.67 73.36 83.02 KDDE(CDAN) 91.51 55.55 73.53 90.30 49.91 70.11 92.45 75.68 84.07 63.62 73.87 84.11 48.23 66.17 84.11 76.27 80.19 DDC 84.70 64.19 74.45 82.97 52.23 67.60 83.93 77.35 80.64 DANN 84.06 65.33 74.70 82.65 55.36 69.01 83.24 77.97 80.61 DAAN 84.61 64.85 74.73 83.29 52.09 67.69 84.06 77.84 80.95 CDAN 82.01 64.03 73.02 80.42 55.73 68.08 82.70 80.21 81.46 KDDE(DDC) 84.38 64.52 74.45 83.24 53.86 68.55 83.74 78.28 81.01 KDDE(CDAN) 83.29 65.50 74.40 81.65 57.73 69.69 83.42 81.41 82.42</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>P?A A</cell><cell>P+A</cell><cell>P</cell><cell>P?C C</cell><cell>P+C</cell><cell>P</cell><cell>P?R R</cell><cell>P+R</cell></row><row><cell cols="2">ResNet-50 92.05 Model R</cell><cell>R?A A</cell><cell>R+A</cell><cell>R</cell><cell>R?C C</cell><cell>R+C</cell><cell>R</cell><cell>R?P P</cell><cell>R+P</cell></row><row><cell>ResNet-50</cell><cell>84.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>KDDE versus Model selection for UDE. Model selection classifies a test example using ResNet-50 if the example is deemed to be from the source domain, or using CDAN otherwise.</figDesc><table><row><cell>Model</cell><cell>C?A P?R</cell></row><row><cell>ResNet-50</cell><cell>62.99 81.18</cell></row><row><cell>CDAN</cell><cell>65.58 82.51</cell></row><row><cell cols="2">Model selection (Domain classifier) 66.29 82.71</cell></row><row><cell>Model selection (Oracle)</cell><cell>66.13 82.94</cell></row><row><cell>Model ensemble</cell><cell>66.78 83.25</cell></row><row><cell>KDDE (CDAN)</cell><cell>68.01 84.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Performance on DomainNet, with clipart (c) and painting (p) as the source domain, respectively. Among the six UDA / UDE tasks, KDDE(DDC) tops the performance five times, followed by KDDE(CDAN).<ref type="bibr" target="#b31">32</ref>.07 54.62 77.16 48.22 62.69 77.16 38.50 57.83 DDC 75.36 36.52 55.94 75.77 54.09 64.93 75.10 41.22 58.16 DANN 71.35 33.51 52.43 73.92 52.98 63.45 73.45 40.41 56.93 CDAN 72.45 34.33 53.39 73.34 53.23 63.29 72.25 39.08 55.67 KDDE(DDC) 76.57 37.71 57.14 76.77 55.52 66.15 76.20 42.17 59.19 KDDE(CDAN) 75.69 36.27 55.98 77.25 55.60 66.43 75.53 41.81 58.67 39.72 54.72 69.71 53.28 61.50 69.71 33.30 51.51 DDC 65.40 44.86 55.13 68.98 58.48 63.73 65.01 37.93 51.47 DANN 59.89 41.74 50.82 66.78 55.24 61.01 61.70 36.83 49.27 CDAN 63.54 43.09 53.32 65.58 55.30 60.44 61.83 37.64 49.74 KDDE(DDC) 67.45 46.73 57.09 70.39 59.91 65.15 66.20 39.60 52.90 KDDE(CDAN) 66.34 45.07 55.71 69.68 57.64 63.66 65.19 39.53 52.36</figDesc><table><row><cell>Model</cell><cell>c</cell><cell>c?p p</cell><cell>c+p</cell><cell>c</cell><cell>c?r r</cell><cell>c+r</cell><cell>c</cell><cell>c?s s</cell><cell>c+s</cell></row><row><cell cols="2">ResNet-50 77.16 Model p</cell><cell>p?c c</cell><cell>p+c</cell><cell>p</cell><cell>p?r r</cell><cell>p+r</cell><cell>p</cell><cell>p?s s</cell><cell>p+s</cell></row><row><cell>ResNet-50</cell><cell>69.71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Performance on DomainNet, with real (r) and sketch (s) as the source domain, respectively. Among the six UDA tasks, both KDDE(DDC) and KDDE(CDAN) tops the performance three times. As for the six UDE tasks, KDDE(DDC) performs the best, followed by KDDE(CDAN). 49.60 66.28 82.96 45.71 64.34 82.96 34.50 58.73 DDC 81.16 50.08 65.62 82.14 46.50 64.32 80.23 36.34 58.29 DANN 77.25 49.32 63.29 78.34 43.25 60.80 76.85 37.84 57.35 CDAN 79.10 50.99 65.05 80.63 46.30 63.47 78.03 40.02 59.03 KDDE(DDC) 82.19 52.68 67.44 83.28 48.77 66.03 81.25 38.71 59.98 KDDE(CDAN) 81.37 53.56 67.47 82.68 49.00 65.84 80.59 41.93 61.26 49.92 59.22 68.51 31.19 49.85 68.51 41.84 55.18 DDC 66.57 54.26 60.42 66.48 41.15 53.82 67.04 52.97 60.01 DANN 64.36 53.13 58.75 64.61 39.88 52.25 63.97 50.27 57.12 CDAN 63.48 52.04 57.76 63.36 39.89 51.63 63.11 50.55 56.83 KDDE(DDC) 68.33 56.46 62.40 68.15 43.47 55.81 68.54 54.75 61.65 KDDE(CDAN) 66.57 55.34 60.96 66.70 42.51 54.61 68.19 53.51 60.85</figDesc><table><row><cell>Model</cell><cell>r</cell><cell>r?c c</cell><cell>r+c</cell><cell>r</cell><cell>r?p p</cell><cell>r+p</cell><cell>r</cell><cell>r?s s</cell><cell>r+s</cell></row><row><cell cols="2">ResNet-50 82.96 Model s</cell><cell>s?c c</cell><cell>s+c</cell><cell>s</cell><cell>s?p p</cell><cell>s+p</cell><cell>s</cell><cell>s?r r</cell><cell>s+r</cell></row><row><cell>ResNet-50</cell><cell>68.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>Performance of three ResNet-50 models on OCT-11k, where ResNet-50 indicates the sourceonly model, followed by its counterparts trained by CDAN and KDDE(CDAN), respectively. Confusion matrices are shown in colored cells. KDDE(CDAN) is the best in terms of the overall performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Source domain (Zeiss)</cell><cell></cell><cell cols="3">Target domain (Topcon)</cell><cell></cell><cell cols="4">Expanded domain (Zeiss+Topcon)</cell></row><row><cell>Model</cell><cell cols="2">positive negative</cell><cell cols="4">Accuracy AUC positive negative</cell><cell cols="4">Accuracy AUC positive negative</cell><cell cols="2">Accuracy AUC</cell></row><row><cell>positive ResNet-50?n egative</cell><cell>491 90</cell><cell>42 277</cell><cell>0.8533</cell><cell>0.9326</cell><cell>462 9</cell><cell>334 95</cell><cell>0.6189</cell><cell>0.8593</cell><cell>953 99</cell><cell>376 372</cell><cell>0.7361</cell><cell>0.8534</cell></row><row><cell>positive CDAN?n egative</cell><cell>471 110</cell><cell>49 270</cell><cell>0.8233</cell><cell>0.8926</cell><cell>453 18</cell><cell>204 225</cell><cell>0.7533</cell><cell>0.8526</cell><cell>924 128</cell><cell>253 495</cell><cell>0.7883</cell><cell>0.8439</cell></row><row><cell>positive KDDE(CDAN)?n egative</cell><cell>492 89</cell><cell>45 274</cell><cell>0.8511</cell><cell>0.9200</cell><cell>437 34</cell><cell>148 281</cell><cell>0.7978</cell><cell>0.8985</cell><cell>929 123</cell><cell>193 555</cell><cell>0.8244</cell><cell>0.8914</cell></row><row><cell cols="2">4.6 Ablation Study</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">4.6.1 Alternative knowledge distillation loss?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>Performance of KDDE (CDAN) with different losses on Office-Home. The effect of knowledge distillation at the intermediate features space. Features that are discriminative of both domains shall allow an instance to be surrounded by instances of the same class.</figDesc><table><row><cell>Loss</cell><cell cols="3">Source domain Target domain Expanded domain</cell></row><row><cell>2</cell><cell>79.60</cell><cell>61.77</cell><cell>70.69</cell></row><row><cell>cross-entropy</cell><cell>80.78</cell><cell>63.19</cell><cell>71.99</cell></row><row><cell>KL divergence</cell><cell>81.44</cell><cell>63.90</cell><cell>72.67</cell></row><row><cell>4.6.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 .</head><label>10</label><figDesc>Performance of cross-domain image retrieval, using the 2,048-d features produced by the feature extractor of ResNet-50, DDC and KDDE (DDC), respectively. "Query S on T " means using each image in a source domain as a query example and performing retrieval in the target domain, while "Query T on S" indicates retrieval in the opposite direction. Performance metric: Precision at (P@N), = 5, 10.<ref type="bibr" target="#b40">41</ref>.09 40.24 38.07 77.67 74.13 65.19 63.26 DDC 43.43 36.20 43.86 40.75 72.83 68.23 64.95 61.86 KDDE(DDC) 49.86 42.98 47.29 45.08 78.23 74.61 68.10 66.<ref type="bibr" target="#b37">38</ref> Visualization of cross-domain image retrieval. Each row corresponds to the top-5 retrieved images for queries from (a) the Clipart domain and (b) the Art domain, respectively. Note that image retrieval is fully content-based. Labels below each image are for illustration only.while KDDE is better than CDAN+ on both source and multiple target domains. Consequently, KDDE obtains the overall best performance on the expanded domain.4.6.4 Tackling UDE by tuning the trade-off parameter ?We report in</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>C ? A</cell><cell></cell><cell></cell><cell>P ? R</cell></row><row><cell></cell><cell>Model</cell><cell cols="6">Query S on T Query T on S Query S on T Query T on S</cell></row><row><cell></cell><cell></cell><cell cols="6">P@5 P@10 P@5 P@10 P@5 P@10 P@5 P@10</cell></row><row><cell cols="3">ResNet-50 47.87 ResNet-50 Query Sneakers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sneakers</cell></row><row><cell cols="4">Sneakers Sneakers Sneakers Sneakers</cell><cell>Eraser</cell><cell>Sneakers</cell><cell>Sink</cell><cell>Helmet</cell><cell>Bucket</cell><cell>Candles</cell></row><row><cell>DDC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eraser</cell><cell>Sneakers</cell><cell>Knives</cell><cell cols="2">Sneakers Sneakers</cell><cell>Sneakers</cell><cell>Sink</cell><cell>Sneakers Sneakers Webcam</cell></row><row><cell>KDDE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(DDC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Sneakers Sneakers Sneakers Sneakers Sneakers</cell><cell cols="3">Sneakers Sneakers Sneakers Sneakers Sneakers</cell></row><row><cell>Fig. 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>CDAN+) 79.05?+0.14 57.67?+10.61 68.19?+10.64 66.94?+7.29 67.96?+7.17</figDesc><table><row><cell>Model</cell><cell>A</cell><cell>C</cell><cell>A?{C,P,R} P</cell><cell>R</cell><cell>A+C+P+R</cell></row><row><cell>ResNet-50</cell><cell>75.20</cell><cell>45.23</cell><cell>58.45</cell><cell>69.35</cell><cell>62.06</cell></row><row><cell>CDAN+</cell><cell cols="2">70.64?-4.56 50.23?+5.00</cell><cell>61.35?+2.90</cell><cell cols="2">67.17?-2.18 62.35?+0.29</cell></row><row><cell cols="3">KDDE(CDAN+) 72.59?-2.61 50.14?+4.91</cell><cell>63.49?+5.04</cell><cell cols="2">69.49?+0.14 63.93?+1.87</cell></row><row><cell>Model</cell><cell>C</cell><cell>A</cell><cell>C?{A,P,R} P</cell><cell>R</cell><cell>C+A+P+R</cell></row><row><cell>ResNet-50</cell><cell>78.91</cell><cell>47.06</cell><cell>57.55</cell><cell>59.65</cell><cell>60.79</cell></row><row><cell>CDAN+</cell><cell cols="2">77.09?-1.82 54.89?+7.83</cell><cell>66.53?+8.98</cell><cell cols="2">64.62?+4.97 65.78?+4.99</cell></row><row><cell>KDDE(Model</cell><cell>P</cell><cell>A</cell><cell>P?{A,C,R} C</cell><cell>R</cell><cell>P+A+C+R</cell></row><row><cell>ResNet-50</cell><cell>92.05</cell><cell>50.33</cell><cell>43.86</cell><cell>70.31</cell><cell>64.14</cell></row><row><cell>CDAN+</cell><cell cols="2">90.08?-1.97 54.24?+3.91</cell><cell>47.91?+4.05</cell><cell cols="2">70.90?+0.59 65.78?+1.64</cell></row><row><cell cols="3">KDDE(CDAN+) 91.33?-0.72 56.28?+5.95</cell><cell>50.50?+6.64</cell><cell cols="2">72.04?+1.73 67.54?+3.40</cell></row><row><cell>Model</cell><cell>R</cell><cell>A</cell><cell>R?{A,C,P} C</cell><cell>P</cell><cell>R+A+C+P</cell></row><row><cell>ResNet-50</cell><cell>84.11</cell><cell>63.62</cell><cell>48.23</cell><cell>76.27</cell><cell>68.06</cell></row><row><cell>CDAN+</cell><cell cols="2">82.01?-2.10 62.40?-1.22</cell><cell>57.14?+8.91</cell><cell cols="2">76.32?+0.05 69.47?+1.41</cell></row><row><cell cols="3">KDDE(CDAN+) 82.65?-1.46 63.30?-0.32</cell><cell>57.27?+9.04</cell><cell cols="2">77.97?+1.70 70.30?+2.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 .</head><label>12</label><figDesc>Performance of DDC w.r.t. the trade-off parameter , on three tasks (C?A, P?R and R?C) of Office-Home. Varied positions of light-blue cells, which indicate the best-performed on specific domains, show the difficulty in selecting a proper for tackling the UDE tasks. 47.06 62.99 92.05 70.31 81.18 84.11 48.23 66.17 70.11 KDDE(DDC, =10) 80.05 54.57 67.31 92.67 73.36 83.02 83.24 53.86 68.55 72.96</figDesc><table><row><cell>Model</cell><cell>C</cell><cell>C?A A</cell><cell>C+A</cell><cell>P</cell><cell>P?R R</cell><cell>P+R</cell><cell>R</cell><cell>R?C C</cell><cell>R+C</cell><cell>Averaged accuracy on expanded domains</cell></row><row><cell cols="10">ResNet-50 =0 =0.01 =0.1 78.91 DDC 78.68 51.55 65.12 92.05 71.54 81.80 83.01 52.73 67.87 79.41 52.28 65.85 92.40 72.54 82.47 83.56 53.82 68.69 79.86 52.37 66.12 92.58 72.13 82.36 82.97 53.77 68.37 =1 79.23 51.47 65.35 92.72 73.04 82.88 83.42 52.82 68.12</cell><cell>71.59 72.34 72.28 72.12</cell></row><row><cell>=10</cell><cell cols="9">80.23 50.90 65.57 92.49 71.95 82.22 82.97 52.23 67.60</cell><cell>71.80</cell></row><row><cell>=20</cell><cell cols="9">79.23 51.47 65.35 92.09 71.27 81.68 83.20 52.14 67.67</cell><cell>71.57</cell></row><row><cell>=100</cell><cell>7.96</cell><cell>2.53</cell><cell>5.25</cell><cell>13.50</cell><cell>5.64</cell><cell>9.57</cell><cell>6.33</cell><cell>3.41</cell><cell>4.87</cell><cell>6.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 .</head><label>13</label><figDesc>Performance of MCD and KDDE (MCD). Per model we run the experiment three times, reporting the average value and standard deviation of the resultant accuracy scores. 53?0.66 51.90?0.96 64.72?0.53 91.45?0.32 70.67?0.71 81.06?0.51 KDDE(MCD) 79.20?0.35 56.33?0.65 67.77?0.49 92.25?0.16 73.91?0.10 83.08?0.04 4.6.6 Reproducibility test. Due to the large-scale datasets used in our study, producing</figDesc><table><row><cell>Model</cell><cell>C</cell><cell>C?A A</cell><cell>C+A</cell><cell>P</cell><cell>P?R R</cell><cell>P+R</cell></row><row><cell>MCD</cell><cell>77.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 .</head><label>14</label><figDesc>Averaged performance of the individual models on Office-Home. Per model we repeat the training and evaluation procedures three times, reporting the averaged accuracy and standard deviation. 64?0.50 44.73?0.45 59.68?0.48 74.64?0.50 59.19?0.70 66.91?0.11 74.64?0.50 69.17?0.24 71.91?0.33 DDC 73.03?0.58 48.58?0.48 60.80?0.27 73.60?0.17 62.99?0.33 68.29?0.17 74.31?0.35 70.52?0.31 72.42?0.04 DANN 71.94?0.75 49.58?0.34 60.76?0.54 72.92?0.41 62.15?1.02 67.54?0.33 74.71?0.45 70.60?0.38 72.66?0.41 DAAN 73.68?0.33 49.00?0.08 61.34?0.18 74.28?0.29 63.93?0.33 69.10?0.04 74.74?0.48 71.25?0.30 73.00?0.38 CDAN 70.06?1.10 47.15?1.37 58.61?1.07 70.80?0.91 64.64?2.10 67.72?1.28 72.57?0.69 69.63?0.87 71.10?0.59 KDDE(DDC) 73.38?0.26 49.50?0.62 61.44?0.42 74.33?0.66 64.57?0.79 69.45?0.45 75.58?0.58 71.84?0.56 73.71?0.57 KDDE(CDAN) 68.84?1.08 47.98?0.84 58.41?0.94 71.53?0.80 66.20?1.93 68.87?1.32 73.68?0.29 71.19?1.14 72.43?0.71 98?0.09 48.20?1.28 63.59?0.65 78.98?0.09 58.31?0.69 68.65?0.37 78.98?0.09 59.85?0.35 69.42?0.17 DDC 79.74?0.44 51.93?0.91 65.84?0.24 80.03?0.38 61.75?0.74 70.89?0.18 79.53?0.66 64.01?0.51 71.77?0.56 DANN 78.06?0.23 53.10?0.91 65.58?0.52 78.86?0.69 60.41?1.24 69.64?0.82 78.89?0.03 62.93?0.33 70.91?0.15 DAAN 79.08?0.48 52.80?0.74 65.94?0.43 79.51?0.45 62.21?0.34 70.87?0.19 79.74?0.07 64.51?0.41 72.13?0.19 CDAN 78.09?0.30 53.59?1.28 65.84?0.59 78.27?0.64 65.34?0.69 71.81?0.33 79.12?0.14 64.66?0.64 71.89?0.33 KDDE(DDC) 80.09?0.48 55.57?0.89 67.83?0.47 80.43?0.33 64.06?0.79 72.25?0.53 80.47?0.42 66.64?1.34 73.55?0.85 KDDE(CDAN) 78.92?0.77 56.09?1.72 67.51?0.85 80.03?0.23 67.55?1.00 73.79?0.39 80.48?0.29 66.12?0.66 73.30?0.41 05?0.23 52.20?1.63 72.13?0.81 92.05?0.23 42.94?0.89 67.49?0.50 92.05?0.23 70.11?0.34 81.08?0.09 DDC 92.20?0.14 52.99?0.62 72.59?0.29 91.69?0.12 45.39?0.42 68.54?0.25 92.30?0.20 72.42?0.44 82.36?0.12 DANN 90.32?0.36 51.55?1.56 70.93?0.94 90.28?0.49 47.52?0.43 68.90?0.36 91.76?0.12 71.56?0.90 81.66?0.39 DAAN 92.06?0.50 54.21?0.87 73.14?0.37 91.63?0.24 45.24?1.05 68.44?0.61 92.38?0.21 72.37?0.22 82.38?0.18 CDAN 90.44?0.59 52.37?1.14 71.40?0.86 89.11?0.49 48.33?1.27 68.72?0.40 90.97?0.20 73.98?0.41 82.48?0.18 KDDE(DDC) 92.07?0.28 54.16?0.50 73.12?0.20 91.79?0.18 47.14?0.79 69.47?0.47 92.84?0.25 74.30?0.83 83.57?0.52 KDDE(CDAN) 90.81?0.67 53.40?1.96 72.10?1.31 89.96?0.45 49.73?0.83 69.84?0.33 92.14?0.31 75.23?0.42 83.68?0.33 05?0.07 63.73?0.49 73.89?0.22 84.05?0.07 49.47?1.08 66.76?0.51 84.05?0.07 76.14?0.74 80.09?0.36 DDC 84.49?0.26 64.52?0.50 74.50?0.30 83.21?0.28 53.23?0.87 68.22?0.54 84.02?0.33 77.79?0.42 80.91?0.24 DANN 83.76?0.41 65.33?0.33 74.55?0.13 82.00?0.69 55.07?0.96 68.54?0.78 82.83?0.43 78.02?0.08 80.43?0.23 DAAN 84.59?0.30 64.22?0.62 74.41?0.40 83.07?0.19 52.59?0.65 67.83?0.25 83.80?0.25 77.79?0.42 80.80?0.30 CDAN 82.29?0.90 64.19?0.51 73.24?0.70 80.12?0.38 54.82?1.38 67.47?0.68 82.45?0.22 80.12?0.37 81.29?0.25 KDDE(DDC) 84.65?0.30 65.22?0.66 74.94?0.47 83.33?0.08 54.03?0.33 68.68?0.19 83.91?0.16 79.27?0.88 81.59?0.52 KDDE(CDAN) 82.94?0.31 64.55?0.90 73.74?0.59 80.25?1.33 56.73?1.02 68.49?1.04 82.79?0.56 80.79?0.73 81.79?0.58</figDesc><table><row><cell>Model</cell><cell></cell><cell>A</cell><cell>A?C C</cell><cell>A+C</cell><cell>A</cell><cell>A?P P</cell><cell>A+P</cell><cell>A</cell><cell>A?R R</cell><cell>A+R</cell></row><row><cell cols="2">ResNet-50 74.Model</cell><cell>C</cell><cell>C?A A</cell><cell>C+A</cell><cell>C</cell><cell>C?P P</cell><cell>C+P</cell><cell>C</cell><cell>C?R R</cell><cell>C+R</cell></row><row><cell cols="2">ResNet-50 78.Model</cell><cell>P</cell><cell>P?A A</cell><cell>P+A</cell><cell>P</cell><cell>P?C C</cell><cell>P+C</cell><cell>P</cell><cell>P?R R</cell><cell>P+R</cell></row><row><cell cols="2">ResNet-50 92.Model</cell><cell>R</cell><cell>R?A A</cell><cell>R+A</cell><cell>R</cell><cell>R?C C</cell><cell>R+C</cell><cell>R</cell><cell>R?P P</cell><cell>R+P</cell></row><row><cell>ResNet-50</cell><cell>84.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Multimedia Comput. Commun. Appl., Vol. 1, No. 1, Article . Publication date: April 2021.Unsupervised Domain Expansion for Visual Categorization</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://ai.bu.edu/visda-2019</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation of DNN acoustic models using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Asami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikazu</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Masataki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Joseph R Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenad</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Askham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain Transfer SVM for video concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal Mackiewicz</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>March</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarially robust distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-domain learning methods for high-level visual concept classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zavesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contrastive Adaptation Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Goldbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Carolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Valentim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured Knowledge Distillation for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation via teacher-student learning for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial teacher-student learning for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<meeting><address><addrLine>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moment matching for multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">FitNets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-Supervised Domain Adaptation via Minimax Entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distilling Knowledge From a Deep Pose Regressor Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhamad</forename><surname>Risqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P B De</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Gusmao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Almalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge distillation for recurrent neural network language modeling with trust regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Yuh</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<title level="m">Contrastive representation distillation. ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation in Semantic Segmentation: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Maracani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep Domain Confusion: Maximizing for Domain Invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>ArXiv abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Hashing Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-Stream CNN with Loose Pair Training for Multi-modal AMD Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transferable Normalization: Towards Improving Transferability of Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar Jinsong Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transfer Learning with Dynamic Adversarial Adaptation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Training Efficient Saliency Prediction Models with Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Examining CNN Representations with Respect to Dataset Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
