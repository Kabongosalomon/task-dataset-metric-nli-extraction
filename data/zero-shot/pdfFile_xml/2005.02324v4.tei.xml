<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural CRF Model for Sentence Alignment in Text Simplification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounica</forename><surname>Maddela</surname></persName>
							<email>maddela.4@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural CRF Model for Sentence Alignment in Text Simplification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1. We apply our CRF aligner to construct two new text simplification datasets, NEWSELA-AUTO and WIKI-AUTO, which are much larger and of better quality compared to the existing datasets. A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text simplification aims to rewrite complex text into simpler language while retaining its original meaning <ref type="bibr">(Saggion, 2017)</ref>. Text simplification can provide reading assistance for children <ref type="bibr">(Kajiwara et al., 2013)</ref>, non-native speakers <ref type="bibr">(Petersen and Ostendorf, 2007;</ref><ref type="bibr">Pellow and Eskenazi, 2014)</ref>, nonexpert readers <ref type="bibr">(Elhadad and Sutaria, 2007;</ref><ref type="bibr">Siddharthan and Katsos, 2010)</ref>, and people with language disorders <ref type="bibr">(Rello et al., 2013)</ref>. As a preprocessing step, text simplification can also improve the performance of many natural language processing (NLP) tasks, such as parsing <ref type="bibr">(Chandrasekar et al., 1996)</ref>, semantic role labelling <ref type="bibr">(Vickrey and Koller, 2008)</ref>, information extraction <ref type="bibr">(Miwa et al., 2010)</ref> , summarization <ref type="bibr">(Vanderwende et al., 2007;</ref><ref type="bibr" target="#b3">Xu and Grishman, 2009)</ref>, and machine translation <ref type="bibr">(Chen et al., 2012;</ref><ref type="bibr">?tajner and Popovic, 2016)</ref>.</p><p>Automatic text simplification is primarily addressed by sequence-to-sequence (seq2seq) models whose success largely depends on the quality and quantity of the training corpus, which consists of pairs of complex-simple sentences. Two widely used corpora, NEWSELA <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> and WIK-ILARGE <ref type="bibr" target="#b6">(Zhang and Lapata, 2017)</ref>, were created by automatically aligning sentences between comparable articles. However, due to the lack of reliable annotated data, 2 sentence pairs are often aligned using surface-level similarity metrics, such as Jaccard coefficient <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> or cosine distance of TF-IDF vectors <ref type="bibr">(Paetzold et al., 2017)</ref>, which fails to capture paraphrases and the context of surrounding sentences. A common drawback of text simplification models trained on such datasets is that they behave conservatively, performing mostly deletion, and rarely paraphrase <ref type="bibr">(Alva-Manchego et al., 2017)</ref>. Moreover, WIKILARGE is the concatenation of three early datasets <ref type="bibr" target="#b8">(Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011;</ref><ref type="bibr">Coster and Kauchak, 2011)</ref> that are extracted from Wikipedia dumps and are known to contain many errors <ref type="bibr" target="#b2">(Xu et al., 2015)</ref>.</p><p>To address these problems, we create the first high-quality manually annotated sentence-aligned datasets: NEWSELA-MANUAL with 50 article sets, and WIKI-MANUAL with 500 article pairs. We design a novel neural CRF alignment model, which utilizes fine-tuned BERT to measure semantic similarity and leverages the similar order of content be- <ref type="figure">Figure 1</ref>: An example of sentence alignment between an original news article (right) and its simplified version (left) in Newsela. The label a i for each simple sentence s i is the index of complex sentence c ai it aligns to. tween parallel documents, combined with an effective paragraph alignment algorithm. Experiments show that our proposed method outperforms all the previous monolingual sentence alignment <ref type="bibr">approaches (?tajner et al., 2018;</ref><ref type="bibr">Paetzold et al., 2017;</ref><ref type="bibr" target="#b2">Xu et al., 2015)</ref> by more than 5 points in F1.</p><p>By applying our alignment model to all the 1,882 article sets in Newsela and 138,095 article pairs in Wikipedia dump, we then construct two new simplification datasets, NEWSELA-AUTO (666,645 sentence pairs) and WIKI-AUTO (488,332 sentence pairs). Our new datasets with improved quantity and quality facilitate the training of complex seq2seq models. A BERT-initialized Transformer model trained on our datasets outperforms the stateof-the-art by 3.4% in terms of SARI, the main automatic metric for text simplification. Our simplification model produces 25% more rephrasing than those trained on the existing datasets. Our contributions include:</p><p>1. Two manually annotated datasets that enable the first systematic study for training and evaluating monolingual sentence alignment; 2. A neural CRF sentence alinger and a paragraph alignment algorithm that employ finetuned BERT to capture semantic similarity and take advantage of the sequential nature of parallel documents; 3. Two automatically constructed text simplification datasets which are of higher quality and 4.7 and 1.6 times larger than the existing datasets in their respective domains; 4. A BERT-initialized Transformer model for automatic text simplification, trained on our datasets, which establishes a new state-of-theart in both automatic and human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural CRF Sentence Aligner</head><p>We propose a neural CRF sentence alignment model, which leverages the similar order of content presented in parallel documents and captures editing operations across multiple sentences, such as splitting and elaboration (see <ref type="figure">Figure 1</ref> for an example). To further improve the accuracy, we first align paragraphs based on semantic similarity and vicinity information, and then extract sentence pairs from these aligned paragraphs. In this section, we describe the task setup and our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Given a simple article (or paragraph) S of m sentences and a complex article (or paragraph) C of n sentences, for each sentence s i (i ? [1, m]) in the simple article, we aim to find its corresponding sentence c a i (a i ? [0, n]) in the complex article. We use a i to denote the index of the aligned sentence, where a i = 0 indicates that sentence s i is not aligned to any sentence in the complex article. The full alignment a between article (or paragraph) pair S and C can then be represented by a sequence of alignment labels a = (a 1 , a 2 , . . . , a m ). <ref type="figure">Figure  1</ref> shows an example of alignment labels. One specific aspect of our CRF model is that it uses a varied number of labels for each article (or paragraph) pair rather than a fixed set of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural CRF Sentence Alignment Model</head><p>We learn P (a|S, C), the conditional probability of alignment a given an article pair (S, C), using linear-chain conditional random field:</p><formula xml:id="formula_0">P (a|S, C) = exp(?(a, S, C)) a?A exp(?(a, S, C)) = exp( |S| i=1 ?(a i , a i?1 , S, C)) a?A exp( |S| i=1 ?(a i , a i?1 , S, C)))<label>(1)</label></formula><p>where |S| = m denotes the number of sentences in article S. The score |S| i=1 ?(a i , a i?1 , S, C) sums over the sequence of alignment labels a = (a 1 , a 2 , . . . , a m ) between the simple article S and the complex article C, and could be decomposed into two factors as follows:</p><formula xml:id="formula_1">?(a i , a i?1 , S, C) = sim(s i , c a i ) + T (a i , a i?1 )<label>(2)</label></formula><p>where sim(s i , c a i ) is the semantic similarity score between the two sentences, and T (a i , a i?1 ) is a pairwise score for alignment label transition that a i follows a i?1 .</p><p>Semantic Similarity A fundamental problem in sentence alignment is to measure the semantic similarity between two sentences s i and c j . Prior work used lexical similarity measures, such as Jaccard similarity <ref type="bibr" target="#b2">(Xu et al., 2015)</ref>, <ref type="bibr">TF-IDF (Paetzold et al., 2017)</ref>, and continuous n-gram features <ref type="bibr">(?tajner et al., 2018)</ref>. In this paper, we fine-tune BERT (Devlin et al., 2019) on our manually labeled dataset (details in ?3) to capture semantic similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment Label Transition</head><p>In parallel documents, the contents of the articles are often presented in a similar order. The complex sentence c a i that is aligned to s i , is often related to the complex sentences c a i?1 and c a i+1 , which are aligned to s i?1 and s i+1 , respectively. To incorporate this intuition, we propose a scoring function to model the transition between alignment labels using the following features:</p><formula xml:id="formula_2">g 1 = |a i ? a i?1 | g 2 = 1(a i = 0, a i?1 = 0) g 3 = 1(a i = 0, a i?1 = 0) g 4 = 1(a i = 0, a i?1 = 0)<label>(3)</label></formula><p>where g 1 is the absolute distance between a i and a i?1 , g 2 and g 3 denote if the current or prior sentence is not aligned to any sentence, and g 4 indicates whether both s i and s i?1 are not aligned to any sentences. The score is computed as follows:</p><formula xml:id="formula_3">T (a i , a i?1 ) = FFNN([g 1 , g 2 , g 3 , g 4 ]) (4)</formula><p>where <ref type="bibr">[, ]</ref> represents concatenation operation and FFNN is a 2-layer feedforward neural network. We provide more implementation details of the model in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference and Learning</head><p>During inference, we find the optimal alignment?: a = argmax a P (a|S, C)</p><p>using Viterbi algorithm in O(mn 2 ) time. During training, we maximize the conditional probability of the gold alignment label a * :</p><formula xml:id="formula_5">log P (a * |S, C) =?(a * , S, C)? log a?A exp(?(a, S, C)) (6)</formula><p>The second term sums the scores of all possible alignments and can be computed using forward algorithm in O(mn 2 ) time as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Paragraph Alignment</head><p>Both accuracy and computing efficiency can be improved if we align paragraphs before aligning sentences. In fact, our empirical analysis revealed that sentence-level alignments mostly reside within the corresponding aligned paragraphs (details in ?4.4 and <ref type="table">Table 3</ref>). Moreover, aligning paragraphs first provides more training instances and reduces the label space for our neural CRF model.</p><p>We propose Algorithm 1 and 2 for paragraph alignment. Given a simple article S with k paragraphs S = (S 1 , S 2 , . . . , S k ) and a complex article C with l paragraphs C = (C 1 , C 2 , . . . , C l ), we first apply Algorithm 1 to calculate the semantic similarity matrix simP between paragraphs by averaging or maximizing over the sentence-level similarities ( ?2.2). Then, we use Algorithm 2 to generate the paragraph alignment matrix alignP . We align paragraph pairs if they satisfy one of the two conditions: (a) having high semantic similarity and appearing in similar positions in the article pair (e.g., both at the beginning), or (b) two continuous paragraphs in the complex article having relatively high semantic similarity with one paragraph in the simple side, (e.g., paragraph splitting or fusion). The difference of relative position in documents Algorithm 1: Pairwise Paragraph Similarity</p><formula xml:id="formula_6">Initialize: simP ? R 2?k?l to 0 2?k?l for i ? 1 to k do for j ? 1 to l do simP [1, i, j] = avg sp?S i max cq ?C j simSent(sp, cq) simP [2, i, j] = max sp?S i ,cq ?C j simSent(sp, cq) end end return simP</formula><p>Algorithm 2: Paragraph Alignment Algorithm</p><formula xml:id="formula_7">Input :simP ? R 2?k?l Initialize: alignP ? I k?l to 0 k?l for i ? 1 to k do jmax = argmax j simP [1, i, j] if simP [1, i, jmax] &gt; ?1 and d(i, jmax) &lt; ?2 then alignP [i, jmax] = 1 end for j ? 1 to l do if simP [2, i, j] &gt; ?3 then alignP [i, j] = 1 end if j &gt; 1 &amp; simP [2, i, j] &gt; ?4 &amp; simP [2, i, j ? 1] &gt; ?4 &amp; d(i, j) &lt; ?5 &amp; d(i, j ? 1) &lt; ?5 then alignP [i, j] = 1 alignP [i, j ? 1] = 1 end end end return alignP</formula><p>is defined as d(i, j) = | i k ? j l |, and the thresholds ? 1 -? 5 in Algorithm 2 are selected using the dev set. Finally, we merge the neighbouring paragraphs which are aligned to the same paragraph in the simple article before feeding them into our neural CRF aligner. We provide more details in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constructing Alignment Datasets</head><p>To address the lack of reliable sentence alignment for Newsela <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> and Wikipedia <ref type="bibr" target="#b8">(Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011)</ref>, we designed an efficient annotation methodology to first manually align sentences between a few complex and simple article pairs. Then, we automatically aligned the rest using our alignment model trained on the human annotated data. We created two sentence-aligned parallel corpora (details in ?5), which are the largest to date for text simplification.  ? This number includes all complex-simple sentence pairs (including aligned, partially-aligned, or notaligned) across all 10 combinations of 5 readability levels (level 0-4), of which 20,343 sentence pairs between adjacent readability levels were manually annotated and the rest of labels were derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Aligned Newsela Corpus</head><p>Newsela corpus <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> consists of 1,932 English news articles where each article (level 0) is re-written by professional editors into four simpler versions at different readability levels (level 1-4). We annotate sentence alignments for article pairs at adjacent readability levels (e.g., 0-1, 1-2) as the alignments between non-adjacent levels (e.g., 0-2) can be then derived automatically. To ensure efficiency and quality, we designed the following three-step annotation procedure:</p><p>1. Align paragraphs using CATS toolkit <ref type="bibr">(?tajner et al., 2018)</ref>, and then correct the automatic paragraph alignment errors by two in-house annotators. 3 Performing paragraph alignment as the first step significantly reduces the number of sentence pairs to be annotated from every possible sentence pair to the ones within the aligned paragraphs. We design an efficient visualization toolkit for this step, for which a screenshot can be found in Appendix E.2. 2. For each sentence pair within the aligned paragraphs, we ask five annotators on the <ref type="figure">Figure  Eight 4</ref> crowdsourcing platform to classify into one of the three categories: aligned, partiallyaligned, or not-aligned. We provide the annotation instructions and interface in Appendix E.1. We require annotators to spend at least ten seconds per question and embed one test question in every five questions. Any worker whose accuracy drops below 85% on test questions is removed. The inter-annotator agreement is 0.807 measured by Cohen's kappa (Artstein and Poesio, 2008). 3. We have four in-house annotators (not authors) verify the crowdsourced labels.</p><p>We manually aligned 50 article groups to create the NEWSELA-MANUAL dataset with a 35/5/10 split for train/dev/test, respectively. We trained our aligner on this dataset (details in ?4), then automatically aligned sentences in the remaining 1,882 article groups in Newsela <ref type="table" target="#tab_1">(Table 1)</ref> to create a new sentence-aligned dataset, NEWSELA-AUTO, which consists of 666k sentence pairs predicted as aligned and partially-aligned. NEWSELA-AUTO is considerably larger than the previous NEWSELA <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> dataset of 141,582 pairs, and contains 44% more interesting rewrites (i.e., rephrasing and splitting cases) as shown in <ref type="figure">Figure</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Aligned Wikipedia Corpus</head><p>We also create a new version of Wikipedia corpus by aligning sentences between English Wikipedia and Simple English Wikipedia. Previous work <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> has shown that Wikipedia is much noisier than the Newsela corpus. We provide this dataset in addition to facilitate future research.</p><p>We first extract article pairs from English and Simple English Wikipedia by leveraging Wikidata, a well-maintained database that indexes named entities (and events etc.) and their Wikipedia pages in different languages. We found this method to be more reliable than using page titles (Coster and Kauchak, 2011) or cross-lingual links <ref type="bibr" target="#b8">(Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011)</ref>, as titles can be ambiguous and cross-lingual links may direct to a disambiguation or mismatched page (more details in Appendix B). In total, we extracted 138,095 article pairs from the 2019/09 Wikipedia dump, which is two times larger than the previous datasets <ref type="bibr">(Coster and Kauchak, 2011;</ref><ref type="bibr" target="#b8">Zhu et al., 2010)</ref> of only 60?65k article pairs, using an improved version of the WikiExtractor library. <ref type="bibr">5</ref> Then, we crowdsourced the sentence alignment annotations for 500 randomly sampled document pairs (10,123 sentence pairs total). As document length in English and Simple English Wikipedia articles vary greatly, 6 we designed the following annotation strategy that is slightly different from Newsela. For each sentence in the simple article, we select the sentences with the highest similarity scores from the complex article for manual annotation, based on four similarity measures: lexical similarity from CATS <ref type="bibr">(?tajner et al., 2018)</ref>, cosine similarity using TF-IDF <ref type="bibr">(Paetzold et al., 2017)</ref>, cosine similarity between BERT sentence embeddings, and alignment probability by a BERT model fine-tuned on our NEWSELA-MANUAL data ( ?3.1). As these four metrics may rank the same sentence at the top, on an average, we collected 2.13 complex sentences for every simple sentence and annotated the alignment label for each sentence pair. Our pilot study showed that this method captured 93.6% of the aligned sentence pairs. We named this manually labeled dataset WIKI-MANUAL with a train/dev/test split of 350/50/100 article pairs.</p><p>Finally, we trained our alignment model on this   <ref type="table">Table 3</ref>: Ablation study of our aligner on dev set. annotated dataset to automatically align sentences for all the 138,095 document pairs (details in Appendix B). In total, we yielded 604k non-identical aligned and partially-aligned sentence pairs to create the WIKI-AUTO dataset. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates that WIKI-AUTO contains 75% less defective sentence pairs than the old WIKILARGE <ref type="bibr" target="#b6">(Zhang and Lapata, 2017)</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation of Sentence Alignment</head><p>In this section, we present experiments that compare our neural sentence alignment against the stateof-the-art approaches on NEWSELA-MANUAL ( ?3.1) and WIKI-MANUAL ( ?3.2) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Existing Methods</head><p>We compare our neural CRF aligner with the following baselines and state-of-the-art approaches:  <ref type="bibr" target="#b2">(Xu et al., 2015)</ref>, which uses Jaccard coefficient for sentence similarity and a greedy approach for alignment. 3. MASSAlign (Paetzold et al., 2017), which combines TF-IDF cosine similarity with a vicinity-driven dynamic programming algorithm for alignment. 4. CATS toolkit <ref type="bibr">(?tajner et al., 2018)</ref>, which uses character n-gram features for sentence similarity and a greedy alignment algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We report Precision, Recall and F1 on two binary classification tasks: aligned + partially-aligned vs. not-aligned (Task 1) and aligned vs. partiallyaligned + not-aligned (Task 2). It should be noted that we excluded identical sentence pairs in the evaluation as they are trivial to classify. <ref type="table" target="#tab_3">Table 2</ref> shows the results on NEWSELA-MANUAL test set. For similarity-based methods, we choose a threshold based on the maximum F1 on the dev set. Our neural CRF aligner outperforms the stateof-the-art approaches by more than 5 points in F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In particular, our method performs better than the previous work on partial alignments, which contain many interesting simplification operations, such as sentence splitting and paraphrasing with deletion. Similarly, our CRF alignment model achieves 85.1 F1 for Task 1 (aligned + partially-aligned vs. not-aligned) on the WIKI-MANUAL test set. It outperforms one of the previous SOTA approaches CATS <ref type="bibr">(?tajner et al., 2018)</ref> by 15.1 points in F1. We provide more details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We analyze the design choices crucial for the good performance of our alignment model, namely CRF component, the paragraph alignment and the BERTbased semantic similarity measure. <ref type="table">Table 3</ref> shows the importance of each component with a series of ablation experiments on the dev set.  <ref type="table">Table 4</ref>: Statistics of our newly constructed parallel corpora for sentence simplification compared to the old datasets <ref type="bibr" target="#b2">(Xu et al., 2015;</ref><ref type="bibr" target="#b6">Zhang and Lapata, 2017)</ref>.</p><p>CRF Model Our aligner achieves 93.2 F1 and 88.1 F1 on Task 1 and 2, respectively, which is around 3 points higher than its variant without the CRF component (BERT f inetune + ParaAlign). Modeling alignment label transitions and sequential predictions helps our neural CRF aligner to handle sentence splitting cases better, especially when sentences undergo dramatic rewriting.</p><p>Paragraph Alignment Adding paragraph alignment (BERT f inetune + ParaAlign) improves the precision on Task 1 from 93.3 to 98.4 with a negligible decrease in recall when compared to not aligning paragraphs (BERT f inetune ). Moreover, paragraph alignments generated by our algorithm (Our Aligner) perform close to the gold alignments (Our Aligner + gold ParaAlign) with only 0.9 and 0.3 difference in F1 on Task 1 and 2, respectively.</p><p>Semantic Similarity BERT f inetune performs better than other neural models, including Infersent <ref type="bibr">(Conneau et al., 2017)</ref>, <ref type="bibr">ESIM (Chen et al., 2017)</ref>, BERTScore <ref type="bibr" target="#b5">(Zhang et al., 2020)</ref> and pretrained BERT embedding <ref type="bibr">(Devlin et al., 2019)</ref>. For BERTScore, we use idf weighting, and treat simple sentence as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Automatic Sentence Simplification</head><p>In this section, we compare different automatic text simplification models trained on our new parallel corpora, NEWSELA-AUTO and WIKI-AUTO, with their counterparts trained on the existing datasets. We establish a new state-of-the-art for sentence simplification by training a Transformer model with initialization from pre-trained BERT checkpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with existing datasets</head><p>Existing datasets of complex-simple sentences, NEWSELA <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> and WIKILARGE <ref type="bibr" target="#b6">(Zhang and Lapata, 2017)</ref>, were aligned using lexical similarity metrics. NEWSELA dataset <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> was aligned using JaccardAlign ( ?4.1). WIK-ILARGE is a concatenation of three early datasets <ref type="bibr" target="#b8">(Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011;</ref><ref type="bibr">Coster and Kauchak, 2011)</ref> where sentences in Simple/Normal English Wikipedia and editing history were aligned by TF-IDF cosine similarity. For our new NEWSELA-AUTO, we partitioned the article sets such that there is no overlap between the new train set and the old test set, and vice-versa. Following <ref type="bibr" target="#b6">Zhang and Lapata (2017)</ref>, we also excluded sentence pairs corresponding to the levels 0-1, 1-2 and 2-3. Similar to <ref type="bibr">(?tajner et al., 2015)</ref>, for our WIKI-AUTO dataset, we eliminated sentence pairs with high (&gt;0.9) or low (&lt;0.1) lexical overlap based on GLEU scores <ref type="bibr">(Wu et al., 2016)</ref>. We observed that sentence pairs with low GLEU are often inaccurate paraphrases with only shared named entities and the pairs with high GLEU are dominated by sentences merely copied without simplification. We used the benchmark TURK corpus <ref type="bibr" target="#b4">(Xu et al., 2016)</ref> for evaluation on Wikipedia, which consists of 8 human-written references for sentences in the validation and test sets. We discarded sentences in TURK corpus from WIKI-AUTO. <ref type="table">Table 4</ref> shows the statistics of the existing and our new datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines and Simplification Models</head><p>We compare the following seq2seq models trained using our new datasets versus the existing datasets:   <ref type="bibr" target="#b2">(Xu et al., 2015)</ref>. We report SARI, the main automatic metric for simplification, precision for deletion and F1 scores for adding and keeping operations. Add scores are low partially because we are using one reference. Bold typeface and underline denote the best and the second best performances respectively. For Flesch-Kincaid (FK) grade level and average sentence length (Len), we consider the values closest to reference as the best.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>In this section, we evaluate different simplification models trained on our new datasets versus on the old existing datasets using both automatic and human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Automatic Evaluation</head><p>We report SARI <ref type="bibr" target="#b4">(Xu et al., 2016)</ref>, Flesch-Kincaid (FK) grade level readability (?), and average sentence length (Len). While SARI compares the generated sentence to a set of reference sentences in terms of correctly inserted, kept and deleted n-grams (n ? {1, 2, 3, 4}), FK measures the readability of the generated sentence. We also report the three rewrite operation scores used in SARI: the precision of delete (del), the F1-scores of add (add), and keep (keep) operations. <ref type="table" target="#tab_7">Tables 5 and 8</ref> show the results on Newsela and Wikipedia datasets respectively. Systems trained on our datasets outperform their equivalents trained on the existing datasets according to SARI. The difference is notable for Transformer bert with a 6.4% and 3.7% increase in SARI on NEWSELA-AUTO test set and TURK corpus, respectively. Larger size and improved quality of our datasets enable the training of complex Transformer models. In fact, Transformer bert trained on our new datasets outperforms the existing state-of-the-art systems for automatic text simplification. Although improvement in SARI is modest for LSTM-based models (LSTM and EditNTS), the increase in F1 scores for addition and deletion operations indicate that the models trained on our datasets make more meaningful changes to the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Human Evaluation</head><p>We also performed human evaluation by asking five Amazon Mechanical Turk workers to rate fluency, adequacy and simplicity (detailed instructions in Appendix D.2) of 100 random sentences generated by different simplification models trained on NEWSELA-AUTO and the existing dataset. Each  worker evaluated these aspects on a 5-point Likert scale. We averaged the ratings from five workers. <ref type="table" target="#tab_10">Table 7</ref> demonstrates that Transformer bert trained on NEWSELA-AUTO greatly outperforms the one trained on the old dataset. Even with shorter sentence outputs, our Transformer bert retained similar adequacy as the LSTM-based models. Our Transformer bert model also achieves better fluency, adequacy, and overall ratings compared to the SOTA systems <ref type="table" target="#tab_9">(Table 6</ref>). We provide examples of system outputs in Appendix D.3. Our manual inspection <ref type="figure" target="#fig_2">(Figure 3</ref>) also shows that Transfomer bert trained on NEWSELA-AUTO performs 25% more paraphrasing and deletions than its variant trained on the previous NEWSELA <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Text simplification is considered as a text-totext generation task where the system learns how to simplify from complex-simple sentence pairs. There is a long line of research using methods based on hand-crafted rules <ref type="bibr">(Siddharthan, 2006;</ref><ref type="bibr">Niklaus et al., 2019)</ref>, statistical machine translation <ref type="bibr">(Narayan and Gardent, 2014;</ref><ref type="bibr" target="#b4">Xu et al., 2016;</ref><ref type="bibr" target="#b1">Wubben et al., 2012)</ref>, or neural seq2seq models <ref type="bibr" target="#b6">(Zhang and Lapata, 2017;</ref><ref type="bibr" target="#b7">Zhao et al., 2018;</ref><ref type="bibr">Nisioi et al., 2017)</ref>. As the existing datasets were built using lexical similarity metrics, they frequently omit paraphrases and sentence splits. While training on such datasets creates conservative systems that rarely paraphrase, evaluation on these datasets exhibits an unfair preference for deletion-based simplification over paraphrasing.</p><p>Sentence alignment has been widely used to extract complex-simple sentence pairs from parallel articles for training text simplification systems. Previous work used surface-level similarity metrics, such as TF-IDF cosine similarity <ref type="bibr" target="#b8">(Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011;</ref><ref type="bibr">Coster and Kauchak, 2011;</ref><ref type="bibr">Paetzold et al., 2017)</ref>, Jaccard-similarity <ref type="bibr" target="#b2">(Xu et al., 2015)</ref>, and other lexical features <ref type="bibr">(Hwang et al., 2015;</ref><ref type="bibr">?tajner et al., 2018)</ref>. Then, a greedy <ref type="bibr">(?tajner et al., 2018)</ref> or dynamic programming <ref type="bibr">(Barzilay and Elhadad, 2003;</ref><ref type="bibr">Paetzold et al., 2017)</ref> algorithm was used to search for the optimal alignment. Another related line of research <ref type="bibr">(Smith et al., 2010;</ref><ref type="bibr">Tufis , et al., 2013;</ref><ref type="bibr">Tsai and Roth, 2016;</ref><ref type="bibr">Gottschalk and Demidova, 2017;</ref><ref type="bibr">Aghaebrahimian, 2018;</ref><ref type="bibr">Thompson and Koehn, 2019)</ref> aligns parallel sentences in bilingual corpora for machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a novel neural CRF model for sentence alignment, which substantially outperformed the existing approaches. We created two high-quality manually annotated datasets (NEWSELA-MANUAL and WIKI-MANUAL) for training and evaluation. Using the neural CRF sentence aligner, we constructed two largest sentencealigned datasets to date (NEWSELA-AUTO and WIKI-AUTO) for text simplification. We showed that a BERT-initalized Transformer trained on our new datasets establishes new state-of-the-art performance for automatic sentence simplification. A Neural CRF Alignment Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation Details</head><p>We used PyTorch 9 to implement our neural CRF alignment model. For the sentence encoder, we used Huggingface implementation <ref type="bibr">(Wolf et al., 2019)</ref> of BERT base 10 architecture with 12 layers of Transformers. When fine-tuning the BERT model, we use the representation of [CLS] token for classification. We use cross entropy loss and update the weights in all layers. <ref type="table" target="#tab_14">Table 9</ref> summarizes the hyperparameters of our model. <ref type="table" target="#tab_1">Table 10</ref> provides the thresholds for our paragraph alignment Algorithm 2, which were chosen based on NEWSELA-MANUAL dev data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Value</head><p>Parameter Value hidden units 768 # of layers 12 learning rate 0.00002 # of heads 12 max sequence length 128 batch size 8  For Wikipedia data, we tailored our paragraph alignment algorithm (Algorithm 3 and 4). <ref type="table" target="#tab_1">Table  11</ref> provides the thresholds for Algorithm 4, which were chosen based on WIKI-MANUAL dev data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sentence Aligned Wikipedia Corpus</head><p>We present more details about our pre-processing steps for creating the WIKI-MANUAL and WIKI-AUTO corpora here. In Wikipedia, Simple English</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Sentence Alignment on Wikipedia</head><p>In this section, we compare different approaches for sentence alignment on the WIKI-MANUAL dataset. <ref type="table" target="#tab_1">Tables 12 and 13</ref> report the performance for Task 1 (aligned + partially-aligned vs. not-aligned) on dev and test set. To generate prediction for MAS-SAlign, CATS and two BERT f inetune methods, we first utilize the method in ?3.2 to select candidate sentence pairs, as we found this step helps to improve their accuracy. Then we apply the similarity metric from each model to calculate the similarity of each candidate sentence pair. We tune a threshold for max f1 on the dev set and apply it to the test set. Candidate sentence pairs with a similarity larger than the threshold will be predicted as aligned, otherwise not-aligned. Sentence pairs that are not selected as candidates will also be predicted as not-aligned.      <ref type="table" target="#tab_1">Table 14</ref> shows the values of other hyperparameters. For the LSTM baseline, we replicated the LSTM encoder-decoder model used by <ref type="bibr" target="#b6">Zhang and Lapata (2017)</ref>. We preprocessed the data by replacing the named entities in a sentence using spaCy 13 toolkit. We also replaced all the words with frequency less than three with &lt;UNK&gt;. If our model predicted &lt;UNK&gt;, we replaced it with the aligned source word <ref type="bibr">(Jean et al., 2015)</ref>. <ref type="table" target="#tab_1">Table 15</ref> summarizes the hyperparameters of LSTM model. We used 300dimensional GloVe word embeddings <ref type="bibr">(Pennington et al., 2014)</ref> to initialize the embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Example System Outputs Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated by LSTM baseline Complex (input)</head><p>In Seattle , eight activists between ages 10 and 15 petitioned Washington state last year to adopt stricter science-based regulations to protect them against climate change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>In Seattle, eight youths between 10 to 15 years old petitioned the state of Washington to change the law. New (this work) in seattle , eight activists between ages 10 and 15 asked washington state last year to keep the environment safe. (Phrasal Praphrase + Deletion) Old <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> in seattle , eight activists between ages 10 and 15 asked washington state last year to adopt stricter science -based rules to protect them against climate change. (Lexical Paraphrase) Complex (input)</p><p>He recognized that another recommendation would be controversial with police groups: independent investigations after police shootings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>He admitted that police would not like one of the recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New (this work)</head><p>he thought another suggestion would be against the police. (Phrasal Paraphrase + Deletion) Old <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> he recognized that another suggestion would be controversial with police groups. (Lexical Paraphrase + Deletion) Complex (input)</p><p>The Philadelphia Museum of Art has two famous selfie spots , both from the movie " Rocky. " Simple (reference)</p><p>The Philadelphia Museum of Art has two big selfie spots. New (this work) the philadelphia museum of art has two picture spots. (Lexical Paraphrase + Deletion) Old <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> the philadelphia museum of art has two famous spots. (Deletion) Generated by Transformer bert Complex (input) Some Chicago residents got angry about it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>The plan made some people angry. New (this work) some people in chicago were angry. (Phrasal Paraphrase) Old <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> some chicago residents got angry. (Deletion) Complex (input)</p><p>Emissions standards have been tightened , and the government is investing money in solar , wind and other renewable energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>China has also put a great deal of money into solar, wind and other renewable energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New (this work)</head><p>the government is putting aside money for new types of energy. (Phrasal Paraphrase + Deletion) Old <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> the government is investing in money , wind and other equipment. (Lexical Paraphrase + Deletion) Complex (input) On Feb. 9 , 1864 , he was sitting for several portraits , including the one used for the $5 bill.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>On Feb. 9, 1864, several artists painted pictures of him. New (this work) on feb. 9, 1864 , he was sitting for several portraits. (Deletion) Old <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> on feb 9, 1864 , he was sitting for several , including the $ 5 bill for the bill. (Deletion) <ref type="table" target="#tab_1">Table 16</ref>: Examples of simplified sentences generated by LSTM and Transformer bert models trained on our new NEWSELA-AUTO (this work) and old existing NEWSELA <ref type="bibr" target="#b2">(Xu et al., 2015)</ref> datasets. The source sentences are from our new NEWSELA-AUTO test set. Models trained on our new data rephrase the input sentence more often than the models trained on old data. Bold indicates deletions or paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex (input)</head><p>Now at age 9, his teachers say Richie reads at the level of a student in high school, and his vocabulary is well above those of his classmates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>He reads like a high school student. LSTM now he is age 9. EditNTS (Dong et al., 2019) he say his classmates are using a special job. <ref type="bibr">Rerank (Kriz et al., 2019)</ref> but it is well above those of his classmates. Transfomer bert (this work) now at age 9 , his teachers say that richie reads high schoolwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex (input)</head><p>He can recall the special feeling when, at age 7, he built his first kite and saw it waft into the air.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>He can remember the special feeling when he built his first kite . LSTM he can remember the people when he was age 7. EditNTS <ref type="bibr">(Dong et al., 2019)</ref> , at age 7, he built his first kite. <ref type="bibr">Rerank (Kriz et al., 2019)</ref> he could remember the special feeling when. Transfomer bert (this work) he can remember the special feeling when he was 7 years old.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex (input)</head><p>Following the action , two middle-aged brothers slid down a hill holding signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple (reference)</head><p>For example , two grownup brothers slid down a hill holding signs . LSTM &lt;unk&gt; middle -aged brothers slid down a hill holding signs. EditNTS (Dong et al., 2019) two middle-aged brothers , 14 , heard down a hill signs. <ref type="bibr">Rerank (Kriz et al., 2019)</ref> he made a hill holding signs. Transfomer bert (this work) two middle-aged brothers slid down a hill holding signs.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Manual inspection of 100 random sentence pairs from our corpora (NEWSELA-AUTO and WIKI-AUTO) and the existing Newsela<ref type="bibr" target="#b2">(Xu et al., 2015)</ref> and Wikipedia<ref type="bibr" target="#b6">(Zhang and Lapata, 2017)</ref> corpora. Our corpora contain at least 44% more complex rewrites (Deletion + Paraphrase or Splitting + Paraphrase) and 27% less defective pairs (Not Aligned or Not Simpler).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2. 4 https://www.figure-eight.com/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Manual inspection of 100 random sentences generated by Transformer bert trained on NEWSELA-AUTO and existing NEWSELA datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Instructions and an example question for our crowdsourcing annotation on the Figure Eight platform. E.2 In-house Annotation Interface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Annotation interface for correcting the crowdsourced alignment labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of our manually and automatically created sentence alignment annotations on Newsela.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of different sentence alignment methods on the NEWSELA-MANUAL test set. ? Previous work was designed only for Task 1 and used alignment strategy (greedy algorithm or dynamic programming) to improve either precision or recall.</figDesc><table><row><cell></cell><cell></cell><cell>Task 1</cell><cell></cell><cell></cell><cell>Task 2</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="3">Neural sentence pair models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Infersent</cell><cell cols="6">92.8 69.7 79.6 87.8 74.0 80.3</cell></row><row><cell>ESIM</cell><cell cols="6">91.5 71.2 80.0 82.5 73.7 77.8</cell></row><row><cell>BERTScore</cell><cell cols="6">90.6 76.5 83.0 83.2 74.3 78.5</cell></row><row><cell>BERT embedding</cell><cell cols="6">84.7 53.0 65.2 77.0 74.7 75.8</cell></row><row><cell>BERT f inetune</cell><cell cols="6">93.3 84.3 88.6 90.2 80.0 84.8</cell></row><row><cell cols="7">+ ParaAlign 98.4 84.2 90.7 91.9 79.0 85.0</cell></row><row><cell cols="2">Neural CRF aligner</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Our CRF Aligner 96.5 90.1 93.2 88.6 87.7 88.1</cell></row><row><cell cols="7">+ gold ParaAlign 97.3 91.1 94.1 88.9 88.0 88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. sent. len (complex) 25.4 25.8 26.6 25.2 avg. sent. len (simple) 13.8 15.7 18.7 18.5</figDesc><table><row><cell></cell><cell cols="2">Newsela</cell><cell cols="2">Wikipedia</cell></row><row><cell></cell><cell cols="4">Auto Old Auto Old</cell></row><row><cell># of article pairs</cell><cell cols="4">13k 7.9k 138k 65k</cell></row><row><cell># of sent. pairs (train)</cell><cell cols="4">394k 94k 488k 298k</cell></row><row><cell># of sent. pairs (dev)</cell><cell cols="2">43k 1.1k</cell><cell>2k</cell><cell>2k</cell></row><row><cell># of sent. pairs (test)</cell><cell>44k</cell><cell>1k</cell><cell cols="2">359 359</cell></row><row><cell>avg</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We provide the model and training details in Appendix D.1. Models trained on old dataset (original NEWSELA corpus released in<ref type="bibr" target="#b2">(Xu et al., 2015))</ref> </figDesc><table><row><cell>1. A BERT-initialized Transformer, where the encoder and decoder follow the BERT base ar-chitecture. The encoder is initialized with the same checkpoint and the decoder is randomly initialized (Rothe et al., 2020). 2. A randomly initialized Transformer with the same BERT base architecture as above. 3. A BiLSTM-based encoder-decoder model used in Zhang and Lapata (2017). 4. EditNTS (Dong et al., 2019), 7 a state-of-the-art neural programmer-interpreter (Reed and de Freitas, 2016) approach that predicts ex-plicit edit operations sequentially. In addition, we compared our BERT-initialized Transformer model with the released system out-puts from Kriz et al. (2019) and EditNTS (Dong et al., 2019). We implemented our LSTM and Evaluation on old test set SARI add keep del FK Len SARI add keep del FK Len 11.9 0.0 35.5 0.0 12 24.3 12.5 0.0 37.7 0.0 11 22.9 33.1 1.8 22.1 75.4 6.8 14.2 34.1 2.0 25.5 74.8 6.7 14.2 35.6 2.8 32.1 72.0 8.2 16.9 36.2 2.5 34.9 71.3 7.7 16.3 35.5 1.8 30.0 75.4 7.1 14.1 36.1 1.7 32.8 73.8 7.0 14.1 34.4 2.4 25.2 75.8 7.0 14.5 35.1 2.7 27.8 74.8 6.8 14.3 Models trained on our new dataset (NEWSELA-AUTO) Complex (input) Transformer rand LSTM EditNTS Transformer bert Transformer rand 35.6 3.2 28.4 75.0 7.1 14.4 35.2 2.5 29.7 73.5 7.0 14.2 LSTM 35.8 3.9 30.5 73.1 7.0 14.3 36.4 3.3 33.0 72.9 6.6 14.0 EditNTS 35.8 2.4 29.4 75.6 6.3 11.6 35.7 1.8 31.1 74.2 6.1 11.5 Transformer bert 36.6 4.5 31.0 74.3 6.8 13.3 36.8 3.8 33.1 73.4 6.8 13.5 Transformer models using Fairseq. 8 Evaluation on our new test set Simple (reference) ----6.6 13.2 ----6.2 12.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Automatic evaluation results on NEWSELA test sets comparing models trained on our dataset NEWSELA-AUTO against the existing dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Train</cell><cell>F</cell><cell>A</cell><cell>S Avg.</cell></row><row><cell>LSTM</cell><cell cols="4">old 3.57 3.27 3.11 3.31</cell></row><row><cell>LSTM</cell><cell cols="4">new 3.55 2.98 3.12 3.22</cell></row><row><cell>Transformer bert</cell><cell cols="4">old 2.91 2.56 2.67 2.70</cell></row><row><cell>Transformer bert</cell><cell cols="4">new 3.76 3.21 3.18 3.39</cell></row><row><cell>Simple (reference)</cell><cell>-</cell><cell cols="3">4.34 3.34 3.37 3.69</cell></row></table><note>Human evaluation of fluency (F), adequacy (A) and simplicity (S) on the old NEWSELA test set. ?We used the system outputs shared by the authors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Human evaluation of fluency (F), adequacy</cell></row><row><cell>(A) and simplicity (S) on NEWSELA-AUTO test set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Automatic evaluation results on Wikipedia</cell></row><row><cell>TURK corpus comparing models trained on WIKI-</cell></row><row><cell>AUTO and WIKILARGE (Zhang and Lapata, 2017).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Gustavo Paetzold, Fernando Alva-Manchego, and Lucia Specia. 2017. MASSAlign: Alignment and annotation of comparable documents. In Proceedings of the IJCNLP 2017, System Demonstrations. Paolo Rosso, and Simone Paolo Ponzetto. 2018. CATS: A tool for customized alignment of text simplification corpora. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation.</figDesc><table><row><cell>David Pellow and Maxine Eskenazi. 2014. An open corpus of everyday documents for simplification tasks. In Proceedings of the 3rd Workshop on Pre-dicting and Improving Text Readability for Target Reader Populations.</cell><cell>Sanja?tajner, Marc Franco-Salvador, Sanja?tajner and Maja Popovic. 2016. Can text simpli-fication help machine translation? In Proceedings of the 19th Annual Conference of the European Associ-ation for Machine Translation.</cell></row><row><cell>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 confer-ence on empirical methods in natural language pro-cessing.</cell><cell>Brian Thompson and Philipp Koehn. 2019. Vecalign: Improved sentence alignment in linear time and space. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-</cell></row><row><cell>Sarah E Petersen and Mari Ostendorf. 2007. Text sim-</cell><cell>ral Language Processing.</cell></row><row><cell>plification for language learners: A corpus analy-sis. In Proceedings of Workshop on Speech and Lan-guage Technology for Education.</cell><cell>Chen-Tse Tsai and Dan Roth. 2016. Cross-lingual wikification using multilingual embeddings. In Pro-ceedings of the 2016 Conference of the North Amer-</cell></row><row><cell>Scott E. Reed and Nando de Freitas. 2016. Neural</cell><cell>ican Chapter of the Association for Computational</cell></row><row><cell>programmer-interpreters. In 4th International Con-</cell><cell>Linguistics.</cell></row><row><cell>ference on Learning Representations.</cell><cell></cell></row><row><cell></cell><cell>Dan Tufis , , Radu Ion, S , tefan Dumitrescu, and Dan</cell></row><row><cell>Luz Rello, Ricardo Baeza-Yates, and Horacio Saggion.</cell><cell>S , tef?nescu. 2013. Wikipedia as an SMT training</cell></row><row><cell>2013. The impact of lexical simplification by verbal</cell><cell>corpus. In Proceedings of the International Confer-</cell></row><row><cell>paraphrases for people with and without dyslexia. In</cell><cell>ence Recent Advances in Natural Language Process-</cell></row><row><cell>Proceedings of the 14th International Conference on</cell><cell>ing.</cell></row><row><cell>Computational Linguistics and Intelligent Text Pro-</cell><cell></cell></row><row><cell>cessing.</cell><cell>Lucy Vanderwende, Hisami Suzuki, Chris Brockett,</cell></row><row><cell>Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for se-quence generation tasks. Transactions of the Asso-</cell><cell>and Ani Nenkova. 2007. Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion. Inf. Process. Manage.</cell></row><row><cell>ciation for Computational Linguistics.</cell><cell>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob</cell></row><row><cell>Horacio Saggion. 2017. Automatic text simplification. Synthesis Lectures on Human Language Technolo-gies.</cell><cell>Uszkoreit, Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro-cessing systems.</cell></row><row><cell>Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Com-putation.</cell><cell>David Vickrey and Daphne Koller. 2008. Sentence sim-plification for semantic role labeling. In Proceed-ings of the 46th Annual Meeting of the Association</cell></row><row><cell>Advaith Siddharthan and Napoleon Katsos. 2010. Re-</cell><cell>for Computational Linguistics.</cell></row><row><cell>formulating discourse connectives for non-expert</cell><cell></cell></row><row><cell>readers. In Human Language Technologies: The</cell><cell></cell></row><row><cell>2010 Annual Conference of the North American</cell><cell></cell></row><row><cell>Chapter of the Association for Computational Lin-</cell><cell></cell></row><row><cell>guistics.</cell><cell></cell></row><row><cell>Jason R. Smith, Chris Quirk, and Kristina Toutanova.</cell><cell></cell></row><row><cell>2010. Extracting parallel sentences from compara-</cell><cell></cell></row><row><cell>ble corpora using document level alignment. In Hu-</cell><cell></cell></row><row><cell>man Language Technologies: The 2010 Annual Con-</cell><cell></cell></row><row><cell>ference of the North American Chapter of the Asso-</cell><cell></cell></row><row><cell>ciation for Computational Linguistics.</cell><cell></cell></row><row><cell>Sanja?tajner, Hannah B?chara, and Horacio Saggion.</cell><cell></cell></row><row><cell>2015. A deeper exploration of the standard PB-SMT</cell><cell></cell></row><row><cell>approach to text simplification and its evaluation. In</cell><cell></cell></row><row><cell>Proceedings of the 53rd Annual Meeting of the Asso-</cell><cell></cell></row><row><cell>ciation for Computational Linguistics and the 7th In-</cell><cell></cell></row><row><cell>ternational Joint Conference on Natural Language</cell><cell></cell></row><row><cell>Processing.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Parameters of our neural CRF sentence alignment model.</figDesc><table><row><cell cols="2">Threshold Value</cell></row><row><cell>?1</cell><cell>0.1</cell></row><row><cell>?2</cell><cell>0.34</cell></row><row><cell>?3</cell><cell>0.9998861788416304</cell></row><row><cell>?4</cell><cell>0.998915818299745</cell></row><row><cell>?5</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>The thresholds in paragraph alignment Algorithm 2 for Newsela data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>The thresholds in paragraph alignment Algorithm 4 for Wikipedia data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Performance of different sentence alignment methods on the WIKI-MANUAL dev set for Task 1.</figDesc><table><row><cell></cell><cell></cell><cell>Test set</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>MASSAlign (Paetzold et al., 2017)</cell><cell cols="3">68.6 72.5 70.5</cell></row><row><cell>CATS (?tajner et al., 2018)</cell><cell cols="3">68.4 74.4 71.3</cell></row><row><cell cols="4">BERT f inetune (NEWSELA-MANUAL) 80.6 78.8 79.6</cell></row><row><cell>BERT f inetune (WIKI-MANUAL)</cell><cell cols="3">86.3 82.4 84.3</cell></row><row><cell>+ ParaAlign</cell><cell cols="3">86.6 82.4 84.5</cell></row><row><cell>Our CRF Aligner (WIKI-MANUAL)</cell><cell cols="3">89.3 81.6 85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Performance of different sentence alignment methods on the WIKI-MANUAL test set for Task 1.</figDesc><table><row><cell>D Sentence Simplification</cell></row><row><cell>D.1 Implementation Details</cell></row><row><cell>We used Fairseq 11 toolkit to implement our Trans-</cell></row><row><cell>former (Vaswani et al., 2017) and LSTM (Hochre-</cell></row><row><cell>11 https://github.com/pytorch/fairseq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Parameters of our Transformer model.</figDesc><table><row><cell cols="2">Parameter Value Parameter Value</cell></row><row><cell>hidden units 256</cell><cell>batch size 64</cell></row><row><cell>embedding dim 300</cell><cell>max len 100</cell></row><row><cell># of layers 2</cell><cell>dropout 0.2</cell></row><row><cell>lr 0.001</cell><cell>optimizer Adam</cell></row><row><cell>clipping 5</cell><cell>epochs 30</cell></row><row><cell>min vocab freq 3</cell><cell>seed 13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>Parameters of our LSTM model.</figDesc><table><row><cell cols="2">iter and Schmidhuber, 1997) baselines. For the</cell></row><row><cell>Transformer baseline, we followed BERT base</cell><cell>12</cell></row><row><cell cols="2">architecture for both encoder and decoder. We</cell></row><row><cell cols="2">initialized the encoder using BERT base uncased</cell></row><row><cell cols="2">checkpoint. Rothe et al. (2020) used a similar</cell></row><row><cell cols="2">model for sentence fusion and summarization. We</cell></row><row><cell cols="2">trained each model using Adam optimizer with a</cell></row><row><cell cols="2">learning rate of 0.0001, linear learning rate warmup</cell></row><row><cell cols="2">of 40k steps and 200k training steps. We tokenized</cell></row><row><cell>the data with BERT WordPiece tokenizer.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>Examples of simplifications generated by our best model, Transformer bert , and other baselines, namely, EditNTS (Dong et al., 2019), Rerank (Kriz et al., 2019) and LSTM on the old NEWSELA test set. Both LSTM and Transformer bert are trained on NEWSELA-AUTO. For EditNTS and Rerank, we use the system outputs shared by their original authors. Bold indicates new phrases introduced by the model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and data are available at: https://github. com/chaojiang06/wiki-auto. Newsela data need to be requested at: https://newsela.com/data/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> Hwang et al. (2015)  annotated 46 article pairs from Simple-Normal Wikipedia corpus; however, its annotation is noisy, and it contains many sentence splitting errors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We consider any sentence pair not in the aligned paragraph pairs as not-aligned. This assumption leads to a small number of missing sentence alignments, which are manually corrected in Step 3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/attardi/wikiextractor 6 The average number of sentences in an article is 9.2 ? 16.5 for Simple English Wikipedia and 74.8 ? 94.4 for English Wikipedia.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/yuedongP/EditNTS 8 https://github.com/pytorch/fairseq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://pytorch.org/ 10 https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://github.com/google-research/bert 13 https://spacy.io/ D.2 Human Evaluation Figure 4: Instructions provided to Amazon Mechanical Turk workers to evaluate generated simplified sentences. We used the same instructions as described in Kriz et al. (2019).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank three anonymous reviewers for their helpful comments, Newsela for sharing the data, Ohio Supercomputer Center (Center, 2012)  and NVIDIA for providing GPU computing resources. We also thank Sarah Flanagan, Bohan Zhang, Raleigh Potluri, and Alex Wing for help with data annotation. This research is supported in part by the NSF awards IIS-1755898 and IIS-1822754, ODNI and IARPA via the BETTER program contract 19051600004, ARO and DARPA via the Social-Sim program contract W911NF-17-C-0095, <ref type="figure">Figure  Eight</ref> AI for Everyone Award, and Criteo Faculty Research Award to Wei Xu. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, IARPA, ARO, DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>is considered as a language by itself. When extracting articles from Wikipedia dump, we removed the meta-page and disambiguation pages. We also removed sentences with less than 4 tokens and sentences that end with a colon.</p><p>After the pre-processing and matching steps, there are 13,036 article pairs in which the simple article contains only one sentence. In most cases, that one sentence is aligned to the first sentence in the complex article. However, we find that the patterns of these sentence pairs are very repetitive (e.g., XXX is a city in XXX. XXX is a football player in XXX.). Therefore, we use regular expressions to filter out the sentences with repetitive patterns. Then, we use a BERT model fine-tuned on the WIKI-MANUAL dataset to compute the se-mantic similarity of each sentence pair and keep the ones with a similarity larger than a threshold tuned on the dev set. After filtering, we ended up with 970 aligned sentence pairs in total from these 13,036 article pairs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Wubben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Antal van den Bosch, and Emiel Krahmer</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Problems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A parse-and-trim approach with information significance for Chinese sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on Language Generation and Summarisation</title>
		<meeting>the 2009 Workshop on Language Generation and Summarisation</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERTScore: Evaluating text generation with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating transformer and paraphrase rules for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Saptono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bambang</forename><surname>Parmanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
