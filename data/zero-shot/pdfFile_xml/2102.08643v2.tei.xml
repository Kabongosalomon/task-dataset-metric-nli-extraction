<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEMPORAL MEMORY ATTENTION FOR VIDEO SEMANTIC SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TEMPORAL MEMORY ATTENTION FOR VIDEO SEMANTIC SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-video semantic segmentation</term>
					<term>memory</term>
					<term>self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video semantic segmentation requires to utilize the complex temporal relations between frames of the video sequence. Previous works usually exploit accurate optical flow to leverage the temporal relations, which suffer much from heavy computational cost. In this paper, we propose a Temporal Memory Attention Network (TMANet) to adaptively integrate the long-range temporal relations over the video sequence based on the self-attention mechanism without exhaustive optical flow prediction. Specially, we construct a memory using several past frames to store the temporal information of the current frame. We then propose a temporal memory attention module to capture the relation between the current frame and the memory to enhance the representation of the current frame. Our method achieves new state-of-theart performances on two challenging video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and 76.5% mIoU on CamVid with ResNet-50.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Image semantic segmentation is a dense prediction task that needs to predict a category label for each pixel of a given image. Video semantic segmentation is a much more challenging task, which needs to assign a category label for each pixel in each frame of a given video sequence.</p><p>Video semantic segmentation is an important task for visual understanding, which has attracted a lot of attention from the research community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The most straightforward solution for video semantic segmentation is to apply an image semantic segmentation model to each frame of the videos as image semantic segmentation does. However, video frames have strong relation with each other. Simply applying an image segmentation model on a video sequence frame by frame doesn't make full use of the temporal relation between video frames. Modeling the temporal relation of video frames will improve the performance of the video segmentation model. Previous works building the temporal relation of a video sequence can be categorized into two streams: optical-flowbased methods and non-optical-flow-based methods. Optical flow represents the motion of an object between consecutive frames. The optical-flow-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> usually contain two networks: 1) an optical flow network, which predicts the motion of objects between consecutive frames by a well pre-trained optical flow network (e.g. FlowNet-2.0 <ref type="bibr" target="#b5">[6]</ref>), and 2) a segmentation network, which generates the segmentation results for the pre-defined key frame and uses the predicted optical flow to propagate the segmentation result from the key frame to other frames. Optical-flow based methods share the same point that video segmentation model needs high-quality optical flow predictions, and poor optical flow predictions will lead to poor segmentation results.</p><p>The non-optical-flow-based methods raise a new direction to generate the video representation and achieve better performance recently. Per-frame prediction method <ref type="bibr" target="#b6">[7]</ref> on video semantic segmentation introduces a novel temporal consistency loss to improve the temporal consistency of video prediction and employs a light model with knowledge distillation to retain high performance and attain high inference speed simultaneously. TDNet <ref type="bibr" target="#b3">[4]</ref> proposes to distribute several subnetworks over sequential frames and recompose the extracted features for segmentation via an attention propagation module. The non-optical-flow based methods discard the optical flow prediction, which is more efficient for video semantic segmentation. Our proposed method belongs to non-opticalflow-based method.</p><p>Memory networks have been introduced to enhance the reasoning ability of the model in VideoQA <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and video object segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, but have never been introduced in video semantic segmentation as we know. <ref type="bibr" target="#b7">[8]</ref> uses episodic memory to conduct multiple cycles of inference by interacting the question with video features conditioned on current memory. STA <ref type="bibr" target="#b9">[10]</ref> designs a spatial-temporal at- tention mechanism to capture the temporal information for video object segmentation. Memory networks utilize a memory component to store and retrieve information required by the query from the memory. In video representation, it is straightforward to construct a memory that consists of the previous frames and a query represents the current frame. Then, we can retrieve information from the previous frames by computing the correlation between the previous frames and the current frame to enhance the representation of the current frame. Motivated by this, we propose a Temporal Memory Attention network (TMANet) to better capture the temporal relation of video frames and enhance the video representation without the help of opticalflow. Take the street scene in <ref type="figure" target="#fig_0">Fig.1</ref> as an example, the person appearing on the current frame also appears in the previous frames, which exists high relationships between adjacent frames. Our model aims to adaptively integrate similar information from the previous frames, thus enhances the representation of the current frame and improves the segmentation results.</p><p>Our main contributions are as follows: (1) We propose a novel Temporal Memory Attention Network, which is the first work applying the memory and self-attention mechanism in video semantic segmentation. (2) We design a novel Temporal Memory Attention module to capture the temporal correlation in the video sequence efficiently. (3) The proposed method achieves new state-of-the-art performances on two challenging datasets, namely Cityscapes and CamVid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>Given a video sequence that contains multiple frames where one frame is annotated with labels, we consider the previous frames without annotation labels as the memory frames and the current frame with annotation label as query frame. It should be noted that the memory contains multiple frames, while the query contains one frame. Both the memory and the query frame are then fed into a shared backbone to extract features following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. The output of the backbone is of high dimension but in low resolution. To reduce computational cost and encode different representation of the memory and the query, the extracted features from the backbone are fed into encoding layers for channel reduction and feature encoding. The key feature is learned to encode visual semantics for matching robust appearance variations, the value feature stores detailed information for producing semantic prediction, and the number of channel in the key feature is much smaller than that of the value feature. Next, the key and value feature go through our proposed Temporal Memory Attention (TMA) module to build the long-range temporal context information. Then, the value features of query is combined with the long-range temporal context information to enhance the query representation. After feature aggregation, a segmentation head is followed to output the final segmentation result for the current frame.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>, given a memory sequence containing T frames and a query with a single frame X ? R 3?H?W , we concatenate the memory frames along the temporal dimension to get a new memory M ? R T ?3?H?W . First, features are extracted via a shared deep backbone. Then, we feed them into different encoding layers to generate features with different semantic information,</p><formula xml:id="formula_0">M K ? R T ?C K ?H?W , M V ? R T ?C V ?H?W for memory and Q K ? R C K ?H?W , Q V ? R C V ?H?W for query.</formula><p>After that, the key and value are input to the Temporal Memory Attention module to capture the long-range temporal relations. We add a simple feature aggregation following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref> to aggregate the temporal information in memory and important information in query. Finally, We add a segmentation head implemented by 1x1 convolution to generate segmentation map (R C?H?W ), where C is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoding Layer</head><p>Directly using the original output of the backbone is computationally expensive because of the high-dimensional channel. The simplest way for channel reduction is applying a 1x1 convolution on the feature maps. However, 1x1 convolution is not able to capture the spatial information and leads to performance decreasing. The 3x3 convolution or larger kernel can capture spatial information with a larger receptive field, but it will bring more parameters and computational cost. Therefore, we propose to apply a 1x1 convolution for channel reduction and add a 3x3 convolution for spatial information encoding to balance the performance and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Temporal Memory Attention Module</head><p>As for images, long-range context refers to the relation between a unique pixel and other pixels <ref type="bibr" target="#b13">[14]</ref>, while the longrange context of videos is the relation between different frames <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>. As represented in <ref type="figure" target="#fig_1">Fig.2</ref>, we propose a Temporal Memory Attention module to build the temporal relations of video frames for video semantic segmentation.</p><p>After embedding the memory sequence as mentioned above, we accordingly obtain T key features and T value features. We then concatenate them along the temporal dimension generating a 4-dimension matrix, and then permute and reshape them to M K ? R C K ?M and M V ? R M ?C V , respectively. M = T ? H ? W is the number of pixels in the memory. Similarly, we reshape and transpose the key of query to Q K ? R N ?C K , where N = H ? W is the number of pixels in the query. Next, we multiply M K and Q K , and then apply a softmax layer to calculate the temporal memory attention S ? R N ?M ,</p><formula xml:id="formula_1">S ij = exp(Q i K ? M j K ) M j=1 exp(Q i K ? M j K )<label>(1)</label></formula><p>where S ij measures the impact of the i th position in the key of query on the j th position in the key of memory. It should be noted that larger impact from the query to the memory indicates greater relation between them. After obtaining the temporal attention map S, we multiply S and M V to integrate the temporal relation to memory, thus enhancing the embedding of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Feature Aggregation</head><p>After obtaining the long-range temporal context information via temporal memory attention module, we combine the longrange temporal context information with the information from current frame, as follows:</p><formula xml:id="formula_2">f = ?(M V , Q V )<label>(2)</label></formula><p>where f is the aggregated feature, and ? is the employed feature aggregation method.</p><p>Feature aggregation can be implemented by a decoder structure <ref type="bibr" target="#b15">[16]</ref> , feature concatenation or feature summation.</p><p>In this paper, we employ feature concatenation for simplicity. After feature aggregation, we exploit a segmentation head to generate the final segmentation result for the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Implementation Details</head><p>To evaluate our proposed method, we carry out comprehensive experiments on two benchmark datasets Cityscapes <ref type="bibr" target="#b16">[17]</ref> and CamVid <ref type="bibr" target="#b17">[18]</ref>.</p><p>Cityscapes <ref type="bibr" target="#b16">[17]</ref> contains 5000 high-quality fine annotated images, which can be split into 2975, 500 and 1275 snippets for training, validation and testing, respectively. Each snippet contains 30 frames, and only the 20 th frame of each snippet is annotated with 19 classes for semantic segmentation. CamVid <ref type="bibr" target="#b17">[18]</ref> contains 4 videos with 11 category labels for semantic segmentation and is annotated every 30 frames. The annotated frames are grouped into 467, 100 and 233 snippets for training, validation and testing, respectively. We adopt mean Intersection-over-Union (mIoU) as our evaluation metric on Cityscapes and CamVid.</p><p>We implement our method based on PyTorch on 4 GPUs of Tesla V100. Inspired by <ref type="bibr" target="#b18">[19]</ref>, we employ the poly learning rate policy and employ SGD as the optimizer, where the initial learning rate is multiplied by (1 ? iter total iter ) 0.9 for each iteration. Momentum and weight decay are set to 0.9 and 5e-4 for all experiments on Cityscapes and CamVid. We train our model with Sync-BN <ref type="bibr" target="#b19">[20]</ref>, where batch size and learning rate are set to 8 and 0.01 for both datasets, respectively. We set the total iteration to 80,000 for all experiments. For data augmentation, we apply random resize with a ratio between 0.5 and 2, random cropping (768x768, 640x640 for Cityscapes and CamVid respectively) and random horizontal flipping for input images and sequences for all experiments. We apply sliding window strategy to generate video snippets in the testing stage. Following <ref type="bibr" target="#b12">[13]</ref>, we add the auxiliary segmentation loss at the low-level feature of the backbone (e.g. the stage 3 output of ResNet). We adopt the above settings for all experiments if without specific clarification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation Study</head><p>All the ablation experiments are conducted on the Cityscapes dataset. We use FCN-50 <ref type="bibr" target="#b21">[22]</ref> as our baseline. To save computational resources and training time, we adopt ResNet-50 as the backbone and set output stride to 16 for all ablation experiments.</p><p>Following <ref type="bibr" target="#b9">[10]</ref>, we set the channel of value features in both memory and query as four times than that of key features (e.g. when the channel of key features is set to 64, the channel of value features is set to 256). Besides, it is important to determine how many past frames should be selected into memory. We conduct experiments to analyze different numbers of channel and different memory lengths. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we can observe that a significant improvement from 77.52 to 78.28 is obtained when the length of memory increases from 2 to 4. While when the length of memory increases to 6, the improvement is too slight to be ignored. It can be interpreted as information redundancy that the memory storing the information of 4 frames is enough for the feature representation enhancement of the current frame. Though the model performs best when the channel number is set to 128, we choose 64 as the number of channel in key features for computational efficiency which has similar performance as 128 channels.</p><p>To build the memory, we need to select multiple frames from the past video sequence. There exist two selection methods as follows: 1) random selecting multiple frames from the past video sequence (random selecting n frames from last 10 frames), 2) continuously selecting multiple frames from the past video sequence. As shown in <ref type="table">Table 2</ref>, the continuous selecting strategy performs better than random selecting strategy. The possible reason is that random selecting strategy may involve some long-range relation which is harmful to the current frame representation because of the long distance from the current frame. While the continuous selecting strategy selects multiple frames from the current frame continuously, the representation between frames is highly related, which will enhance the feature representation of the current frame. We also analyze different feature aggregation methods, e.g. concatenation and summation. As shown in <ref type="table">Table  2</ref>, feature concatenation performs better than feature summation. The main reason appears to be that concatenated features involve more channels and can represent more information.</p><p>The encoding layer plays an important role in the frame- work, thus we also compare different encoding layers and the results are listed in <ref type="table" target="#tab_1">Table 3</ref>. It can be seen that a combination of 1x1 convolution and 3x3 convolution performs best than other configurations. <ref type="table">Table 4</ref> shows the performance and GFLOPs of our method and other state-of-the-art methods. Considering the inference time varies from different hardware environments, we provide the computational cost of GFLOPs for fair comparison. Compared with other optical-flow based methods and non opticalflow based methods, our method achieves better performance on both Cityscapes and Camvid datasets with lower computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with State-of-the-arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we propose a Temporal Memory Attention Network (TMANet) for video semantic segmentation, which is the first work using memory and self-attention to build the temporal relation in video semantic segmentation. Specially, we introduce a Temporal Memory Attention module to capture the temporal relations between frames. Our method achieves state-of-the-art performance on Cityscapes and CamVid dataset without complicated testing augmented skills. In the future, we will continue to decrease the computation complexity and enhance the efficiency of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENTS</head><p>This work was supported by National Natural Science Foundation of China (61922086, 61872366) and Beijing Natural Science Foundation (4192059, JQ20022).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of the behavior of TMANet. It collects related information from the previous frames to enhance the representation of the current frame. The orange arrows represent the highly related positions between the frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of our proposed TMANet. We select T frames from a given video as the memory sequence. The current frame and memory sequence are fed into a shared backbone to extract features. The encoding layers further embed the features to keys and values. The Temporal Memory Attention module captures temporal relation between Q K , M K and M V , generating an enhanced memory embeddingM V . The embedding of current frame Q V is concatenated withM V to generate final segmentation result through a segmentation head. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results of different channel numbers of key features and memory lengths on the Cityscapes validation set. Sequence2 denotes 2 frames in the Memory. Key256 denotes the channel number of key is 256.</figDesc><table><row><cell>Method</cell><cell></cell><cell>mIoU (%)</cell></row><row><cell>Baseline</cell><cell></cell><cell>70.69</cell></row><row><cell cols="2">Sequence2-Key256</cell><cell>77.77</cell></row><row><cell cols="2">Sequence2-Key128</cell><cell>77.95</cell></row><row><cell>Sequence2-Key64</cell><cell></cell><cell>77.87</cell></row><row><cell>Sequence2-Key32</cell><cell></cell><cell>77.65</cell></row><row><cell>Sequence1-Key64</cell><cell></cell><cell>78.08</cell></row><row><cell>Sequence4-Key64</cell><cell></cell><cell>78.26</cell></row><row><cell>Sequence6-Key64</cell><cell></cell><cell>78.28</cell></row><row><cell cols="4">Table 2. Comparison results of different feature aggregation</cell></row><row><cell cols="4">methods and sampling methods (default is random) on the</cell></row><row><cell>Cityscapes validation set.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>Sample</cell><cell>mIoU (%)</cell></row><row><cell>Sequence4-Key64, concat</cell><cell></cell><cell>random</cell><cell>78.39</cell></row><row><cell cols="3">Sequence4-Key64, concat continuous</cell><cell>78.45</cell></row><row><cell>Sequence4-Key64, sum</cell><cell></cell><cell>random</cell><cell>78.18</cell></row><row><cell>Sequence4-Key64, sum</cell><cell cols="2">continuous</cell><cell>78.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison results of different encoding layers on the Cityscapes validation set.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>mIoU (%)</cell></row><row><cell cols="3">Sequence4-Key64, 3x3 conv</cell><cell></cell><cell>78.26</cell></row><row><cell cols="3">Sequence4-Key64, 1x1 conv</cell><cell></cell><cell>77.88</cell></row><row><cell cols="4">Sequence4-Key64, 1x1 conv, 3x3 conv</cell><cell>78.39</cell></row><row><cell>Table 4.</cell><cell cols="4">Comparison results with state-of-the-arts on</cell></row><row><cell cols="4">Cityscapes and CamVid validation set.</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">mIoU (%) Cityscapes CamVid</cell><cell>GFLOPs</cell></row><row><cell cols="2">DFF [2]</cell><cell>69.2</cell><cell>-</cell><cell>&gt;919</cell></row><row><cell cols="2">GRFP [5]</cell><cell>73.6</cell><cell>66.1</cell><cell>-</cell></row><row><cell cols="2">Netwarp [21]</cell><cell>-</cell><cell>67.1</cell><cell>&gt;919</cell></row><row><cell cols="2">LVS [3]</cell><cell>76.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TDNet-50 [4]</cell><cell>79.9</cell><cell>76.0</cell><cell>1082</cell></row><row><cell cols="2">Ours-50</cell><cell>80.3</cell><cell>76.5</cell><cell>754</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yule</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5997" to="6005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6819" to="6828" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient semantic video segmentation with per-frame inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11433</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6576" to="6585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1999" to="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video object segmentation using spacetime memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced memory network for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="801" to="818" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7151" to="7160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic video cnns through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4453" to="4462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
