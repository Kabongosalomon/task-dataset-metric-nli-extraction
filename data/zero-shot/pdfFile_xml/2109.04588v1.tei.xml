<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<email>vandurme@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Murray</surname></persName>
							<email>kenton@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pretrained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BIBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En?De and 38.61 for De?En on the IWSLT'14 dataset, and 31.26 for En?De and 34.94 for De?En on the WMT'14 dataset, which exceeds all published numbers 12 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models (LMs), trained on a large-scale unlabeled data to capture rich representations of the input, such as ELMO <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, XLNET <ref type="bibr" target="#b48">(Yang et al., 2019)</ref> and XLM <ref type="bibr" target="#b10">(Conneau and Lample, 2019)</ref> have increasingly attracted attention in various NLP tasks. Either utilizing context-aware representations of input tokens <ref type="bibr" target="#b31">(Peters et al., 2018)</ref> or fine-tuning the pre-trained parameters <ref type="bibr">(Devlin 1</ref> Code is available at: https://github.com/ fe1ixxu/BiBERT. 2 Our BiBERT is released at: https://huggingface. co/jhu-clsp/bibert-ende. et al., 2019) both lead to significant improvement for downstream tasks. <ref type="figure">Figure 1</ref>: The overview of methods: a series of additive improvements to the use of contextualized embeddings on IWSLT'14 dataset. Experimenting over various pretrained language models, we show that our BIBERT, a bilingual English-German language model, vastly outperforms all other methods (Section 2). Adding stochastic layer selection to BIBERT improves performance (Section 3). Finally, innovative dual-directional training and fine-tuning with the previous two methods yield around 2 BLEU point gains over the previous state-of-the-art result ) (Section 4).</p><p>Inspired by the superior performance of BERT on many other tasks, researchers have investigated leveraging using this pre-trained masked language model to enhance translation models, e.g., initializing the parameters of the model's encoder with BERT parameters <ref type="bibr" target="#b34">(Rothe et al., 2020)</ref>, and incorporating the output of BERT to each layer of the encoder <ref type="bibr" target="#b50">(Zhu et al., 2020;</ref><ref type="bibr" target="#b42">Weng et al., 2020)</ref>. In this paper, we demonstrate simply using the output of a pre-trained language model as the input of NMT systems can achieve state-of-the-art results on IWLST'14 <ref type="bibr" target="#b6">(Cettolo et al., 2014)</ref> and WMT'14 <ref type="bibr" target="#b3">(Bojar et al., 2014)</ref> English?German (En?De) translation tasks in the case of without using back translation <ref type="bibr" target="#b36">(Sennrich et al., 2016;</ref>  <ref type="bibr">3</ref> . After conducting a thor-ough evaluation of numerous pre-trained language models, we demonstrate that specialized bilingual models perform the best. We then introduce two further refinements, stochastic layer selection and dual-directional training that yield further improvements. The overview of methods are shown in <ref type="figure">Figure 1</ref>. Overall, our best systems beat published state-of-the-art BLEU scores by around 2 points.</p><p>Our main contributions are listed as follows:</p><p>? We release our English-Germean bilingual pre-trained language model, BIBERT, and demonstrate that it outperforms both monolingual and multi-lingual language models for machine translation (Section 2).</p><p>? Expanding upon our bilingual language model results, we introduce stochastic layer selection which incorporates information from more layers in the pre-trained language model to improve machine translation (Section 3). 2 Contextualized Embeddings for NMT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Method</head><p>In this section, we focus on investigating the effectiveness of using the output (contextualized embeddings) of the last layer of pre-trained language models on building NMT models. Our basic NMT models are six-layer transformer translation models, though it is model agnostic assuming there are encoder embeddings <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>. Specifically, our method relies on extracting contextualized embeddings of source sentences from the final layer of a frozen pre-trained language model and feeding them to the embedding layer of the NMT encoder. Rather than randomly initializing the source embedding layer, we use the output of these pre-trained models and do not allow these parameters to update during training. To allow for a deep analysis, we concentrate on one language pair, English?German (En?De). In the with additional monolingual data, we only use the provided bitexts during machine translation training.</p><p>following subsections, we first explore how much translation performance can be improved by simply using contextualized embeddings, and then explore the internal factors of various pre-trained language models that may affect NMT models. We then introduce our bilingual pre-trained language model and demonstrate that using its contextualized embeddings achieves state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Existing Pre-Trained Models</head><p>We first describe four influential pre-trained models that we incorporate into NMT -two monolingual and two multilingual models.</p><p>ROBERTA An optimized version of BERT which is trained on a larger dataset, with a dynamic masked language model training regiment that also removes the next sentence prediction . This model matches or exceeds the performance of BERT on multiple NLP tasks.</p><p>GottBERT A state-of-the-art pure German Roberta model <ref type="bibr" target="#b35">(Scheible et al., 2020)</ref> trained on 145G German text data portion of OSCAR (Ortiz <ref type="bibr" target="#b26">Su?rez et al., 2020)</ref>, a huge multilingual corpus extracted from Common Crawl. This has been shown to outperform the other two existing German monolingual models (i.e., German BERT 4 from deepset and dbmz BERT 5 ) on NER and text classification tasks.</p><p>MBERT (cased) A multilingual BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> pre-trained on 104 highest-resource languages in Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XLM-R (base)</head><p>A transformer-based <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> masked language model trained on 100 languages, using more than two terabytes of filtered CommonCrawl data, which outperforms MBERT on a variety of cross-lingual benchmarks <ref type="bibr">(Conneau et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">How Do Pre-Trained LMs Affect NMT?</head><p>First we investigate how contextualized embeddings of aforementioned pre-trained language models help NMT models, and explore possible positive and negative factors that may affect NMT models.</p><p>Dataset We initially consider a low-resource scenario and then show further experiments in a highresource scenario in Section 5. We conduct experi-ments on the IWSLT'14 English-German dataset, which has 160K parallel bilingual sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>Our model configuration is transformer_iwslt_de_en, a six-layer transformer architecture <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>, with FFN dimension size 1024 and 4 attention heads. We use an embedding dimension of 768 to match the dimension of pre-trained language models. For a consistent comparison with previous works, the evaluation metric is the commonly used tokenized BLEU <ref type="bibr" target="#b29">(Papineni et al., 2002)</ref>   <ref type="table">Table 1</ref>: IWSLT'14 En?De BLEU scores utilizing contextualized embeddings from various pre-trained language models. random represents the embedding layer of the NMT encoder that is randomly initialized but uses the same vocabulary of the assigned pretrained language model. pre-trained means the embedding layer of the NMT encoder use the output of the assigned frozen pre-trained language model during MT training. Numbers in the bracket show the increment/deduction compared with the corresponding model compared to randomly initialized embeddings.</p><p>Observations The main IWSLT'14 results are shown in <ref type="table">Table 1</ref>. We first conduct experiments with randomly initialized embeddings to obtain baselines. Feeding the output of a pre-trained language model into an NMT model necessitates that the vocabulary of the encoder should be the same as the one used for the language model. To ensure that improvements are not the result of choosing a better vocabulary, we train randomly initialized baseline systems using identical vocabularies for each encoder. For these experiments, the decoder's vocabulary size is fixed to 8K in order to make fair comparisons. We investigate decoder vocabulary size selection in more detail in Section 2.5. When the embedding layer of the MT encoder is randomly initialized, as opposed to using the pre-trained language model, we ob-serve similar BLEU scores for all baselines from English-to-German (around 27.6) and German-to-English (around 33.7). By replacing the embedding layer with contextualized embeddings, GOT-TBERT boosts the BLEU scores of De?En from 33.56 to 36.32, and ROBERTA strengthens the En?De translation from 27.3 to 28.74. However, the MBERT and XLM-R only provide modest improvement in De?En translation and even degenerate the performance of En?De translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curse of Multilinguality</head><p>We first note the deterioration caused by MBERT and XLM-R on En?De over the randomly initialized baselines, as well as the comparatively small gains versus the monolingual models of De?En. We hypothesize that contextualized embeddings from MBERT and XLM-R are hurt by the curse of multilinguality <ref type="bibr">(Conneau et al., 2020)</ref>, i.e., low-resource language performance can be improved by adding higherresource languages during pre-training, but unfortunately high-resource performance suffers and degrades. MBERT and XLM-R are trained on 100 and 104 languages respectively and the curse of multilinguality may lead to model capacity issues that degenerate the contextualized embeddings of high-resource languages such as English and German. We attribute the slightly higher improvements of XLM-R over MBERT to the larger amounts of data used in pre-training. The large monolingual models, ROBERTA and GOTTBERT significantly beat a randomized baseline, but also significantly beat the multilingual models. Note that even though XLM-R has 55.6B English tokens used for pre-training, it still helps less than ROBERTA using around 28B English tokens, which is possibly due to interference and constrained capacity <ref type="bibr" target="#b0">(Arivazhagan et al., 2019;</ref><ref type="bibr" target="#b17">Johnson et al., 2017;</ref><ref type="bibr" target="#b39">Tan et al., 2019)</ref>. Therefore, a suitable pre-trained language model for NMT intuitively should be trained on a large amount of data, but with special care to avoid using too many languages during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Customized Pre-Trained LM</head><p>Pre-trained monolingual language models can improve performance of machine translation systems, yet machine translation is inherently a bilingual task. We hypothesize that a pre-trained language model can further improve the translation performance if its training data is composed of a mixture of texts in both source and target lan-guages. In other words, we expect the source and target language data to enrich the contextualized information for each other to better facilitate translation for both directions (En?De). Therefore, we propose our bilingual pre-trained language models, dubbed BIBERT.</p><p>Our BIBERT EN-DE is based on the RoBERTa architecture  and implemented using the fairseq framework . In order to make a direct comparison, BIBERT EN-DE is trained on the same German texts as GOTTBERT -just with an additional 146GB of English texts. These are a subset of the English portion in OS-CAR -the same dataset the German texts come from. We combine English and German data and shuffle them before training. We train the model using the same number of update steps on German texts as GOTTBERT 6 . We train a unified 52K vocabulary using the WordPiece tokenizer <ref type="bibr" target="#b45">(Wu et al., 2016)</ref>, with 67GB English and 67GB German texts which are randomly sampled from the training set. BIBERT EN-DE is trained on TPU v3-8 for four weeks. More details about optimization for BIBERT EN-DE are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Vocabulary Size Selection</head><p>The vocabulary is fixed for the encoder but still indeterminate for the decoder. In a low-resource machine translation setting, performance is highly sensitive to decoder vocabulary size selection. <ref type="bibr" target="#b14">Gowda and May (2020)</ref> demonstrated that a decoder vocabulary using 8K BPE operations performed best across a large grid search. To ensure that 8K vocabulary size is also a suitable choice for the IWSLT'14 (160K parallel sentences) dataset when combined with our method, we search over four candidate decoder vocabulary sizes (8K, 16K, 24K, and 32K) for all aforementioned pre-trained language models. As shown in <ref type="figure">Figure 2</ref>, 8K yields the highest BLEU score for all of our NMT models for De?En. Thus we select 8K as the vocabulary size of the decoder and use this for all subsequent experiments on IWSLT'14 unless otherwise noted. Interestingly, we also notice that the performance of the translation model with BIBERT EN-DE is robust for De?En, and basically unaffected by the vocabulary size. Analysis Based on the superior performance of BIBERT EN-DE , we hypothesize that contextualized embeddings output from BIBERT EN-DE contain richer German information than GOTTBERT and better assist the model in translation by learning extra English data. Furthermore, we theorize training on German texts also enhances the quality of English contextualized embeddings -note that even though ROBERTA and BIBERT EN-DE are not directly comparable due to different English pre-training data, BIBERT EN-DE still had a 0.68 BLEU point improvement over ROBERTA even while using less English training data. Some other explanations for the superior performance of BIBERT EN-DE are 1) it learns the aligned embeddings for the tokens with similar meanings across two languages. Hence, the source embeddings can offer the encoder a hint of aligned target embeddings to help translation. 2) Embeddings of overlapping En-De sub-word units 7 fed to NMT encoders may facilitate translation by bilingual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithms</head><p>De ? En Adversarial MLE <ref type="bibr" target="#b41">(Wang et al., 2019)</ref> 35.18 DynamicConv  35.20 Macaron Net <ref type="bibr" target="#b22">(Lu* et al., 2020)</ref> 35.40 BERT-Fuse <ref type="bibr" target="#b50">(Zhu et al., 2020)</ref> 36.11 MAT  36.22 Mixed Representations  36.41 UniDrop  36.88 Ours, GOTTBERT 36.32 Ours, BIBERT 37.58  <ref type="table" target="#tab_3">Table 2</ref> shows a comparison of our work with the recent literature on IWSLT'14 German to English translation. These works propose improvements to transformer models in different aspects, e.g., incorporating BERT into every layer of encoders and decoders with additional multi-head attentions <ref type="bibr" target="#b50">(Zhu et al., 2020)</ref>, multi-branch encoders , mixed representations from different tokenizers  and uniting different dropout techniques into NMT models . Our straightforward method of simply using the final layer of BIBERT EN-DE outperforms all of them. Furthermore, even the model that only uses the monolingual GOTTBERT achieves a competitive result (36.32) compared with the previous state-of-the-art approach (36.88). Our method is easy to implement, so it can be used in conjunction with other methods in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Comparison with Existing Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Time Costs</head><p>Leveraging an external pre-trained language model leads to higher computational complexity. Our approach takes approximately 20% additional time during training and 13% extra time during inference. Considering the significant BLEU gains, we argue that they justify the higher time costs. 7 Such as ##n, which uses shared En-De information.</p><p>3 Layer Selection <ref type="bibr" target="#b16">Jawahar et al. (2019)</ref> demonstrates that different layers of BERT capture differing linguistic information in a rich, hierarchical structure that mimics classical, compositional tree-like structures. Information in the lower layer (e.g., phrase-level information) gets gradually diluted in higher layers. Thus, to potentially leverage more information encapsulated in the pre-trained language models, we are also interested in exploring how other layers of contextualized embeddings can improve NMT models -rather than simply using the last layer.</p><p>We denote X as the collection of source language sentences. For each source sentence x ? X , let H i B (x) denote the contextualized embeddings of x obtained from the i th layer of the pre-trained language model. In our settings, we consider top K layers of the pre-trained language model, i.e., we consider H</p><formula xml:id="formula_0">i B (x) ?i ? [M ? K + 1, M ],</formula><p>where K is a hyperparameter, and M is the total number of layers of the pre-trained language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stochastic Layer Selection</head><p>During training of deep neural networks, various methods of stochastically freezing groups of parameters in a model for individual training examples have been shown to improve performance. For instance, dropout <ref type="bibr" target="#b38">(Srivastava et al., 2014)</ref> samples parameters from a Bernoulli distribution to not update, and drop-net <ref type="bibr" target="#b50">(Zhu et al., 2020)</ref> and drop-branch  randomly active a candidate net and freeze the others in a uniform distribution. We propose stochastic layer selection, a novel approach to encapsulate more features and information from more layers of the pre-trained language models. Specifically, for each batch, we randomly pick the output from one layer rather than all of them as the input for the NMT encoder <ref type="figure" target="#fig_0">(Figure 3)</ref>. We denote the input embeddings of sentence x to the NMT encoder as H E (x), which is defined in the following way during training:</p><formula xml:id="formula_1">H E (x) = K i=1 1( i ? 1 K &lt; p ? i K )H M ?i+1 B (x)<label>(1)</label></formula><p>where 1(?) is the indicator function and p is a random variable which is uniformly sampled from [0,1]. In the inference step, the output is the expectation of outputs of all layers used for training, i.e.,</p><formula xml:id="formula_2">E p?uniform[0,1] [H E (x)]</formula><p>, which leads to the modifi- :</p><formula xml:id="formula_3">H E (x) = 1 K K i=1 H M ?i+1 B (x).</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments and Results</head><p>Based on the results of <ref type="table">Table 1</ref>, we select the pre-trained model performing best for NMT, BIBERT EN-DE , and use it as the basis for all subsequent experiments. To be consistent with the results in Section 2, we once again use the IWSLT'14 dataset. <ref type="figure" target="#fig_1">Figure 4</ref> illustrates the impact of stochastic layer selection. We conduct experiments for En?De with the number of layers K ranging from 2 to M (M = 12 for BIBERT EN-DE ). Note that setting K = 1 reduces to the case of only selecting the last layer as in Section 2. In all cases, the stochastic layer selection obtains substantial gains compared with our previous best scores in En?De (29.65) and De?En (37.58) in Section 2. In both situations of En?De and De?En, the translation model gets the highest score (37.94 for De?En and 30.04 for En?De) when stochastic layer selection uses 8 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">One Model, Dual-Directional Translations</head><p>In this section, different from ordinary oneway translation models, we introduce our dualdirectional translation models, i.e., a model can translate both En?De and De?En. The model architecture is the same as the one in Section 3. One of the biggest advantages of the shared English-German vocabulary of BIBERT EN-DE is that our encoder has the capability of receiving De?En (right, red bars) BLEU as a function of number of layers K considered in the stochastic layer selection module for NMT models. Note that when K = 1, it reduces to the case of selecting the last layer. However, any value of K &gt; 1 selected for stochastic layer selection beats this very strong baseline with K = 8 obtaining the highest BLEU scores in both directions. contextualized embeddings of both source and target tokens. During the training step, we feed source sentences to the model and expect the generation of a target translation, yet also, inversely, feed target sentences and expect translations in the source language. The motivation behind the dual-directional translation model is that we expect the contextualized representations of source and target sentences could enhance each other to build a better encoder for the translation model. From the aspect of data augmentation, the target sentences play a role in augmented data in the task of translating from the source language to the target language, vice versa. With the method of swapping source and target sentences once as an additional dataset, our experiments show superior performance for both direc- tional translations. Two advantages of this method are 1) obtaining improvement without extra bitexts, and 2) only slight modification for data preprocessing and no changes for the model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Preprocessing</head><p>For consistent comparisons, the dataset is still IWSLT'14 En?De. The details of data preprocessing for the dual-directional translation model are illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. Using only the same exact parallel sentences in our bitext for training, we simply leverage the dataset in reverse, by swapping our original target sentences to use as new source sentences and original source sentences as new target sentences. We then concatenate and jointly shuffle the original and new data to acquire our mixed training data. We use a joint English-German vocabulary of size 12k for the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-tuning</head><p>Inspired by the findings of , where training on a mix of in-and out-of-domain of data initially, and then gradually fine-tuning until only in-domain data is used, substantially improved model performance, we treat our concatenated sentences as mixed domain data, and the source and target languages are separate language domains. Each language data can be the out-of-domain data for the other language. Following this perspective, we first train our dual-directional model on mixed data, and then fine-tune it on the source or target data to obtain one-way translation models 8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments and Results</head><p>We additionally conduct one-way translation models with 12K bilingual vocabulary to have a fair baseline for dual-directional models. Overall results are shown in <ref type="table" target="#tab_5">Table 3</ref>. We first discuss the models trained without stochastic layer selection. The dual-directional model substantially outperforms the one-way model by obtaining a gain of 0.52 in En?De and 0.72 in De?En. Moreover, fine-tuning on the in-domain data further improves BLEU from 29.89 to 30.33 in En?De and from 37.97 to 38.12 in De?En. Both positive results indicated by the dual-directional model and finetuning approach show their effectiveness in helping translation. A similar discussion holds for the models with the stochastic layer selection method.    29.3 -Evolved Transformer <ref type="bibr" target="#b37">(So et al., 2019)</ref> 29.8 -BERT Initialization (12 layers) <ref type="bibr" target="#b34">(Rothe et al., 2020)</ref> 30.6 33.6 BERT-Fuse <ref type="bibr" target="#b50">(Zhu et al., 2020)</ref> 30.  16 attention heads. We replace hidden size 1024 with 768 to keep the dimensions consistent with BIBERT EN-DE . The evaluation strategy is the same as IWSLT'14 tasks. Following the findings that En?De translation has similar results for vocabularies ranging from 32K to 64K in high-resource scenarios (4.5M training samples) <ref type="bibr" target="#b14">(Gowda and May, 2020)</ref>, we use a bilingual vocabulary with 52K size for the decoder, which is larger than the ones (8K and 12K) used in IWSLT experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We compare our methods with prior existing works that achieve highest scores by only using provided bi-texts in <ref type="table" target="#tab_7">Table 4</ref>. With BIBERT EN-DE contextualized embeddings and stochastic layer selection, our model achieves state-of-the-art BLEU both on En?De (30.91) and De?En (34.94). Interestingly, dual-directional translation training does not show the same strong effectiveness as it did in the low-resource scenario. One possible reason is that model capacity is not large enough to handle mixed domain data <ref type="bibr" target="#b0">(Arivazhagan et al., 2019)</ref>. However, it still additively improves En?De to 31.26 BLEU. It is worth mentioning that our NMT model achieves better performance with less training parameters -the hidden size of our NMT model is 768 but 1024 for the prior existing works.</p><p>6 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pre-Trained Embeddings</head><p>Traditional pre-trained embeddings are investigated in type level, e.g., word2vec <ref type="bibr" target="#b25">(Mikolov et al., 2013)</ref>, glove <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref> and fastText <ref type="bibr" target="#b2">(Bojanowski et al., 2017)</ref>. <ref type="bibr" target="#b31">Peters et al. (2018)</ref> moved further from this line and proposed context-aware embeddings output from pre-trained bidirectional LSTM (ELMO). Following the attention-based transformer module <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>, the architectures of GPT models <ref type="bibr" target="#b32">(Radford et al., 2018</ref><ref type="bibr" target="#b33">(Radford et al., , 2019</ref><ref type="bibr">Brown et al., 2020)</ref> and BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> respectively are based on stacking deep transformer decoders and encoders and significantly boost downstream tasks. Beyond pure English models, pre-trained language models for other languages have also showed up, e.g., CAMEMBERT for French <ref type="bibr" target="#b35">(Martin et al., 2020)</ref> and ARABERT for Arabic <ref type="bibr" target="#b1">(Baly et al., 2020)</ref>. Multilingual representations, e.g. <ref type="bibr">MBERT and XLMS (Conneau and Lample, 2019)</ref> have been shown to be effective to facilitate cross-lingual learning. XLM-R <ref type="bibr">(Conneau et al., 2020)</ref>, a model learning cross-lingual representation at scale achieved state-of-the-art results on multiple cross-lingual benchmarks. Recently, an English-Arabic bilingual BERT <ref type="bibr" target="#b20">(Lan et al., 2020)</ref> outperformed ARABERT, MBERT and XLM-R on supervised and zero-shot transfer settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MT with Context-Aware Representations</head><p>Imamura and Sumita (2019) removed the NMT encoder part and directly fed the output of BERT to the attention mechanism in the decoder. They train the model with two optimization stages, i.e., only training the decoder and fine-tuning BERT. Similarly, <ref type="bibr" target="#b7">Clinchant et al. (2019)</ref> have incorporated BERT into NMT models by replacing the embedding layer with BERT parameters and initializing encoder with BERT, but they still notice that NMT model with BERT is not as robust as expected. <ref type="bibr" target="#b34">Rothe et al. (2020)</ref> also leveraged pretrained checkpoints (e.g., BERT and GPT) to initialize 12-layer NMT encoder and decoder and achieved state-of-the-art results. Interestingly, they showed that the models with decoder initialized by GPT fail to improve the translation performance and are even worse than the one whose decoder is randomly initialized. Similarly, <ref type="bibr" target="#b23">Ma et al. (2020)</ref> initialize both transformer encoder and decoder by XLM-R but fine-tune it on multiple bilingual corpora to obtain a multilingual translation model. The preliminary experiments from <ref type="bibr" target="#b50">Zhu et al. (2020)</ref> indicate that NMT models simply fed by the output of BERT outperform the models initialized by BERT or XLM. However, only limited experiments and little analysis on this method has been done in their work. They mainly focused on the BERT-fuse approach, i.e., the output of BERT is fed to each layer of NMT encoder and decoder with extra multi-head attentions. Instead of only using the last layer of BERT, <ref type="bibr" target="#b42">Weng et al. (2020)</ref> introduced layer-aware attention mechanism to capture compound contextual information from BERT. Moreover, they also proposed the knowledge distillation paradigm to learn pre-trained representation in the training process. On an English-Arabic translation task,  use a precursor of this method though it lacks all of the refinements described here. However, it was shown to further help in downstream cross-lingual information extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have shown that our BIBERT trained on a large amount of mixed texts of the source and target languages can better help NMT models improve translation performance compared with other existing pre-trained language models and achieve state-ofthe-art results by simply using the output of the last layer. Moreover, we introduce the stochastic layer selection method and demonstrated its effectiveness in improving translation performance. Finally, experiments on the dual-directional translation model illustrate that source and target data can augment each other to further boost performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The overall framework of stochastic layer selection method. Top K layers of the pre-trained language model are considered and fed to the NMT encoder. cation of Equation 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>IWSLT'14 En?De (left, blue bars) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Workflow of data preprocessing. We swap source and target sentences, and concatenate swapped sentence pairs and original sentene pairs. Finally, we shuffle the concatenated data for dual-directional translation model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Note that GOTTBERT uses 100K update steps, but our training data is roughly double that due to the extra English data, so we adopt 200K update steps. En?De BLEU as a function of vocabulary size with various pre-trained language models on IWSLT'14 test set. Models obtain highest scores with 8K vocabulary size. 2.6 BIBERT Performance BIBERT EN-DE results As indicated in the last row of Table 1, our bilingual model help the transformer model achieve 29.65 score for En?Dea gain of 2.12 over the baseline. For De?En, our model achieves 37.58 score with a gain of 4.06. Recall that BIBERT EN-DE uses the same settings as GOTTBERT -the only difference is the addition of extra English training data from the OSCAR corpus -yet BIBERT EN-DE yields an additional 1.30 BLEU point improvement over GOTTBERT.</figDesc><table><row><cell>(a) De?En BLEU on IWSLT'14 test set</cell></row><row><cell>(b) En?De BLEU on IWSLT'14 test set</cell></row><row><cell>Figure 2:</cell></row></table><note>6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison of our work and most recent ex- isting methods on IWSLT'14 De?En.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Compared with our previous models in Section 3 (30.04 En?De and 37.94 in De?En), our best model achieves new state-of-the-art results both in En?De and De?En, which respectively obtain 30.45 and 38.61 BLEU.</figDesc><table><row><cell>Methods</cell><cell cols="2">En?De De?En</cell></row><row><cell>No Stochastic Layer Selection:</cell><cell></cell><cell></cell></row><row><cell>One-Way (vocab size=12K)</cell><cell>29.37</cell><cell>37.25</cell></row><row><cell>Dual-Directional Training</cell><cell>29.89</cell><cell>37.97</cell></row><row><cell>+ Fine-Tuning</cell><cell>30.33</cell><cell>38.12</cell></row><row><cell>Stochastic Layer Selection, K = 8:</cell><cell></cell><cell></cell></row><row><cell>One-Way (vocab size=12K)</cell><cell>30.00</cell><cell>37.69</cell></row><row><cell>Dual-Directional Training</cell><cell>30.30</cell><cell>38.37</cell></row><row><cell>+ Fine-Tuning</cell><cell>30.45</cell><cell>38.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>: Comparison of dual-directional and ordi-</cell></row><row><cell cols="2">nary (one-way) translation models, with and without</cell></row><row><cell cols="2">stochastic layer selection, on IWSLT'14 En?De.</cell></row><row><cell cols="2">5 High-Resource Scenario</cell></row><row><cell cols="2">5.1 Dataset and Training Details</cell></row><row><cell cols="2">For the high-resource scenario, we evaluate</cell></row><row><cell cols="2">models on the WMT'14 English-German</cell></row><row><cell cols="2">dataset, which contains 4.5M parallel sentence</cell></row><row><cell>pairs.</cell><cell>We combine newstest2012 and</cell></row><row><cell cols="2">newstest2013 as the validation set and</cell></row><row><cell cols="2">use newstest2014 (3003 sentence pairs)</cell></row><row><cell cols="2">as the test set. Our model configuration is</cell></row><row><cell cols="2">transformer_vaswani_wmt_en_de_big,</cell></row><row><cell cols="2">a 'big' transformer with 4096 FFN dimension and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>WMT'14 En?De results on newstest2014 test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Though we use a language model that has been trained arXiv:2109.04588v1 [cs.CL] 9 Sep 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://deepset.ai/german-bert 5 https://github.com/dbmdz/berts</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">In the view of the same amount of training data for the source and target language, we only apply one-stage finetuning, which is slightly different from the multiple stages of fine-tuning in.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank anonymous reviewers for their valuable comments. We thank Kelly Marchisio and Kevin Duh for their helpful suggestions. We also thank the Google TFRC program for providing free TPU access. This work was supported in part by IARPA BETTER (#2019-19051600005). The views and conclusions contained in this work are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, or endorsements of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 IWSLT'14 Training Details</head><p>We use 4 NVIDIA 2080 Ti GPUs with 2048 tokens per GPU and accumulate the gradient 4 times. The learning rate is 0.0004. The optimizer is Adam (Kingma and Ba, 2014) with inverse_sqrt learning rate scheduler. At inference time, we use beam search with width 4 and use a length penalty of 0.6 (Boulanger- <ref type="bibr" target="#b4">Lewandowski et al., 2013;</ref><ref type="bibr" target="#b45">Wu et al., 2016;</ref><ref type="bibr" target="#b19">Koehn and Knowles, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 WMT'14 Training Details</head><p>We use 4 NVIDIA V100 GPUs with a batch size of 4096 tokens per GPU. Following the recommendation of the training settings from , we accumulate the gradient 32 times to simulate 128 GPU training settings. We set the initial learning rate as 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Optimization for BIBERT</head><p>We use fairseq to pre-train our bilingual language models on an 8-core TPU v3-8. We train the models in 200K update steps using a batch size of 8192. We use Adam optimizer (Kingma and Ba, 2014) with a learning rate of 4e-4, ? 1 = 0.9, ? 2 = 0.999, L2 weight decay of 0.01. The learning rate is warmed up over the first 20K steps to a peak value of 4e-4, from which the learning rate polynomially decayed. We apply a dropout rate of 0.1 to all layers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arabert: Transformer-based model for arabic language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fady</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Hajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</title>
		<meeting>the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audio chord recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT-International Workshop on Spoken Language Processing</title>
		<imprint>
			<publisher>Sebastian St?ker</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2" to="17" />
		</imprint>
	</monogr>
	<note>Fran?ois Yvon</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the use of BERT for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kweon Woo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilina</forename><surname>Nikoulina</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5611</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
		<respStmt>
			<orgName>Hong Kong. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<imprint>
			<pubPlace>Guillaume Wenzek, Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10270</idno>
		<title level="m">Multi-branch attentive transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding the optimal vocabulary size for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamme</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.352</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="3955" to="3964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recycling a pre-trained BERT encoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5603</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="23" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An empirical study of pre-trained transformers for Arabic information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4727" to="4734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding and improving transformer from a multi-particle dynamic system point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Yan Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Xlm-t: Scaling up multilingual machine translation with pretrained cross-lingual transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Hany Hassan Awadalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singhal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15547</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Camembert: a tasty french language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
	<note>Djam? Seddah, and Beno?t Sagot. 2020</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A monolingual approach to contextualized word embeddings for mid-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1703" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00313</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Thomczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patric</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Jaravine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Boeker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02110</idno>
		<title level="m">Gottbert: a pure german language model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6555" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Acquiring knowledge from pre-trained model to neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanbo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9266" to="9273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence generation with mixed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10388" to="10398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">UniDrop: A simple yet effective technique to improve transformer without extra cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3865" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradual fine-tuning for lowresource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Yarmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Steven</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Domain Adaptation for NLP</title>
		<meeting>the Second Workshop on Domain Adaptation for NLP<address><addrLine>Kyiv, Ukraine</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Everything is all it takes: A multipronged strategy for zero-shot cross-lingual information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Yarmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Steven</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Incorporating bert into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
