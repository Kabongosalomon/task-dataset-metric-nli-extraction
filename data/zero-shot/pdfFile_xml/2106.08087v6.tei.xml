<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">4 Pingan Health Technology 5 Ping An Health Cloud Company Limited 6 Ping An International Smart City Technology Co</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhuan</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Shang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangping</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotong</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics, Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics, Ministry of Education</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Life Sciences and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>10 Yidu Cloud Technology Inc 11</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Zhengzhou University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongying</forename><surname>Zan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunli</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">4 Pingan Health Technology 5 Ping An Health Cloud Company Limited 6 Ping An International Smart City Technology Co</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>12 Harbin Institute of Technology (Shenzhen) 13 Peng Cheng Laboratory, 14 Philips Research China</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the development of biomedical language understanding benchmarks, Artificial Intelligence applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that stateof-the-art neural models perform far worse than the human ceiling 1 . Our benchmark is released at https://tianchi.aliyun. com/dataset/dataDetail?dataId= 95414&amp;lang=en-us. * Equal contribution and shared co-first authorship.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial intelligence is gradually changing the landscape of healthcare, and biomedical research <ref type="bibr" target="#b6">(Yu et al., 2018)</ref>. With the fast advancement of biomedical datasets, biomedical natural language processing (BioNLP) has facilitated a broad range A key driving force behind such improvements and rapid iterations of models is the use of general evaluation datasets and benchmarks <ref type="bibr">(Gijsbers et al., 2019)</ref>. Pioneer benchmarks, such as BLURB <ref type="bibr">(Gu et al., 2020)</ref>, <ref type="bibr">PubMedQA (Jin et al., 2019)</ref>, and others, have provided us with the opportunity to conduct research on biomedical language understanding and developing real-world applications. Unfortunately, most of these benchmarks are developed in English, which makes the development of the associated machine intelligence Anglo-centric. Meanwhile, other languages, such as Chinese, have unique linguistic characteristics and categories that need to be considered. Even though Chinese speakers account for a quarter of the world population, there have been no existing Chinese biomedical language understanding evaluation benchmarks.</p><p>To address this issue and facilitate natural language processing studies in Chinese, we take the first step in introducing a comprehensive Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark with eight biomedical language understanding tasks. These tasks include named entity recognition, information extraction, clinical diagnosis normalization, short text classification, question answering (in transfer learning setting), intent classification, semantic similarity, and so on. We evaluate several pre-trained Chinese language models on CBLUE and report their performance. The current models still perform by far worse than the standard of single-human perfor-mance, leaving room for future improvements. We also conduct a comprehensive analysis using case studies to indicate the challenges and linguistic differences in Chinese biomedical language understanding. We intend to develop a universal GLUElike open platform for the Chinese BioNLP community, and this work helps accelerate research in that direction. Overall, the main contributions of this study are as follows:</p><p>? We propose the first Chinese biomedical language understanding benchmark, an openended, community-driven project with diverse tasks. The proposed benchmark serves as a platform for the Chinese BioNLP community and encourages new dataset contributions.</p><p>? We report a systematic evaluation of 11 Chinese pre-trained language models to understand the challenges derived by these tasks. We release the source code of the baselines as a toolkit for future research purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several benchmarks have been developed to evaluate general language understanding over the past few years. GLUE <ref type="bibr" target="#b0">(Wang et al., 2019b)</ref> is one of the first frameworks developed as a formal challenge affording straightforward comparison between taskagnostic transfer learning techniques. SuperGLUE <ref type="bibr">(Wang et al., 2019a)</ref>, styled after GLUE, introduce a new set of more difficult language understanding datasets. Other similarly motivated benchmarks include DecaNLP <ref type="bibr">(McCann et al., 2018)</ref>, which recast a set of target tasks into a general questionanswering format and prohibit task-specific parameters, and SentEval <ref type="bibr">(Conneau and Kiela, 2018)</ref>, which evaluate explicitly fixed-size sentence embeddings. Non-English benchmarks include <ref type="bibr">Rus-sianSuperGLUE (Shavrina et al., 2020)</ref> and CLUE <ref type="bibr" target="#b5">(Xu et al., 2020)</ref>, which is a community-driven benchmark with nine Chinese natural language understanding tasks. These benchmarks in the general domain provide a north star goal for researchers and are part of the reason we can confidently say we have made great strides in our field. For BioNLP, many datasets and benchmarks have been proposed <ref type="bibr">Li et al., 2016;</ref><ref type="bibr" target="#b4">Wu et al., 2019)</ref> which promote the biomedical language understanding <ref type="bibr">(Beltagy et al., 2019;</ref><ref type="bibr">Lewis et al., 2020;</ref><ref type="bibr">Lee et al., 2020)</ref>. <ref type="bibr">Tsatsaronis et al. (2015)</ref> propose biomedical language understanding datasets as well as a competition on largescale biomedical semantic indexing and question answering. <ref type="bibr">Jin et al. (2019)</ref> propose PubMedQA, a novel biomedical question answering dataset collected from PubMed abstracts. <ref type="bibr">Pappas et al. (2018)</ref> propose BioRead, which is a publicly available cloze-style biomedical machine reading comprehension (MRC) dataset. <ref type="bibr">Gu et al. (2020)</ref> create a leaderboard featuring the Biomedical Language Understanding &amp; Reasoning Benchmark (BLURB). Unlike a general domain corpus, the annotation of a biomedical corpus needs expert intervention and is labor-intensive and time-consuming. Moreover, most of the benchmarks are based on English; ignoring other languages means that potentially valuable information may be lost, which can be helpful for generalization.</p><p>In this study, we focus on Chinese to fill the gap and aim to develop the first Chinese biomedical language understanding benchmark. Note that Chinese biomedical text is linguistically different from English and has its domain characteristics, necessitating an evaluation BioNLP benchmark designed explicitly for Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CBLUE Overview</head><p>3.1 Design Principle CBLUE consists of 8 biomedical language understanding tasks.The task descriptions and statistics of CBLUE are shown <ref type="table" target="#tab_0">Table 1</ref>. Unlike CLUE <ref type="bibr" target="#b5">(Xu et al., 2020)</ref> as shown in <ref type="table">Table 2</ref>, CBLUE has a diverse data source (the annotation is expensive), richer task setting, thus, more challenging for NLP models. We introduce the design principle of CBLUE as follows: 1) Diverse tasks: CBLUE contain widespread token-level, sequence-level, sequence-pair tasks.</p><p>2) Variety of differently distributed data: CBLUE collect data from various sources, including clinical trials, EHRs, medical forum, textbooks, and search engine logs with a real-world distribution.</p><p>3) Quality control in long-term maintenance: We asked domain experts (doctors from Class A tertiary hospitals) to annotate datasets and carefully review data to ensure data quality.  <ref type="table">Table 2</ref>: Difference between CBLUE, CLUE and BLURB. There are three major differences: a) CBLUE has a much more diverse task setting with different data sources in the biomedical domain including clinical trials, EHRs, medical forum, text books and search engine logs; b) CBLUE has a long-tailed distribution which is challenging; c) CBLUE contains a specific transfer learning scenario supported by the CHIP-STS dataset, in which the testing set has a different distribution fromthe training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tasks</head><p>CMeEE For this task, the dataset is first released in CHIP2020 2 . Given a pre-defined schema, the task is to identify and extract entities from the given sentence and classify them into nine categories: disease, clinical manifestations, drugs, medical equipment, medical procedures, body, medical examinations, microorganisms, and department.</p><p>CMeIE For this task, the dataset is also released in CHIP2020 <ref type="bibr">(Guan et al., 2020)</ref>. The goal of the task is to identify both entities and relations in a sentence following the schema constraints. There are 53 relations defined in the dataset, including 10 synonymous sub-relationships and 43 other subrelationships.</p><p>CHIP-CDN For this task, the dataset is to standardize the terms from the final diagnoses of Chinese electronic medical records. Given the original phrase, the task is to normalize it to standard terminology based on the International Classification of Diseases (ICD-10) standard for Beijing Clinical Edition v601.</p><p>2 http://cips-chip.org.cn/ CHIP-CTC For this task, the dataset is to classify clinical trials eligibility criteria, which are fundamental guidelines of clinical trials defined to identify whether a subject meets a clinical trial or not <ref type="bibr">(Zong et al., 2021)</ref>. All text data are collected from the website of the Chinese Clinical Trial Registry (ChiCTR) 3 , and a total of 44 categories are defined. The task is like text classification; although it is not a new task, studies and corpora for the Chinese clinical trial criterion are still limited, and we hope to promote future research for social benefits.</p><p>CHIP-STS For this task, the dataset is for sentence similarity in the non-i.i.d. (non-independent and identically distributed) setting. Specifically, the task aims to evaluate the generalization ability between disease types on Chinese disease questions and answer data. Given question pairs related to 5 different diseases (The disease types in the training and testing set are different), the task is to determine whether the semantics of the two sentences are similar.</p><p>KUAKE-QIC For this task, the dataset is for intent classification. Given search engine queries, the task is to classify each of them into one of 11 medical intent categories defined in <ref type="table">KUAKE-QIC.  Those include diagnosis, etiology analysis, treatment plan, medical advice, test result analysis and  others.</ref> KUAKE-QTR For this task, the dataset is used to estimate the relevance of the title of a query document. Given a query (e.g., "Symptoms of vitamin B deficiency"), the task aims to find the relevant title (e.g., "The main manifestations of vitamin B deficiency").</p><p>KUAKE-QQR For this task, the dataset is used to evaluate the relevance of the content expressed in two queries. Similar to KUAKE-QTR, the task aims to estimate query-query relevance, which is an essential and challenging task in real-world search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Collection</head><p>Since machine learning models are mostly datadriven, data plays a critical role, and it is pretty often in the form of a static dataset <ref type="bibr">(Gebru et al., 2018)</ref>. We collect data for different tasks from diverse sources, including clinical trials, EHRs, medical books, and search logs from real-world search engines. As biomedical data may contain private information such as the patient's name, age, and gender, all collected datasets are anonymized and reviewed by the IRB committee of each data provider to preserve privacy. We introduce the data collection details followingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection from Clinical Trials</head><p>Clinical trial eligibility criteria text is collected from ChiCTR, a non-profit organization that provides information about clinical trial registration for public research use. In each trial registry file, eligibility criteria text is organized as a paragraph in the inclusion criteria and exclusion criteria. Some meaningless texts are excluded, and the remaining texts are annotated to generate the CHIP-CTC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection from EHRs</head><p>We obtain the final diagnoses of the medical records from several Class A tertiary hospitals and sample a few diagnosis items from different medical departments to construct the CHIP-CDN dataset for research purposes. The diagnosis items are randomly sampled from the items which are not covered by the common medical synonyms dict.</p><p>No privacy information is involved in the final diagnoses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection from Medical Forum and Textbooks</head><p>Due to the COVID-19 pandemic, online consultation has become more and more popular via the Internet. To promote data diversity, we select the online questions by patients to build the CHIP-STS dataset. Note that most of the questions are chief complaints. To ensure the authority and practicability of the corpus, we also select medical textbooks of Pediatrics <ref type="bibr" target="#b2">(Wang et al., 2018)</ref>, Clinical Pediatrics <ref type="bibr">(Shen and Gui, 2013)</ref> and Clinical Practice 4 . We collect data from these sources to construct the CMeIE and CMeEE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection from Search Engine Logs</head><p>We also collect search logs from real-world search engines like the Alibaba KUAKE Search Engine 5 . First, we filter the search queries in the raw search logs by the medical tag to obtain candidate medical texts. Then, we sample the documents for each query with non-zero relevance scores (i.e., to determine if the document is relevant to the query). Specifically, we divide all the documents into three categories, namely high, middle, and tail documents, and then uniformly sample the data to guarantee diversity. We leverage the data from search logs to construct KUAKE-QTC, KUAKE-QTR, and KUAKE-QQR datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Annotation</head><p>Each sample is annotated by three to five domain experts, and the annotation with the majority of votes is taken to estimate human performance. During the annotation phase, we add control questions to prevent dishonest behaviors by the domain experts. Consequently, we reject any annotations made by domain experts who fail in the training phase and do not adopt the results of those who achieved low performance on the control tasks. We maintain strict and high criteria for approval and review at least 10 random samples from each worker to decide whether to approve or reject all their HITs. We also calculate the average interrater agreement between annotators using Fleiss' Kappa scores <ref type="bibr">(Fleiss, 1971)</ref>, finding that five out of six annotations show almost perfect agreement (? = 0.9). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Characteristics</head><p>Utility-preserving Anonymization Biomedical data may be considered as a breach in the privacy of individuals because they usually contain sensitive information. Thus, we conduct utilitypreserving anonymization following <ref type="bibr">(Lee et al., 2017)</ref> to anonymize the data before releasing the benchmark.</p><p>Real-world Distribution To promote the generalization of models, all the data in our CBLUE benchmark follow real-world distribution without up/downsampling. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), our dataset follows long-tail distribution following Zipf's law and will inevitably be long-tailed. However, long-tail distribution has no significant effect on performance. Further, some datasets, such as CMedIE, have label hierarchy with both coarsegrained and fine-grained relation labels, as shown in <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_0">(b).</formula><p>Diverse Tasks Setting Our CBLUE benchmark includes eight diverse tasks, including named entity recognition, relation extraction, and singlesentence/sentence-pair classification. Besides the independent and i.i.d. scenarios, our CBLUE benchmark also contains a specific transfer learning scenario supported by the CHIP-STS dataset, in which the testing set has a different distribution from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Leaderboard</head><p>We provide a leaderboard for users to submit their own results on CBLUE. The evaluation system will give final scores for each task when users submit their prediction results. The platform offers 60 free GPU hours from Aliyun 6 to help researchers develop and train their models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Distribution and Maintenance</head><p>Our CBLUE benchmark was released online on April 1, 2021. Up to now, more than 300 researchers have applied the dataset, and over 80 teams have submitted their model predictions to our platform, including medical institutions (Peking Union Medical College Hospital, etc.), universities (Tsinghua University, Zhejiang University, etc.), and companies (Baidu, JD, etc.). We will continue to maintain the benchmark by attending to new requests and adding new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Reproducibility</head><p>To make it easier to use the CBLUE benchmark, we also offer a toolkit implemented in PyTorch (Paszke et al., 2019) for reproducibility. Our toolkit supports mainstream pre-trained models and a wide range of target tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Baselines We conduct experiments with baselines based on different Chinese pre-trained language models. We add an additional output layer (e.g., MLP) for each CBLUE task and fine-tune the pre-trained models.</p><p>Models We evaluate CBLUE on the following public available Chinese pre-trained models:</p><p>? BERT-base <ref type="bibr">(Devlin et al., 2018)</ref>. We use the base model with 12 layers, 768 hidden layers, 12 heads, and 110 million parameters.</p><p>? BERT-wwm-ext-base <ref type="bibr">(Cui et al., 2019)</ref>. A Chinese pre-trained BERT model with whole word masking.</p><p>? RoBERTa-large <ref type="bibr">(Liu et al., 2019)</ref>. Compared with BERT, RoBERTa removes the next sentence prediction objective and dynamically changes the masking pattern applied to the training data.</p><p>? RoBERTa-wwm-ext-base/large. RoBERTawwm-ext is an efficient pre-trained model which integrates the advantages of RoBERTa and BERT-wwm.</p><p>? ALBERT-tiny/xxlarge <ref type="bibr">(Lan et al., 2019)</ref>. AL-BERT is a pre-trained model with two objectives: Masked Language Modeling (MLM) and Sentence Ordering Prediction (SOP).</p><p>? <ref type="bibr">ZEN (Diao et al., 2019)</ref>. A BERT-based Chinese text encoder enhanced by N-gram representations, where different combinations of characters are considered during training.</p><p>? Mac-BERT-base/large <ref type="bibr" target="#b5">(Cui et al., 2020)</ref>. Mac-BERT is an improved BERT with novel MLM as a correction pre-training task.</p><p>? PCL-MedBERT 7 . A pre-trained medical language model proposed by the Peng Cheng Laboratory.</p><p>We implement all baselines with PyTorch (Paszke et al., 2019). All the training details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark Results</head><p>We report the results of our baseline models on the CBLUE benchmark in <ref type="table" target="#tab_2">Table 3</ref>. We notice that larger pre-trained models obtain better performance. Since Chinese text is composed of terminologies, carefully designed masking strategies may be helpful for representation learning. However, we observe that models which use whole word masking do not always yield better performance than others in some tasks, such as CTC, QIC, QTR, and QQR, indicating that tasks in our benchmark are challenging and more sophisticated technologies should be developed. Further, we find that ALBERT-tiny achieves comparable performance to base models in CDN, STS, QTR, and QQR tasks, illustrating that smaller models may also perform well in specific tasks. We think this is caused by the different distribution between pretraining corpus and Chinese medical text; thus, large PTLMs may not obtain satisfactory performance. Finally, we notice that PCL-MedBERT, which tends to be state-of-the-art in Chinese biomedical text processing tasks, and does not perform as well as we expected. This further demonstrates the difficulty  <ref type="table">Table 4</ref>: Human performance of two-stage evaluation scores with the best-performed model. "avg" refers to the mean score from the three annotators. "majority" indicates the performance taken from the majority vote of amateur humans. Bold text denotes the best result among human and model prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22%</head><p>Need of our benchmark, and contemporary models may find it difficult to quickly achieve outstanding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Performance</head><p>For all of the tasks in CBLUE, we ask human amateur annotators with no medical experience to label instances from the testing set and compute the annotators' majority vote against the gold label annotated by specialists. Similar to SuperGLUE <ref type="bibr">(Wang et al., 2019a)</ref>, we first need to train the annotators before they work on the testing data. Annotators are asked to annotate some data from the development set; then, their annotations are validated against the gold standard. Annotators need to correct their annotation mistakes repeatedly so that they can master the specific tasks. Finally, they annotate instances from the testing data, and these annotations are used to compute the final human scores. The results are shown in <ref type="table" target="#tab_2">Table 4 and the  last row of Table 3</ref>. In all tasks, humans have better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case studies</head><p>We choose two datasets: CMeEE and KUAKE-QIC, a sequence labeling and classification task, respectively, to conduct case studies. As shown in <ref type="figure">Figure 2</ref>, we report the statistics of the proportion of various types of error cases 8 . For CMeEE, we notice that entity overlap 9 , ambiguity 10 , need domain knowledge 11 , annotation error 12 are major reasons that result in the prediction failure. Furthermore, there exist many instances with entity overlap, which may lead to confusion for the named entity recognition task. While in the analysis for KUAKE-QIC, almost half of bad cases are due to multiple triggers 13 and colloquialism. Colloquialism 14 is natural in search queries, which means that some descriptions of the Chinese medical text are too simplified, colloquial, or inaccurate. We show some cases on CMeEE in <ref type="table" target="#tab_5">Table 5</ref>. In the second row, we notice that given the instance of "????????????????? 8 See definitions of errors in the appendix. 9 There exist multiple overlapping entities in the instance. <ref type="bibr">10</ref> The instance has a similar context but different meaning, which mislead the prediction. <ref type="bibr">11</ref> There exist biomedical terminologies in the instance which require domain knowledge to understand. <ref type="bibr">12</ref> The annotated label is wrong. <ref type="bibr">13</ref> There exist multiple indicative words which mislead the prediction. 14 The instance is quite different from written language (e.g., with many abbreviations)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>???? ???? ???? ????</head><p>The systolic blood pressure of the elderly is 160, and the diastolic blood pressure is only more than 40. What is the reason? How to treat? <ref type="table">Table 6</ref>: Case studies in KUAKE-QIC. We evaluate the performance of baselines with 3 sampled instances. The correlation between Query and Title is divided into 3 levels (0-2), which means 'poorly related or unrelated', 'related' and 'strongly related'. BERT = BERT-base, BERT-ext = BERT-wwm-ext-base, MedBERT = PCL-MedBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis Diagnosis Diagnosis Treatment</head><p>??? (Rash can be reduced by the host producing specificanti-toxin antibodies.)", ROBERTA and PCL-MedBERT obtain different predictions. The reason is that there exist medical terms such as "????? (anti-toxin antibodies)". ROBERTA can not identify those tokens correctly, but PCL-MedBERT, pre-trained on the medical corpus, can successfully make it. Moreover, PCL-MedBERT can extract entities "? ?,? ?,? ? (eletions, translocations, inversions)" from the long sentences, which is challenging for other models.</p><p>We further show some cases on KUAKE-QIC in <ref type="table">Table 6</ref>. In the first case, we notice that both BERT and BERT-ext fail to obtain the intent label of the query "?????????????? ????????? (Does it matter if the ratio of lymphocytes is high and the ratio of neutrophils is low?)", while MedBERT can obtain the correct prediction. Since "?????? (ratio of lymphocytes)" and "?????? (ratio of neutrophils)" are biomedical terms, and the general pre-trained language model has to leverage domain knowledge to understand those phrases.</p><p>As shown in <ref type="table" target="#tab_5">Table 5</ref> and <ref type="table">Table 6</ref>, compared with other languages, the Chinese language is very colloquial even in medical texts. Furthermore, polysemy is prevalent in chinese language. The meaning of a word changes according to its tone, which usually causes confusion and difficulties for machine reading. In summary, we conclude that tasks in CBLUE are not easy to solve since the Chinese language has unique characteristics, and more robust models should be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark. We evaluate 11 current language representation models on CBLUE and analyzed their results. The results illustrate the limited ability of state-ofthe-art models to handle some of the more challenging tasks. In contrast to English benchmarks such as GLUE/SuperGLUE and BLURB, whose model performance already matches human performance, we observe that this is far from the truth for Chinese biomedical language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to express gratitude to the anonymous reviewers for their hard work and kind com- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>We collected all the data with authorization from the organization that owned the data and signed the agreement. We release the benchmark following the CC BY-NC 4.0 license. All collected datasets are anonymized and reviewed by the IRB committee of each data provider to preserve privacy. Since we collect data following real-world distribution, there may exist popularity bias that cannot be ignored. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Broader Impact</head><p>The COVID-19 (coronavirus disease 2019) pandemic has had a significant impact on society, both because of the severe health effects of COVID-19 and the public health measures implemented to slow its spread. A lack of information fundamentally causes many difficulties experienced during the outbreak; attempts to address these needs caused an information overload for both researchers and the public. Biomedical natural language processing-the branch of artificial intelligence that interprets human language-can be applied to address many of the information needs making urgent by the COVID-19 pandemic. Unfortunately, most language benchmarks are in English, and no biomedical benchmark currently exists in Chinese. Our benchmark CBLUE, as the first Chinese biomedical language understanding benchmark, can serve as an open testbed for model evaluations to promote the advancement of this technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Negative Impact</head><p>Although we ask domain experts and doctors to annotate all the corpus, there still exist some instances with wrong annotated labels. If a model was chosen based on numbers on the benchmark, this could cause real-world harm. Moreover, our benchmark lowers the bar of entry to work with biomedical data. While generally a good thing, it may dilute the pool of data-driven work in the biomedical field even more than it already is, making it hard for experts to spot the relevant work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limitations</head><p>Although our CBLUE offers diverse settings, there are still some tasks not covered by the benchmark, such as medical dialogue generation <ref type="bibr" target="#b5">(Liu et al., 2020;</ref><ref type="bibr">Lin et al., 2020;</ref> or medical diagnosis <ref type="bibr" target="#b3">(Wei et al., 2018)</ref>. We encourage researchers in both academics and industry to contribute new datasets. Besides, our benchmark is static; thus, models may still achieve outstanding performance on tasks but fail on simple challenge examples and falter in real-world scenarios. We leave this as future works to construct a platform including dataset creation, model development, and assessment, leading to more robust and informative benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D CBLUE Background</head><p>Standard datasets and shared tasks have played essential roles in promoting the development of AI technology. Taking the Chinese BioNLP community as an example, the CHIP (China Health Information Processing) conference releases biomedicalrelated shared tasks every year, which has extensively advanced Chinese biomedical NLP technology. However, some datasets are no longer available after the end of shared tasks, which has raised issues in the data acquisition and future research of the datasets.</p><p>In recent years, we can obtain state-of-the-art performance for many downstream tasks with the help of pre-trained language models. A significant trend is the emergence of multi-task leaderboards, such as GLUE (General Language Understanding Evaluation) and CLUE (Chinese Language Understanding Evaluation). These leaderboards provide a fair benchmark that attracts the attention of many researchers and further promotes the development of language model technology. For example, Microsoft has released BLURB (Biomedical Language Understanding &amp; Reasoning Evaluation) at the end of 2020 in the medical field. Recently, the Tianchi platform has launched the CBLUE (Chinese Biomedical Language Understanding Evaluation) public benchmark under the guidance of the CHIP Society. We believe that the release of the CBLUE will further attract researchers' attention to the medical AI field and promote the development of the community. CBLUE 1.0 15 comprises the previous shared tasks of the CHIP conference and the dataset from Alibaba QUAKE Search Engine, including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentencepair classification. <ref type="bibr">15</ref> We release the benchmark following the CC BY-NC 4.0 license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed Task Introduction E.1 Chinese Medical Named Entity</head><p>Recognition Dataset (CMeEE)</p><p>Task Background As an essential subtask of information extraction, entity recognition has achieved promising results in recent years. Biomedical texts such as textbooks, encyclopedias, clinical trials, medical literature, electronic health records, and medical examination reports contain rich medical knowledge. Named entity recognition is the process of extracting medical terminologies, such as diseases and symptoms, from the above mentioned unstructured or semi-structured texts, and it can help significantly improve the efficiency of scientific research. CMeEE dataset is proposed for this purpose, and the original dataset was released at the CHIP2020 conference.</p><p>Task Description This task is defined as given the pre-defined schema and an input sentence to identify medical entities and to classify them into 9 categories, including disease (dis), clinical symptoms(sym), drugs (dru), medical equipment (equ), medical procedures (pro), body (bod), medical examination items (ite), microorganisms (mic), <ref type="bibr">department (dep)</ref>. For the detailed annotation instructions, please refer to the CBLUE official website, and examples are shown in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>Annotation Process The annotation guide was conducted by two medical experts from Class A tertiary hospitals and optimized during the trail annotation process. A total of 32 annotators had participated in the annotation process, including 2 medical experts who are also the owner of the annotation guideline, 4 experts from the biomedical informatics field, 6 medical M.D., and 22 master students from computer science majors. The annotation lasts for about three months (from October 2018 to December 2018), as well as an additional month's time for curation. The total expense is about 50,000 RMB. The annotation process was divided into two stages.</p><p>? Stage1: This stage was called the trail annotation phase. The medical experts gave training to the annotators to make sure they had a comprehensive understanding of the task. Two rounds of trail annotation were conducted by the annotators, with the purpose of getting familiar with the annotation task as well as It is of great diagnostic value to detect the specific antigen of a certain pathogen with immunoassay, a simple and quick assay that is intended for early diagnosis and proves more reliable than the antibody assay. discovering the unclear points of the guideline, and annotation problems were discussed, and the medical experts improved the annotation guidelines according to the feedback iteratively.</p><p>? Stage2: For the first phase, each record was assigned to two annotators to label independently, and the medical experts and biomedical informatics experts would give in time help. The annotation results were compared automatically by the annotation tools (developed for CMeEE and CMeIE tasks), and any disagreement was recorded and handed over to the next phase. In the second phase, medical experts and the annotators had a discussion for the disagreements records as well as other annotation problems, and the annotators made corrections. After the two stages, the IAA score (Kappa score) is 0.8537, which satisfied the research goal.</p><p>PII and IRB The corpus is collected from authorized medical textbooks or Clinical Practice, and no personally identifiable information or offensive content is involved in the text. No PII is included in the above-mentioned resources. The dataset does not refer to ethics, which has been checked by the IRB committee of the provider.</p><p>The original dataset format is a self-defined plain text format. To simplify the data pre-processing step, the CBLUE team has converted the data format to the unified JSON format with the permission of the data provider.</p><p>Evaluation Metrics This task uses strict Micro-F1 metrics. Task Background Entity and relation extraction is an essential information extraction task for natural language processing and knowledge graph (KG), which is used to detect pairs of entities and their relations from unstructured text. The technology of this task can apply to the medical field. For example, with entity and relation extraction, unstructured and semi-structured medical texts can construct medical knowledge graphs, which can serve lots of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Statistic</head><p>Task Description Given the schema and sentence, in which defines the relation (Predicate) and its related Subject and Object, such as ("subject_type": "? ?"?"predicate": "? ? ? ?"?"object_type": "? ?"). The task requires the model to automatically analyze the sentence and then extract all the T riples = [(S1, P 1, O1), (S2, P 2, O2)...] in the sentence.  <ref type="table" target="#tab_10">Table 8</ref>.</p><p>Annotation Process The annotation guide was conducted by two medical experts from Class A tertiary hospitals and optimized during the trail annotation process. A total of 20 annotators had participated in the annotation process, including 2 medical experts who are also the owner of the annotation guideline, 2 experts from the biomedical informatics field, 4 medical M.D., and 14 master students from computer science majors. The annotation lasts for about four months (from October 2018 to December 2018), which contains the annotation time as well as the curation time. The total expense is about 40,000 RMB. Similar to the CMeEE dataset, the annotation process for CMeIE also contains the trail annotation stage and the formal annotation stage following the same process. Besides, an additional step called the Chinese segmentation validation step was added for this dataset. The data provider has developed a segmentation tool for the medical texts which could generate the segment as well as the POS tagging, and some specified POS types (like 'disease,' 'drug') could help validate if there were potential missing named entities for this task automatically, which could help assist the annotators to check the missing labels. The final IAA for this dataset is 0.83, which could satisfy the research purpose.</p><p>PII and IRB The corpus is collected from authorized medical textbooks or Clinical Practice, and no personally identifiable information or offensive content is involved in the text.</p><p>No PII is included in the above-mentioned resources. The dataset does not refer to ethics, which has been checked by the IRB committee of the provider.</p><p>Evaluation Metrics The SPO results given by the participants need to be accurately matched. The strict Micro-F1 is used for evaluation.</p><p>Dataset Statistic This task has 14,339 training set data, 3,585 validation set data, 4,482 test set data. The dataset is from the pediatric corpus and common disease corpus. The pediatric corpus originates from 518 pediatric diseases, and the common disease corpus is derived from 109 common diseases. The dataset contains nearly 75,000 triples, 28,000 disease sentences, and 53 schemas.</p><p>Dataset Provider The dataset is provided by:</p><p>? </p><formula xml:id="formula_1">'? ? ? ?- treatment_department', 'subject': '? ? ? ? ?-abdominal_aortic_aneurysm', 'sub- ject_type': '? ?-disease', 'object': '? ? ? ? ? ? ? ?-primary_medical_care_clinic', 'object_type': '??-others'} ??_???? disease_other treatment ???? adjuvant therapy {'predicate': '? ? ? ?-adjuvant_therapy', 'subject': '? ? ? ? ? ? ?- utaneous_squamous_cell_carcinoma', 'sub- ject_type': '??-disease', 'object': '???? ?-non_surgical_destructio', 'object_type': '? ???-other_treatment'} ?? chemotherapy {'predicate': '? ?-chemotherapy', 'subject': '? ?-tumour', 'subject_type': '? ? ? ? ? ??-cutaneous_squamous_cell_carcinoma', 'ob- ject': '? ? ? ?-local_chemotherapy', 'ob- ject_type': '????-other_treatment'} ???? radiotherapy</formula><p>{'predicate': '????-radiation_therapy', 'subject': '??????-non_cancer_pain', 'sub-ject_type': '??-disease', 'object': '???-external_irradiation', 'object_type':</p><formula xml:id="formula_2">'????- other_treatment'} ??_???? disease_surgical treatment ???? surgical treatment {'predicate': '? ? ? ?-surgical_treatment', 'subject': '? ? ? ? ? ? ?-cutaneous _squamous_cell_carcinoma', 'subject_type': '? ?-disease', 'object': '? ? ? ? ? ?- surgical_resection(traditional_therapy)',</formula><p>'ob-ject_type': '????-surgical_treatment'} Task Background Clinical term normalization is a crucial task for both research and industry use. Clinically, there might be up to hundreds of different synonyms for the same diagnosis, symptoms, or procedures; for example, "heart tack" and "MI" both stand for the standard terminology "myocardial infarction". The goal of this task is to find the standard phrases (i.e., ICD codes) for the given clinical term. With the help of the standard code, it can help ease the burden of researchers for the statistical analysis of clinical trials; also, it can be helpful for the insurance companies on the DRGs or DIP-related applications. This task is proposed for this purpose, and the originally shared task was released at the CHIP2020 conference.</p><p>Task Description The task aims to standardize the terms from the final diagnoses of Chinese electronic medical records. No privacy information is involved in the final diagnosis. Given the original terms, it is required to predict its corresponding standard phrase from the standard vocabulary of "International Classification of Diseases (ICD-10) for Beijing Clinical Edition v601". For the detailed annotation instructions, please refer to the CBLUE official website. Examples are shown in <ref type="table" target="#tab_14">Table 9</ref>.</p><p>Annotation Process The Chinese Diagnostic Normalization Data Set (CHIP-CDN) was annotated by the medical team of Yidu Cloud. They are all composed of people with medical backgrounds and clinician qualification certificates. This work took about 2 months, and since the work was done by internal staff, the estimated cost was around 100,000 RMB in total. The Chinese Diagnostic Normalization Data Set (CHIP-CDN) is completed by one round of labeling, one round of full audit, and one round of random quality inspection. Labeling and review are completed by ordinary labeling personnel with clinical qualifications, and random quality inspections are completed by high-level terminology experts.</p><p>PII and IRB The corpus is collected from EMR(electronic medical records), and only the final diagnoses part is chosen for research purposes. The dataset does not refer to ethics.</p><p>As shown in the example table, the final diagnosis has no PII included.</p><p>The original dataset format is a self-defined xlsx format. To unify the data pre-processing step, the CBLUE team has converted the data format to the JSON format with the permission of the data provider.</p><p>Evaluation Metrics The F1 score is calculated with (original diagnosis terms, standard phrases) pairs. Say, if the test set has m golden pairs, and the predicted result has n pairs, where k pairs are predicted correctly, then: P = k/n, R = k/m, F 1 = 2 * P * R/(P + R).</p><p>(1)</p><p>Dataset Statistic 8,000 training instances and 10,000 testing instances are provided. We split the original training set into 6,000 and 2,000 for the training and validation set, respectively.</p><p>Dataset Provider The dataset is provided by Yidu Cloud Technology Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Clinical Trial Criterion Dataset (CHIP-CTC)</head><p>Task Background Clinical trials refer to scientific research conducted by human volunteers to determine the efficacy, safety, and side effects of a drug or a treatment method. It plays a crucial role in promoting the development of medicine and improving human health. Depending on the purpose of the experiment, the subjects may be patients or healthy volunteers. The goal of this task is to predict whether a subject meets a clinical trial or not. Recruitment of subjects for clinical trials is generally done through manual comparison of medical records and clinical trial screening criteria, which is time-consuming, laborious, and inefficient. In recent years, methods based on natural language processing have got successful in many biomedical applications. This task is proposed with the purpose of automatically classifying clinical trial eligibility criteria for the Chinese language, and the original task is released at the CHIP2019 conference. All the data comes from real clinical trials collected from the website of the Chinese Clinical Trial Registry (ChiCTR) 16 , which is a non-profit organization providing registration for public research use. Each</p><p>Task Description A total of 44 pre-defined semantic categories are defined for this task, and the goal is to predict a given text to the correct category. For the detailed annotation instructions, please refer to the CBLUE official website. Examples of labeled data are shown in <ref type="table" target="#tab_0">Table 10</ref>.</p><p>Annotation Process The CHIP-CTC corpus was annotated by three annotators. The first annotator is Zuofeng Li, a principal scientist in Philips Research China, with more than a decade of research   Jinxuan Yang (Ph.D. candidate) in the biomedical informatics field from Tongji University. The annotation started in July 2019 and took about 1 month. Further, the corpus was used in the CHIP 2019 shared task. The annotation was related to the annotator's research project, and no payment was required.</p><p>One experienced biomedical researcher (Z.L) and two raters (Z.Z and J.Y, Ph.D. candidate for biomedical informatics) of biomedical domains labeled the CHIP-CTC corpus with the 44 categories. First, they studied these categories' definitions, investigated a large number of expression patterns of criteria sentences, and chose criteria examples of each category. Next, the two raters independently annotated the same 1000 sentences, then they checked annotations and discussed contradictions with Z.L until consensus was achieved. This step repeated 20 iterations, and 20000 criteria sentences were annotated, which were later used to cal-culate the inter-annotator agreement score (0.9920 by Cohen's kappa score). Finally, the remaining 18341 sentences were assigned to the two raters for annotation.</p><p>PII and IRB The corpus is collected from the Chinese Clinical Trial Registry (ChiCTR) website, which is a non-profit organization providing registration for public research use. For each registered clinical trial case on this website, it is already approved by the ethics committee of the organization. In addition, the annotation and corpus have also been reviewed and approved by Internal Committee on Biomedical Experiments (ICBE) in Philips. It is encouraged to use the corpus for academic research.</p><p>For each registered clinical trial report, no PII is included.</p><p>The original dataset format is a self-defined csv format. To unify the data pre-processing step, the CBLUE team has converted the data format to the JSON format with the permission of the data provider.</p><p>Evaluation Metrics The evaluation of this task uses Macro-F1. Suppose we have n categories, C 1 , ..., C i , ..., C n . The accuracy rate P i is the number of records correctly predicted to class C i / the number of records predicted to be class C i . Recall rate R i = the number of records correctly predicted as the class C i / the number of records of the real C i class.</p><formula xml:id="formula_3">Average ? F 1 = (1/n) n i=1 2 * P i * Ri P i + Ri (2)</formula><p>Dataset Statistic This task has 22,962 training sets, 7,682 validation sets, and 10000 test sets.</p><p>Dataset Provider The dataset is provided by the School of Life Sciences and Technology, Tongji University, and Philips Research China.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Semantic Textual Similarity Dataset (CHIP-STS)</head><p>Task Background CHIP-STS task aims to learn similar knowledge between disease types based on the Chinese online medical questions. Specifically, given question pairs from 5 different diseases, it is required to determine whether the semantics of the two sentences are similar or not. The originally shared task was released at the CHIP2019 conference.</p><p>Task Description The category represents the name of the disease type, including diabetes, hypertension, hepatitis, aids, and breast cancer. The label indicates whether the semantics of the questions are the same. If they are the same, they are marked as 1, and if they are not the same, they are marked as 0. Examples of labeling are shown in <ref type="table" target="#tab_0">Table 11</ref>.</p><p>Annotation Process The CHIP-STS corpus was annotated by five undergraduate annotators from medical colleges under the guidance of one surgeon and one physician. The task is relatively simple since it is a two-class classification one; the annotation process, as well as the time of verification, lasts for two weeks. A total of 30,000 sentences pairs are annotated, and the annotation expense is 25,000 RMB. There are five types of diseases, so each annotator was assigned two types of disease to the label to guarantee that each type of disease was annotated by two raters. During the trail annotation process, each annotator was given 100 records to label, which aimed to test if they could understand the tasks thoroughly. Following that, the annotators start to label the process, and medical experts would give necessary help, like explaining the disease mechanism to assist the raters. Finally, each record was labeled by two different labelers, and the disagreed pairs were selected for discussion and case study; the annotators would recheck the previous labeled results according to the experts' feedback. The IAA score was 0.93.</p><p>PII and IRB The corpus is collected from online questions from the medical forum, and it doesn't refer to the ethics, which has been checked by the IRB committee of the provider.</p><p>During the annotation step, sentences with PHI information are discarded by the annotators manually. The CBLUE team has also validated the dataset record by record to guarantee there is no PII included.</p><p>The original dataset format is a self-defined csv format. To unify the data pre-processing step, the CBLUE team has converted the data format to the JSON format with the permission of the data provider.</p><p>Evaluation Metrics The evaluation of this task is Macro-F1.</p><p>Dataset Statistic This task has 16,000 training sets, 4,000 validation sets, and 10,000 tests set data.</p><p>Dataset Provider The dataset is provided by Ping An Technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 KUAKE-Query Intent Classification</head><p>Dataset (KUAKE-QIC)</p><p>Task Background In medical search scenarios, the understanding of query intent can significantly improve the relevance of search results. In particular, medical knowledge is highly specialized, and classifying query intentions can also help integrate medical knowledge to enhance the performance of search results. This task is proposed for this purpose.   <ref type="table" target="#tab_0">Table 12</ref>.</p><p>Annotation Process The KUAKE-QIC corpus was annotated by six annotators who graduated from medical college; they were employed by Alibaba as full-time employees for the KUAKE department. They must get past the test for the specified annotation tasks before the annotation starts. This task cost about 2 weeks, and the annotation fee was 6,600 RMB with 22,000 labeled records, that's to say, 0.3 RMB / per record. The annotation process was divided into three steps:</p><p>The first step was the trail annotation step; 2,000 records were selected for this stage. The annotators were grouped into 2 groups, each with 3 persons. The data provider had a strict metric for quality control, say, the IAA between the three persons within the same group must exceed 0.9.</p><p>The second stage is the formal annotation phase, and during this stage, 6 annotators were divided into three groups, each with 2 persons. A total of 20,000 records were annotated; IAA for this step was 0.9230.</p><p>The last step was the quality control step, the sampling strategy was adopted, and 300 records were sampled for validation; some common annotation problems were raised by the medical experts, and the data would be fixed in a batch mode. In addition, some disagreed cases were made final decisions by the medical experts.</p><p>PII and IRB The corpus is collected from user queries from the KUAKE search engine, and it doesn't refer to the ethics, which has been checked by the IRB committee of the provider.</p><p>During the annotation step, sentences with PHI information or offensive information (like sexual queries) are discarded by the annotators manually.</p><p>The dataset also got passed the data disclosure process of Alibaba.</p><p>The CBLUE team has also validated the dataset record by record to guarantee there is no PII included.</p><p>Evaluation Metrics Accuracy is used for the evaluation of this task. Task Description The correlation between Query and Title is divided into 4 levels (0-3), 0 is the worst, and 3 stands for the best match. For the detailed annotation instructions, please refer to the CBLUE official website. Examples are shown in <ref type="table" target="#tab_0">Table 13</ref>.</p><p>Annotation Process The KUAKE-QTR corpus was annotated by a total of nine annotators, among which seven were from third-party crowd-sourcing undergraduates from medical colleges, and two were from Alibaba full-time employees with medical backgrounds. The crowd-sourcing annotators were required to get trained and pass the annotation test before they could execute the task. The annotations lasted for 2 weeks, and a total of 28,000 RMB was used.   Similar to the KUAKE-QIC task, the KUAKE-QTR annotation process was divided into three steps with minor changes:</p><p>The training and examination stage: The seven annotators got trained by the two FTE (full-time employee) experts to understand the tasks, then each one was given 200 records to label, which have ground-truth answer annotated by FTE experts. The precision must be above 85% to get past the test.</p><p>The second step was the formal annotation step, and Each annotator was given 3,000 records to label, among which 100 were with golden labels. The annotation tools would automatically evaluate the annotation quality by comparing the label between the annotators' ones and the golden ones. Help would be given to the annotators if necessary. Only the precision exceeding the threshold 0.85 would be handed to the next round.</p><p>The last step was the quality control step, the sampling strategy was adopted, and 100 records were sampled for validation by the FTE medical experts; bad cases would be returned to the crowdsourcing annotators to be fixed.</p><p>PII and IRB The corpus is collected from user queries from the KUAKE search engine, and it doesn't refer to the ethics, which has been checked by the IRB committee of the provider.</p><p>During the annotation step, sentences with PHI information or offensive information (like sexual queries) are discarded by the annotators manually. The dataset also got passed the data disclosure process of Alibaba.</p><p>The CBLUE team has also validated the dataset record by record to guarantee there is no PII included. One record with the NULL label was discarded with the permission of the provider.</p><p>Evaluation Metrics Same as the KUAKE-QIC task, accuracy is used for the evaluation of this task.</p><p>Dataset Statistic This task has 24,174 training set data, 2,913 validation set data, and 54,65 test set data.</p><p>Dataset Provider This dataset is provided by Alibaba QUAKE Search Engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8 KUAKE -Query Query Relevance</head><p>Dataset (KUAKE-QQR)</p><p>Task Background KUAKE Query-Query Relevance is a dataset that evaluates the relevance between two given queries to resolve the long-tail challenges for search engines. Similar to KUAKE-QTR, query-query relevance is an essential and challenging task in real-world search engines.</p><p>Task Description The correlation between Query and Title is divided into 3 levels (0-2), 0 is the worst, and 2 stands for the best correlation. For the detailed annotation instructions, please refer to the CBLUE official website. Examples are shown in <ref type="table" target="#tab_0">Table 14</ref>.</p><p>Annotation Process The same as KUAKE-QTR except for the expense, which is 22,000 RMB in total.</p><p>PII and IRB The same as KUAKE-QTR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Error Analysis for Other Tasks</head><p>We introduce the error definition as follows and illustrate some error cases for other tasks in <ref type="table" target="#tab_8">Table  27</ref> to 32.</p><p>Ambiguity indicates that the instance has a similar context but different meaning, which mislead the prediction.</p><p>Need domain knowledge indicates that there exist biomedical terminologies in the instance which require domain knowledge to understand.</p><p>Need syntactic knowledge indicates that there exists complex syntactic structure in the instance, and the model fails to understand the correct meaning.</p><p>Entity overlap indicates there exist multiple overlapping entities in the instance.</p><p>Long sequence indicates that the input instance is very long.</p><p>Annotation error indicates that the annotated label is wrong.</p><p>Wrong entity boundary indicates that the instance has the wrong entity boundary.</p><p>Rare words indicates that there exist lowfrequency words in the instance.</p><p>Multiple triggers indicates that there exist multiple indicative words which mislead the prediction.</p><p>Colloquialism (very common in the search queries) indicates that the instance is quite different         <ref type="table" target="#tab_0">Table 21</ref>: Hyper-parameters for the training of pre-trained models with a sequence classifier for the ranking model of the CHIP-CDN task. We encode the pairs of the original term and standard phrase from candidates recalled during the recall stage and then pass the pooled output to the classifier, which predicts the relevance between the original term and standard phrase.  <ref type="table">Table 22</ref>: Hyper-parameters for the training of pre-trained models with a sequence classifier for the prediction of the number of standard phrases corresponding to the original term in the CHIP-CDN task.    <ref type="table" target="#tab_10">Table 28</ref>: Error cases in CHIP-CDN. We evaluate roberta-wwm-ext and PCL-MedBERT on 3 sampled sentences, with their gold labels and model predictions. There may be multiple predicted values, separated by a "##". RO = roberta-wwm-ext, MB = PCL-MedBERT.   <ref type="table" target="#tab_2">Table 30</ref>: Error cases in CHIP-STS. We evaluate performance of baselines with 3 sampled instances. The similarity between queries is divided into 2 levels (0-1), which means 'unrelated' and 'related'. BE = BERT-base, BE+ = BERT-wwm-ext-base, MB = PCL-MedBERT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Analysis of the named entity recognition and information extraction datasets. (a) illustrates the entity (coarse-grained) distribution in CMeEE and the impact of data distribution on the model's performance. We set entity type Body with the maximum number of entities to 1.0, and others to the ratio of number or F1 score to Body. (b) shows the relation hierarchy in CMeIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ments. This work is funded by Special Project of New Generation Artificial Intelligence of the Ministry of Science and Technology of China (2021ZD0113402), National Natural Science Foundations of China (61876052 and U1813215), National Natural Science Foundation of Guangdong, China (2019A1515011158), Strategic Emerging Industry Development Special Fund of Shenzhen (20200821174109001), Pilot Project in 5G + Health Application of Ministry of Industry and Information Technology &amp; National Health Commission (5G + Luohu Hospital Group: an Attempt to New Health Management Styles of Residents), Zhengzhou collaborative innovation major special project (20XTZX11020), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Task descriptions and statistics in CBLUE. CMeEE and CMeIE are sequence labeling tasks. Others are single sentence or sentence pair classification tasks.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell></cell><cell></cell><cell cols="2">Train Dev</cell><cell>Test</cell><cell>Metrics</cell></row><row><cell>CMeEE</cell><cell>NER</cell><cell></cell><cell></cell><cell cols="2">15,000 5,000 3,000</cell><cell>Micro F1</cell></row><row><cell>CMeIE</cell><cell cols="3">Information Extraction</cell><cell cols="2">14,339 3,585 4,482</cell><cell>Micro F1</cell></row><row><cell>CHIP-CDN</cell><cell cols="3">Diagnosis Normalization</cell><cell>6,000</cell><cell>2,000 10,192 Micro F1</cell></row><row><cell>CHIP-STS</cell><cell cols="2">Sentence Similarity</cell><cell></cell><cell cols="2">16,000 4,000 10,000 Macro F1</cell></row><row><cell>CHIP-CTC</cell><cell cols="3">Sentence Classification</cell><cell cols="2">22,962 7,682 10,000 Macro F1</cell></row><row><cell cols="3">KUAKE-QIC Intent Classification</cell><cell></cell><cell>6,931</cell><cell>1,955 1,994</cell><cell>Accuracy</cell></row><row><cell cols="6">KUAKE-QTR Query-Document Relevance 24,174 2,913 5,465</cell><cell>Accuracy</cell></row><row><cell cols="4">KUAKE-QQR Query-Query Relevance</cell><cell cols="2">15,000 1,600 1,596</cell><cell>Accuracy</cell></row><row><cell cols="6">Benchmark Language Domain Data Distribution</cell><cell>Label Distribution</cell></row><row><cell>CBLUE</cell><cell>Chinese</cell><cell cols="4">medical long-tailed (CMeEE) non-i.i.d (CHIP-STS)</cell></row><row><cell>CLUE</cell><cell>Chinese</cell><cell>general</cell><cell>uniform</cell><cell></cell><cell>i.i.d</cell></row><row><cell>BLURB</cell><cell>English</cell><cell cols="2">medical uniform</cell><cell></cell><cell>i.i.d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of baseline models on CBLUE benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Query</cell></row></table><note>Case studies in CMeEE. We evaluate roberta-wwm-ext and PCL-MedBERT on 3 sampled sentences, with their gold labels and model predictions. Ite (medical examination items), Pro (medical procedure), Bod (body), and Sym (clinical symptoms) are labeled for medical named words. O means that the model fails to extract the entity from sentences. RO=roberta-wwm-ext, MB=PCL-MedBERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text. Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pretrained models for chinese natural language processing. arXiv preprint arXiv:2004.13922. Shijin Wang, and Guoping Hu. 2019. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101. Hal Daum? III, and Kate Crawford. 2018. Datasheets for datasets. CoRR, abs/1803.09010. Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing. CoRR, abs/2007.15779. T. Guan, H. Zan, X. Zhou, H. Xu, and K Zhang. 2020. CMeIE: Construction and Evaluation of Chinese Medical Information Extraction Dataset. Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 9241-9250. Association for Computational Linguistics. Hui Zong, Jinxuan Yang, Zeyu Zhang, Zuofeng Li, and Xiaoyan Zhang. 2021. Semantic categorization of chinese eligibility criteria in clinical trials using machine learning methods. BMC Medical Informatics Decis. Mak., 21(1):128.</figDesc><table><row><cell>Nat-</cell><cell></cell></row><row><cell>ural Language Processing and Chinese Computing,</cell><cell></cell></row><row><cell>9th CCF International Conference, NLPCC 2020,</cell><cell></cell></row><row><cell>Zhengzhou, China, October 14-18, 2020, Proceed-</cell><cell></cell></row><row><cell>ings, Part I.</cell><cell></cell></row><row><cell>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W.</cell><cell></cell></row><row><cell>Cohen, and Xinghua Lu. 2019. Pubmedqa: A</cell><cell></cell></row><row><cell>dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empiri-cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2567-2577. Association for Computational Linguistics.</cell><cell>In Proceedings of the 2019 Conference on Empiri-cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3613-3618. Association for Computational Linguistics.</cell></row><row><cell>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learn-ing of language representations. arXiv preprint arXiv:1909.11942.</cell><cell>Alexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representa-tions. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. Eu-ropean Language Resources Association (ELRA).</cell></row><row><cell></cell><cell>Yiming Cui, Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin,</cell></row><row><cell></cell><cell>Ziqing Yang, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and</cell></row><row><cell></cell><cell>Kristina Toutanova. 2018. Bert: Pre-training of deep</cell></row><row><cell></cell><cell>bidirectional transformers for language understand-</cell></row><row><cell></cell><cell>ing. In NAACL-HLT.</cell></row><row><cell></cell><cell>Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and</cell></row><row><cell></cell><cell>Yonggang Wang. 2019. Zen: pre-training chinese</cell></row><row><cell></cell><cell>text encoder enhanced by n-gram representations.</cell></row><row><cell></cell><cell>arXiv preprint arXiv:1911.00720.</cell></row><row><cell></cell><cell>Joseph L Fleiss. 1971. Measuring nominal scale agree-</cell></row><row><cell></cell><cell>ment among many raters. Psychological bulletin,</cell></row><row><cell></cell><cell>76(5):378.</cell></row><row><cell></cell><cell>Timnit Gebru, Jamie Morgenstern, Briana Vec-</cell></row><row><cell></cell><cell>chione, Jennifer Wortman Vaughan, Hanna M. Wal-</cell></row><row><cell></cell><cell>lach, Pieter Gijsbers, Erin LeDell, Janek Thomas, S?bastien</cell></row><row><cell></cell><cell>Poirier, Bernd Bischl, and Joaquin Vanschoren.</cell></row><row><cell></cell><cell>2019. An open source automl benchmark. arXiv</cell></row><row><cell></cell><cell>preprint arXiv:1907.00909.</cell></row></table><note>Yu Gu, Robert Tinn, Hao Cheng, Michael Lu- cas, Naoto Usuyama, Xiaodong Liu, Tristan Nau- mann,Hyukki Lee, Soohyung Kim, Jong Wook Kim, and Yon Dohn Chung. 2017. Utility-preserving anonymization for health data publishing. BMC Medical Informatics Decis. Mak., 17(1):104:1- 104:12. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomed- ical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240. Patrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy- anov. 2020. Pretrained language models for biomed- ical and clinical tasks: Understanding and extend- ing the state-of-the-art. In Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 146-157, Online. Association for Computa- tional Linguistics.on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Examples in CMeEE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>This task has 15,000 training set data, 5,000 validation set data, 3,000 test set data. The corpus contains 938 files and 47,194 sentences. The average number of words contained per file is 2,355. The dataset contains 504 common pediatric diseases, 7,085 body parts, 12,907 clinical symptoms, and 4,354 medical procedures in total.</figDesc><table><row><cell>Dataset Provider The dataset is provided by:</cell></row><row><cell>? Key Laboratory of Computational Linguis-</cell></row><row><cell>tics, Peking University, Ministry of Education,</cell></row><row><cell>China</cell></row><row><cell>? Laboratory of Natural Language Processing,</cell></row><row><cell>Zhengzhou University, China</cell></row><row><cell>? The Research Center for Artificial Intelli-</cell></row><row><cell>gence, Peng Cheng Laboratory, China</cell></row><row><cell>? Harbin Institute of Technology, Shenzhen,</cell></row><row><cell>China</cell></row><row><cell>E.2 Chinese Medical Information Extraction</cell></row><row><cell>Dataset (CMeIE)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>shows the examples in the data set, and</cell></row><row><cell>53 SCHEMAs include 10 kinds of genus rela-</cell></row><row><cell>tions, 43 other sub-relations. The details are in</cell></row><row><cell>the 53_schema.json file. For the detailed annota-</cell></row><row><cell>tion instructions, please refer to the CBLUE official</cell></row><row><cell>website, and examples are shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell>Examples in CMeIE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Examples in CHIP-CDN</figDesc><table><row><cell cols="2">ID Clinical trial sentence</cell><cell>Category</cell></row><row><cell>S1</cell><cell>??&gt;80? Age: &gt; 80</cell><cell>Age</cell></row><row><cell>S2</cell><cell>??????????? Recent intracranial/intraspinal surgery</cell><cell>Therapy or Surgery</cell></row><row><cell>S3</cell><cell>??&lt;2.7mmol/L Blood glucose &lt; 2.7 mmol/L</cell><cell>Laboratory Examinations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Examples in CHIP-CTC</cell></row><row><cell>experience in the biomedical domain. Other an-</cell></row><row><cell>notators were Zeyu Zhang (Ph.D. candidate) and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: Examples in CHIP-STS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Examples in KUAKE-QIC</figDesc><table><row><cell>Query</cell><cell>Title</cell><cell>Level</cell></row><row><cell>????b??? Symptoms of Vitamin B deficiency</cell><cell>???b???????? Vitamin B deficiency? What are the major symptoms of</cell><cell>3</cell></row><row><cell>??????????</cell><cell>??????????</cell><cell></cell></row><row><cell>How can I treat a soft tissue</cell><cell>What's the treatment for a soft tissue</cell><cell>2</cell></row><row><cell>injury in the thigh?</cell><cell>injury in the leg?</cell><cell></cell></row><row><cell>???????????? What causes lower leg cramps?</cell><cell>??????????? leg cramps? How can I treat pains caused by lower</cell><cell>1</cell></row><row><cell>?????????? What is the cause of picky eating?</cell><cell>?????????? What is the cause of picky eating?</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13 :</head><label>13</label><figDesc>Examples in KUAKE-QTR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Evaluation Metrics Same with the KUAKE-QIC and KUAKE-QTR tasks, accuracy is used for the evaluation metrics.This section details the training procedures and hyper-parameters for each of the data sets. We utilize Pytorch to conduct experiments, and all running hyper-parameters are shown in the following Tables. There are two stages in CMeIE, namely,</figDesc><table><row><cell>F Experiments Details</cell></row><row><cell>entity recognition (CMeEE-ER) and relation clas-</cell></row><row><cell>sification (CMeEE-RE). So we detail the hyper-</cell></row><row><cell>parameters in CMeEE-ER and CMeEE-RE, respec-</cell></row><row><cell>tively.</cell></row><row><cell>Requirements</cell></row><row><cell>? python3</cell></row><row><cell>? pytorch 1.7</cell></row><row><cell>? transformers 4.5.1</cell></row><row><cell>? jieba</cell></row><row><cell>? gensim</cell></row><row><cell>Hyper-parameters for Specific Task is shown</cell></row><row><cell>in Table 15-26</cell></row><row><cell>Dataset Statistic This task has 15,000 training</cell></row><row><cell>set data, 1,600 validation set data, and 1,596 test</cell></row><row><cell>set data.</cell></row><row><cell>Dataset Provider This dataset is provided by Al-</cell></row><row><cell>ibaba QUAKE Search Engine.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Examples in KUAKE-QQR</figDesc><table><row><cell>Method</cell><cell>Value</cell></row><row><cell>warmup_proportion</cell><cell>0.1</cell></row><row><cell>weight_decay</cell><cell>0.01</cell></row><row><cell>adam_epsilon</cell><cell>1e-8</cell></row><row><cell>max_grad_norm</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>Common hyper-parameters for all CBLUE tasks from written language (e.g., with many abbreviations), thus, challenging the prediction model.Irrelevant description indicates that the instance has lots of irrelevant information, which mislead the prediction. China contributed the dataset of CMeIE.Linfeng Li, Jun Yan from Yidu Cloud Technology Inc., Beijing, China contributed the dataset of CHIP-CDN.Hui Zong from School of Life Sciences and Technology, Tongji University and Philips Research China contributed the dataset of CHIP-CTC.</figDesc><table><row><cell>Contributions</cell></row><row><cell>Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Lei</cell></row><row><cell>Li from Zhejiang University, AZFT Joint Lab for</cell></row><row><cell>Knowledge Engine, Hangzhou Innovation Center</cell></row><row><cell>wrote the paper.</cell></row><row><cell>Mosha Chen, Chuanqi Tan, Fei Huang, Luo</cell></row><row><cell>Si from Alibaba Group and Zheng Yuan from</cell></row><row><cell>the Center for Statistical Science, Tsinghua Uni-</cell></row><row><cell>versity contributed the CBLUE benchmark leader-</cell></row><row><cell>board and transformed the eight datasets from self-</cell></row><row><cell>defined data format to unified JSON format.</cell></row><row><cell>Kunli Zhang from School of Information Engi-</cell></row><row><cell>neering, Zhengzhou University, Peng Cheng Lab-</cell></row><row><cell>oratory, China and Baobao Chang from Key Lab-</cell></row><row><cell>oratory of Computational Linguistics, Ministry of</cell></row><row><cell>Education, Peking University, Peng Cheng Labora-</cell></row><row><cell>tory, China contributed the dataset of CMeEE.</cell></row><row><cell>Hongying Zan from School of Information En-</cell></row><row><cell>gineering, Zhengzhou University, Peng Cheng Lab-</cell></row><row><cell>oratory, China and Zhifang Sui from Key Labo-</cell></row><row><cell>ratory of Computational Linguistics, Ministry of</cell></row><row><cell>Education, Peking University, Peng Cheng Labora-</cell></row><row><cell>tory,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 16 :</head><label>16</label><figDesc>Hyper-parameters for the training of pre-trained models with a token classification head on top for named entity recognition of the CMeEE task.</figDesc><table><row><cell>Model</cell><cell cols="4">epoch batch_size max_length learning_rate</cell></row><row><cell>bert-base</cell><cell>7</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>bert-wwm-ext</cell><cell>7</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>roberta-wwm-ext</cell><cell>7</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>roberta-wwm-ext-large</cell><cell>7</cell><cell>16</cell><cell>80</cell><cell>4e-5</cell></row><row><cell>roberta-large</cell><cell>7</cell><cell>16</cell><cell>80</cell><cell>2e-5</cell></row><row><cell>albert-tiny</cell><cell>10</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>albert-xxlarge</cell><cell>7</cell><cell>16</cell><cell>80</cell><cell>1e-5</cell></row><row><cell>zen</cell><cell>7</cell><cell>20</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-base</cell><cell>7</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-large</cell><cell>7</cell><cell>20</cell><cell>80</cell><cell>2e-5</cell></row><row><cell>PCL-MedBERT</cell><cell>7</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 17 :</head><label>17</label><figDesc>Hyper-parameters for the training of pre-trained models with a token-level classifier for subject and object recognition of the CMeIE task.</figDesc><table><row><cell>Model</cell><cell cols="4">epoch batch_size max_length learning_rate</cell></row><row><cell>bert-base</cell><cell>8</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>bert-wwm-ext</cell><cell>8</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>roberta-wwm-ext</cell><cell>8</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>roberta-wwm-ext-large</cell><cell>8</cell><cell>16</cell><cell>80</cell><cell>4e-5</cell></row><row><cell>roberta-large</cell><cell>8</cell><cell>16</cell><cell>80</cell><cell>2e-5</cell></row><row><cell>albert-tiny</cell><cell>10</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>albert-xxlarge</cell><cell>8</cell><cell>16</cell><cell>80</cell><cell>1e-5</cell></row><row><cell>zen</cell><cell>8</cell><cell>20</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-base</cell><cell>8</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-large</cell><cell>8</cell><cell>20</cell><cell>80</cell><cell>2e-5</cell></row><row><cell>PCL-MedBERT</cell><cell>8</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 18 :</head><label>18</label><figDesc>Hyper-parameters for the training of pre-trained models with a classifier for the entity pairs relation prediction of the CMeIE task.</figDesc><table><row><cell>Model</cell><cell cols="4">epoch batch_size max_length learning_rate</cell></row><row><cell>bert-base</cell><cell>5</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>bert-wwm-ext</cell><cell>5</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>roberta-wwm-ext</cell><cell>5</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>roberta-wwm-ext-large</cell><cell>5</cell><cell>20</cell><cell>50</cell><cell>3e-5</cell></row><row><cell>roberta-large</cell><cell>5</cell><cell>20</cell><cell>50</cell><cell>4e-5</cell></row><row><cell>albert-tiny</cell><cell>10</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>albert-xxlarge</cell><cell>5</cell><cell>20</cell><cell>50</cell><cell>1e-5</cell></row><row><cell>zen</cell><cell>5</cell><cell>20</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-base</cell><cell>5</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-large</cell><cell>5</cell><cell>20</cell><cell>50</cell><cell>2e-5</cell></row><row><cell>PCL-MedBERT</cell><cell>5</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 19 :</head><label>19</label><figDesc>Hyper-parameters for the training of pre-trained models with a sequence classification head on top for screening criteria classification of the CHIP-CTC task.</figDesc><table><row><cell>Param</cell><cell>Value</cell></row><row><cell>recall_k</cell><cell>200</cell></row><row><cell>num_negative_sample</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 20 :</head><label>20</label><figDesc>Hyper-parameters for the CHIP-CDN task. We model the CHIP-CDN task with two stages: recall stage and ranking stage. num_negative_sample sets the number of negative samples sampled for the training ranking model during the ranking stage. recall_k sets the number of candidates recalled in the recall stage.</figDesc><table><row><cell>Model</cell><cell cols="4">epoch batch_size max_length learning_rate</cell></row><row><cell>bert-base</cell><cell>3</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>bert-wwm-ext</cell><cell>3</cell><cell>32</cell><cell>128</cell><cell>5e-5</cell></row><row><cell>roberta-wwm-ext</cell><cell>3</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>roberta-wwm-ext-large</cell><cell>3</cell><cell>32</cell><cell>40</cell><cell>4e-5</cell></row><row><cell>roberta-large</cell><cell>3</cell><cell>32</cell><cell>40</cell><cell>4e-5</cell></row><row><cell>albert-tiny</cell><cell>3</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>albert-xxlarge</cell><cell>3</cell><cell>32</cell><cell>40</cell><cell>1e-5</cell></row><row><cell>zen</cell><cell>3</cell><cell>20</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-base</cell><cell>3</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row><row><cell>macbert-large</cell><cell>3</cell><cell>32</cell><cell>40</cell><cell>2e-5</cell></row><row><cell>PCL-MedBERT</cell><cell>3</cell><cell>32</cell><cell>128</cell><cell>4e-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 25 :</head><label>25</label><figDesc>Hyper-parameters of training the sequence classifier for the KUAKE-QTR task.</figDesc><table><row><cell>Model</cell><cell cols="4">epoch batch_size max_length learning_rate</cell></row><row><cell>bert-base</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>3e-5</cell></row><row><cell>bert-wwm-ext</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>3e-5</cell></row><row><cell>roberta-wwm-ext</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>3e-5</cell></row><row><cell>roberta-wwm-ext-large</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>3e-5</cell></row><row><cell>roberta-large</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>2e-5</cell></row><row><cell>albert-tiny</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>5e-5</cell></row><row><cell>albert-xxlarge</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>3e-5</cell></row><row><cell>zen</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>2e-5</cell></row><row><cell>macbert-base</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>2e-5</cell></row><row><cell>macbert-large</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>2e-5</cell></row><row><cell>PCL-MedBERT</cell><cell>3</cell><cell>16</cell><cell>30</cell><cell>2e-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 26 :</head><label>26</label><figDesc>Hyper-parameters of training the sequence classifier for the KUAKE-QQR task.</figDesc><table><row><cell>Sentence</cell><cell>Golden</cell><cell>RO</cell><cell>ME</cell></row><row><cell>???????????</cell><cell>???????| ??</cell><cell>?????|???</cell><cell>?????| ???</cell></row><row><cell>???????????</cell><cell>??| ???</cell><cell>?|???</cell><cell>?|???</cell></row><row><cell>??</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Another study showed that</cell><cell>medial knee</cell><cell>medial knee</cell><cell>medial knee</cell></row><row><cell>load-reducing shoes were</cell><cell>osteoarthritis, adjuvant</cell><cell>osteoarthritis, adjuvant</cell><cell>osteoarthritis, adjuvant</cell></row><row><cell>not effective for medial knee</cell><cell>therapy, load-reducing</cell><cell>therapy, load-reducing</cell><cell>therapy, load-reducing</cell></row><row><cell>osteoarthritis.</cell><cell>shoes</cell><cell>shoes</cell><cell>shoes</cell></row><row><cell>???????????</cell><cell>??|??(??)|?</cell><cell>?|?|?</cell><cell>??|??(??)|?</cell></row><row><cell>????????</cell><cell>??</cell><cell></cell><cell>??</cell></row><row><cell>Mental illness: anxiety and</cell><cell>anxiety, related cause,</cell><cell>None|None|None</cell><cell>anxiety, related cause,</cell></row><row><cell>depression are related to in-</cell><cell>insomnia</cell><cell></cell><cell>insomnia</cell></row><row><cell>somnia.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>???????????</cell><cell>???|??(?</cell><cell>?|?|?</cell><cell>?|?|?</cell></row><row><cell>??????</cell><cell>?)|??</cell><cell></cell><cell></cell></row><row><cell>In the late stage of rabies</cell><cell>rabies, transform,</cell><cell>None|None|None</cell><cell>None|None|None</cell></row><row><cell>infection, patients often ap-</cell><cell>comatose</cell><cell></cell><cell></cell></row><row><cell>pear comatose.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 27 :</head><label>27</label><figDesc>Error cases in CMeIE. We evaluate roberta-wwm-ext and PCL-MedBERT on 3 sampled sentences, with their gold labels and model predictions. Each label consists of subject | predicate | Object. None means that the model fails to predict. RO = roberta-wwm-ext, MB = PCL-MedBERT.</figDesc><table><row><cell>Sentence</cell><cell>Label</cell><cell>RO</cell><cell>MB</cell></row><row><cell>????????????</cell><cell>????</cell><cell>????</cell><cell>????</cell></row><row><cell>Right first toe traumatic toe cutting</cell><cell>Single toe cut</cell><cell>Toe injury</cell><cell>Single toe cut</cell></row><row><cell>C3-4????</cell><cell>??????</cell><cell>????</cell><cell>????</cell></row><row><cell>C3-4 spinal cord injury</cell><cell>Neck spinal cord</cell><cell>Spinal cord injury</cell><cell>Spinal cord injury</cell></row><row><cell></cell><cell>injury</cell><cell></cell><cell></cell></row><row><cell>???????</cell><cell>??????</cell><cell>?????##??</cell><cell>?????##??</cell></row><row><cell></cell><cell>?##????</cell><cell>???##??</cell><cell>???##??</cell></row><row><cell></cell><cell>?##??</cell><cell></cell><cell></cell></row><row><cell>Tumor bone metastatic gastritis</cell><cell>Junior malignant</cell><cell>Reflux</cell><cell>Pelvic</cell></row><row><cell></cell><cell>tumor##Metastatic</cell><cell>gastritis##Metastatic</cell><cell>tumor##Metastatic</cell></row><row><cell></cell><cell>tumor##Gastritis</cell><cell>tumor##Gastritis</cell><cell>tumor##Gastritis</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>Table 29 :</head><label>29</label><figDesc>Error cases in CHIP-CTC. We evaluate roberta-wwm-ext and PCL-MedBERT on 3 sampled sentences, with their gold labels and model predictions. RO = roberta-wwm-ext, MB = PCL-MedBERT.</figDesc><table><row><cell>Query-A</cell><cell>Query-B</cell><cell cols="3">Model BE BE+ MB</cell><cell>Gold</cell></row><row><cell>???????????</cell><cell>????????</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>Can sweat spread the hepatitis B</cell><cell>How is hepatitis B transmitted?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>virus?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>????????</cell><cell>???????????</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>What type of diabetes?</cell><cell>What type of diabetes am I?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>????????</cell><cell>????????</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>How to prevent AIDS?</cell><cell>AIDS Prevention and Control</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Regulations.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://chictr.org.cn/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.nhc.gov.cn/ 5 https://www.myquark.cn/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://tianchi.aliyun.com/ notebook-ai/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">http://chictr.org.cn/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Buzhou Tang, Qingcai Chen from Harbin Institute of Technology (Shenzhen), Peng Cheng Laboratory, China advised the project, suggested tasks, and led the research.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dewey</forename><forename type="middle">A</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devvret</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Stilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wilhelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oren Etzioni</title>
		<imprint/>
	</monogr>
	<note>and Sebastian Kohlmeier. 2020. CORD-19: the covid-19 open research dataset. CoRR, abs/2004.10706</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pediatrics 9th edn. People&apos;s Medical Publishing House</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Task-oriented dialogue system for automatic diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaixiao</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangying</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lam</surname></persName>
		</author>
		<editor>RE-COMB</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CLUE: A chinese language understanding evaluation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yechen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yina</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoweihua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-08" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4762" to="4772" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Artificial intelligence in healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun-Hsing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="719" to="731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meddialog: Large-scale medical dialogue datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqian</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.743</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference</title>
		<meeting>the 2020 Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
