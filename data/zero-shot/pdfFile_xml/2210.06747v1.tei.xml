<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCANet: Differential Convolution Attention Network for RGB-D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhi</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Tian</surname></persName>
							<email>tianchunqi@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoru</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoyu</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DCANet: Differential Convolution Attention Network for RGB-D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combining RGB images and the corresponding depth maps in semantic segmentation proves the effectiveness in the past few years. Existing RGB-D modal fusion methods either lack the non-linear feature fusion ability or treat both modal images equally, regardless of the intrinsic distribution gap or information loss. Here we find that depth maps are suitable to provide intrinsic fine-grained patterns of objects due to their local depth continuity, while RGB images effectively provide a global view. Based on this, we propose a pixel differential convolution attention (DCA) module to consider geometric information and localrange correlations for depth data. Furthermore, we extend DCA to ensemble differential convolution attention (EDCA) which propagates long-range contextual dependencies and seamlessly incorporates spatial distribution for RGB data. DCA and EDCA dynamically adjust convolutional weights by pixel difference to enable self-adaptive in local and long range, respectively. A two-branch network built with DCA and EDCA, called Differential Convolutional Network (DCANet), is proposed to fuse local and global information of two-modal data. Consequently, the individual advantage of RGB and depth data are emphasized. Our DCANet is shown to set a new state-of-the-art performance for RGB-D semantic segmentation on two challenging benchmark datasets, i.e., NYUDv2 and SUN-RGBD. * Corresponding author.</p><p>? Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is an essential task in computer vision, which infers semantic labels of every pixel in a scene. With the widespread use of 3D sensors such as Kinect, Xition etc., the 3D geometry information of objects can be easily obtained to boost the advancement of RGB-D <ref type="figure">Figure 1</ref>. The intrinsic differences between RGB and depth data and the illumination of our DCANet. While the chair and the table are inseparable according to the 2D appearance in RGB image, they can be easily distinguished in depth map based on geometric information. In DCANet, we exploit DCA to capture local-range geometric consistency in depth map and EDCA to focus on longrange dependence for RGB. semantic segmentation. After encoding the real-world geometric information, the RGB-D images can be applied to overcome the challenge of 2D only displaying the photometric appearance properties in the projected image space and enrich the representation of RGB images. The information of RGB and depth images are presented in entirely different forms. In particular, RGB images capture the photometric appearance properties in the projected image space, while the depth maps can produce plentiful complementary information for the appearance cues of local geometry. As a result, it is vital to enhance and fuse the strengths of RGB and depth data in semantic segmentation task.</p><p>In a real scenario, there are too many challenging images with complex appearances. Take <ref type="figure">Fig. 1</ref> as an example, while the chair and the table are inseparable in the RGB image, they can be easily distinguished in depth. Obviously, it is not feasible to separate the table with chair using only 2D information such as shapes and colors. While in the view of depth maps, there is local consistency information, which will not be limited by the similar confusing appear-ance. In fact, the depth data provide more fine-grained local geometry difference information and theoretically leading to better segmentation performance compared to only using RGB images. In contrast, as verified in the classic selfattention <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b63">63]</ref> mechanisms that RGB data focuses on more global information.</p><p>The existing methods <ref type="bibr">[3, 9-11, 20, 24, 26, 29, 36, 39]</ref> try to fuse RGB-D data by introducing new convolution layer and pooling layer, attention mechanism, noise-cancelling module, etc., to obtain better semantic segmentation results. These methods ignore the intrinsic differences between RGB and depth features, using homogeneous operators instead. The weights of both types of data are equally treated so as to make the same contribution to the segmentation, which is obviously not appropriate. Besides, the information of RGB images and depth maps is mainly achieved from the combined final channel, where specific semantic information in different channels is not considered.</p><p>To address the aforementioned problems, we propose two attention mechanisms, namely differential convolution attention (DCA) and ensemble differential convolution attention (EDCA) to improve the cross-modal ability between RGB and depth data in semantic segmentation. DCA dynamically augments the standard convolution with a pixel difference term and forces pixels with a similar difference to the center of the kernel to contribute more to the output than other pixels. DCA incorporates local geometric information and improve local-range adaptability for depth data. EDCA absorbs the advantage of dynamic convolution of DCA to propagate long-range contextual dependencies and seamlessly incorporate spatial distribution for RGB data. Meanwhile, both DCA and EDCA avoid common drawbacks such as ignoring adaptability in channel dimension. Our main contributions are summarized as follows.</p><p>? We propose a DCA module which incorporates localrange intricate geometric patterns and enables self-adaptive by considering subtle discrepancy of pixels in local regions for depth data.</p><p>? We extend DCA to EDCA for achieving long-range correlations and seamlessly incorporating spatial distribution for RGB data.</p><p>? Based on DCA and EDCA, we propose a DCANet that achieves a new state-of-the-art performance on NYUDv2 <ref type="bibr" target="#b47">[47]</ref> and SUN-RGBD <ref type="bibr" target="#b48">[48]</ref> datasets. We also provide a detailed analysis of design choices and model variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RGB-D Semantic Segmentation</head><p>With the help of additional depth information, the combination of such two complementary modalities achieves great performance in semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b47">47]</ref>. Many works simply concatenate the features of RGB and depth images to enhance the semantic information of each pixel <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b47">47]</ref>. The fusion method can be classified into three types: early fusion, middle fusion and late fusion. Cao et al. <ref type="bibr" target="#b2">[3]</ref> concatenate the RGB and depth data decomposed by a shape and a base component in the depth feature in the early stage. However, due to the complexity of these two modalities, a single model cannot fit their data well due to their differences. Jiao et al. <ref type="bibr" target="#b26">[27]</ref> design two encoder-decoder modules for fully consideration the RGB and depth information, where both modal are fused in late stage. In this method, the interaction between the different features of RGB and depth data is insufficient, since the rich information of the modalities is gradually compressed and even lost. After overcoming the drawback of early stage and late stage fusion strategy, middle stage fusion performs better by fusing the intermediate information of the two different modalities. Gupta et al. <ref type="bibr" target="#b17">[18]</ref> concatenate the geocentric embedding for depth images and with depth images to contribute the final semantic information in the middle stage. Notably, the distribution gap is reduced in the middle stage fusion strategy, and multi-modal features are combined with ample interaction. As a result, recent studies mainly focus on middle stage fusion. Chen et al. <ref type="bibr" target="#b8">[9]</ref> propose a spatial information-guided convolution, which generates convolution kernels with different sampling distributions to enhance the spatial adaptability of network and receptive field regulation. Chen et al. <ref type="bibr" target="#b9">[10]</ref> unify the most informative crossmodality features from data for both modalities into an efficient representation. Lin et al. <ref type="bibr" target="#b28">[29]</ref> split the image into multiple branches based on geometry information, where each branch of the network semantically segments relevant similar features.</p><p>Our method applies two branches and each branch focuses on extracting modality-specific features, such as color and texture from RGB images and geometric, illuminationindependent features from depth images. To be specific, similar to middle stage fusion, attentive depth features generated by the DCA are fused into the attentive RGB from the EDCA at each of the resolution stages in the encoders. The depth and RGB data focus on local and long range information, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Modules</head><p>What has greatly contributed to the popularity of attention modules is the fact that they can be applied to model the global dependencies of features almost in any stage of the network. Woo et al. <ref type="bibr" target="#b56">[56]</ref> adaptively refined the information in spatial and channel dimensions through the convolutional block attention module. Inspired by the self-attention network in Natural Language Processing <ref type="bibr" target="#b51">[51]</ref>, such selfattention related module achieves widespread focus in computer vision <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b61">61]</ref>. Many researchers focus on the global and local dependencies. In <ref type="bibr" target="#b54">[54]</ref>, Wang et al. pro- pose a non-local model to extend the self-attention to a more general type of non-local filtering method for capturing the long-range dependencies. Fu et al. <ref type="bibr" target="#b14">[15]</ref> propose two attention modules to capture spatial and channel interdependencies, respectively. Cao et al. <ref type="bibr" target="#b3">[4]</ref> propose a lightweight non-local network based on a query independent formulation for global context modeling. Zhu et al. <ref type="bibr" target="#b63">[63]</ref> integrates the features of different levels while considering long-range dependencies and reducing redundant parameters.</p><p>Our method integrates DCA and EDCA to build relationship between different points for depth and RGB deta, respectively. The DCA module supports that the same objects have more substantial depth similarity in a local-range of depth data, and we make use of the pixel-wise difference to force pixels with more consistent geometry to make more contributions to the corresponding output. The EDCA module enables long-range dependencies for RGB data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>RGB-D semantic segmentation requires fusing features from RGB and depth modalities, which are inherently different. Specifically, RGB data has long-range contextual dependencies and global spatial consistency, while depth data contains local geometric consistency. The intrinsic characteristics of the two modalities should be considered separately to identify the strengths of each, while enhanc- ing the two feature representations. To this end, we put forward two attention modules called DCA and EDCA to capture the intrinsic features of depth and RGB data, respectively. In this section, we elaborate the details of the proposed DCA and EDCA, followed by the the description of the proposed differential convolution attention network (DCANet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Differential Convolution Attention</head><p>The attention mechanism can be considered as an adaptive selection process that selects discriminative features based on input features and automatically ignores noisy responses <ref type="bibr" target="#b15">[16]</ref>. The key point of the attention mechanism is to learn the relationship between different points and generate an attention map that indicates the importance of different points. The well-known method for establishing relationship between different points is self-attention mechanism <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b61">61]</ref>, which is used to capture long-range dependence. However, due to its intrinsic properties, the depth data is only relevant in a local region and long-range dependencies may introduce more interference terms. For this, we explore convolution to build relevance and produce attention map by considering a local region in depth data.</p><p>Given a feature map F ? R h?w?c ; h, w, and c are the height, width and the channel of input feature map, respectively. For simplicity, we note X ? R h?w?1 as the input feature map. For each point p ? R 2 on X, the vanilla convolution is calculated as:</p><formula xml:id="formula_0">Y(p) = k?k i=1 K i ? X(p + p i ),<label>(1)</label></formula><p>where p i enumerates the local locations around p. K is the learnable weights of the convolution kernel with the size of k ? k (the bias terms are ignored for simplicity). In Eq.(1), the convolution kernel K of the vanilla convolution is fixed for any input, which cannot perceive the changes of the input dynamically. However, for depth data, we expect the attention map generated by convolution to sense the geometric information on-the-fly while learning the correlations between different points in a local region. To this end, we explore a pixel difference term to weight the vanilla convolution kernel called differential convolution kernel K * :</p><formula xml:id="formula_1">K * i = K i ? exp(?|X(p) ? X(p + p i )|),<label>(2)</label></formula><p>The difference term in K * implies the geometric information in depth data and is then regularized to (0,1], which ensures that the larger the difference between any two points, the smaller the correlation, and vice versa. It is intuitive that the depth at a point is locally continuous. With the blessing of the difference term, the differential convolution kernel K * depends not only on the input features, but also on the convolution position. Thus it is geometry-aware for depth data. With differential convolution kernel K * , the differential convolution (DC) for input feature map X ? R h?w?1 can be written as:</p><formula xml:id="formula_2">Y(p) = k?k i=1 K * i ? X(p + p i ),<label>(3)</label></formula><p>As mentioned above, we use differential convolution kernel K * to calculate the relevance between different points in a local receptive field, and the field size is input-dependent.</p><p>In our experiments, the receptive field size for depth data is 9 ? 9. To reduce computations, we apply the depth-wise separable convolution <ref type="bibr" target="#b11">[12]</ref> to decouple a differential convolution into a differential depth-wise convolution and a pointwise convolution (1 ? 1 convolution). For generalized input feature map F ? R h?w?c , our DCA module is defined as:</p><formula xml:id="formula_3">Attention = Conv 1?1 (DC-DW(F)), Output = Attention ? F.<label>(4)</label></formula><p>Here, Conv 1?1 represents 1 ? 1 convolution, and DC-DW denotes differential depth-wise convolution whose differential kernel is generated by Eq.(2). Attention ? R h?w?c means attention map which has the same size of input feature map F. Each value in the attention map integrates the geometric information in the local range of the depth image to indicate the importance of each feature. ? denotes element-wise product. The whole process of DCA is illustrated at the top part of <ref type="figure" target="#fig_0">Fig. 2</ref>. Convolution kernel that introduces difference a term can dynamically rebalance the convolution weights according to the input. And the proposed DCA module forces points with more consistent geometry to make more contributions to the corresponding output for depth data. In summary, DCA achieves flexibility not only in the local spatial dimension, but also in the channel dimension, and integrates geometric information of the local extent. It is worth noting that channels-wise information often represents different objects in CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">43]</ref>, which is also crucial for segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ensemble Differential Convolution Attention</head><p>As mentioned above, RGB data has long-range contextual dependencies and global spatial consistency. Although self-attention <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b63">63]</ref> is the practical methods to learn relationship between different points to capture long-range dependence, it only obtains spatial-wise adaptability and lacks the channel-wise adaptability. The proposed DCA module has flexibility in both spatial dimension and channel dimension, and it considers local-range correlations which is appropriate for depth data. Therefore, as for RGB data it is intuitive to extend DCA to propagate long-range contextual dependencies.</p><p>The most straightforward approach is to use larger kernel differential depth-wise convolution in DCA. In order to capture long-range relationship with less computational costs and parameters than directly apply larger kernel operations, we decomposed large kernel-based DC to a differential depth-wise convolution, a differential depth-wise dilation convolution and a point-wise convolution, called ensemble differential convolution (EDC). With EDC, the propose EDCA can be written as:</p><formula xml:id="formula_4">F 1 = DC-DW(F), F 2 = DC-DWD(F 1 ), Attention = Conv 1?1 (F 1 + F 2 ), Output = Attention ? F.<label>(5)</label></formula><p>Similar to DCA, F ? R h?w?c is the input feature map. Conv 1?1 represents 1 ? 1 convolution and ? denotes element-wise product. DC-DW and DC-DWD mean differential depth-wise convolution and differential depth-wise dilation convolution with differential convolution kernel K * , respectively. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the proposed EDCA module.</p><p>The size of EDC kernels are also input dependent. In our experiments, the DC kernel size of DC-DW is 5 ? 5, and that of DC-DWD is 9 ? 9 with dilation 3. With the above settings, the receptive field size of EDC is approximated to 29?29. <ref type="figure" target="#fig_1">Fig. 3 (d)</ref> shows the convolution strategies in EDC, for convenience, we show the 5 ? 5 convolution and 5 ? 5 convolution with dilation 3. Accordingly, EDCA can obtain long-range dependence, while the differential term dynamically adjust the convolution weights and provides spatial distribution information for RGB data. In summary, the discriminative features are boosted and the noisy responses are ignored based on the spatial and channel-wise adaptability of EDCA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Understanding DCA and EDCA</head><p>As verified by the prior works that pixels with same semantic labels have similar depths <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b53">53]</ref> in a local region. The DCA integrates geometric perception ability to vanilla convolution and generates an attention map which indicates the importance of each point in depth data. EDCA absorbs the advantage of dynamic convolution of DCA to propagate long-range contextual dependencies and seamlessly incorporate spatial distribution for RGB data.</p><p>As shown in Tab. 1, our proposed DCA and EDCA combine the advantages of convolution and self-attention. By augmenting the convolution kernel with a pixel difference term, DCA captures geometry with a local receptive field. Compared with vanilla convolution, the learnable weights of DCA are adjusted by the geometric variance. Based on this, with the help of our decomposed large kernel, the EDCA is extended to further capture refined pixel discrepancy in the satisfied receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">DCANet Architecture</head><p>The architecture of DCANet for RGB-D semantic segmentation is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Our DCANet adopts DeepLabv3+ <ref type="bibr" target="#b7">[8]</ref> as the baseline for RGB-D semantic segmentation task, where the encoder is ResNet-101 <ref type="bibr" target="#b18">[19]</ref> and retain the original decoder of DeepLabv3+. We apply a twobranch structure in our DCANet, one for RGB and another for depth data.</p><p>At each of the four resolution stages in the ResNet-101, depth features are fused into the RGB encoder by attention and fusion block. Specifically, the channel dimension of both modalities are first squeezed to 1/8 for dimensionality reduction. Next, we apply the DCA for depth data and EDCA for RGB data simultaneously. Third, the outputs of DCA and EDCA are convolved to match the dimensionality of the original features and performed an element-wise sum with the original features separately. Finally, the depth data extracting complementary geometric information is integrated into the RGB data by element-wise sum to obtain better feature representations. The outputs of attention and fusion block are as follows:</p><p>Depth out = W 2 (DCA(W 1 (Depth in ))) + Depth in , RGB out = W 2 (EDCA(W 1 (RGB in ))) + RGB in , RGB out = RGB out + Depth out <ref type="bibr" target="#b5">(6)</ref> where W 1 (W 1 ) and W 2 (W 2 ) represent 1 ? 1 convolution to squeeze and recover the channel, respectively. Notably, the fused output RGB feature of last block is propagated to the segmentation decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and metrics</head><p>Evaluation is performed on two popular RGB-D datasets:</p><p>NYUDv2 <ref type="bibr" target="#b47">[47]</ref>: NYUDv2 contains 1449 RGB-D images with pixel-wise labels. We follow the 40-class settings and the standard split with 795 training images and 654 testing images.</p><p>SUN-RGBD <ref type="bibr" target="#b48">[48]</ref>: This dataset has 37 categories of objects and consists 10335 RGB-D images, with 5285 as training and 5050 as testing.</p><p>We evaluate the results using two common metrics, i.e., Pixel Accuracy (Pixel Acc.), and Mean Intersection Over Union (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use dilated ResNet-101 <ref type="bibr" target="#b18">[19]</ref> pretrained on ImageNet <ref type="bibr" target="#b46">[46]</ref> as the backbone network for feature extraction and adding another auxiliary loss in the last stage of ResNet-101. We keep all the other settings of DeepLabv3+ <ref type="bibr" target="#b7">[8]</ref> the same. We implement our network using the PyTorch deep learning framework <ref type="bibr" target="#b39">[40]</ref>, and all the models are trained with two Nvidia Tesla V100 GPUs. We employ the "poly" policy <ref type="bibr" target="#b33">[34]</ref> with initial learning rate 0.008, crop size 480?480, batch size 8, fine-tuning batch normalization parameters <ref type="bibr" target="#b24">[25]</ref> and data augmentation method (i.e., random scaling, random cropping, and left-right flipping) during training. For the optimizer, we use the SGD with a momentum of 0.9 and a weight decay of 0.0001. In addition, we train the NYUDv2 dataset for 500 epochs and train the SUN-RGBD dataset for 200 epochs. For fair comparisons with other methods, we adopted both single-scale and multi-scale testing strategies during inference. If not otherwise noted, the experiments are single-scale testing, and ' * ' in tables denote the multi-scale strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>DC kernel size of DCA. Our DCA module applies the DC kernel of 9 ? 9, dilation 1 to capture local geometry information for depth data. To confirm the effectiveness of applying 9?9 DC kernel, we hence attempt DCA with other   Effectiveness of DCA and EDCA modules. We conduct ablation studies on NYUDv2 dataset to prove the indispensability of the DCA and EDCA modules. We perform two parallel DeepLabv3+ (ResNet-101) as baseline. As shown in Tab. 3, the two attention modules improve the performance remarkably. Compared with baseline, employing only DCA on depth data improves mIoU by 3.5%, while using only EDCA on RGB data brings 3.9% improvement. <ref type="table">Table 4</ref>. Superiority of EDCA compared with Self-Attention <ref type="bibr" target="#b54">[54]</ref> and EDCA-on NYUDv2 test set. EDCA-denotes EDCA without differential term. All the three modules are for RGB data to capture long-range dependence and no operations are performed on the depth data. When we apply both modules together, the performance is further improved to 77.3% (Pixel Acc.) and 52.1% (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>The results indicate that both modules are critical for the performance improvement and work best when combined. EDCA vs. Self-Attention. Self-attention mechanism, e.g., Non-local neural networks <ref type="bibr" target="#b54">[54]</ref>, are the well-known method to capture long-range dependence. We compare the performance of self-attention with our proposed EDCA. As illustrated in Tab. 4, EDCA outperforms self-attention in mIoU and Pixel Acc. by 2% and 0.8%, respectively. Selfattentive mechanisms are spatially adaptive, but not simultaneously channel-adaptive as EDCA. Nevertheless, channel adaptability plays a crucial role in segmentation tasks. Furthermore, we also verify the effectiveness of differential term in EDCA by removing the differential term in EDCA, called EDCA-. The results in Tab. 4 show that differential term brings 1.2% improvement in mIoU. This term in EDCA provides long range spatial distribution information for RGB data while dynamically sensing the scene.</p><p>Suitability of DCA and EDCA. In the proposed DCANet, we apply DCA on depth data to capture localrange dependence and geometric information and EDCA on RGB data to garner long-range correlations and spatial distribution information. We also confirm the suitability of such two modules by applying EDCA for depth data and DCA for RGB. As shown in Tab. 5, applying DCA on depth improves mIoU by 1.7% compared to EDCA and 1.6% improvement in mIoU using EDCA over DCA on RGB. The results illustrate that DCA and EDCA are appropriate for Depth and RGB data, respectively. This also explains that depth maps are more suitable for providing intrinsic geometric information of objects due to their local depth continuity, while RGB images effectively provide a global view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Different Architectures</head><p>Our proposed DCA and EDCA are general modules for RGB-D semantic segmentation, which can be easily plugged into CNNs as attention modules in semantic segmentation. Our method is also assessed with respect to  <ref type="figure">Figure 6</ref>. Visualization of response maps. For the left part, the second column shows the feature maps generated by baseline; the third column is generated by EDCA (the counterpart of the right part is generated by DCA) ; the fourth column shows the refined feature maps.</p><p>achieves the mIoU of 48.4, while our model achieve the socre of 52.1, a 3.7% improvement. This because depthaware convolution is used to produce a feature map while our DCA and EDCA are employed to generate an attention map that indicates the importance of different points. Moreover, depth-aware convolution only compares the similarity of local regions in the depth map and ignores the longrange dependence and global spatial consistency in RGB data, which can be captured by our EDCA. SUN RGB-D. Tab. 8 shows testing results on SUN RGB-D dataset. The DCANet achieves the best results compared with other state-of-the-art methods under both single-scale and multi-scale testing. <ref type="figure">Fig. 5</ref> illustrates the qualitative results of the NYUDv2 and SUN RGB-D dataset. From the results, we can confirm that the local geometric information in depth image and global dependence in RGB image are well enhanced by our DCA and EDCA modules. As illustrated in the second row on the right part, our DCANet successfully recognize the whole lamp including its bracket while it is even unrecognizable under strong lighting conditions. That is because our model effectively combines the advantages of both modal data. Specifically, when the 2D information of the object is unreliable, the model will make reasonable use of the corresponding geometric information. Similar examples can be found in the second row on the left part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Visualization of DCANet</head><p>To validate the effectiveness of the DCA and EDCA of our model, we apply the response maps of the baseline model and our DCANet. As shown in <ref type="figure">Fig. 6</ref>, the refined feature maps demonstrate the segmentation effectiveness of our method on capturing pixel-level subtle information (edge areas), where the pixel differential convolution matters. The attention maps of the RGB and depth data also explain that the DCA provide intrinsic fine-grained local geometry difference information for depth data, while EDCA effectively provide a global view for RGB data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Considering the intrinsic difference between RGB and depth data, we present a state-of-the-art differential convolution attention network by introducing two plug-and-play modules: DCA and EDCA. DCA dynamically perceives subtle geometric information that occurs in local regions in depth data. EDCA absorbs the advantage of dynamic convolution of DCA to propagate long-range contextual dependencies and seamlessly incorporate spatial distribution for RGB data. Attention maps generated by DCA and EDCA are employed to boost feature representations and further improve model performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The instances of DCA and EDCA when taking a 3 ? 3 local grid as an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The explains of convolution strategies in EDC, 5?5 convolution is used for convenience. (a) 5 ? 5 convolution, Conv5?5.(b) 5 ? 5 convolution with dilation 3, DConv5?5. (c) The combination of (a) and (b), DConv5?5(Conv5?5(?)). Compared with (a), (b) has a larger receptive field, but cause information lost. In (c) (left), the red dashed box is Conv5?5 which just fills the dilation of DConv5?5 and makes it approximate size of 17 ? 17 as the blue dashed box in (c) (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The overview of our network. The network consists of two ResNet-101 encoders, where DCA and EDCA are plugged into CNNs as an attention module for each block of each ResNet-101 encoder in RGB and Depth branches, respectively. We employ the original decoder DeepLabv3+. During training, each pair of feature maps are fused by the attention and fusion block and propagated to the next stage of the encoder for further feature transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Desirable characteristics of convolution, self-attention, DCA, and EDCA. Notably, DCA and EDCA are applied for depth and RGB data, respectively.</figDesc><table><row><cell>Properties</cell><cell>Convolution self-attention DCA EDCA</cell></row><row><cell>Geometry Structure</cell><cell></cell></row><row><cell>Local-range dependence</cell><cell></cell></row><row><cell>Long-range dependence</cell><cell></cell></row><row><cell>Spatial adaptability</cell><cell></cell></row><row><cell>Channel adaptability</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The results of DCA with different DC kernel sizes on NYUDv2 test set.</figDesc><table><row><cell cols="3">DC kernel size Pixel Acc. mIoU</cell></row><row><cell>3 ? 3</cell><cell>75.3</cell><cell>49.1</cell></row><row><cell>5 ? 5</cell><cell>75.7</cell><cell>49.7</cell></row><row><cell>7 ? 7</cell><cell>76.0</cell><cell>50.1</cell></row><row><cell>9 ? 9</cell><cell>76.5</cell><cell>50.9</cell></row><row><cell>11 ? 11</cell><cell>76.4</cell><cell>50.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on DCA and EDCA modules on NYUDv2 test set.DC kernel sizes on depth data and no operations are performed on the RGB data. The results shown in Tab.2 prove that larger DC kernels do not bring significant performance gains due to the local geometric nature of depth data and our setup works.</figDesc><table><row><cell cols="3">Method DCA EDCA Pixel Acc.% mIoU%</cell></row><row><cell>Baseline</cell><cell>75.1</cell><cell>47.4</cell></row><row><cell>Model1</cell><cell>76.5</cell><cell>50.9</cell></row><row><cell>Model2</cell><cell>76.9</cell><cell>51.3</cell></row><row><cell>DCANet</cell><cell>77.3</cell><cell>52.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Suitability of DCA and EDCA of the NYUDv2 test set. Note: Our proposed method applies DCA for Depth and EDCA for RGB data.</figDesc><table><row><cell>RGB</cell><cell cols="3">Depth Pixel Acc. mIoU</cell></row><row><cell>EDCA</cell><cell></cell><cell>76.9</cell><cell>51.3</cell></row><row><cell>DCA</cell><cell></cell><cell>76.2</cell><cell>49.7</cell></row><row><cell></cell><cell>DCA</cell><cell>76.5</cell><cell>50.9</cell></row><row><cell></cell><cell>EDCA</cell><cell>76.1</cell><cell>49.2</cell></row><row><cell cols="5">Table 6. Single-scale testing performance comparison with differ-</cell></row><row><cell cols="3">ent baseline methods on NYUDv2 test set.</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Setting Pixel Acc. mIoU</cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>75.1</cell><cell>47.4</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>ours</cell><cell>77.3</cell><cell>52.1</cell></row><row><cell>Deeplabv3+ [8]</cell><cell></cell><cell>+ baseline</cell><cell>2.2 74.5</cell><cell>4.7 46.5</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>ours</cell><cell>76.8</cell><cell>51.2</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>2.3</cell><cell>4.7</cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>73.3</cell><cell>45.4</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>ours</cell><cell>76.2</cell><cell>50.2</cell></row><row><cell>Deeplabv3 [7]</cell><cell></cell><cell>+ baseline</cell><cell>2.9 72.7</cell><cell>4.8 45.2</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>ours</cell><cell>75.8</cell><cell>49.4</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>3.1</cell><cell>4.2</cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>72.8</cell><cell>44.3</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>ours</cell><cell>75.6</cell><cell>49.2</cell></row><row><cell>PSPNet [33]</cell><cell></cell><cell>+ baseline</cell><cell>2.8 72.2</cell><cell>4.9 43.6</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>ours</cell><cell>75.1</cell><cell>48.5</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>2.9</cell><cell>4.9</cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>74.4</cell><cell>46.5</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>ours</cell><cell>76.1</cell><cell>50.1</cell></row><row><cell>FPN [62]</cell><cell></cell><cell>+ baseline</cell><cell>1.7 74.1</cell><cell>3.6 46.2</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>ours</cell><cell>75.7</cell><cell>49.9</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>1.6</cell><cell>3.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison with the state-of-the-art methods on NYUDv2 test set. '*' means multi-scale testing. The comparison results are shown in Tab. 7. Our method achieves leading performance. Compared with these methods, our model focuses more on the variability within RGB and depth data and apply different modules to enhance the feature representation. The depth-aware convolution proposed by D-CNN<ref type="bibr" target="#b53">[53]</ref> is more similar to our approach. For fair comparison, under single testing, D-CNN</figDesc><table><row><cell>Method</cell><cell cols="2">Pixel Acc.(%) mIoU(%)</cell></row><row><cell>LSD-GF [11]</cell><cell>71.9</cell><cell>45.9</cell></row><row><cell>D-CNN [53]</cell><cell>-</cell><cell>48.4</cell></row><row><cell>MMAF-Net [14]</cell><cell>72.2</cell><cell>44.8</cell></row><row><cell>ACNet [23]</cell><cell>-</cell><cell>48.3</cell></row><row><cell>ShapeConv [3]</cell><cell>75.8</cell><cell>50.2</cell></row><row><cell>RDF [39]*</cell><cell>76.0</cell><cell>50.1</cell></row><row><cell>M2.5D [58]*</cell><cell>76.9</cell><cell>50.9</cell></row><row><cell>SGNet [9]*</cell><cell>76.8</cell><cell>51.1</cell></row><row><cell>SA-Gate [10]*</cell><cell>77.9</cell><cell>52.4</cell></row><row><cell>InverseForm [2]*</cell><cell>78.1</cell><cell>53.1</cell></row><row><cell>ShapeConv [3]*</cell><cell>76.4</cell><cell>51.3</cell></row><row><cell>DCANet</cell><cell>77.3</cell><cell>52.1</cell></row><row><cell>DCANet*</cell><cell>78.2</cell><cell>53.3</cell></row><row><cell cols="3">Table 8. Performance comparison with the state-of-the-art meth-</cell></row><row><cell cols="3">ods on SUN RGB-D test set. '*' means multi-scale testing.</cell></row><row><cell>Method</cell><cell cols="2">Pixel Acc.(%) mIoU(%)</cell></row><row><cell>3DGNN [42]</cell><cell>-</cell><cell>44.1</cell></row><row><cell>D-CNN [53]</cell><cell>-</cell><cell>42.0</cell></row><row><cell>MMAF-Net [14]</cell><cell>81.0</cell><cell>47.0</cell></row><row><cell>SGNet [9]</cell><cell>81.0</cell><cell>47.5</cell></row><row><cell>ShapeConv [3]</cell><cell>82.0</cell><cell>47.6</cell></row><row><cell>ACNet [23]</cell><cell>-</cell><cell>48.1</cell></row><row><cell>3DGNN [42]*</cell><cell>-</cell><cell>45.9</cell></row><row><cell>CRF [29]*</cell><cell>-</cell><cell>48.1</cell></row><row><cell>RDF [39]*</cell><cell>81.5</cell><cell>47.7</cell></row><row><cell>SA-Gate [10]*</cell><cell>82.5</cell><cell>49.4</cell></row><row><cell>SGNet [9]*</cell><cell>82.0</cell><cell>47.6</cell></row><row><cell>ShapeConv [3]*</cell><cell>82.2</cell><cell>48.6</cell></row><row><cell>DCANet</cell><cell>82.2</cell><cell>48.1</cell></row><row><cell>DCANet*</cell><cell>82.6</cell><cell>49.6</cell></row><row><cell cols="3">several representative semantic segmentation architectures:</cell></row><row><cell cols="3">Deeplabv3+ [8], Deeplabv3 [7], PSPNet [33] and FPN [62]</cell></row><row><cell cols="3">with different backbones (ResNet-50, ResNet-101 [19]) on</cell></row><row><cell cols="3">NYUDv2 dataset to verify the generalizability. As is shown</cell></row><row><cell cols="3">in Tab. 6, our method outperforms the baseline by a desir-</cell></row><row><cell cols="3">able margin under all settings, demonstrating the general-</cell></row><row><cell cols="2">ization capability of our method.</cell><cell></cell></row><row><cell cols="2">4.5. Comparing with State-of-the-arts</cell><cell></cell></row><row><cell>NYUDv2.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverseform: A loss function for structured boundary-aware segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhankar</forename><surname>Borse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5901" to="5911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shapeconv: Shape-aware convolutional layer for indoor rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeezeexcitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial information guided convolution for real-time rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bidirectional cross-modality feature propagation with separation-and-aggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic segmentation with context encoding and multi-path decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3520" to="3533" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal attention-based fusion model for semantic segmentation of rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahimeh</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4837" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining semantic and geometric features for object class segmentation of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzad</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babette</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carme</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="55" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoder-decoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometryaware distillation for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ferdous Sohel, Roberto Togneri, and Imran Naseem. Integrating geometrical context for semantic labeling of indoor scenes using rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth-aware mirror segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Topi</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ki-Sang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Amodal instance segmentation with kins dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3014" to="3023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Xu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal token fusion for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12186" to="12195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Malleable 2.5 d convolution: Learning receptive fields along the depth-axis for rgb-d scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="555" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Inverted pyramid multitask transformer for dense scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07997</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
