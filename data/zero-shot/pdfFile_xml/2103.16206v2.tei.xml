<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XVFI: eXtreme Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XVFI: eXtreme Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Both authors contributed equally to this work. ? Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) 96.6 (b) 71.0 (c) 34.9 (d) 40.6 (e) 196.5 (f) 152.2 <ref type="figure">Figure 1</ref>. Some examples of our X4K1000FPS dataset, which contain diverse motions in 4K-resolution of 1000-fps. The numbers below the examples are the magnitude means of optical flows between two input frames in 30 fps. This is a video figure that can be best viewed with motion using Adobe? Reader. It should be noted that they are rendered in down-scales at 15 fps for visualization convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that first handles the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidirectional optical flow learning between two input frames (BiOF-I) and for bidirectional optical flow learning from target to input frames (BiOF-T). The optical flows are stably approximated by a complementary flow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essential information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algorithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Some examples of our X4K1000FPS dataset, which contain diverse motions in 4K-resolution of 1000-fps. The numbers below the examples are the magnitude means of optical flows between two input frames in 30 fps. This is a video figure that can be best viewed with motion using Adobe? Reader. It should be noted that they are rendered in down-scales at 15 fps for visualization convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that first handles the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidirectional optical flow learning between two input frames (BiOF-I) and for bidirectional optical flow learning from target to input frames (BiOF-T). The optical flows are stably approximated by a complementary flow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essential information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algorithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation (VFI) converts low frame rate (LFR) contents to high frame rate (HFR) videos by synthesizing one or more intermediate frames between given two consecutive frames, and then the videos of fast motion can be smoothly rendered in an increased frame rate, thus yielding reduced motion judder <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>. Therefore, it is widely used for various practical applications, such as adaptive streaming <ref type="bibr" target="#b45">[46]</ref>, novel view interpolation synthesis <ref type="bibr" target="#b10">[11]</ref>, frame rate up conversion <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">50]</ref>, slow motion generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> and video restoration <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref>. However, VFI is significantly challenging, which is attributed to diverse factors such as occlusions, large motions and change of light. Recent deeplearning-based VFI has been actively studied, showing remarkable performances <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref>. However, they are often optimized for existing LFR benchmark datasets of low resolution (LR), which may lead to poor VFI performance, especially for videos of 4K resolution (4096?2160) or higher with very large motion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. Such 4K videos often contain frames of fast motion with extremely large pixel displacements for which conventional convolutional neural networks (CNNs) do not effectively work with receptive fields of limited sizes.</p><p>To solve the above issues for deep learning-based VFI methods, we directly photographed 4K videos to construct a high-quality HFR dataset of high resolution, called X4K1000FPS. <ref type="figure">Fig. 1</ref> shows some examples of our X4K1000FPS dataset. As shown, our videos of 4K resolution have extremely large motions and occlusions. . VFI results for extreme motions. Our XVFI-Net can generate a more stable intermediate frame with very large motions than two recent SOTA methods, FeFlow <ref type="bibr" target="#b12">[13]</ref> and DAIN <ref type="bibr" target="#b3">[4]</ref>, which are newly trained on our dataset for fair comparisons.</p><p>We also first propose an extreme VFI model, called XVFI-Net, that is effectively designed to handle such a challenging dataset of 4K@1000fps. Instead of directly capturing extreme motions through consecutive feature spaces with deformable convolution as recent trends in video restoration <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20]</ref>, or using very largesized pretrained networks with extra information such as contexts, depths, flows and edges <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13]</ref>, our XVFI-Net is simple but effective, which is based on a recursive multi-scale shared structure. The XVFI-Net has two cascaded modules: one for the bidirectional optical flow learning between two input frames (BiOF-I) and the other for the bidirectional optical flow estimating from target to the inputs (BiOF-T). The BiOF-I and BiOF-T modules are trained in combination with multi-scale losses. However, once trained, the BiOF-I module can start from any downscaled input upward while the BiOF-T module only operates at the original input scale at inference, which is computationally efficient and helps to generate an intermediate frame at any target time instance. Structurally, the XVFI-Net is adjustable in terms of the number of scales for inference according to the input resolutions or the motion magnitudes, even if training is once over. We also propose a novel optical flow estimation from time t to those of the inputs, called a complementary flow reversal (CFR) that effectively fills the holes by taking complementary flows. Extensive experiments are conducted for fair comparison and our XVFI-Net that has a relatively smaller complexity outperforms previous VFI SOTA methods on our X4K1000FPS, especially for extreme motions as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. A further experiment on the previous LR-LFR benchmark dataset also demonstrates the robustness of our XVFI-Net. Our contributions are summarized as:</p><p>? We first propose a high-quality of HFR video dataset of 4K resolution, called X4K1000FPS (4K@1000fps) which contains a wide variety of textures, extremely large motions, zoomings and occlusions.</p><p>? We propose the CFR that can generate stable optical flow estimation from time t to the input frames, boosting both qualitative and quantitative performances.</p><p>? Our proposed XVFI-Net can start from any downscaled input upward, which is adjustable in terms of the number of scales for inference according to the input resolutions or the motion magnitudes.</p><p>? Our XVFI-Net achieves state-of-the-art performance on the testset of X4K1000FPS with a significant margin compared to the previous VFI SOTA methods while having computational efficiency with a small number of filter parameters. All source codes and proposed X4K1000FPS dataset are publicly available at https://github.com/JihyongOh/XVFI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Frame Interpolation</head><p>Most VFI methods can be categorized into optical flowor kernel-based <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref> and pixel hallucination-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21]</ref> methods. Flow-based VFI. Super-SloMo <ref type="bibr" target="#b17">[18]</ref> first linearly combines predicted optical flows between two input frames to approximate flows from the target intermediate frame to the input frames. Quadratic video frame interpolation <ref type="bibr" target="#b47">[48]</ref> utilizes four input frames to cope with nonlinear motion modeling by quadratic approximation, which limits the VFI generalization when two input frames are given. It also proposes flow reversal (projection) for more accurate image warping. On the other hand, DAIN <ref type="bibr" target="#b3">[4]</ref> gives different weights of overlapped flow vectors depending on the object depth of the scene via a flow projection layer. However, DAIN employs and fine-tunes both PWC-Net <ref type="bibr" target="#b40">[41]</ref> and MegaDepth <ref type="bibr" target="#b25">[26]</ref>, which is computationally burdened for inferring intermediate HR frames. AdaCoF proposes a generalized warping module to deal with complex motion <ref type="bibr" target="#b24">[25]</ref>. However, it is not adaptive to handle the frames of higher resolutions due to a fixed dilation degree, after once trained. Pixel Hallucination-based VFI. FeFlow <ref type="bibr" target="#b12">[13]</ref> has benefited from deformable convolution <ref type="bibr" target="#b8">[9]</ref> to the center frame generator by replacing optical flows with offset vectors. Zooming Slow-Mo <ref type="bibr" target="#b46">[47]</ref> also interpolates middle frames with the help of deformable convolution in the feature domain. However, since these methods directly hallucinate pixels unlike the flow-based VFI methods, the predicted frames tend to be blurry when fast-moving objects are present.</p><p>Most importantly, the aforementioned VFI methods are difficult to operate on the entire HR frames at once, due to their heavy computational complexity. On the other hand, our XVFI-Net is designed to efficiently operate on the entire 4K frame input at once with a smaller number of parameters and is capable of effectively capturing large motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Networks for Large Pixel Displacements</head><p>PWC-Net <ref type="bibr" target="#b40">[41]</ref> is a state-of-the-art optical flow estimator that has been adopted in several VFI methods for pretrained flow estimators <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>. Since PWC-Net has a 6-level feature pyramid structure to have larger sizes of receptive fields, it enables to effectively predict large motions. IM-Net <ref type="bibr" target="#b33">[34]</ref> also adopts a multi-scale structure to cover large displacements of objects in adjacent frames while the coverage is limited in the size of the adaptive filters. Despite of the multi-scale pyramid structures, the above methods lack adaptivity because the coarsest level of each network is fixed after once trained, i.e. each scale level consists of its own (not shared) parameters. The RRPN <ref type="bibr" target="#b50">[51]</ref> shares weight parameters across different scale levels in a flexible recurrent pyramid structure. However, it only infers the centered frame, not at arbitrary time instances. So it can only synthesize recursively the intermediate frames of time at a power of 2. As a result, the prediction errors are accumulated as intermediate frames are generated iteratively between the two input frames. Therefore, RRPN has limited temporal flexibility for VFI at arbitrary target time instance t.</p><p>Distinguished from the above methods, our proposed XVFI-Net has a scalable structure with shared parameters for various input resolutions. Different from RRPN <ref type="bibr" target="#b50">[51]</ref>, the XVFI-Net is structurally divided into the BiOF-I and BiOF-T modules, which allows predicting an intermediate frame at arbitrary time t with the help of the complementary flow reversal in an efficient way. That is, the BiOF-T module can be skipped at the down-scaled levels in inference so that our model can infer the intermediate frame of 4K at once, without any patch-wise iteration unlike all other previous methods, which can be applied in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed X4K1000FPS Dataset</head><p>Although numerous methods for VFI have been both trained and evaluated over the diverse benchmark datasets, such as Adobe240fps <ref type="bibr" target="#b39">[40]</ref>, DAVIS <ref type="bibr" target="#b34">[35]</ref>, UCF101 <ref type="bibr" target="#b38">[39]</ref>, Middlebury <ref type="bibr" target="#b2">[3]</ref> and Vimeo90K <ref type="bibr" target="#b48">[49]</ref>, none of the datasets contains rich amounts of 4K videos with HFR. These limits the study of elaborate VFI methods required for VFI applications for targeting very high resolution videos.</p><p>To tackle the challenging extreme VFI task, we provide a rich set of 4K@1000fps video that we photographed using a Phantom Flex4K? camera with the 4K spatial resolution of 4096?2160 at 1,000 fps, producing 175 video scenes, each with 5,000 frames by shooting for 5 seconds.</p><p>In order to select valuable data samples for VFI, we estimated bidirectional occlusion maps and optical flows of every 32 frames of the scenes using IRR-PWC <ref type="bibr" target="#b15">[16]</ref>. The occlusion map indicates part of the objects to be occluded in the next frames. The occlusion makes optical flow estimation and frame interpolation challenging <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref>. Thus, we manually selected 15 scenes as our testset, called X-TEST, by considering the degrees of occlusion, optical flow magnitudes and scene diversity. Each scene for X-TEST simply contains one test sample that consists of two input frames in a temporal distance of 32 frames and approximately corresponds to 30 fps. The test evaluation is set to interpolate 7 intermediate frames, which results in the consecutive frames of 240 fps. For the training dataset, called X-TRAIN, we cropped and selected 4,408 clips of 768?768-sized and the lengths of 65 consecutive frames by considering the amounts of occlusion. More details are described in Supplementary Material. <ref type="table">Table 1</ref> compares the statistics of datasets: Vimeo90K <ref type="bibr" target="#b48">[49]</ref>, Adobe240fps <ref type="bibr" target="#b39">[40]</ref>, our X-TEST and X-TRAIN. We estimated the occlusion range in [0,255] and optical flow magnitudes <ref type="bibr" target="#b15">[16]</ref> between input pairs and calculated their percentiles for each dataset. As shown in <ref type="table">Table 1</ref>, our datasets contain comparable occlusion but significantly larger motion, compared to the previous VFI datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method : XVFI-Net Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Design Considerations</head><p>Our XVFI-Net aims at interpolating an intermediate frame I t at an arbitrary time t between two consecutive input frames, I 0 and I 1 , of HR with extreme motion. Scale Adaptivity. An architecture with a fixed number of scale levels like PWC-Net <ref type="bibr" target="#b40">[41]</ref> is difficult to adapt to various spatial resolutions of the input video, because the structure in each scale level is not shared across different scale levels, so the new architecture with an increased scale depth needs to be retrained. In order to have a scale adaptivity to various spatial resolutions of input frames, our XVFI-Net is designed to have optical flow estimation starting at any desired coarse scale level, adapting to the degree of motion magnitudes in the input frames. To do so, our XVFI-Net shares its parameters across different scale levels. Capturing Large Motion. In order to effectively capture a large motion between two input frames, the Feature Extrac-tion Block of XVFI-Net first reduces the spatial resolution of two input frames by a module scale factor M via a strided convolution, thus yielding the spatially reduced feature that is then converted to two contextual feature maps C 0 0 and C 0 1 . The Feature Extraction Block in <ref type="figure" target="#fig_2">Fig. 3</ref> is composed of the strided convolution and two residual blocks <ref type="bibr" target="#b14">[15]</ref>. Then, XVFI-Net at each scale level estimates optical flows from target frame I t to two input frames in the reduced spatial size by M . The predicted flows are upscaled (?M ) to warp the input frames at each scale level to time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">XVFI-Net Architecture</head><p>BiOF-I module. </p><formula xml:id="formula_0">= F s+1 01 ? 2 ,F s 10 = F s+1 10 ? 2 .</formula><p>To update the initial flows in the current scale, C s 0 and C s 1 are first warped by the initial flows, that is,C s 01 = W (F s 01 , C s 1 ) andC s 10 = W (F s 10 , C s 0 ), respectively, where W is a backward warping operation <ref type="bibr" target="#b16">[17]</ref>. ThenC s 01 ,C s 10 , C s 0 , C s 1 together withF s 01 ,F s 10 are fed to an auto-encoder-based BiFlownet as in <ref type="figure" target="#fig_1">Fig. 4</ref> to output residual flows over the initial flows and a trainable importance mask z <ref type="bibr" target="#b30">[31]</ref>. Then F s 01 , F s 10 are obtained. They are then fed as input to the BiOF-T module and are also used as the initial flows to the next scale s ? 1. BiOF-T module. Hereafter, we omit superscript s for the notion of feature tensors at each scale, unless mentioned. Although the linear approximation with optical flows F 01 , F 10 <ref type="bibr" target="#b17">[18]</ref> or the flow reversal of F 0t , F 1t <ref type="bibr" target="#b47">[48]</ref> allows to estimate the flows F t0 , F t1 at arbitrary time t, there are few shortcomings. The linear approximation is inaccurate to predict F t0 and F t1 for fast-moving objects because the anchor points of F 01 and F 10 are severely misaligned. On the other hand, the flow reversal can align the anchor points but holes may appear in estimated F t0 and F t1 . To stabilize the performance of the flow reversal, we take complementary advantages of both the linear approximation and flow reversal. So, a stable optical flow estimate from time t to 0 or 1 can be computed by a normalized linear combination of a negative anchor flow and a complementary flow, which we call a complementary flow reversal (CFR). The resulting complementary reversed optical flow maps,F t0 andF t1 , from time t to 0 and 1 are given by,  </p><formula xml:id="formula_1">x t0 = (1 ? t) N0 w 0 ?(?F y 0t ) + t N1 w 1 ? F y 1?(1-t) (1 ? t) N0 w 0 + t N1 w 1 (1) F x t1 = (1 ? t) N0 w 0 ? F y 0?(1-t) + t N1 w 1 ? (?F y 1t ) (1 ? t) N0 w 0 + t N1 w 1<label>(2)</label></formula><p>where x denotes a pixel location at time t and y is at time 0 or 1.</p><formula xml:id="formula_2">w i = z y i ? G(|x ? (y + F y it )|)</formula><p>is a Gaussian weight depending on the distance between x at time t and y+F y it at time i (= 0 or 1) while also considering the learnable importance mask of each flow by z y i <ref type="bibr" target="#b30">[31]</ref>. Also, ?F y 0t (or ?F y 1t ) and F y 1?(1-t) (or F y 0?(1-t) ) in Eq. 1 (or Eq. 2) are defined as a negative anchor flow and a complementary flow, respectively. Furthermore, the anchor flows are normalized flows that can be calculated as F 0t = tF 01 and F 1t = (1 ? t)F 10 to intermediate time t. It should be noted in Eq. 1 and Eq. 2 that the complementary flows are also normalized as F 1?(1-t) = tF 10 and F 0?(1-t) = (1 ? t)F 01 which complementally fill the holes occurred in the reversed flows. By doing so, we can fully exploit the temporal-densely captured X4K1000FPS dataset to train our XVFI-Net for VFI at arbitrary time t. The neighborhoods of x are defined as,</p><formula xml:id="formula_3">N 0 = {y | round(y + F y 0t ) = x} (3) N 1 = {y | round(y + F y 1t ) = x}.<label>(4)</label></formula><p>To refine the bidirectional flow approximatesF t0 ,F t1 , we rewarp the feature maps (C 0 , C 1 ) toC t0 andC t1 byF t0 andF t1 , respectively. We concatenate and feed C 0 , C 1 ,C t0 ,C t1 , andF t0 ,F t1 to the auto-encoder-based TFlownet as in <ref type="figure" target="#fig_1">Fig. 4</ref> (similarly to refineF 01 ,F 10 ). The outputs of TFlownet are used to compose refined flows F t0 , F t1 which are then bilinearly up-scaled (?M ) back to the size   of I s t . The flow estimation in the spatially reduced size by M has three advantages: (i) enlarged receptive fields, (ii) lowered computational costs and (iii) smooth optical flows. This strategy maximizes the benefit of flow-based VFI that can fully utilize the texture information of the original input frames by warping them with the estimated flows, compared to the hallucination-based methods that suffer from a lack of sharpness in restoration from down-scaled feature maps. The above up-scaled flows are used to warp the input frames I s 0 and I s 1 to be? s t0 and? s t1 , respectively. The</p><formula xml:id="formula_4">C s 0 , C s 1 ,C s t0 ,C s t1 , F s t0 , F s t1 , I s 0 , I s 1 ,? s t0 and? s t1</formula><p>are all aggregated to be fed into the U-Net [36]-based Refinement Block. Then, both the generated occlusion mask m s and residual image? s r are finally used to blend the warped frames? s t0 and? s t1 , which is given by,</p><formula xml:id="formula_5">I s t = (1 ? t)?m s ?? s t0 + t?(1 ? m s )?? s t1 (1 ? t) ? m s + t ? (1 ? m s ) +? s r<label>(5)</label></formula><p>where? s t is the final result of each scale level s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adjustable and Efficient Scalability</head><p>Adjustable Scalability. <ref type="figure" target="#fig_2">Fig. 3</ref> shows a VFI framework of our XVFI-Net that can begin from any scale level by ?1/2 s recurrent down-scaling the contextual feature map C 0 0 and C 0 1 , and predicts the coarsest optical flow to capture extreme motion effectively. Then the estimated flows F s 01 , F s 10 are transmitted to the next scale s ? 1, and the flow is updated gradually to the original scale s = 0. We aim that the number of scales can be decided for inference, adaptive to the spatial resolution and degree of motion magnitudes for the input frames, even after once trained. To generalize the XVFI-Net learning for the input of any scale level, a multiscale reconstruction loss in Eq. 7 is applied for every output I s t for the selected scale depth S trn during training. Efficient Scalability. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the computation through the BiOF-T module is always taken place at the original scale (s = 0) during inference no matter which scale level the BiOF-I starts from, which are denoted as the arrows in the light orange color. Since F s 01 and F s 10 are the only information that passes across different scale levels through the BiOF-I module (from the previous scale to the next scale level) as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we only pass the two optical flows recursively until reaching the original scale level. Then, the BiOF-T module processes F s=0 10 and F s=0 01 to estimate F s=0 t1 and F s=0 t0 only at the original scale level. This is architecturally very beneficial because (i) the BiOF-I module is responsible to stably capture extreme motion by recursively learning the bidirectional flows between input time instances 0 and 1 across multiple scale levels, and (ii) the BiOF-T module finely predicts the bidirectional flows in the original scale only from any target time t to times 0 and 1 based on the stably estimated flows F s=0 10 and F s=0 01 , unlike the RRPN <ref type="bibr" target="#b50">[51]</ref>. Loss Functions. We adopt a multi-scale reconstruction loss to train the shared parameters of our XVFI-Net. To further encourage the smoothness of the obtained optical flow, the first-order edge-aware smoothness loss is used for F 0 t0 and F 0 t1 at the original scale <ref type="bibr" target="#b18">[19]</ref>. The total loss function is a weighted sum of the two loss functions as follows:</p><formula xml:id="formula_6">L total = L r + ? s ? L s<label>(6)</label></formula><formula xml:id="formula_7">L r = Strn s=0 ? s t ? I s t 1<label>(7)</label></formula><formula xml:id="formula_8">L s = i=0,1 exp(?e 2 c ? x I 0 tc ) ? ? x F 0 ti<label>(8)</label></formula><p>where c, e 2 and x denote color channel index, an edge weighting factor and a spatial coordinate, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment Results</head><p>The proposed X-TRAIN dataset contains 4,408 clips of the sizes of 768?768 and the lengths of 65 consecutive frames. Each training sample is randomly fetched on the fly from each clip. A training sample is defined as a triplet with two input frames (I 0 , I 1 ) and one target frame (I t , 0 &lt; t &lt; 1). The temporal distance between I 0 and I 1 is randomly selected in the range <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref> where I t is also randomly determined between the selected I 0 and I 1 . By doing so, our training samples are stochastically well obtained by fully exploiting our X-TRAIN dataset of temporally dense video clips to learn various t accordingly.</p><p>The weights of the XVFI-Net are initialized with Xavier initialization <ref type="bibr" target="#b11">[12]</ref> and the mini-batch size is set to 8. XVFI-Net is trained via total of 110,200 iterations (200 epochs) by using the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with the initial learning rate of 10 ?4 , reduced by a factor of 4 at [100, 150, 180]-th epoch. The hyperparameter M , ? s and e are set to 4, 0.5 and 150, respectively. We also randomly crop 384 ? 384-sized patches from the original size of X-TRAIN and randomly flip both spatial and temporal directions for data augmentation. Training takes about a half-day with an NVIDIA TI-TAN RTX? GPU with PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison to the Previous Methods</head><p>We compare our XVFI-Net with three previous VFI methods, <ref type="bibr">DAIN [4]</ref>, FeFlow <ref type="bibr" target="#b12">[13]</ref> and AdaCoF <ref type="bibr" target="#b24">[25]</ref>, whose training codes are publicly available. DAIN can generate the interpolated frame at arbitrary time t at once and the latter two can only synthesize the intermediate frame at the power of 2 in an iterative manner during the inference.</p><p>For a fair comparison, we retrain the three previous methods on X-TRAIN under their original hyperparameter settings except the patch size of 384?384 and the total iterations of 110,200. For further comparison, we also use the original pretrained models of the three methods, which are denoted as the subscript o to distinguish from their retrained models with the subscript f on X-TRAIN. The lowest scale depths for our XVFI-Net are set to 3 for training (S trn ) and 3 or 5 for testing (S tst ). We evaluate their performances for 7 interpolated frames per scene (multi-frame interpolation ?8) on X-TEST in terms of three evaluation metrics: PSNR, SSIM <ref type="bibr" target="#b44">[45]</ref> and tOF <ref type="bibr" target="#b7">[8]</ref> that measures the temporal consistency for the pixel-wise difference of motions (the lower, the better). We also evaluate each method for 7 interpolated frames per clip on the Adobe240fps dataset <ref type="bibr" target="#b39">[40]</ref>, where 200 nonuplets clips are randomly extracted with 1280 ? 720 (HD) at 240fps. Quantitative Comparison. <ref type="table">Table 2</ref> shows the quantitative comparisons of the VFI methods on both X-TEST and Adobe240fps. Please note that all runtimes (R t ) in <ref type="table">Table 2</ref> are measured for 1024?1024-sized frames because DAIN and FeFlow are too heavy to run for each of 4K input frames (4096?2160) at once. As shown in <ref type="table">Table 2</ref>, our proposed XVFI-Net with S tst set to both 3 and 5 clearly outperforms all the previous methods with large margins on both X-TEST and Adobe240fps, even though the number of model parameters (#P) of our model is significantly smaller    <ref type="table">Table 2</ref>. Quantitative comparisons on both X-TEST (4K) and Adobe240fps (HD) <ref type="bibr" target="#b39">[40]</ref> for multi-frame interpolation (?8). than those of the others. It is also worthwhile to note that our model can infer the intermediate frames of 4K at once, without any patch-wise iteration. In particular, XVFI-Net (S tst =5) outperforms DAIN f by 2.6dB, 0.049 and 1.32 in terms of PSNR, SSIM and tOF, respectively, for X-TEST, by utilizing only 22.9% of DAIN's parameters.</p><p>Especially for the X-TEST that contains significantly extreme motions in 4K frames, our XVFI-Net can effectively capture large motion in earlier stages and then precisely interpolate the 4K input frames better than the previous methods. It is noted that FeFlow is inappropriate for large motion alignment in the feature domain, which results in blurry output and is computationally heavy for 4K input frames. In addition, the center-frame interpolation methods such as AdaCoF, FeFlow and others <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32]</ref> tend to synthesize intermediate frames generally worse than those of arbitrary time VFI methods such as DAIN and XVFI-Net as shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. The errors of the center-frame interpolation methods tend to be accumulated iteratively due to inaccurate predictions. On the other hand, our model can accurately generate intermediate frames at arbitrary time t. Qualitative Comparison. <ref type="figure">Fig. 6</ref> shows the visual comparison for VFI performances. The first column images in <ref type="figure">Fig. 6</ref> show overlapped images of two 4K input frames. As shown, huge pixel displacements are observed between two input frames, which is very challenging for VFI. The interpolated AdaCoF f</p><p>FeFlow results in <ref type="figure">Fig. 6</ref> correspond to the center time (t = 0.5) of the two input frames which is the most challenging frame interpolation. As shown in <ref type="figure">Fig. 6</ref>, our XVFI-Net (S tst = 5) surprisingly captures very complex structures of objects with extremely fast motions, which are failed by all the previous methods.</p><formula xml:id="formula_9">f DAIN f XVFI-Net (S tst =3) XVFI-Net (S tst =5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>Flow Approximation. We compare the three flow approximations that enable to produce intermediate frames at arbitrary t: (a) the linear approximation <ref type="bibr" target="#b17">[18]</ref> with F 01 , F 10 , (b) flow reversal <ref type="bibr" target="#b47">[48]</ref> of F 0t and F 1t , and (c) our proposed complementary flow reversal (CFR). In this comparison, we approximated F t0 with the three methods using the estimated optical flows F 01 , F 10 which are obtained by IRR-PWC <ref type="bibr" target="#b15">[16]</ref> between the input I 0 and I 1 . The importance mask z's in Eq. 1 and 2 are excluded in this comparison. <ref type="figure">Fig.  7</ref> visualizes an example of the approximated optical flows by the three methods and the pseudo ground truth which is estimated between I t and I 0 by IRR-PWC <ref type="bibr" target="#b15">[16]</ref>. To evaluate the flow approximations quantitatively, the averaged endpoint errors (EPEs) for the three methods are calculated between the approximated flows and the pseudo ground truth on the testset of Vimeo90K <ref type="bibr" target="#b48">[49]</ref>, which are shown in <ref type="table" target="#tab_3">Table  3</ref>. The linear approximation reveals misalignment due to the different anchor frames, which is indicated by yellow arrows in <ref type="figure">Fig. 7</ref>. The flow reversal resolves the misalignment problem, but is inferior to the linear approximation because it causes holes that are not projected from any flow vector, as shown in the second optical flow map (red arrows). Also, the EPE value of the flow reversal is the worst among the three methods. On the other hand, our proposed CFR can appropriately fill in the holes since the bidirectional flows complement each other, as shown in <ref type="figure">Fig. 7</ref>, which is consistent with the lowest EPE value of CFR in <ref type="table" target="#tab_3">Table 3</ref>. In order to investigate the efficacy of the proposed CFR for VFI, we trained three VFI models from scratch by adopting each of the three flow approximations in their BiOF-T modules, without any help of pretrained networks. The lowest scale depths for both training S trn and test S tst are set to 3. The VFI performances on our X-TEST (PSNR/SSIM/tOF) for the three models are listed in <ref type="table" target="#tab_3">Table  3</ref>, showing a superiority of our proposed CFR. Adjustable Scalability. The lowest scale depth S tst for the inference can be adaptive to the degree of motion magnitudes and spatial resolution of the input frames, even after once trained, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We show the adjustable scalability of our framework with S trn = 1, 3, 5 for S tst = 1, 3, 5. For this, we train XVFI-Net variants by fully utilizing 512?512-sized patches because the spatial resolution of the training inputs should be multiple of 512 for S trn = 5  where the number 512 is determined as 2 Strn=5 ? M (= 4) ? 4 (via the bottlenecks of the autoencoders). <ref type="table" target="#tab_4">Table 4</ref> compares the performances of the XVFI-Net variants. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the performances are generally boosted by increasing the value of S tst with the help of effectively enlarging receptive field sizes and elaborately refining the resulting flows, especially in capturing extremely large motions and detailed structures. This trend is also observed in <ref type="table">Table 2</ref> for the XVFI-Net trained with 384 ? 384-sized patches of S trn = 3. Furthermore, as shown in the rightmost four columns of <ref type="figure">Fig. 6</ref>, the details of the objects, letters and textures are more precisely synthesized for S tst = 5 than 3 qualitatively. Both quantitative and qualitative results clearly show the effectiveness of the XVFI-Net's adjustable scalability. On the other hand, the occlusions and flow magnitudes of the Adobe240fps dataset <ref type="bibr" target="#b39">[40]</ref> are much smaller than those of X-TEST as shown in <ref type="table">Table 1</ref>. It is noted in Table 2 that our XVFI-Net with S tst = 3 shows better performance than that with S tst = 5 on the Adobe240fps dataset with smaller resolutions than X-TEST, which also clearly supports the efficacy of our adjustable scalability. Robustness of our XVFI-Net Framework. To show the robustness of our XVFI-Net framework for LR-LFR benchmark dataset, we construct a variant of XVFI-Net, called XVFI-Net v , with M = 2 for the dataset with lower resolution frames. The XVFI-Net v is then trained on a standard VFI dataset that is the Vimeo90K <ref type="bibr" target="#b48">[49]</ref> training set with 51,313 triplets (t = 0.5) of 448 ? 256 size. The training went through 200 epochs with randomly cropped 256?256sized patches and a mini-batch size of 16, where both S trn and S tst are set to 1. We compare our XVFI-Net v with four SOTA methods: DAIN <ref type="bibr" target="#b3">[4]</ref>, FeFlow <ref type="bibr" target="#b12">[13]</ref>, AdaCoF <ref type="bibr" target="#b24">[25]</ref> and BMBC <ref type="bibr" target="#b32">[33]</ref>, where their pretrained models and testing code are publicly available. <ref type="figure" target="#fig_10">Fig. 8</ref> shows the PSNR/SSIM and runtime (s) performances of our and SOTA methods with their model sizes (M) evaluated on Vimeo90K testset. As shown, our XVFI-Net v outperforms BMBC, DAIN and AdaCoF with a significantly smaller model size (5.5 million parameters), by taking advantage of the recursive multi-scale and shared structure. However, the XVFI-Net v shows lower performance than that of FeFlow but has a much smaller model size only with 5.4% of the number of the FeFlow's parameters, thus leading to about ?7 faster runtime. As a result, our XVFI-Net framework designed for high-resolution VFI with extremely large motion shows its robustness to the LR-LFR benchmark dataset by simply adjusting module scale factor M , S trn and S tst .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We first proposed a high-quality HFR dataset in HR, called X4K1000FPS with a wide range of motions. The proposed XVFI-Net can handle large pixel displacements with an adjustable scalability for inference to cope with the input resolutions or the motion magnitudes, even if training is once over. The XVFI-Net showed state-of-the-art performance on HR datasets compared to the previous methods and its robustness to the LR-LFR benchmark dataset.</p><p>Although our proposed X4K1000FPS dataset was obtained by using one single camera, such an extreme HFR 4K dataset is very valuable for the research community of VFI because such kinds of cameras are few. Besides, we delicately select clips as X-TRAIN/X-TEST to be publicly available by considering both occlusions and flow magnitudes for a new challenging VFI task, called the eXtreme Video Frame Interpolation (XVFI). We hope that this research would be a valuable milestone to extend the current VFI for more recent real-world applications with HR video. <ref type="figure">Figure 9</ref>. More examples of our X4K1000FPS dataset, which contain diverse motions in 4K-resolution of 1000 fps. The numbers below the examples are the magnitude means of optical flows between two input frames in 30 fps. This is a video figure that can be best viewed with motion using Adobe? Reader. It should be noted that they are rendered in down-scales at 15 fps for visualization convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Proposed X4K1000FPS Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Photographing Videos</head><p>In order to provide a wide range of object motions and various camera motion types at different speeds in diverse locations, the shooting rules were guided as follows: (i) shooting various objects independently moving while the camera is stationary, (ii) shooting videos from a moving car (fast translated videos), (iii) shooting while walking (moving at normal speed), (iv) shooting with the camera in irregular motion trajectories at non-uniform speeds, (v) shooting with zooming out or in and panning at the same time. Besides, the contents of the videos also include various objects (crowds, cars, trains, plants, animals, boats, traffic signs, signboards, waterfalls, buildings, etc.) in various types of places such as stadiums, stations, beaches, markets, parks, rivers, playgrounds, etc. <ref type="figure">Fig. 9</ref> shows some representative thumbnails of spatiotemporally down-sampled 4K video at 15 fps for a visualization purpose. As shown in <ref type="figure">Fig. 1</ref> in the main paper and <ref type="figure">Fig. 9</ref> in this Supplementary Material, very extreme scenes including various camera motions, zooming, translations, speeds, occlusions and objects are contained in the X4K1000FPS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Test Dataset: X-TEST</head><p>We manually select the consecutive 32 frames for each test scene by considering the degrees of occlusion, optical flow magnitudes and diversity among 5,000 frames. We compose nonuplets by sampling every 4 frames from 32 frames of each test scene. Since the frame rates of videos are often given in multiples of 30 in VFI benchmark datasets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref> or real-world industries, we approximate that our test videos are set to 960 fps (= 32?30 fps) instead of 1,000 fps. Therefore, two input frames that are 32 frames apart are regarded as part of a 30 fps (= 960 / 32) video and converted to 240 fps by ?8 multi-frame interpolation in the evaluation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Train Dataset: X-TRAIN</head><p>To compose a valuable training dataset from our enormous videos for video frame interpolation, we select training samples based on the value of the occlusion map estimated by IRR-PWC <ref type="bibr" target="#b15">[16]</ref>. The occlusion maps are approximated on the spatially down-sampled (?1/4) and temporally sub-sampled (?1/32) frames of the original 4Kresolution 1000 fps videos. Each target frame is fed into IRR-PWC with the previous and the next frames of the target frame, respectively. The resulting two occlusion maps are averaged to get the bidirectional occlusion map.</p><p>Then, we divide the 4K-resolution frame into overlapping patches of 768 ? 768, which forms an 81 ? 31 grid, except for the boundary of the 4K-resolution frame. This is because the boundary patches have undesirably large occlusion values when there are translation motions. Similarly, about 5,000 frames are divided into overlapping 154 clips of 65 consecutive frames except for the boundary period in the temporal dimension of the scenes. Thus, about 386K (= 81 ? 31 ? 154) candidate training samples per scene of the patch size of 768?768 and the lengths of 65 frames are extracted from a 4K-resolution 1000 fps video. After that, the candidate training samples whose bidirectional occlusion value is the top 10% of those of all candidate training samples remained, and the others are discarded. Finally, total 4,408 training samples are sparsely selected as training data to prevent similar samples from being selected to maintain the diversity of the training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Architecture of XVFI-Net</head><p>In addition to <ref type="figure" target="#fig_1">Fig. 3 and 4</ref> in the main paper, we present the detailed architectures of sub-networks of XVFI-Net in the case of the module scale factor M = 4 in <ref type="table" target="#tab_6">Table 5</ref> to <ref type="table" target="#tab_11">Table 9</ref>. The series of rows represents the consecutive operations. The first column represents each layer's operation, and H,W and C indicate the spatial ratio with respect to bicubicly downsampled (?1/2 s ) input frames I s i for each scale s and the number of channels of the output tensors, respectively. The last column denotes the names of some output tensors, which are worth mentioning. We omit the names of the output tensors if they are just intermediate tensors in the sub-networks. When the multiple tensors are input to each layer, they are concatenated channel-wise. 'resblock' represents a residual block which consists of conv2d -relu -conv2d -identity addition. The stride of the convolutional layer is set to 1, if not mentioned. The convolution filter sizes are 3 ? 3 and 4 ? 4 for the strides of 1 and 2, respectively.</p><p>As shown in <ref type="table" target="#tab_6">Table 5</ref>, the Feature Extraction Block is a simple residual block-based sub-network. On the other hand, the flow estimation sub-networks, the BiFlownet and TFlownet, have a simple auto-encoder architecture to enlarge the receptive field as shown in <ref type="table" target="#tab_7">Table 6</ref>, 7 and 8. The Refinement Block has a U-Net <ref type="bibr" target="#b35">[36]</ref>-based architecture as in <ref type="table" target="#tab_11">Table 9</ref>. The parameters of each sub-network are shared for all scale levels except for the BiFlownet at the lowest scale depth S, which is isolated in <ref type="table" target="#tab_7">Table 6</ref>. The bidirectional flows are estimated directly from two input features C S 0 , C S 1 at the lowest scale level S, because there does not exist any provided initial flow.</p><p>Efficiency of XVFI-Net During Inference. The BiOF-T module can start from any down-scaled level, while the BiOF-T module can be skipped in the down-scaled levels (s = 1, . . . , S tst ) as described in <ref type="figure" target="#fig_2">Fig. 3</ref> in the main paper. By doing so, our XVFI-Net framework can accelerate run time about 22% faster compared to the full-recursion framework where both BiOF-I and -T modules are processed together in all scale levels for 4K video, when S tst = 5. Besides, the additional runtimes induced by the smaller down-scaled levels (s &gt; 0) are negligible since the runtimes of the BiOF-I module at down-scaled levels are much smaller than those of BiOF-I and BiOF-T modules at the original scale (s = 0).</p><p>As shown in the first scene with even extreme back and forth motions of a propeller with zoom-out, our XVFI-Net can surprisingly capture such a complex motion for VFI, but the SOTAs fail to interpolate the sophisticated tips of the propeller pointed by the yellow arrows. For the second scene, even while riding a fast-moving car, XVFI-Net better captures far tiny structures such as electric wires seen at the left part and a closer pole with a large pixel displacement pointed by the yellow arrows. For the third scene, the rightmost front car moves very fast, so all the previous methods fail to capture it, denoted by the yellow arrow, yielding severe artifacts (structural distortions). On the other hand, XVFI-Net precisely captures the especially right edges of the rightmost car. Finally, in the last scene even with the extremely hand-shaken frames, the XVFI-Net can also synthesize repeating similar stairs but all SOTAs tend to generate baggy artifacts. As a result, XVFI-Net significantly better handles large pixel displacements due to extreme motion and huge spatial resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Qualitative Results</head><p>Visual Comparisons for VFI methods. We provide additional qualitative results on X-TEST (4K) in <ref type="figure">Fig. 10</ref>, Adobe240fps <ref type="bibr" target="#b39">[40]</ref> (HD) in <ref type="figure">Fig. 11</ref> and Vimeo90K <ref type="bibr" target="#b48">[49]</ref> in <ref type="figure" target="#fig_0">Fig. 12</ref> by each setting described in the main paper.</p><p>Visualization of Components of XVFI-Net. <ref type="figure" target="#fig_2">Fig. 13</ref> shows the visualization of optical flows and occlusion masks of XVFI-Net v . As expected, estimated flows at the upper level seem finer than those at the lower level (F 1 t0 ) as shown in <ref type="figure" target="#fig_2">Fig. 13</ref>. The coarse-to-fine structure gradually helps the whole XVFI framework boost the final VFI performance at original scale s = 0 based on the occlusion masks and the iteratively updated flows that are all learned from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure Cases</head><p>Since we delicately select the scenes to compose extremely challenging X-TEST, there exist inevitably failure patches within the same 4K frame result, where all compared methods including XVFI-Net (S tst = 5), fail to accurately interpolate the intermediate frames. <ref type="figure" target="#fig_1">Fig. 14</ref> shows the 4K failure results (t = 0.5) including several failure patches of ours because the input videos have very large magnitude means of optical flows (196.5) attributed to large camera shaking with the fast moving cars. <ref type="figure" target="#fig_7">Fig. 15</ref> shows the failure cropped patches. First, the tiny electric line, which is hard to be distinguishable from static background, is failed to be accurately interpolated by all methods including ours, as   indicated by a red arrow. Second, rotations of fast moving car's wheels are also challenging to be delicately synthesized while considering the degree of rotations, as pointed by two green arrows. On top of these, blurriness and abrupt brightness or color change in the input frames would also make all VFI methods challenging. Please note that we have also provided all interpolated results for all compared methods of both original and retrained versions on X-TEST to be publicly available at https://github.com/JihyongOh/XVFI for easier comparison. Acknowledgement. We specially thank Sung-Jun Yoon and Hyun-Ho Kim for photographing 4K videos on the spot.     <ref type="figure">Figure 10</ref>. Visual comparisons for VFI results (t = 0.5) on X-TEST for our and retrained SOTA methods with X-TRAIN. (*,*): occlusions and optical flow magnitudes between the two input frames measured by IRR-PWC <ref type="bibr" target="#b15">[16]</ref>, respectively. Best viewed in zoom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overlapped two inputs (1280x720)</head><p>AdaCoF f FeFlow f DAIN f XVFI-Net (S tst =5) 114pixels 236pixels 48pixels 182pixels <ref type="figure">Figure 11</ref>. Visual comparisons for VFI results (t = 0.5) on Adobe240fps for our and retrained SOTA methods with X-TRAIN. Best viewed in zoom.</p><formula xml:id="formula_10">AdaCoF o DAIN o BMBC o Overlapped inputs</formula><p>FeFlow o Ours G.T <ref type="figure" target="#fig_0">Figure 12</ref>. Visual comparisons of AdaCoF <ref type="bibr" target="#b24">[25]</ref>, DAIN <ref type="bibr" target="#b3">[4]</ref>, BMBC <ref type="bibr" target="#b32">[33]</ref>, FeFlow <ref type="bibr" target="#b12">[13]</ref>, our XVFI-Netv and the corresponding ground truth on the testset of Vimeo90K <ref type="bibr" target="#b48">[49]</ref> triplet. Best viewed in zoom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interploated frame Overlapped inputs</head><p>Occlusion Overlapped two inputs (4096x2160) AdaCoF f FeFlow f <ref type="figure" target="#fig_1">Figure 14</ref>. Failure cases of 4K result (t = 0.5) on X-TEST for our and retrained SOTA methods with X-TRAIN, including the corresponding ground truth. Best viewed in zoom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overlapped two inputs (4096x2160)</head><p>AdaCoF f FeFlow f DAIN f XVFI-Net (S tst =5) GT <ref type="figure" target="#fig_7">Figure 15</ref>. Failure cases of cropped results (t = 0.5) on X-TEST for our and retrained SOTA methods with X-TRAIN, including the corresponding ground truth. Best viewed in zoom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>arXiv:2103.16206v2 [cs.CV] 5 Aug 2021Overlapped 4K inputs (crop)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>shows our XVFI-Net architecture in scale s, where I s denotes bicubicly down-scaled by 1/2 s . First, contextual pyramid C = {C s } is recurrently extracted from C 0 0 and C 0 1 via a stride 2 convolution, and then utilized as an input for XVFI-Net at each scale level s (s = 0, 1, 2, ...), where s = 0 denotes the scale of the original input frames. Let F s tat b denotes optical flow from time t a to t b at scale s. F s 01 and F s 10 are the bidirectional flows between input frames at scale s. F s t0 and F s t1 are the bidirectional flows from I s t to I s 0 and I s 1 , respectively. The estimated flows F s+1 01 , F s+1 10 from the previous scale (s + 1) are ?2 bilinearly up-scaled to be set as the initial flows for the current scale s, i.e.,F s 01</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Adjustable and efficient scalability of our XVFI-Net framework. Even if the lowest scale depth Strn during training is set to 1 in this example, inference can start from any scale level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of our proposed XVFI-Net in scale s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>PSNR profiles for multi-frame interpolation results (?8) on X-TEST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Visual comparisons for VFI results (t = 0.5) on X-TEST for our and retrained SOTA methods with X-TRAIN. (*,*): occlusions and optical flow magnitudes between the two input frames measured by<ref type="bibr" target="#b15">[16]</ref>, respectively. Best viewed in zoom.(a) Linear comb. (b) Reversal (c) CFR (ours) Overlapped (d) Pseudo GT Approximated optical flows Ft0 by three different flow approximation methods. (a) Linear combination, (b) flow reversal, (c) CFR (proposed). Best viewed in zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>PSNR/SSIM vs runtime (s) on Vimeo90K [49] with model size (M) indicated in each circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Visualization of optical flows and occlusion masks of XVFI-Netv. The coarse and fine flows are extracted at scale 1 and 0, respectively. DAIN f XVFI-Net (S tst =5) GT 288pixels 179pixels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, 50 th and 75 th represent percentiles of each datset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Occlusion [16] 25 th 50 th 75 th</cell><cell cols="3">Flow magnitude [16] 25 th 50 th 75 th</cell></row><row><cell>Vimeo90K [49]</cell><cell>6.8</cell><cell cols="2">11.9 18.1</cell><cell>3.1</cell><cell>4.9</cell><cell>7.1</cell></row><row><cell>Adobe240fps [40]</cell><cell>0.8</cell><cell>1.7</cell><cell>3.2</cell><cell>3.8</cell><cell>8.9</cell><cell>16.3</cell></row><row><cell>X-TEST (ours)</cell><cell>2.1</cell><cell>5.6</cell><cell>17.7</cell><cell cols="3">23.9 81.9 138.5</cell></row><row><cell>X-TRAIN (ours)</cell><cell>6.9</cell><cell cols="2">10.1 15.7</cell><cell>5.5</cell><cell>18.0</cell><cell>59.5</cell></row><row><cell cols="7">25 Table 1. The occlusion and optical flow magnitude statistics of VFI</cell></row><row><cell cols="7">datasets: 3,782 test triplets of Vimeo90K [49], randomly selected</cell></row><row><cell cols="7">200 clips of Adobe240fps [40], 15 clips of X-TEST and 4,408</cell></row><row><cell>clips of X-TRAIN.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>th</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The ratio of number of iterations of the original version to that ofretrained version in the fair condition. #P: The number of parameters (M). R</figDesc><table><row><cell>Methods (?N)</cell><cell cols="2">X-TEST (PSNR/SSIM/tOF) (PSNR/SSIM) Adobe240fps</cell><cell>#P ?</cell><cell>R t ?</cell></row><row><cell>AdaCoF o (?5.8)</cell><cell>23.90/0.727/6.89</cell><cell>25.26/0.785</cell><cell cols="2">21.8 0.005</cell></row><row><cell>AdaCoF f [25]</cell><cell>25.81/0.772/6.42</cell><cell>25.21/0.791</cell><cell cols="2">21.8 0.005</cell></row><row><cell>FeFlow o (?5.3)</cell><cell>24.00/0.756/6.59</cell><cell>25.18/0.785</cell><cell cols="2">102.5 1.681</cell></row><row><cell>FeFlow f [13]</cell><cell>25.16/0.783/6.54</cell><cell>24.17/0.780</cell><cell cols="2">102.5 1.681</cell></row><row><cell>DAIN o (?9.3)</cell><cell>26.78/0.807/3.83</cell><cell>29.89/0.911</cell><cell>24</cell><cell>1.375</cell></row><row><cell>DAIN f [4]</cell><cell>27.52/0.821/3.47</cell><cell>29.99/0.910</cell><cell>24</cell><cell>1.375</cell></row><row><cell>Ours (S tst =3)</cell><cell>28.86/0.858/2.67</cell><cell>30.29/0.912</cell><cell>5.5</cell><cell>0.074</cell></row><row><cell>Ours (S tst =5)</cell><cell>30.12/0.870/2.15</cell><cell>30.18/0.911</cell><cell>5.5</cell><cell>0.075</cell></row><row><cell>?N:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>t : The runtime on 1024?1024-sized frames in sec.RED: Best performance, BLUE: Second best performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The endpoint error (EPE) between the approximated Ft0 and the pseudo ground truth is obtained by IRR-PWC<ref type="bibr" target="#b15">[16]</ref> on Vimeo90K<ref type="bibr" target="#b48">[49]</ref> testset. Note that the VFI performances are measured on X-TEST in terms of PSNR, SSIM and tOF for three models that adopt each approximation method.</figDesc><table><row><cell>Methods</cell><cell>Metrics</cell><cell>EPE?</cell><cell cols="2">PSNR? SSIM? tOF?</cell></row><row><cell cols="2">(a) Linear comb.</cell><cell>0.0752</cell><cell>28.73</cell><cell>0.8518 2.89</cell></row><row><cell cols="2">(b) Flow reversal</cell><cell>0.0892</cell><cell>28.30</cell><cell>0.8425 2.98</cell></row><row><cell cols="2">(c) CFR (ours)</cell><cell>0.0721</cell><cell>28.86</cell><cell>0.8582 2.67</cell></row><row><cell>S tst</cell><cell></cell><cell cols="3">(PSNR(dB)? / SSIM[45]? / tOF[8]?)</cell></row><row><cell>S trn</cell><cell>1</cell><cell></cell><cell>3</cell><cell>5</cell></row><row><cell>1</cell><cell cols="4">26.85/0.806/4.90 28.40/0.852/3.46 27.14/0.842/3.69</cell></row><row><cell>3</cell><cell cols="4">23.61/0.729/6.56 29.22/0.863/2.68 30.35/0.879/1.98</cell></row><row><cell>5</cell><cell cols="4">22.37/0.699/6.71 23.70/0.724/6.39 29.48/0.864/2.08</cell></row><row><cell cols="3">RED: Best performance of each row</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on adjustable scalability depending on the lowest scale depth Strn and Stst measured on X-TEST.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>The detailed architecture of the Feature Extraction Block of XVFI-Net. C s i is obtained by applying the last convolutional layer to C 0 i s times recurrently. The parameters are temporally shared for the two input frames (i = 0, 1).</figDesc><table><row><cell>Operation</cell><cell>H,W (? 1/2 S )</cell><cell>C</cell><cell>Remarks</cell></row><row><cell>input [C S 0 , C S 1 ]</cell><cell>1/4</cell><cell>64?2</cell><cell>-</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/8</cell><cell>128</cell><cell>-</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/16</cell><cell>256</cell><cell>-</cell></row><row><cell>NN upscale -conv2d -relu</cell><cell>1/8</cell><cell>128</cell><cell>-</cell></row><row><cell>NN upscale -conv2d -relu</cell><cell>1/4</cell><cell>64</cell><cell>-</cell></row><row><cell>conv2d</cell><cell>1/4</cell><cell cols="2">2+2+1+1 F S 01 , F S 10 , z S 01 , z S 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>The detailed architecture of the auto-encoder-based BiFlownet of XVFI-Net at the lowest scale depth.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>The detailed architecture of the auto-encoder-based BiFlownet of XVFI-Net except for the lowest scale depth.</figDesc><table><row><cell>Operation</cell><cell>H,W (? 1/2 s )</cell><cell>C</cell><cell>Remarks</cell></row><row><cell>input [C s 0 , C s 1 ,C s t0 ,C s t1 ,F s t0 ,F s t1 ]</cell><cell>1/4</cell><cell>64?4+2?2</cell><cell>-</cell></row><row><cell>conv2d (filter 1?1) -relu</cell><cell>1/4</cell><cell>64</cell><cell>-</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/8</cell><cell>128</cell><cell>-</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/16</cell><cell>256</cell><cell>-</cell></row><row><cell>NN upscale -conv2d -relu</cell><cell>1/8</cell><cell>128</cell><cell>-</cell></row><row><cell>NN upscale -conv2d -relu</cell><cell>1/4</cell><cell>64</cell><cell>-</cell></row><row><cell>conv2d -add to [F s t0 ,F s t1 ]</cell><cell>1/4</cell><cell>2+2+1</cell><cell>F s t0 , F s t1 , m s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>The detailed architecture of the auto-encoder-based TFlownet of XVFI-Net.</figDesc><table><row><cell>Operation</cell><cell>H,W (? 1/2 s )</cell><cell>C</cell><cell>Remarks</cell></row><row><cell>input ([C s 0 , C s 1 ,C s t0 ,C s t1 ])</cell><cell>1/4</cell><cell>256</cell><cell>-</cell></row><row><cell>pixel-shuffle [38] (?4)</cell><cell>1</cell><cell>16</cell><cell>PS</cell></row><row><cell>input [PS, F s t0 ? 2 , F s t1 ? 2 , I s 0 , I s 1 ,? s t0 ,? s t1 ]</cell><cell>1</cell><cell>16+2?2+3?4</cell><cell>-</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/2</cell><cell>64</cell><cell>enc 1</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/4</cell><cell>128</cell><cell>enc 2</cell></row><row><cell>conv2d (stride 2) -relu</cell><cell>1/8</cell><cell>256</cell><cell>-</cell></row><row><cell>conv2d -relu</cell><cell>1/8</cell><cell>256</cell><cell>-</cell></row><row><cell>NN upscale -concat to enc 2</cell><cell>1/4</cell><cell>384</cell><cell>-</cell></row><row><cell>conv2d -relu</cell><cell>1/4</cell><cell>128</cell><cell>-</cell></row><row><cell>NN upscale -concat to enc 1</cell><cell>1/2</cell><cell>192</cell><cell>-</cell></row><row><cell>conv2d -relu</cell><cell>1/2</cell><cell>64</cell><cell>-</cell></row><row><cell>NN upscale -conv2d</cell><cell>1</cell><cell>1+3</cell><cell>m s ,? s r</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>The detailed architecture of the U-Net [36]-based Refinement Block of XVFI-Net.</figDesc><table><row><cell>(5.6, 97.4) (1.4, 190.6)</cell><cell>181pixels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(19.1, 23.9)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(21.6,72.0)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(3.2, 40.9)</cell><cell>180pixels</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Overlapped two inputs (4096x2160)</cell><cell>AdaCoF f</cell><cell>FeFlow f</cell><cell>DAIN f</cell><cell>XVFI-Net (S tst =5)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast 4k video frame interpolation using a hybrid task-based convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Ha-Eun Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je Woo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">619</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast 4k video frame interpolation using a multi-scale optical flow reconstruction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Ha-Eun Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je Woo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisang</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1251</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A method for motion adaptive frame rate up-conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Castagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petri</forename><surname>Haavisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Ramponi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="436" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadi</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10663" to="10671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning temporal coherence via selfsupervision for gan-based video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayer</forename><surname>Jonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuerey</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nils</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="75" to="76" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Engineering observations from spatiovelocity and spatiotemporal visual models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Daly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Models and Applications to Image and Video Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="179" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Featureflow: Robust video interpolation via structureto-texture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Space-time-aware multi-resolution video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2859" to="2868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What matters in unsupervised optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep space-time video upsampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wug</forename><surname>Seoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fisr: Deep joint frame interpolation and super-resolution with a multiscale temporal loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11278" to="11286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A psychophysical study of improvements in motion-image quality by using high frame rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiji</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Oyaizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Yoshimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Information Display</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effects of motion image stimuli with normal and high frame rates on eeg power spectra: comparison with continuous motion image stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haruo</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Kusakabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Yamakoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Information Display</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="191" to="198" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2041" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study of subjective video quality at various frame rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3407" to="3411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5437" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Im-net for high resolution video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Sabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omry</forename><surname>Sendik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2398" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blurry video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongkuo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5114" to="5123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3360" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling and optimization of high frame rate video transmission over wireless networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2713" to="2726" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3370" to="3379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical motion estimation algorithm using multiple candidates for frame rate up-conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11049</biblScope>
			<biblScope unit="page">1104938</biblScope>
		</imprint>
	</monogr>
	<note>IWAIT</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A flexible recurrent residual pyramid network for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
