<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oriented Objects as pairs of Middle Lines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
							<email>weihaoran18@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Electronic</orgName>
								<orgName type="department" key="dep2">Electrical and Communication Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Key Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Key Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghan</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Electronic</orgName>
								<orgName type="department" key="dep2">Electrical and Communication Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Key Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Key Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Electronic</orgName>
								<orgName type="department" key="dep2">Electrical and Communication Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Key Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Key Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Oriented Objects as pairs of Middle Lines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Oriented Objects</term>
					<term>Object Detection</term>
					<term>Middle Lines</term>
					<term>Anchor- Free</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The detection of oriented objects is frequently appeared in the field of natural scene text detection as well as object detection in aerial images. Traditional detectors for oriented objects are common to rotate anchors on the basis of the RCNN frameworks, which will multiple the number of anchors with a variety of angles, coupled with rotating NMS algorithm, the computational complexities of these models are greatly increased. In this paper, we propose a novel model named Oriented Objects Detection Network (O 2 -DNet) to detect oriented objects by predicting a pair of middle lines inside each target. O 2 -DNet is an one-stage, anchor-free and NMS-free model. The target line segments of our model are defined as two corresponding middle lines of original rotating bounding box annotations which can be transformed directly instead of additional manual tagging. Experiments show that our O 2 -DNet achieves excellent performance on ICDAR 2015 and DOTA datasets. It is noteworthy that the objects in COCO can be regard as a special form of oriented objects with an angle of 90 degrees. O 2 -DNet can still achieve competitive results in these general natural object detection datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When the object such as text in natural scene and aerial target(e.g., airplane, ship, vehicle) appear in an image with a certain degree, the output form of horizontal bounding box which usually used in the detection of natural object no longer meet the detection requirement generally for it may include many redundant pixels which belong to background actually. Moreover, when detecting some objects which have a large aspect ratio and park densely, the cooperation between horizontal bounding box and NMS is easy to cause missed detection as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In order to solve problems aforementioned, an oriented bounding box output form has been proposed, and the following detection of oriented objects has attracted more and more attention in recent years. Many of recent research achievements of oriented objects detection rely on RCNN frameworks heavily. In the field of text detection in natural scene images, R2CNN <ref type="bibr" target="#b0">[1]</ref> adds a new branch to regress two points and the height of oriented bounding box based on Faster R-CNN <ref type="bibr" target="#b1">[2]</ref>. However, during the training stage, horizontal anchors are still be suppressed by NMS within the RPN network when the objects need to be detected have large aspect ratios and park densely. RRPN <ref type="bibr" target="#b2">[3]</ref> proposes rotation anchors to replace the horizontal ones and the corresponding NMS is also replaced by a new NMS algorithm calculated by rotating IOU. However, in order to cover objects of any angles, more anchors with different angles, aspect ratios and scales need to be set which increases the computational complexity of the model. Moreover, the introduction of rotation anchors in RRPN adds additional regression of angle information via Smooth L1 <ref type="bibr" target="#b3">[4]</ref>, but the combination of rotating IOU and Smooth L1 is not perfect because the angle has boundary problem, so the oriented bounding box output by this algorithm is not accurate and often accompanied by angle jitter.</p><p>In the field of aerial images, the detection of oriented object is more difficult campare with text detection in natural scene for the complex background as well as the variation of spatial resolution of images. SCRDet <ref type="bibr" target="#b4">[5]</ref> proposes an IOU Loss to address the boundary problem for oriented bounding box mentioned in RRPN. In addition, a Multi-Dimensional Attention Network is added to deal with the complex background of aerial images. However, SCRDet is still anchor-based and NMS-based and also faces some problems bring by them. For instance, in the testing stage, 300 proposals are taken from 10000 regression boxes by NMS in <ref type="bibr" target="#b4">[5]</ref> but according to our statistics, for DOTA dataset, a crop image of 800 ? 800 size can reach up to 2000+ objects, which will cause missed detection.</p><p>Recently, in view of the disadvantages of the anchor-based models, a number of anchor-free algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> have emerged. CornerNet <ref type="bibr" target="#b5">[6]</ref> and ExtremeNet <ref type="bibr" target="#b6">[7]</ref> can be divided into the process of detection and grouping of keypoints. While it    <ref type="figure" target="#fig_0">Figure (a1)</ref> shows that if the intersection point drift in the drift region, the final bounding box do not drift. If the angle of oriented object is 90, they will be predicted by branch 1, and objects with other angles will be detected by branch 2. In ICDAR 2015, the shape of bounding box of text is not always a rectangular, so we set the vertical loss in Line Loss to 0 when detecting text in natural scene.</p><p>is not suitable to apply them into aerial images which may obtain many objects due to the high time complexity of their grouping algorithm. CenterNet <ref type="bibr" target="#b8">[9]</ref> puts forward a new method of regressing the height and weight of object at the center point. In order to achieve NMS-free, they obtain the center point in heatmap through a method of searching 8-connect of each center point. However, this method still needs to choose K top scoring objects which can not be applied to single image with numerous targets. In addition, it may lead to the drift of the center point, and further cause the bounding box to drift. Different from the anchor-free models based on keypoints detection above, FCOS <ref type="bibr" target="#b7">[8]</ref> belongs to the category of dense sampling, which regress in numerous pixels for one objects. Because the bounding box of an object is regressed by a large number of pixels, NMS is still needed to filter the redundant boxes in FCOS.</p><p>In this paper, we propose a novel anchor-free and NMS-free model named O 2 -DNet as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> to detect oriented objects by a pair of middle lines. Our model is a new form of anchor-free which combines the mothods of keypoints detection and dense sampling. In order to reach the aim of NMS-free, we choose the method of keypoint detection to locate the intersection point of each pair of median lines. For the problem of intersection point drift, we design a drift region inspired by the method of dense sampling to ensure that the intersection point drift in the drift region will not affect the position of the final bounding box. In order to successfully predict the middle line, we design a specific Line Loss according to the characteristics of lines(e.g., length, slope, position) to regress each median line. The Line Loss consists of three parts: the position loss to control the location of the endpoints of each middle line, the parallel loss to control the two endpoints of each middle line and the intersection point of two middle lines are collinear. The last one is vertical loss to control the geometric relationship between two median lines of one object. There is an order for the regression of endpoints of middle lines, so we wil also face the boundary problem. In order to solve it, we design O 2 -DNet as two branches, one for predicting horizontal objects with 90 degrees and the other for oriented objects of other angles. The design of two branches also enables us to apply O 2 -DNet to COCO <ref type="bibr" target="#b9">[10]</ref> without changing any network structure.</p><p>Our contributions and innovations are as follows:</p><p>(1) We propose a noval anchor-free and NMS-free model named O 2 -DNet to detect oriented objects by a pair of middle lines.</p><p>(2) Our O 2 -DNet is a new form of anchor-free which combines keypoints detection and dense sampling, and we design drift region to relax the requirement for accurate extracting keypoints.</p><p>(3) Our O 2 -DNet can detecting oriented objects and horizontal objects under a single network without increasing computational complexities via two branches. For the regression of middle lines, we design a special Line Loss.</p><p>The rest of this paper is organized as follows: In Section 2, we introduce the related works done by researchers before and basic principle in our method. The details of our network and algorithms are shown in Section 3. We place our experiments results and analysis in Section 4. At last, our work is summarized and concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Detectors based on Manually Engineered Features</head><p>Traditional object detectors mainly depend on manually engineered features, they first select features like Histogram of Oriented Gradient (HOG) <ref type="bibr" target="#b10">[11]</ref> generally and then input them to classifier such as Support Vector Machine (SVM) <ref type="bibr" target="#b11">[12]</ref> to identify the existence of object. The generalization capability of these detectors is limited by features extraction and the robustness of this type of detector needs to be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detectors based on DCNNs</head><p>In recent years, the success and development of deep convolution neural networks (DCNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> bring great progress to the field of object detection.</p><p>Compared with tradition detectors aforementioned, detectors based on DC-NNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> can automatically extract features through the backbone networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and the accuracy as well as robustness of models is greatly improved. There are two branches which are anchor-based and anchor-free in DCNNs based detectors at present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anchor-Based vs. Anchor-Free Detectors</head><p>The concept of anchor was proposed in Region Proposals Networks (RPN) of Faster R-CNN <ref type="bibr" target="#b1">[2]</ref>, which acts as extracting proposals and guiding the regression task of networks. Subsequently, the anchor mechanism within RPN is widely used in two-stage detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref>. For one-stage detectors which detect objects <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref> without RPN, YOLOv1 <ref type="bibr" target="#b14">[15]</ref> not use the anchor mechanism can't provide accuracy comparable to that of two-stage detectors. Afterwards, anchor methods are also extensively utilized in one-stage detectors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref> to improve the performance of models. In the detection of oriented objects, most algorithms rely on anchor mechanism heavily. In general, these models output the oriented bounding boxes by rotating anchors to regress an additional angle information, and then obtain the final bounding boxes by the filtering of rotated NMS algorithm.</p><p>The anchor mechanism promotes the development of object detection, but it is still not perfect and also has some problems like mentioned in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Recently, the research of anchor-free has become a hot topic in the field of object detection . At present, the anchor-free detectors can be roughly divided into two categories. One is to locate objects through keypoints such as corner points in CornerNet <ref type="bibr" target="#b5">[6]</ref>, extreme points in ExtremeNet <ref type="bibr" target="#b6">[7]</ref> and center points in CenterNet <ref type="bibr" target="#b8">[9]</ref>, the other is via the regression of a lot of points like FCOS <ref type="bibr" target="#b7">[8]</ref> to get the location of objects. For oriented objects, both of these two anchor-free categories have defects. In the inference stage, they all need to keep K of the highest scoring objects and may cause missed detection in the case of many targets in a single image like small cars in aerial image. O 2 -DNet is an one stage and anchor-free detector, which locates objects through a pair of median lines and their intersection point. In order to solve the top K problem, we combine the two categories of existing anchor-free alogrithm to design O 2 -DNet. The details of our model will be explained in the next section.   lines, and 2 ? (2 ? 4 ? C ? w d ? h d ) regression maps to predict the corresponding two middle lines, where C represents the number of classes in this image, the d is output stride of down sampling module, and the first 2 means two branches of O 2 -DNet. The design of two branches is to deal with the angle boundary problems of oriented objects via the independent prediction of objects with angle of 90 degree. For the prediction of middle lines, we obtain them by regressing their corresponding two endpoints. The form of regressing middle lines is inspired by CenterNet <ref type="bibr" target="#b8">[9]</ref>, it is the relative position of endpoint from intersection point. Moreover, we design special loss functions to control the relationship of endpoints to ensure the predicted median lines more accurate. In addition, in order to reduce the dependence of middle lines extraction on the precision of intersection point extraction, we propose the point drift region to make O 2 -DNet output high-quality oriented bounding boxes when the extraction of intersection points is not accurate enough.</p><formula xml:id="formula_0">3 O 2 -DNet 3.1 Overview</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hourglass Networks</head><p>Hourglass Networks <ref type="bibr" target="#b21">[22]</ref> was first proposed for human keypoints detection. In CornerNet <ref type="bibr" target="#b5">[6]</ref>, Law et al. modified hourglass network and introduced it into the field of object detection. We choose 104-Hourglass Networks modified in CornerNet <ref type="bibr" target="#b5">[6]</ref> as our backbone. For one image as input, HourglassNet regress C channels of heatmaps with each pixel valuey ? (0, 1) which means the confidence of being judged as positive. Compared with CornerNet, O 2 -DNet defines each keypoint with value setting to 1, instead of the Gaussian Kernel. In the stage of inference, in order to avoid missed detection, instead of NMS and top K method to extract keypoints in heatmap used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, we take a simple and rough way to extract keypoints, which is finding the connected domains in heatmap, and then define the center of each connected domain as the target keypoint. It is true that this method is not accurate enough, but it can achieve satisfactory results in the experiments through the collocation with intersection drift region proposed in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Middle Lines and Their Intersection Point</head><p>As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, let L 1 [(x1, y1), (x2, y2)] denote middle line 1 of the object, (x1, y1) and (x2, y2) are the endpoint 1 and 2 of L 1 respectively. Similarly, L 2 [(x3, y3), (x4, y4)] is defined as the middle line 2. For the first branch of O 2 -DNet, we define the horizontal median line as L 1 , and the vertical one as L 2 . We regard the right endpoint and the top one as the endpoint 1 of L 1 and L 2 respectively. For the second branch, the longer one of the two median lines is defined as L 1 , the other is L 2 . Endpoints 1 are also defined as the right one and the top one in L 1 and L 2 in this branch. The intersection point (x, y) of two middle lines can be obtained through simple operators ( x1+x2+x3+x4</p><formula xml:id="formula_1">4 ,<label>y1+y2+y3+y4 4</label></formula><p>) in both two branches.</p><p>Intersection Point We follow the modified focal loss <ref type="bibr" target="#b20">[21]</ref> in CornerNet <ref type="bibr" target="#b5">[6]</ref> to predict the heatmap of intersection point of target middle lines. Because the ground truth of our model with value setting to 1 instead of the Gaussian Kernel like mentioned in Section 3.2, the loss in O 2 -DNet is a little different compared with CornerNet in form. We name the loss of intersection point in our model as L ip :</p><formula xml:id="formula_2">L ip = ? 1 N xy (1 ?? xy ) ? log? xy , if Y xy = 1 Y xy ? log(1 ?? xy ), if Y xy = 0 (1)</formula><p>where ? is a hyper-parameter and the value fixed to 2 in our experiment.? xy represents the pixel value at the coordinate (x, y) in heatmap and Y xy corresponds to the ground truth. N is the number of objects.  Middle Lines The method of regressing middle lines is to regress the relative distance between each endpoint of per middle line with the intersection point. As shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, for middle line 1, we need to regress 4?H ?W maps, and the values of these 4 maps in the position of intersection point are ?x1, ?y1, ?x2 and ?y2 respectively. The form of regression of middle line 2 is the same as middle line 1. The loss to regress each middle line is as follows:</p><formula xml:id="formula_3">L 1 = 1 N 2 ep=1 xy [SmoothL1(?x * x y , ?xx y ) + SmoothL1(?y * x y , ?yx y )] ep (2)</formula><p>where N is the number of objects. ep denotes the endpoint of the corresponding middle line.xy means the coordinate of the regression map. ?x * and ?y * represent the ground truth. The way of regressing each endpoint of per middle line independently may result in two endpoints and the intersection point being not collinear. In order to address this problem, we introduce a loss function as follows:</p><formula xml:id="formula_4">L 2 = 1 N 2 l=1 xy [SmoothL1(?x ep1 xy ? ?y ep2 xy , ?x ep2 xy ? ?y ep1 xy )] l<label>(3)</label></formula><p>where l means two middle lines of per object. ep1 and ep2 denote the endpoint 1 and 2 of each middle line. There are two middle lines of one object, and they are vertical in space generally. In order to control the relationship of two target middle lines, we design L 3 as follows:</p><formula xml:id="formula_5">L 3 = 1 N xy [SmoothL1(?x ep1 l1 xy ? ?x ep1 l2 xy + ?y ep1 l1 xy ? ?y ep1 l2 xy , 0)] l<label>(4)</label></formula><p>where ep1 l1 means endpoint 1 of middle line 1, ep1 l2 means endpoint 1 of middle line 2. The L 1 , L 2 and L 3 make up the Line Loss of our model:</p><formula xml:id="formula_6">L l = L 1 + ?L 2 + ?L 3<label>(5)</label></formula><p>And the total loss of our model can be expressed as:</p><formula xml:id="formula_7">L oss = L ip + ?L l<label>(6)</label></formula><p>where ?, ? and ? are weights of losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drift Region</head><p>The extraction of intersection point in heatmap will affect the accuracy of middle lines extraction. Inspired by FCOS <ref type="bibr" target="#b7">[8]</ref>, we set up circular drift regions in the center of objects according to the size of them. All pixel points in drift region will regress the different values according to their relative distances from endpoints of middle lines. The drift region guarantees that the extraction of intersection point from heatmap will not influence the position of final oriented bounding box. The radius of the drift region is set as follows:</p><formula xml:id="formula_8">R = min[r/stride, min(L 1 , L 2 )/(2 ? stride)]<label>(7)</label></formula><p>where stride is output stride of our model, L 1 and L 2 are the middle line 1 and 2 respectively. Where r is 16 in our model. Unlike remote sensing images, objects in natural scenes sometimes have overlaps to form fuzzy samples, we also follow FCOS to address this problem which is when a pixel belongs to two targets at the same time, we regress the small one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In the stage of experiments, we select three datasets to verify the performance of our model. These datasets involve in different research fields: oriented objects detection of aerial images, text detection in natural scene, the detection of objects in nature images. Their detailed introductions are as follows:</p><p>DOTA DOTA[23] is a common benchmark for the detection of objects in aerial images. It includes two detection tasks: horizontal bounding boxes and oriented bounding boxes, and we only use the oriented one in our experiments. There are 2806 aerial images with size ranges from 800 ? 800 to 4000 ? 4000 pixels total in DOTA. These images are annotated using 15 categories (e.g., aircraft, small car, ship). In practice, we divide each large image to crop images in 800 ? 800 with overlap of 0.25.</p><p>ICDAR 2015 ICDAR 2015 <ref type="bibr" target="#b23">[24]</ref> is a dataset used for the detection of text in natural scene. The training set and test set include 1000 and 500 images with the size of 720 ? 1280, respectively.</p><p>COCO The challenging MS COCO <ref type="bibr" target="#b9">[10]</ref> dataset contains 80k images for training, 40k for validation and 20k for testing and includes 80 categories. The annotations in COCO are horizontal bounding boxes which we used to test the generality of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and Testing Details</head><p>All our experiments are implemented on PyTorch 1.0 <ref type="bibr" target="#b24">[25]</ref> by two NVIDIA Tesla V100 GPUs with 2 ? 32 GB memories. For DOTA, we set the input resolution 800 ? 800 to 511 ? 511 and the output stride to 4 following settings in Cor-nerNet <ref type="bibr" target="#b5">[6]</ref> during the training stage. Adam <ref type="bibr" target="#b25">[26]</ref> is selected as the optimizer for O 2 -DNet. We train our model from scratch to 300k iterations with the batch size setting to 32. The learning rate starts from 0.001 and 10 times lower for every third iterations. Simple random horizontal and vertical flipping as well as color dithering are used to enhance the data in our experiments. The weights of loss ?, ? and ? (Section 3.3) are setting to 1, 1 and 0.5 respectively during training. For ICDAR 2015 and COCO, O 2 -DNet is finetuned on two v100 GPUs for 200k iterations with a batch size of 32 from a pre-trained CornerNet model which trained on 10 GPUs for 500k iterations. Other settings are the same as DOTA. It is worth noting that for the two branches of our model, we do not strictly divide them by 90 degrees, but by an angle range of (88, 92) degrees.</p><p>During test stage, as mentioned in Section 3.2, we need to transform the heatmap into a binary image to extract the intersection point of two target median lines, where the threshold is set to 0.3. When the angle of an object is critical in two branches, it may have output in both two. We take the one with the highest intersection point score as the final output of O 2 -DNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-art Frameworks</head><p>In this part, we first prove the advancement of O 2 -DNet on the oriented objects datasets (DOTA, ICDAR 2015). Then we test the strong generality of our model on the dataset of natural objects (COCO). DOTA As shown in <ref type="table">Table 1</ref>, our O 2 DNet achieve 71.04% mAP on DOTA dataset, better than most two-stage and one-stage models used in the detection of aerial objects at present. For bridges with large aspect ratio and dense parked small vehicles, our anchor-free model achieves the most advanced accuracy on AP due to the better adaptive feature extraction ability than anchor-based models.</p><p>ICDAR 2015 For ICDAR 2015 dataset, most of the annotation of objects is not in the form of rectangle, but in the form of irregular quadrilateral which is close to parallelogram. We shield the L 3 of the Line Loss, which is used to control the two target middle line to remain vertical. As shown in <ref type="table" target="#tab_3">Table 2</ref> COCO In order to verify the general performance of our model, we also make experiments on the COCO dataset of natural scene object detection. For COCO with objects labeled in horizontal bounding boxes, our model will only have output in the first branch. As shown in <ref type="table" target="#tab_5">Table 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this part, we conduct three ablation experiments on the DOTA dataset, which are the influence of different backbones on the performance of our model, the influence of Line Loss on the performance of our model, and the influence of single branch on the performance of our model. <ref type="table" target="#tab_6">Table 4</ref> shows all experimental data. The following is the specific analysis: ResNet101-FPN We replace the backbone 104-Hourglass with ResNet101-FPN <ref type="bibr" target="#b34">[35]</ref> to verify the effect of our model with different backbones. As shown in <ref type="table" target="#tab_6">Table 4</ref>, our model also achieve satisfactory 68.93% mAP in matching with ResNet101-FPN, which means that the performance of O 2 -DNet does not depend entirely on 104-Hourglass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Without Line Loss</head><p>In order to prove the validity of Line Loss, we shield the L 2 , L 3 part of Line Loss, and keep the other settings of O 2 -DNet. <ref type="table" target="#tab_6">Table 4</ref> shows that our model with Line Loss improves 1.92% mAP compared with the model without Line Loss. The Line Loss effectively controls the line segment property of the regression target median lines in our model. The effect of Line Loss is shown in <ref type="figure" target="#fig_8">Fig. 7</ref>.</p><p>(a) (b) Single branch In order to verify that the two branches of O 2 -DNet can solve the boundary problem better, we cut off the first branch and input the 90 degree object into the second branch in the form of the original ground truth defined in Section 3.3. The experimental results show that the mAP of two branches is 2.23% higher than that of single branch. The design of two branches is significant for our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel one-stage and anchor-free model named O 2 -DNet to detect oriented objects. O 2 -DNet locates each object by predicting a pair of middle lines inside them. As a result, our model is competitive compared with state-of-the-art detectors in several fields: oriented objects detection of aerial images, text detection in natural scene, the detection of objects in nature images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Horizontal bounding box vs. Oriented bounding box. Compared with Figure (c), the bounding box in Figure(a) carries too much redundant information. The trucks in Figure (b) and (d) have a large aspect ratio and park densely. When detecting them with horizontal bounding box, NMS algorithm will lead to missed detection due to the intersection-over-union (IOU) of two objects is too large. This problem can be solved via introducing oriented bounding box like Figure (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Branch</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Views of O 2 -DNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig</head><label></label><figDesc>Fig. 3 illustrates the architecture of our method. O 2 -DNet locates per object by detecting a pair of corresponding middle lines. We use 104-Hourgalss[6] as the backbone of our model following the CornerNet [6] for its excellent performance of extracting features. For an image of size w ? h, as input, our model outputs 2?(1?2?C ? w d ? h d ) heatmaps to predict the intersection points of target middle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Architecture of O 2 -DNet. O 2 -DNet locates each object via a pair of median lines and their intersection point. Each middle line is represented by two corresponding endpoints. Two branches are added to detect horizontal and oriented objects respectively. O 2 -DNet outputs different classes of objects to different channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The definition of two middle lines and the order of two endpoints of per middle line. The first line of figures represent objects with angles of 90 degree, and the second line of figures represent oriented objects with another angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The method of regressing middle lines in our model. The relative distance between endpoint and intersection point is represented as ? xi, ? yi, where i is 1,2,3,4 meaning the four corresponding endpoints of middle lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of drift region. Per pixel in drift region will regress the relative distances from 4 endpoints of corresponding two middle lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Effects of Line Loss. Figure (a) shows the output of our model with Line Loss and Figure (b) shows the result of our model without Line Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results output by O 2 -DNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. 3 illustrates the architecture of our method. O 2 -DNet locates per object by detecting a pair of corresponding middle lines. We use 104-Hourgalss[6] as the backbone of our model following the CornerNet [6] for its excellent performance of extracting features. For an image of size w ? h, as input, our model outputs 2?(1?2?C ? w d ? h d ) heatmaps to predict the intersection points of target middle</figDesc><table><row><cell>Branch1</cell><cell></cell><cell></cell></row><row><cell>Branch2</cell><cell></cell><cell></cell></row><row><cell>Down sampling</cell><cell></cell><cell></cell></row><row><cell>Hourglass Network</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Prediction modules</cell><cell></cell></row><row><cell></cell><cell cols="3">N channels for n classes</cell></row><row><cell></cell><cell>L1[(?x1,?y1),(?x2,?y2)]</cell><cell></cell></row><row><cell></cell><cell>?y2</cell><cell></cell><cell>?y3</cell></row><row><cell cols="2">Middle line1 (x, y)</cell><cell>?x2</cell><cell>?y4 ?x1 ?y1 ?x3 (x, y)</cell></row><row><cell>ConvNet</cell><cell></cell><cell cols="2">?x4</cell></row><row><cell>Intersection</cell><cell></cell><cell></cell></row><row><cell>point</cell><cell>L2[(?x3,?y3),(?x4,?y4)]</cell><cell></cell></row><row><cell cols="2">Middle line2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67 RRPN [3] 88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01 R-DFPN [27] 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94 ICN [28] 81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20 68.20 RoI-Transformer [29] 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 SCRDet [5] 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61</figDesc><table><row><cell>Method</cell><cell>Pl</cell><cell>Bd</cell><cell>Br</cell><cell>Gft</cell><cell>Sv</cell><cell>Lv</cell><cell>Sh</cell><cell>Tc</cell><cell>Bc</cell><cell>St</cell><cell>Sbf Ra</cell><cell>Ha Sp</cell><cell>He mAP</cell></row><row><cell>Two-stage models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">R 2 CNN [1] 80.94 One-stage models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD [17]</cell><cell cols="13">39.83 9.09 0.64 13.18 0.26 0.39 1.11 16.24 27.57 9.23 27.16 9.09 3.03 1.05 1.01 10.59</cell></row><row><cell>YOLOv2 [15]</cell><cell cols="13">39.57 20.29 36.58 23.42 8.85 2.09 4.82 44.34 38.35 34.65 16.02 37.62 47.23 25.5 7.45 21.39</cell></row><row><cell>RetinaNet [21]</cell><cell cols="13">88.92 67.67 33.55 56.83 66.11 73.28 75.24 90.87 73.95 75.07 43.77 56.72 51.05 55.86 21.46 62.02</cell></row><row><cell>R 3 Det [30]</cell><cell cols="13">89.54 81.99 48.46 62.52 70.48 74.29 77.54 90.80 81.39 83.54 61.97 59.82 65.44 67.46 60.05 71.69</cell></row><row><cell>O 2 -DNet</cell><cell cols="13">89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04</cell></row><row><cell cols="14">Table 1. State-of-the-art comparisons on DOTA. The abbreviations of the names are</cell></row><row><cell cols="14">defined as: Pl: Plane, Bd: Baseball diamond, Br: Bridge, Gft: Ground field track, Sv:</cell></row><row><cell cols="14">Small vehicle, Lv: Large vehicle, Sh:Ship, Tc: Tennis court, Bc: Basketball court, St:</cell></row><row><cell cols="14">Storage tank, Sbf: Soccer-ball field, Ra: Roundabout, Ha: Harbor, Sp: Swimming pool,</cell></row><row><cell cols="14">and He: Helicopter. The SSD, YOLOv2 and RetinaNet are modified by us to output</cell></row><row><cell cols="3">oriented bounding boxes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>, our O 2 -DNet achieve 82.97% F1, better than other models we choose for comparison. The experimental results show that our model can be used not only in the detection of aerial images but also in natural scene text detection. Comparison with different methods on the ICDAR2015 dataset. Our model achieve 82.97% F1, better than other models in this table.</figDesc><table><row><cell>Method</cell><cell cols="2">Recall Precision F1 Method</cell><cell cols="2">Recall Precision F1</cell></row><row><cell>CTPN [31]</cell><cell>51.56 74.22</cell><cell cols="2">60.85 SegLink [32] 76.80 73.10</cell><cell>75.00</cell></row><row><cell>R 2 CNN [1]</cell><cell>79.68 85.62</cell><cell cols="2">82.54 EAST [33] 78.33 83.27</cell><cell>80.72</cell></row><row><cell cols="2">FOTS RT [34] 85.95 79.83</cell><cell cols="2">82.78 RRPN [3] 82.17 73.23</cell><cell>77.44</cell></row><row><cell>O 2 -DNet</cell><cell>80.52 85.58</cell><cell>82.97</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, our O 2 -DNet achieve 41.3% AP on COCO dataset, leading most one-stage detectors. Backbone AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell cols="2">Two-stage detectors</cell><cell></cell></row><row><cell>Faster R-CNN[35]</cell><cell cols="3">ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2</cell></row><row><cell cols="3">Deformable-CNN[36] Inception-ResNet 37.5 58.0</cell><cell>-19.4 40.1 52.5</cell></row><row><cell>Mask R-CNN[37]</cell><cell cols="3">ResNeXt-101 39.8 62.3 43.4 22.1 43.2 51.2</cell></row><row><cell>Cascade R-CNN[20]</cell><cell>ResNet-101</cell><cell cols="2">42.8 62.1 46.3 23.7 45.5 55.2</cell></row><row><cell>D-RFCN + SNIP[38]</cell><cell>DPN-98</cell><cell cols="2">45.7 67.3 51.1 29.3 48.8 57.1</cell></row><row><cell>PANet[39]</cell><cell cols="3">ResNeXt-101 47.4 67.2 51.8 30.1 51.7 60.0</cell></row><row><cell cols="2">One-stage detectors</cell><cell></cell></row><row><cell>YOLOv2[16]</cell><cell>DarkNet-19</cell><cell cols="2">21.6 44.0 19.2 5.0 22.4 35.5</cell></row><row><cell>SSD[17]</cell><cell>ResNet-101</cell><cell cols="2">31.2 50.4 33.3 10.2 34.5 49.8</cell></row><row><cell>RetinaNet[21]</cell><cell>ResNet-101</cell><cell cols="2">39.1 59.1 42.3 21.8 42.7 50.2</cell></row><row><cell>RefineDet[40]</cell><cell>ResNet-101</cell><cell cols="2">36.4 57.5 39.5 16.6 39.9 51.4</cell></row><row><cell>CornerNet[6]</cell><cell cols="3">104-Hourglass 40.5 56.5 43.1 19.4 42.7 53.9</cell></row><row><cell>ExtremeNet[7]</cell><cell cols="3">104-Hourglass 40.2 55.5 43.2 20.4 43.2 53.1</cell></row><row><cell>CenterNet[9]</cell><cell cols="3">104-Hourglass 42.1 61.1 45.9 24.1 45.5 52.8</cell></row><row><cell>FCOS[8]</cell><cell cols="3">ResNet-101-FPN 41.0 60.7 44.1 24.0 44.1 51.0</cell></row><row><cell>O 2 -DNet</cell><cell cols="3">104-Hourglass 41.3 60.9 45.2 24.2 44.5 52.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>State-of-the-art comparison on COCO test-dev. It shows that our O 2 -DNet has a strong competitiveness in natural scene object detection. In this table, all data are from single scale detection results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Method Pl Bd Br Gft Sv Lv Sh Tc Bc St Sbf Ra Ha Sp He mAP ResNet101-FPN[35] 88.07 81.43 47.03 60.51 68.42 70.96 73.72 90.01 81.15 78.24 59.83 57.52 56.01 62.63 58.37 68.93 Without Line Loss 87.25 82.12 47.04 60.14 67.21 70.01 71.38 90.26 79.89 81.09 58.43 59.12 56.05 66.82 60.06 69.12 Single branch 86.17 81.07 47.06 60.28 66.28 72.44 69.09 88.90 81.29 78.77 59.12 57.97 58.14 64.69 59.98 68.81 O 2 -DNet 89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04 Ablation Studies on DOTA. ResNet101-FPN means that the backbone of our model is replaced by ResNet101-FPN. Without Line Loss denotes that we only keep the L1 part of the total Line Loss. Single branch means that we only keep the second branch of O 2 -DNet.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">R2cnn: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arbitraryoriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<title level="m">Fcos: Fully convolutional one-stage object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international Conference on computer vision &amp; Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>Icdar 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards multiclass object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02700</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning roi transformer for detecting oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L G S X Q L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<title level="m">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
