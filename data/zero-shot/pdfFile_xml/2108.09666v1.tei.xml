<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Embedding for Few-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relational Embedding for Few-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to address the problem of few-shot classification by meta-learning "what to observe" and "where to attend" in a relational perspective. Our method leverages relational patterns within and between images via selfcorrelational representation (SCR) and cross-correlational attention (CCA). Within each image, the SCR module transforms a base feature map into a self-correlation tensor and learns to extract structural patterns from the tensor. Between the images, the CCA module computes crosscorrelation between two image representations and learns to produce co-attention between them. Our Relational Embedding Network (RENet) combines the two relational modules to learn relational embedding in an end-to-end manner. In experimental evaluation, it achieves consistent improvements over state-of-the-art methods on four widely used few-shot classification benchmarks of miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b74">75]</ref> aims to learn new visual concepts from a small number of examples. The task is defined to classify a given query image into target classes, each of which is unseen during training and represented by only a few support images. Recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b86">87]</ref> tackle the problem by meta-learning a deep embedding function such that the distance between images on the embedding space conforms to their semantic distance. The learned embedding function, however, often overfits to irrelevant features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> and thus fails to transfer to new classes not yet observed in training. While deep neural features provide rich semantic information, it remains challenging to learn a generalizable embedding without being distracted by spurious features.</p><p>The central tenet of our approach is that relational patterns, i.e., meta-patterns, may generalize better than individual patterns; an item obtains a meaning in comparison with other items in a system, and thus relevant information can be extracted from the relational structure of items. On this basis, we propose to learn "what to observe" and "where to attend" in a relational perspective and combine them to produce relational embeddings for few-shot learning. We achieve this goal by leveraging relational patterns within and between images via (1) self-correlational representation (SCR) and (2) cross-correlational attention (CCA). The SCR module transforms a base representation into its self-correlation tensor and learns to extract structural patterns from it. Self-correlation of a deep feature map encodes rich semantic structures by correlating each activation of the feature map to its neighborhood. We perform representation learning on top of it to make relevant structural patterns of the image stand out ( <ref type="figure" target="#fig_0">Fig. 1 (a)?(b)</ref>). On the other hand, the CCA module computes cross-correlation between two image representations and learns to produce coattention from it. Cross-correlation encodes semantic correspondence relations between the two images. We learn high-dimensional convolutions on the cross-correlation tensor to refine it via convolutional matching and produce adaptive co-attention based on semantic relations between the query and the support ( <ref type="figure" target="#fig_0">Fig. 1 (b)?(c)</ref>).</p><p>The proposed method combines the two modules to learn relational embeddings in an end-to-end manner; it extracts relational patterns within each image (via SCR), generates relational attention between the images (via CCA), and aggregates the cross-attended self-correlation representations to produce the embeddings for few-shot classification. Experiments on four standard benchmark datasets demonstrate that the proposed SCR and CCA modules are effective at highlighting the target object regions and significantly improve few-shot image classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Few-shot classification. Recent few-shot classification methods are roughly categorized into three approaches. The metric-based approach aims to learn an embedding function that maps images to a metric space such that the relevance between a pair of images is distinguished based on their distance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b86">87]</ref>. The optimization-based approach meta-learns how to rapidly update models online given a small number of support samples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b83">84]</ref>. The two aforementioned lines of work formulate few-shot classification as a meta-learning problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b62">63]</ref>. The transfer-learning approach <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b89">90]</ref> has recently shown that the standard transfer learning procedure <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b84">85]</ref> of early pre-training and subsequent fine-tuning is a strong baseline for few-shot learning with deep backbone networks. Among these, our work belongs to the metric-based approach. The main idea behind a metric-based few-shot classifier is that real images are distributed on some manifolds of interest, thus the embedding function adequately trained on the training classes can be transferred to embed images of unseen target classes by interpolating or extrapolating the features <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b71">72]</ref>. Our work improves the transferability of embedding by learning self-and cross-relational patterns that can better generalize to unseen classes. Self-correlation. Self-correlation or self-similarity reveals a structural layout of an image by measuring similarities of a local patch within its neighborhood <ref type="bibr" target="#b64">[65]</ref>. Early work uses the self-correlation itself as a robust descriptor for visual correspondence <ref type="bibr" target="#b73">[74]</ref>, object detection <ref type="bibr" target="#b6">[7]</ref>, and action recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Recent work of <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b88">89]</ref> adopts selfcorrelation as an intermediate feature transform for a deep neural network and shows that it helps the network learn an effective representation for semantic correspondence <ref type="bibr" target="#b26">[27]</ref>, image translation <ref type="bibr" target="#b88">[89]</ref>, and video understanding <ref type="bibr" target="#b30">[31]</ref>. Inspired by the work, we introduce the SCR module for fewshot classification. Unlike self-correlation used in the previous work, however, our SCR module uses channel-wise self-correlation to preserve rich semantic information for image recognition. Note that while self-attention <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b77">78]</ref> also computes self-correlation values as attention weights for aggregation, it does not use the self-correlation tensor directly for representation learning and thus is distinct from this line of research.</p><p>Cross-correlation. Cross-correlation has long been used as a core component for a wide range of correspondencerelated problems in computer vision. It is commonly implemented as a cost-volume or correlation layer in a neural network, which computes matching costs or similarities between two feature maps, and is used for stereomatching <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b85">86]</ref>, optical flow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b81">82]</ref>, visual correspondence <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref>, semantic segmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b67">68]</ref>, video action recognition <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b76">77]</ref>, video object segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>, among others. Some recent few-shot classification methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b86">87]</ref> adopt cross-correlation between a query and each support to identify relevant regions for classification. However, none of them <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b86">87]</ref> leverage geometric relations of features in cross-correlation and they often suffer from unreliable correlation due to the large variation of appearance. Unlike these previous methods, our CCA module learns to refine the crosscorrelation tensor with 4D convolution, filtering out geometrically inconsistent correlations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58]</ref>, to obtain reliable co-attention. In our experiment, we provide an in-depth comparison with the most related work of <ref type="bibr" target="#b19">[20]</ref>.</p><p>Our contribution can be summarized as follows:</p><p>? We propose to learn the self-correlational representation for few-shot classification, which extracts transferable structural patterns within an image.</p><p>? We present the cross-correlational attention module for few-shot classification, which learns reliable coattention between images via convolutional filtering.</p><p>? Experiments on four standard benchmarks show our method achieves the state of the art, and ablation studies validate the effectiveness of the components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary on few-shot classification</head><p>Few-shot classification aims to classify images into target classes given only a few images for each class. Deep neural networks are vulnerable to overfitting with such a small amount of annotated samples, and most few-shot classification methods <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">75]</ref> thus adopt a meta-learning framework with episodic training for few-shot adaptation. In few-shot classification, a model is optimized using training data D train from classes C train and then evaluated on test data D test from unseen classes C test where C train ? C test = ?. Both D train and D test consist of multiple episodes, each of which contains a query set Q = (I q , y q ) and a support set</p><formula xml:id="formula_0">S = {(I (l) s , y (l) s )} N K l=1</formula><p>of K image-label pairs for each N classes, i.e., N -way K-shot episode <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b74">75]</ref>. During training, we iteratively sample an episode from D train and train the model to learn a mapping from (S, I q ) to y q . During testing, the model uses the learned mapping to classify I q as one of N classes in the support set S sampled from D test .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our approach</head><p>In this section, we introduce the Relational Embedding Network (RENet) that addresses the challenge of generalization to unseen target classes in a relational perspective. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the overall architecture, consisting of two main learnable modules: self-correlational representation (SCR) module and cross-correlational attention (CCA) module. We first present a brief overview of the proposed architecture in Sec. 4.1. We then present technical details of SCR and CCA in Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture overview</head><p>Given a pair of a query and one of support images, I q and I s , a backbone feature extractor provides base representations, Z q and Z s ? R H?W ?C . The SCR module transforms the base representations to self-correlational representations, F q and F s ? R H?W ?C , by analyzing feature correlations within an image representation in a convolutional manner. The CCA module then takes the selfcorrelational representations to generate co-attention maps, A q and A s ? R H?W , which give spatial attention weights on aggregating F q and F s to image embeddings, q and s ? R C . This process illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref> is applied to all support images I s ? S in parallel, and then the query is classified as the class of its nearest support embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-Correlational Representation (SCR)</head><p>The SCR module takes the base representation Z 1 and transforms it to focus more on relevant regions in an image, preparing a reliable input to the CCA module that analyzes feature correlations between a pair of different images. <ref type="figure" target="#fig_4">Figure 3a</ref> illustrates the architecture of the SCR module. <ref type="bibr" target="#b0">1</ref> For notational simplicity, we omit subscripts q and s in this subsection.</p><p>Self-correlation computation. Given a base representation Z ? R H?W ?C , we compute the Hadamard product of a C-dimensional vector at each position x ? [1, H]? <ref type="bibr">[1, W ]</ref> and those at its neighborhood and collect them into a selfcorrelation tensor R ? R H?W ?U ?V ?C . With an abuse of notation, the tensor R can be represented as a function with a C-dimensional vector output:</p><formula xml:id="formula_1">R(x, p) = Z(x) Z(x) Z(x + p) Z(x + p) ,<label>(1)</label></formula><formula xml:id="formula_2">where p ? [?d U , d U ]?[?d V , d V ]</formula><p>corresponds to a relative position in the neighborhood window such that 2d U + 1 = U and 2d V + 1 = V , including the center position. Note that the edges of the feature map are zero-padded for sampling off the edges. The similar type of self-correlation, i.e., self-similarity, has been used as a relational descriptor for images and videos that suppresses variations in appearance and reveals structural patterns <ref type="bibr" target="#b64">[65]</ref>. Unlike the previous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b64">65]</ref>, which reduce a pair of feature vectors into a scalar correlation value, we use the channel-wise correlation, preserving rich semantics of the feature vectors for classification.</p><p>Self-correlational representation learning. To analyze the self-correlation patterns in R, we apply a series of 2D convolutions along U ? V dimensions. As shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>, the convolutional block follows a bottleneck structure <ref type="bibr" target="#b70">[71]</ref> for computational efficiency, which is comprised of a point-wise convolution layer for channel size reduction, two 3?3 convolution layers for transformation, and another point-wise convolution for channel size recovery. Between the convolutions, batch normalization <ref type="bibr" target="#b23">[24]</ref> and ReLU <ref type="bibr" target="#b45">[46]</ref> are inserted. This convolutional block g(?) gradually aggregates local correlation patterns without padding, thus reducing their spatial dimensions from U ? V to 1 ? 1 squeeze self-correla?on computa?on cross-correla?on computa?on a?en?on computa?on a?en?on computa?on such that the output g(R) has the same size with Z, i.e., g : R H?W ?U ?V ?C ? R H?W ?C . This process of analyzing structural patterns may be complementary to appearance patterns in the base representation Z. We thus combine the two representations to produce the self-correlational representation F ? R H?W ?C :</p><formula xml:id="formula_3">F = g(R) + Z,<label>(2)</label></formula><p>which reinforces the base features with relational features and helps the few-shot learner better understand "what to observe" within an image. Our experiments show that SCR is robust to intra-class variations and helps generalization to unseen target classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cross-Correlational Attention (CCA)</head><p>The CCA module takes an input pair of query and support SCRs, F q and F s , and produces corresponding attention maps, A q and A s . These spatial attention maps are used to aggregate each representation to an embedding vector. <ref type="figure" target="#fig_4">Figure 3b</ref> visualizes the pipeline of the CCA module. Cross-correlation computation. We first transform both query and support representations, F q and F s ? R H?W ?C , into more compact representations using a point-wise convolutional layer, reducing its channel dimension C to C . From the outputs,F q andF s ? R H?W ?C , we construct a 4-dimensional correlation tensor C ? R H?W ?H?W :</p><formula xml:id="formula_4">C(x q , x s ) = sim(F q (x q ),F s (x s )),<label>(3)</label></formula><p>where x denotes a spatial position on the feature map and sim(?, ?) means the cosine similarity between two features.</p><p>Convolutional matching. The cross-correlation tensor C may contain unreliable correlations, i.e., matching scores, due to the large appearance variations in the few-shot learning setup. To disambiguate those unreliable matches, we employ the convolutional matching process <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58]</ref> that refines the tensor by 4D convolutions with matching kernels; 4D convolution on the tensor plays the role of geometric matching by analyzing the consensus of neighboring matches in the 4D space. As shown in <ref type="figure" target="#fig_4">Fig. 3b</ref>, the convolutional matching block h(?) consists of two 4D convolutional layers; the first convolution produces multiple correlation tensors with multiple matching kernels, increases channel size to C l , and the second convolution aggregates them to a single 4D correlation tensor, i.e.,? = h(C) ? R H?W ?H?W . Batch normalization and ReLU are inserted between the convolutions. We empirically found that two 4D convolutional layers are sufficient for our CCA module.</p><p>Co-attention computation. From the refined tensor?, we produce co-attention maps, A q and A s , which reveal relevant contents between the query and the support. The attention map for the query A q ? R H?W is computed by</p><formula xml:id="formula_5">A q (x q ) = 1 HW xs exp (?(x q , x s )/?) x q exp (?(x q , x s )/?) ,<label>(4)</label></formula><p>where x is a position at the feature map and ? is a temperature factor. Since?(x q , x s ) is a matching score between the positions x q and x s , the attention value A q (x q ) of Eq. <ref type="formula" target="#formula_5">(4)</ref> can be interpreted as converting the matching score of x q , i.e., a position at the query image, to the average probability of x q being matched to a position at the support image. The attention map for the support A s is similarly computed by switching the query and the support in Eq. (4). These co-attention maps improve few-shot classification accuracy by meta-learning cross-correlational patterns and adapting "where to attend" with respect to the images given at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Learning relational embedding</head><p>In this subsection, we derive relational embeddings q and s ? R C from F q , F s , A q and A s . We then conclude our method by describing the learning objective.</p><p>Attentive pooling. To obtain the final embedding of the query, q ? R C , each position of F q ? R H?W ?C is multiplied by the spatial attention map A q ? R H?W followed by pooling:</p><formula xml:id="formula_6">q = xq A q (x q )F q (x q ).<label>(5)</label></formula><p>Note that the elements of A q sum up to 1, and thus the attentive embedding q is a convex combination of F q attended in the context of the support. The final embedding of the support is computed similarly by attending the support feature map F s by A s followed by pooling:</p><formula xml:id="formula_7">s = xs A s (x s )F s (x s ).<label>(6)</label></formula><p>On an N -way K-shot classification setting, this co-attentive pooling generates a set of N K different views of a query, {q (l) } N K l=1 , and a set of support embeddings attended in the context of the query, {s (l) } N K l=1 .</p><p>Learning objective. The proposed RENet is end-to-end trainable from scratch. While most of the recent fewshot classification methods adopt the two-stage training scheme <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b86">87]</ref> of initial pre-training and subsequent episodic training, we adopt the single-stage training scheme <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49]</ref> that jointly trains the proposed modules as well as the backbone network by combining two losses: the anchor-based classification loss L anchor and the metric-based classification loss L metric . First, L anchor is computed with an additional fully-connected classification layer on top of average-pooled base representation z q . This loss guides the model to correctly classify a query of class c ? C train :</p><formula xml:id="formula_8">L anchor = ? log exp(w c z q + b c ) |Ctrain| c =1 exp(w c z q + b c ) ,<label>(7)</label></formula><p>where [w 1 , ? ? ? , w |Ctrain| ] and [b 1 , ? ? ? , b |Ctrain| ] are weights and biases in the fully-connected layer, respectively. Next, the metric-based loss L metric <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b74">75]</ref> is computed by cosine similarity between a query and support prototype embeddings. Before computing the loss, we average the K query embedding vectors each of which is attended in the context of k th support from n th class to compute {q (n) } N n=1 . Similarly, we average the K support embeddings for each class to obtain a set of prototype embeddings: {s (n) } N n=1 . The metric-based loss guides the model to map a query embedding close to the prototype embedding of the same class:</p><formula xml:id="formula_9">L metric = ? log exp(sim(s (n) ,q (n) )/? ) N n =1 exp(sim(s (n ) ,q (n ) )/? ) ,<label>(8)</label></formula><p>where sim(?, ?) is cosine similarity and ? is a scalar temperature factor. At inference, the class of the query is predicted as that of the nearest prototype. The objective combines the two losses:</p><formula xml:id="formula_10">L = L anchor + ?L metric ,<label>(9)</label></formula><p>where ? is a hyper-parameter that balances the loss terms. Note that the fully-connected layer involved in computing L anchor is discarded during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In this section, we evaluate RENet on standard benchmarks and compare the results with the recent state of the arts. We also conduct ablation studies to validate the effect of the major components. For additional results and analyses, we refer the readers to our appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>For evaluation, we use four standard benchmarks for few-shot classification: miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS. miniImageNet <ref type="bibr" target="#b74">[75]</ref> is a subset of ImageNet <ref type="bibr" target="#b59">[60]</ref>   <ref type="bibr" target="#b75">[76]</ref> is a dataset for fine-grained classification of bird species, consisting of 100/50/50 object classes for train/validation/test splits, respectively. Following the recent work of <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b86">87]</ref>, we use pre-cropped images to human-annotated bounding boxes. CIFAR-FS <ref type="bibr" target="#b2">[3]</ref> is built upon CIFAR-100 <ref type="bibr" target="#b28">[29]</ref> dataset. Following the recent work of <ref type="bibr" target="#b2">[3]</ref>, we use the same train/validation/test splits consisting of 64/16/20 object classes, respectively. For all the datasets, D train , D val , and D test are disjoint in terms of object classes such that C train ?C val = C val ?C test = C test ?C train = ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>We adopt ResNet12 <ref type="bibr" target="#b17">[18]</ref> following the recent few-shot classification work <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b86">87]</ref>. The backbone network takes an image with spatial size of 84 ? 84 as an input and provides a base representation Z ? R 5?5?640 followed by shifting its channel activations by the channel mean of an episode <ref type="bibr" target="#b86">[87]</ref>. For our CCA module, we adopt separable 4D convolutions <ref type="bibr" target="#b81">[82]</ref> with kernel size of 3 ? 3 ? 3 ? 3 for its effectiveness in approximating the original 4D convolutions <ref type="bibr" target="#b57">[58]</ref> as well as efficiency in terms of both memory and time. The output of the 4D convolution? is normalized such that the entities in the pair of spatial map to be zero-mean and unit-variance to stabilize training. We set C = 64 in SCR and C l = 16 in CCA module. For the N -way K-shot evaluation, we test 15 query samples for each class in an episode and report average classification accuracy with 95% confidence intervals of randomly sampled 2,000 test episodes. The hyperparameter ? is set to 0.25, 0.5, 1.5 for ImageNet derivatives, CIFAR-FS, CUB, respectively. ? is set to 2 for CUB and 5 otherwise. We set ? = 0.2, U, V = 5 in our experiments.     atively performs back-propagation steps at each inference, which is very slow; it takes 8 hours to evaluate 2,000 5-way 5-shot episodes while ours takes 1.5 minutes on the same machine with an Intel i7-7820X CPU and an NVIDIA Ti-tanXp GPU. We also find RENet outperforms transfer learning methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b79">80]</ref> that are not explicitly designed to learn cross-relation between a query and supports. However, RENet benefits from explicitly meta-learning crossimage relations and is able to better recognize image relevance adaptively to given few-shot examples.    <ref type="figure">Figure 5</ref>: Effects of the group size in SCR on miniImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation studies</head><p>To investigate the effects of core modules in RENet, we conduct extensive ablation studies either in the absence of each module or by replacing them with others and compare the results in the 5-way 1-shot setting. For ease of comparison, we use a baseline model called GAP baseline that applies global-average pooling to base representations to obtain final embeddings.</p><p>Effects of the proposed modules. <ref type="table" target="#tab_5">Table 3</ref> summarizes the effects of the SCR and CCA modules. Without SCR, the model skips self-correlational learning, replacing its output F with the base representation Z. Without CCA, the model skips computing cross-correlation and obtains final image embeddings by simply averaging either Z or F. Both modules consistently improve classification accuracies on both datasets. From the results, we observe that the effectiveness of CCA is more solid on CUB than that on miniImageNet. As the CCA module provides co-attention from the geometric consensus in cross-correlation patterns, it is particularly beneficial for a task where objects across different classes exhibit small geometric variations. We also experimentally show that the self-correlational representation generalizes well to unseen classes than the base representation does as seen in <ref type="figure" target="#fig_6">Fig. 4</ref>; the SCR achieves lower training accuracy but higher validation accuracy than the GAP baseline.</p><p>Design choices of SCR. To see the effectiveness of channelwise correlation in SCR, we replace the Hadamard product in Eq. (1) with group-wise cosine similarity in computing a self-correlation R ? R H?W ?U ?V ?C/G and interpolate the group size G. Namely, a group size G &gt; 1 compresses the channels of self-correlation, and G = 1 becomes equivalent to the proposed method. <ref type="figure">Figure 5</ref> shows that the selfcorrelation with G = C, which represents the feature relation as a similarity scalar, is already effective, and further, the performance gradually increases as smaller group sizes are used; the model benefits from relational information, and the effect becomes greater with richer relation in the channel-wise correlation as similarly observed in <ref type="bibr" target="#b87">[88]</ref>.</p><p>Design choices of CCA. We vary the components in the CCA module and denote the variants from (b) to (d) in <ref type="table" target="#tab_6">Table 4</ref> to verify our design choice. In this study, we ex-  <ref type="table" target="#tab_10">Table 5</ref>: Accuracy (%) and the number of additional learnable parameters of other relation-based methods. "*" denotes reproduced one under a controlled environment for a fair comparison, and underline denotes the best performance among others.</p><p>clude SCR learning to focus on the impact of the CCA. We first examine a non-parametric baseline (b) by ablating all learnable parameters in the CCA module, i.e., we replac? C in Eq. (4) with the cross-correlation between Z q and Z s . It shows marginal improvement from the GAP baseline (a), which implies that the na?ve cross-correlation hardly gives reliable co-attention maps. Another variant (c) validates that the hidden channel dimension C l <ref type="figure" target="#fig_4">(Fig. 3b</ref>) helps the model capture diverse cross-correlation patterns. The last variant (d) constructs cross-correlation preserving the channel dimension using Hadamard product instead of cosine similarity in Eq. (3). Although it provides much information to the module and requires more learnable parameters ((d): 797.3K vs. (e): 45.8K), it is not very effective than the proposed one (e) possibly because too abundant correlations between two independent images negatively affect model generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with other attention modules</head><p>In <ref type="table" target="#tab_10">Table 5</ref>, we compare the proposed modules with other attention modules by replacing ours with others. We first compare self-attention methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b77">78</ref>] that attend to appearance features based on feature similarity, while our SCR module extracts relational features based on local self-correlation. In the comparison, SCR outperforms selfattention methods, suggesting the effectiveness of learning self-correlation patterns for few-shot learning. We find that learning such relational patterns of "how each feature correlates with its neighbors" is transferable to unseen classes and compensates the lack of data issue of few-shot learning. While SCR outperforms most methods, it closely competes with SCE [23] on CUB. SCE computes cosine similarity between a reference position and its neighbors and concatenates their similarities at the channel dimension in a fixed order. SCE is powerful on the CUB dataset (77.54% ? 78.43%) that has relatively little pose variation across images, however, it is disadvantageous on the miniImageNet dataset (65.33% ? 63.39%). This is because SCE imprints the positional order in channel indices, which limits observing diverse neighborhood relations, whereas SCR uses multiple channels to capture various relations with neighbors.</p><p>In <ref type="table" target="#tab_10">Table 5</ref>, we also observe that CCA performs better than CAN <ref type="bibr" target="#b19">[20]</ref> as well as other self-attention methods with a reasonable amount of additional parameters. CAN first averages a 4D cross-correlation to a 2D correlation tensor and feeds it to multi-layer perceptrons that produce an attention mask, which is repeated similarly by switching the query and the support to generate co-attention maps. We empirically find that the process of averaging the initial 4D correlation collapses fine and crucial match details between images. <ref type="figure">Figure 7</ref> shows two examples that CAN is overwhelmed by dominant backgrounds and hardly attends to small objects. Whereas, the CCA module updates crosscorrelations while retaining the spatial dimensions hence successfully attends to relevant objects.</p><p>Combining the SCR and the CCA modules, our model outperforms all the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative results</head><p>The relational embedding process and its attentional effects are shown in <ref type="figure" target="#fig_0">Figs. 1 and 6</ref>. The columns (a) and (b) visualize the averaged channel activation of the base representation Z and the self-correlational representation F, respectively. The column (c) visualizes the 2D attention map A. The images are randomly sampled from the miniImageNet validation set, and activations are bi-linearly interpolated to the input image size. The results demonstrate that the SCR module can deactivate irrelevant features via learning self-correlation with neighborhood, e.g., the activation of a building behind a truck decreases. The subsequent CCA module generates co-attention maps that focus on the common context between a query and a support, e.g., the hands grasping the bars are co-attended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have proposed the relational embedding network for few-shot classification, which leverages the self-correlational representation and the cross-correlational attention. Combining the two modules, our method has achieved the state of the art on the four standard benchmarks. One of our experimental observations is that selfattention mechanism <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b77">78]</ref> is prone to overfitting to the training set so that it does not generalize to unseen classes in the few-shot learning context. Our work, however, has shown that learning structural correlations between visual features better generalizes to unseen object classes and brings performance improvement to few-shot image recognition, suggesting a promising direction of relational knowledge as a transferable prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we provide additional details and results of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Alternative derivation of relational embedding</head><p>Equations (4), <ref type="bibr" target="#b4">(5)</ref>, and (6) in the main paper describe the process of deriving relational embeddings, q and s ? R C , using pre-computed co-attention maps, A q and A s ? R H?W , where the attention maps themselves provide interpretable visualization, e.g., <ref type="figure" target="#fig_0">Fig. 1(c)</ref> in the main paper. In this section, we derive q and s in an alternative way of not explicitly introducing the attention maps, A q and A s , but multiplying a feature map by cross-correlation, which is used in spatial attention work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68]</ref>. Let us denote the normalized cross-correlation tensor in Eq. (4) b?</p><formula xml:id="formula_11">C = exp (?(x q , x s )/?) x q exp (?(x q , x s )/?) (a.10)</formula><p>and reshape it to a 2D matrix:C ? R HW ?HW . The relational embedding q is equivalently derived by multiplying two matricesC and F q ? R HW ?C followed by average pooling:</p><formula xml:id="formula_12">q = xq ? ? ? ? ? ? 1 HW xsC (x q , x s ) Eq. (4) F q (x q ) ? ? ? ? ? ? (Eq. (5)) = 1 HW xq xsC (x q , x s ) F q (x q ) = 1 HW xs xqC (x q , x s )F q (x q ) = 1 HW xs xqC (x s , x q )F q (x q ) matrix multiplication = 1 HW xsC F q R HW ?C (x s ). (a.11)</formula><p>Here,C F q is considered as softly-aligning the query feature map F q in the light of each position of the support using the cross-correlationC . Likewise, the relational embedding s is computed as</p><formula xml:id="formula_13">s = 1 HW xqC F s (x q ).</formula><p>(a.12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Comprehensive details on implementation</head><p>For training, we use an SGD optimizer with a momentum of 0.9 and a learning rate of 0.1. We train 1-shot models for 80 epochs and decay the learning rate by a factor of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablation studies</head><p>We provide more ablation studies on CUB <ref type="bibr" target="#b75">[76]</ref> and miniImageNet <ref type="bibr" target="#b74">[75]</ref> in the 5-way 1-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Self-correlation computation with relative vs. absolute neighbors</head><p>We validate the importance of relative neighborhood correlations of a self-correlation tensor R in  <ref type="bibr" target="#b60">[61]</ref> 61.76 ? 0.08 248.8K CTM <ref type="bibr" target="#b32">[33]</ref> 64.12 ? 0.82 305.8K FEAT <ref type="bibr" target="#b82">[83]</ref> 66.78 ? 0.20 1640.3K MTL <ref type="bibr" target="#b68">[69]</ref> 61.20 ? 1.80 4301.1K wDAE <ref type="bibr" target="#b16">[17]</ref> 61.07 ? 0.15 11273.2K temperature ? for co-attention computation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Separable vs. vanilla 4D convolution on CCA</head><p>Comparison between the original vanilla 4D convolutional kernels <ref type="bibr" target="#b57">[58]</ref> and separable 4D kernels <ref type="bibr" target="#b81">[82]</ref> is summarized in <ref type="table" target="#tab_10">Table a</ref>.7, where we adopt the separable one for its efficiency. Note that the separable 4D kernels approximate the vanilla 3?3?3?3 kernels by two sequential 3?3?1?1 and 1?1?3?3 kernels followed by a point-wise convolution. The reported GPU time in <ref type="table" target="#tab_10">Table a</ref>.7 is an average time for processing an episode and is measured using a CUDA event wrapper in PyTorch <ref type="bibr" target="#b49">[50]</ref>. While the two kinds of kernels closely compete with each other in terms of accuracy, the separable one consumes less computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Number of parameters</head><p>We measure the number of additional model parameters of recent methods and compare them with RENet in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Temperature ? for co-attention computation</head><p>We investigate the impact of the hyper-parameter ? that controls the smoothness of the output attention map (Eq. (4)). As its name "temperature" suggests, a higher temperature outputs a smoother attention map, while a lower temperature outputs a peakier one. <ref type="figure" target="#fig_8">Figure a.8</ref> shows that the temperature ? has a certain point that maximizes the accuracy by appropriately balancing the smoothness factor. Interestingly, an extremely high temperature ? = 100 degrades accuracy by making all attention scores evenly distributed. It is noteworthy that our full model RENet with a range of ? ? {3, 4, 5, 6, 7} outperforms all existing methods on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 Local window size U V for SCR</head><p>To evaluate the effectiveness of learning relational features from local neighborhood correlation, we vary the local window size U V of a self-correlation tensor R ? R H?W ?U ?V ?C . As shown in <ref type="figure" target="#fig_9">Fig a.9</ref>, the accuracy steadily increases as more neighborhood correlations are learned, which indicates that learning relational structures is favorable for few-shot recognition. Note that SCR with U = V = 1 already outperforms the GAP baseline, which is an effect of learning from l2-normalized features (Eq. 1). Despite the consistent accuracy gain from observing wide local window, we choose U = V = 5 for all experiments to limit the space complexity increased by a factor of U V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Qualitative results</head><p>To demonstrate the effects of our method, we present additional qualitative results. All images are sampled from the miniImageNet validation set in the 5-way 1-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Effects of SCR</head><p>We ablate the SCR module and demonstrate the effects of SCR in <ref type="figure" target="#fig_0">Fig. a.10</ref>. The results show that "CCA w/ SCR" successfully attends to fine characteristics than "CCA w/o SCR" does, implying that the SCR module provides reliable representation for the subsequent CCA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Co-attention maps on multi-object queries</head><p>Given a multi-object image as a query, we examine if the object regions can be adaptively highlighted depending on the support semantics in <ref type="figure" target="#fig_0">Fig. a.11</ref>. The CCA module successfully captures query regions that are semantically related with each support image. This effect accords with the motivation of the CCA module, which is to adaptively provide "where to attend" between two image contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Cross-correlation refinement via h(?)</head><p>We demonstrate the effect of 4D convolutional block h(?) that filters out unreliable matches in the initial crosscorrelation by analyzing neighborhood consensus patterns. We visualize the top 10 matches among 2HW matching candidates computed by argmax of matching scores from each side. As shown in <ref type="figure" target="#fig_0">Fig. a.12</ref>, the initial crosscorrelation C exhibits many spurious matches misled by indistinguishable appearance, e.g., matching two regions of the sky, whereas the updated cross-correlation? shows reliable and meaningful matches, e.g., matching two sails. query 1st support query 1st support query 2nd support query 2nd support inputs co-attention maps inputs co-attention maps  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Relational embedding process and its attentional effects. The base image features are transformed via self-correlation to capture structural patterns within each image and then coattended via cross-correlation to focus on semantically relevant contents between the images. (a), (b), and (c) visualize the activation maps of base features, self-correlational representation, and cross-correlational attention, respectively. See Sec. 5.6 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of RENet. The base representations, Zq and Zs, are transformed to self-correlation tensors, Rq and Rs, which are then updated by the convolutional block g to self-correlational representations, Fq and Fs, respectively. The cross-correlation C is computed between the pair of image representations and then refined by the convolutional block h to?, which is bidirectionally aggregated to generate co-attention maps, Aq and As. These co-attention maps are applied to corresponding image representations, Fq and Fs, and their attended features are aggregated to produce the final relational embeddings, q and s, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. 4.2 and Sec. 4.3 respectively, and describe our training objective in Sec. 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of SCR and CCA modules. (a): The SCR module captures relational patterns in the input selfcorrelation R by convolving it over U ? V dimensions. The result g(R) is added to the base representation Z to form the self-correlational representation F (Eq. 2). (b):The CCA module refines the cross-correlation which will be summarized into co-attention maps, Aq and As (Eq. 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art 5-way 1-shot and 5-way 5-shot accuracy (%) with 95% confidence intervals on (a) CUB-200-2011 and (b) CIFAR-FS. " ?" denotes larger backbones than ResNet12, and "*" denotes reproduced one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Learning curves of the GAP baseline and SCR in terms of accuracy (%) with 95 % confidence intervals on CUB-200-2011. The curves for the first 40 epochs are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Effects of RENet. (a): Channel activation of base representation. (b): Channel activation of SCR. (c): Attention map of CCA. input CCA (ours) CAN (Hou et al.) input CCA (ours) CAN (Hou et al.) Co-attention comparison with CAN<ref type="bibr" target="#b19">[20]</ref>. Our CCA better attends to common objects against confusing backgrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure a. 8 :</head><label>8</label><figDesc>Accuracy (%) of varying ? on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure a. 9 :</head><label>9</label><figDesc>Accuracy (%) of varying U ? V on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure a. 10 :</head><label>10</label><figDesc>Effects of SCR on miniImageNet. "CCA w/ SCR" captures fine details between two images while "CCA w/o SCR" often fails. The "base feature map" and "SCR" columns visualize average channel activations. The "CCA w/ SCR" and "CCA w/o SCR" columns visualize co-attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure a. 11 :</head><label>11</label><figDesc>Co-attention maps on multi-object queries on miniImageNet. The proposed CCA module can adaptively capture multiple objects in a query depending on the context of each support instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure a. 12 :</head><label>12</label><figDesc>Visualization of cross-correlation on miniImageNet. (a): Top 10 matches in C (initial cross-correlation). (b): Top 10 matches in? = h(C) (refined cross-correlation). Unreliable matches are filtered through h(?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>consisting of 60,000 images uniformly distributed over 100 object classes. The train/validation/test splits consist of 64/16/20 object classes, respectively. tieredImageNet [57] is a challenging dataset in which train/validation/test splits are disjoint in terms of super-classes from the ImageNet hierarchy, which typically demands better generalization than other datasets. The respective train/validation/test splits consist of 20/6/8 superclasses, which are super-sets of 351/97/160 sub-classes.</figDesc><table /><note>CUB-200-2011 (CUB)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ResNet12 55.43 ? 0.81 77.18 ? 0.61 TADAM [49] ResNet12 58.50 ? 0.30 76.70 ? 0.30 Shot-Free [56] WRN-28-10 ? 59.60 ? 0.41 73.74 ? 0.19 wDAE-GNN [17] WRN-28-10 ? 61.07 ? 0.15 76.75 ? 0.11 MTL [69] ResNet12 61.20 ? 1.80 75.50 ? 0.80 LEO [61]WRN-28-10 ? 61.76 ? 0.08 77.59 ? 0.12 RFS-simple<ref type="bibr" target="#b72">[73]</ref> </figDesc><table><row><cell>method</cell><cell>backbone</cell><cell cols="2">5-way 1-shot 5-way 5-shot</cell></row><row><cell>cosine classifier [6]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet12</cell><cell>59.04</cell><cell>77.64</cell></row><row><cell>TPN [39]</cell><cell>ResNet12</cell><cell>59.46</cell><cell>75.65</cell></row><row><cell>PPA [53]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet12</cell><cell cols="2">62.02 ? 0.63 79.64 ? 0.44</cell></row><row><cell>DC [37]</cell><cell>ResNet18</cell><cell cols="2">62.53 ? 0.19 79.77 ? 0.19</cell></row><row><cell>ProtoNet [66]</cell><cell>ResNet12</cell><cell cols="2">62.39 ? 0.21 80.53 ? 0.14</cell></row><row><cell>MetaOptNet [32]</cell><cell>ResNet12</cell><cell cols="2">62.64 ? 0.82 78.63 ? 0.46</cell></row><row><cell>SimpleShot [80]</cell><cell>ResNet18</cell><cell cols="2">62.85 ? 0.20 80.02 ? 0.14</cell></row><row><cell>MatchNet [75]</cell><cell>ResNet12</cell><cell cols="2">63.08 ? 0.80 75.99 ? 0.60</cell></row><row><cell>S2M2 [41]</cell><cell cols="3">ResNet34  ? 63.74 ? 0.18 79.45 ? 0.12</cell></row><row><cell>CAN [20]</cell><cell>ResNet12</cell><cell cols="2">63.85 ? 0.48 79.44 ? 0.34</cell></row><row><cell>NegMargin [38]</cell><cell>ResNet12</cell><cell cols="2">63.85 ? 0.81 81.57 ? 0.56</cell></row><row><cell>CTM [33]</cell><cell>ResNet18</cell><cell cols="2">64.12 ? 0.82 80.51 ? 0.13</cell></row><row><cell>DeepEMD [87]</cell><cell>ResNet12</cell><cell cols="2">65.91 ? 0.82 82.41 ? 0.56</cell></row><row><cell>FEAT [83]</cell><cell>ResNet12</cell><cell cols="2">66.78 ? 0.20 82.05 ? 0.14</cell></row><row><cell>RENet (ours)</cell><cell>ResNet12</cell><cell cols="2">67.60 ? 0.44 82.58 ? 0.30</cell></row><row><cell cols="3">(a) Results on miniImageNet dataset.</cell><cell></cell></row><row><cell>method</cell><cell>backbone</cell><cell cols="2">5-way 1-shot 5-way 5-shot</cell></row><row><cell>cosine classifier [6]</cell><cell>ResNet12</cell><cell cols="2">61.49 ? 0.91 82.37 ? 0.67</cell></row><row><cell>Shot-Free [56]</cell><cell>ResNet12</cell><cell>63.52</cell><cell>82.59</cell></row><row><cell>TPN [39]</cell><cell>ResNet12</cell><cell cols="2">59.91 ? 0.94 73.30 ? 0.75</cell></row><row><cell>PPA [53]</cell><cell cols="3">WRN-28-10  ? 65.65 ? 0.92 83.40 ? 0.65</cell></row><row><cell>wDAE-GNN [17]</cell><cell cols="3">WRN-28-10  ? 68.18 ? 0.16 83.09 ? 0.12</cell></row><row><cell>LEO [61]</cell><cell cols="3">WRN-28-10  ? 66.33 ? 0.05 81.44 ? 0.09</cell></row><row><cell>MetaOptNet [32]</cell><cell>ResNet12</cell><cell cols="2">65.99 ? 0.72 81.56 ? 0.53</cell></row><row><cell>ProtoNet [66]</cell><cell>ResNet12</cell><cell cols="2">68.23 ? 0.23 84.03 ? 0.16</cell></row><row><cell>MatchNet [75]</cell><cell>ResNet12</cell><cell cols="2">68.50 ? 0.92 80.60 ? 0.71</cell></row><row><cell>CTM [33]</cell><cell>ResNet18</cell><cell cols="2">68.41 ? 0.39 84.28 ? 1.73</cell></row><row><cell>RFS-simple [73]</cell><cell>ResNet12</cell><cell cols="2">69.74 ? 0.72 84.41 ? 0.55</cell></row><row><cell>CAN [20]</cell><cell>ResNet12</cell><cell cols="2">69.89 ? 0.51 84.23 ? 0.37</cell></row><row><cell>FEAT [83]</cell><cell>ResNet12</cell><cell cols="2">70.80 ? 0.23 84.79 ? 0.16</cell></row><row><cell>DeepEMD [87]</cell><cell>ResNet12</cell><cell cols="2">71.16 ? 0.87 86.03 ? 0.58</cell></row><row><cell>RENet (ours)</cell><cell>ResNet12</cell><cell cols="2">71.61 ? 0.51 85.28 ? 0.35</cell></row></table><note>(b) Results on tieredImageNet dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison ResNet12 66.09 ? 0.92 82.50 ? 0.58 RelationNet [70] ResNet34 ? 66.20 ? 0.99 82.30 ? 0.58 MAML [70] ResNet34 ? 67.28 ? 1.08 83.47 ? 0.59 cosine classifier [6] ResNet12 67.30 ? 0.86 84.75 ? 0.60</figDesc><table><row><cell>with the state-of-the-art 5-way 1-shot and</cell></row><row><cell>5-way 5-shot accuracy (%) with 95% confidence intervals on (a)</cell></row><row><cell>miniImageNet and (b) tieredImageNet. " ?" denotes larger back-</cell></row><row><cell>bones than ResNet12.</cell></row><row><cell>5.3. Comparison to the state-of-the-art methods</cell></row><row><cell>Tables 1 and 2 compare RENet and current few-shot</cell></row><row><cell>classification methods on four datasets. Our model uses</cell></row><row><cell>a smaller backbone than that of several methods [17, 41,</cell></row><row><cell>53, 61] yet sets a new state of the art in both 5-way 1-</cell></row><row><cell>shot and 5-shot settings on miniImageNet, CUB-200-2011,</cell></row><row><cell>and CIFAR-FS datasets while being comparable to Deep-</cell></row><row><cell>EMD [87] on tieredImageNet. Note that DeepEMD iter-</cell></row></table><note>(b) Results on CIFAR-FS dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>+1.33) 78.69 (+1.15) 65.90 (+0.57) 78.49 (+0.95) 67.60 (+2.27) 79.49 (+1.95)</figDesc><table><row><cell>SCR CCA</cell><cell>mini-ImageNet</cell><cell>CUB</cell></row><row><cell></cell><cell>65.33</cell><cell>77.54</cell></row><row><cell></cell><cell>66.66 (</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effects of SCR and CCA.CUB C in ? C l ? C out ImgNet</figDesc><table><row><cell>id</cell><cell>CCA</cell><cell>h(?) channel sizes</cell><cell>mini-</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>65.33 77.54</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell>65.73 77.75</cell></row><row><cell>(c) (d) (e)</cell><cell></cell><cell>1 ? 1 ? 1 64 ? 16 ? 1 1 ? 16 ? 1</cell><cell>65.75 78.05 66.18 78.10 65.90 78.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effects of CCA variants.</figDesc><table><row><cell></cell><cell>67.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.66</cell></row><row><cell>accuracy (%)</cell><cell>65.00 66.00</cell><cell>65.33</cell><cell>66.01</cell><cell>66.33 66.34</cell></row><row><cell></cell><cell>64.00</cell><cell>x</cell><cell>C</cell><cell>C/10 C/64</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell cols="4">self-correlation group size G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>] to compute L anchor . This objective is jointly optimized from scratch with L metric and L anchor as described in Sec. 4.4. For a fair comparison, we adopt the same image sizes, the backbone network, the data augmentation techniques, and the embedding normalization following the recent work of<ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b86">87]</ref>.</figDesc><table><row><cell>self-correlation computation</cell><cell cols="3">category of neighbors ImgNet mini</cell><cell>CUB</cell></row><row><cell>(GAP baseline)</cell><cell></cell><cell></cell><cell>65.33</cell><cell>77.54</cell></row><row><cell>R ? R H?W ?H?W ?C R ? R H?W ?U ?V ?C</cell><cell cols="2">absolute relative</cell><cell>66.41 66.66</cell><cell>76.34 78.69</cell></row><row><cell cols="5">Table a.6: Comparison between absolute and relative neighbor-</cell></row><row><cell cols="4">hood space in computing the self-correlation tensor R.</cell></row><row><cell>4D convolution kernels</cell><cell>mini ImgNet</cell><cell>CUB</cell><cell cols="2">GPU time (ms)</cell></row><row><cell>(GAP baseline)</cell><cell>65.33</cell><cell>77.54</cell><cell cols="2">27.74</cell></row><row><cell>vanilla 4D [58]</cell><cell>65.59</cell><cell>78.89</cell><cell cols="2">60.35</cell></row><row><cell>separable 4D [82]</cell><cell>65.90</cell><cell>78.49</cell><cell cols="2">34.97</cell></row><row><cell cols="5">Table a.7: Comparison between 4D convolutions for h(?).</cell></row><row><cell cols="5">0.05 at each {60, 70} epoch. To train 5-shot models, we run 60 epochs and decay the learning rate at each {40, 50} epoch. We randomly construct a training batch of size 128</cell></row><row><cell cols="5">for the ImageNet family [57, 75] and 64 for CUB [76] &amp;</cell></row><row><cell>CIFAR-FS [3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table a.<ref type="bibr" target="#b5">6</ref>. We set H = W = U = V such that the two models have the same input sizes for a fair comparison. The results show the superiority of the relative neighborhood correlation. An advantage of the relative correlation over the absolute one is that relative correlations provide a translationinvariant neighborhood space. For example, let us consider a self-correlation between a reference position x and its neighbors. While an absolute correlation (x, x ) ?</figDesc><table><row><cell>method</cell><cell>5-way 1-shot accuracy (%)</cell><cell># add. params</cell></row><row><cell cols="2">CAN [20] RENet (ours) 67.60 ? 0.44 63.85 ? 0.48 LEO</cell><cell>0.3K 203.2K</cell></row></table><note>R H?W ?H?W provides a variable neighborhood space as x translates by t: (x + t, x + t), a relative correlation (x, p) ? R H?W ?U ?V provides a consistent view of the neighborhood space no matter how x moves: (x + t, p).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table a</head><label>a</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">.8: Performance comparison in terms of model size and</cell></row><row><cell cols="5">accuracy (%) on miniImageNet.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RENet (ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GAP baseline</cell></row><row><cell></cell><cell>68.00</cell><cell>67.20</cell><cell>67.49</cell><cell>67.60</cell><cell>67.25</cell><cell>67.13</cell><cell>67.08</cell></row><row><cell>accuracy (%)</cell><cell>65.00 66.00 67.00</cell><cell></cell><cell></cell><cell>65.33</cell><cell></cell><cell></cell></row><row><cell></cell><cell>64.00</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table a.8. Table a.8 studies the effect of additional parameters only so we collect publicly available codes of methods that use additional parameterized modules<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b82">83]</ref>, and intentionally omit<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87]</ref> as their trainable parameters are either in the backbone network or in the last fully-connected layer. Compared to the largest model<ref type="bibr" target="#b16">[17]</ref>, ours performs significantly better (67.60 vs. 61.07) while introducing 55 times less additional capacity (203.2K vs. 11.2M).</figDesc><table><row><cell></cell><cell>68.00</cell><cell></cell><cell cols="2">SCR w/o CCA GAP baseline</cell><cell>67.24</cell><cell>67.34</cell></row><row><cell>accuracy (%)</cell><cell>65.00 66.00 67.00</cell><cell>66.07</cell><cell>66.25</cell><cell>66.66 65.33</cell></row><row><cell></cell><cell>64.00</cell><cell cols="4">1 ? 1 local window size U ? V of self-correlation R 3 ? 3 5 ? 5 7 ? 7 9 ? 9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by Samsung Electronics Co., Ltd. (IO201208-07822-01) and the IITP grants (No.2019-0-01906, AI Graduate School Program -POSTECH) (No.2021-0-00537, Visual common sense through self-supervised learning for restoration of invisible parts in images) funded by Ministry of Science and ICT, Korea.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximating cnns with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Concept learners for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Brbic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global and efficient self-similarity for object classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter R Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-view action recognition from temporal selfsimilarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">View-independent action recognition from temporal self-similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="172" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning self-similarity in space and time as generalized motion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for fewshot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local propagation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional hough matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) Workshop Autodiff</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Embedding propagation: Smoother manifold for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issam</forename><surname>Pau Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning, or on learning how to learn: the metameta-... hook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Matching local selfsimilarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Mapping a manifold of perceptual observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Local self-similarity-based registration of human rois in pairs of stereo thermal-visible videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume-Alexandre</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">and Matt Feiszli. Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Few-shot classification with feature map reconstruction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno>2021. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jure?bontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The spatially-correlative loss for various image translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
