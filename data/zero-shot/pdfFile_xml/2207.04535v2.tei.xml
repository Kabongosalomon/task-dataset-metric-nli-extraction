<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEPTHFORMER: MULTISCALE VISION TRANSFORMER FOR MONOCULAR DEPTH ESTIMATION WITH GLOBAL LOCAL INFORMATION FUSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Arora</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEPTHFORMER: MULTISCALE VISION TRANSFORMER FOR MONOCULAR DEPTH ESTIMATION WITH GLOBAL LOCAL INFORMATION FUSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-depth estimation</term>
					<term>transformer</term>
					<term>attention</term>
					<term>adaptive bins</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention-based models such as transformers have shown outstanding performance on dense prediction tasks, such as semantic segmentation, owing to their capability of capturing long-range dependency in an image. However, the benefit of transformers for monocular depth prediction has seldom been explored so far. This paper benchmarks various transformer-based models for the depth estimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We propose a novel attention-based architecture, Depthformer for monocular depth estimation that uses multi-head self-attention to produce the multiscale feature maps, which are effectively combined by our proposed decoder network. We also propose a Transbins module that divides the depth range into bins whose center value is estimated adaptively per image. The final depth estimated is a linear combination of bin centers for each pixel. Transbins module takes advantage of the global receptive field using the transformer module in the encoding stage. Experimental results on NYUV2 and KITTI depth estimation benchmark demonstrate that our proposed method improves the state-of-the-art by 3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE). Code is available at https://github.com/ashutosh1807/Depthformer.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Depth estimation from a single image is fundamental for many applications, from virtual reality to low-cost autonomous driving. Almost all of the current techniques to estimate depth from a single image are based on convolutional neural networks (CNN) with a U-Net-based encoder-decoder architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The encoder is typically an image classification network trained on Imagenet <ref type="bibr" target="#b9">[10]</ref>, and the decoder aggregates multiscale features to produce final dense depth. As earlier works have pointed out, the features extracted from a CNN have a local receptive field <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. For dense prediction tasks such as semantic segmentation and depth estimation, a pixel must have a global receptive field about the scene along with the local information for a more accurate estimation.</p><p>The recent success of transformers in Natural Language Processing tasks has created considerable interest among researchers to introduce them in computer vision tasks owing to their capability of capturing long-range dependencies. However, earlier works in Transformer based vision architectures mainly focused on Classification and Object Detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. With the advent of Multiscale Vision Transformers (MVTs), researchers have also started using transformer-based architectures as encoders for dense prediction tasks like semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Most of the works using MVTs showcase their dense prediction performance for semantic segmentation. On the contrary, depth estimation is even more difficult for two reasons: (1) It is a continuous prediction task, and (2) It is an ill-posed problem owing to the inherent scale ambiguity. To the best of our knowledge <ref type="bibr" target="#b17">[18]</ref> is the only work that proposes to use vision transformer (VIT) during the encoding stage. However, it applies VIT only on feature maps with 1/16th scale extracted using a CNN architecture. Motivated by these observations, we propose a novel transformer-based encoder for depth estimation that uses multiheaded self-attention to produce hierarchical feature maps. We also benchmark the performance of recently proposed vision transformers (VITs) for semantic segmentation on the monocular depth estimation task.</p><p>Using a transformer-based encoder increases the receptive field of the network, but for the dense prediction task, pixels also must understand local information. For example, a pixel must know that it lies on a boundary of an object or that it belongs to a group of pixels on a co-planar surface. Previous works that use MVTs <ref type="bibr" target="#b10">[11]</ref> as encoders have a decoder design in which they upscale the encoded features of varying resolutions to a fixed resolution and fuse them using MLP layers. However, directly upsampling to a higher resolution and fusion results in a loss in local information. Motivated by this, we propose a novel decoder that iteratively upsamples feature maps and fuses them with the encoder features, starting from the lowest resolution and moving towards the high resolution. Iterative upsampling and fusion helps propagate global information to high-resolution local information preserving features.</p><p>In alignment with the state-of-the-art (SoTA) <ref type="bibr" target="#b18">[19]</ref>, we model the depth estimation task as an ordinal regression by the decoder network to produce output feature map Fout. Fout is fed to the Transbins module predicts the bin widths. Pixel-wise probability distribution over the bins centers is finally predicted by using a 1 ? 1 convolution followed by softmax activation.</p><p>problem that discretizes the depth range into a number of intervals or bins adaptively per image. We propose a novel module Transbins, that takes advantage of fused global information into the feature maps by an MVT and produces adaptive bins discretized over the depth range. To demonstrate the efficacy of our proposed network Depthformer, we conduct extensive experiments on two canonical depth estimation benchmarks, an outdoor driving dataset KITTI <ref type="bibr" target="#b19">[20]</ref> and an indoor dataset NYUV2 <ref type="bibr" target="#b20">[21]</ref>, achieving SoTA on both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions: To summarise, our contributions are -(1)</head><p>We propose a novel encoder-decoder network that uses self-attention to predict multiscale feature maps that are effectively fused by our decoder. (2) We propose a novel Transbins module that predicts adaptive bins discretized over the depth range whose centers are used to predict final depth. (3) We benchmark the performance of earlier proposed multiscale vision transformers (MVTs) on monocular depth estimation. (4) Our network Depthformer achieves state-of-the-art performance on both outdoor dataset KITTI and indoor dataset NYUV2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head><p>Problem Definition: Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>, we model depth estimation as an ordinal regression task. Given an input I, our network predicts: (1) Bin widths, b, that discretize continuous depth range into a number of intervals, n bins , adaptively per image. (2) Pixel-wise probability distribution over the adaptive bins. The final depth, d, at a pixel is the linear combination of the probability scores at the pixel and per image depth-bin-centers.</p><p>Architecture: This section introduces the architecture for the encoder and decoder of our proposed Depthformer and the training loss that we follow. Our encoder produces the feature maps of varying resolutions using a MVT given an input image. The decoder then fuses these multiscale features to predict feature map F out . Next, the proposed Transbins module uses F out as input to predict adaptive bins b discretized over the depth range. The probability distribution over the bins is estimated by applying a convolution operation on F out followed by the softmax activation. We describe a more detailed architecture below.</p><p>Encoder: Given an image I of size H ? W ? 3, our encoder aims to create feature maps at multiple resolutions using the Transformer framework. Input I is first convolved with learned C 1 kernels of size K 1 and a stride of 4 to produce a feature map of size H 4 ? W 4 ? C 1 . This feature map is flattened to produce a sequence of HW 4 2 feature vectors, each of dimension C 1 that are fed to the transformer block T 1 . Transformer T 1 applies n 1 self-attention -MLP layers to produce the output, which is reshaped to a feature map</p><formula xml:id="formula_0">E 1 of size H 4 ? W 4 ? C 1 .</formula><p>This process is performed repeatedly, with a stride of 2 for the convolution operation in the following layers. Hence, the encoder produces feature maps</p><formula xml:id="formula_1">E 1 , E 2 , E 3 , E 4 of resolutions { 1 4 , 1 8 , 1 16 , 1 32 } respec- tively with channels [C 1 , C 2 , C 3 , C 4 ] = {64</formula><p>, 128, 320, 512} respectively, that are then fed to the decoder.</p><p>Note that, the self attention in the Transformer Block (T i ) is optimised by Spatial attention Reduction (SRA) as suggested in <ref type="bibr" target="#b11">[12]</ref>. SRA is implemented via a convolution layer with kernel size R i and stride R i to project the keyvalue pairs, hence resulting in n R 2 compressed key-value pairs where n is the number of input vectors. For the convolution operation with kernel size K, stride S and padding P , we use K 1 = 7, S 1 = 4, P 1 = 3 for the first layer and K i = 3, S i = 2, P i = 1 for i = 2, 3, 4. A kernel size K greater than the stride S, encourages shared information be-tween the adjacent feature vectors to produce a smoother feature maps. The number of self-attention MLP layers for the four transformer blocks are: [N 1 , N 2 , N 3 , N 4 ] = {3, 8, 27, 3}. Spatial reduction ratios for the four transformers are: [R 1 , R 2 , R 3 , R 4 ] = {8, 4, 2, 1}. <ref type="figure" target="#fig_0">Fig. 1</ref> gives an overview of our encoder design.</p><p>Decoder: Earlier works using MVT for dense prediction <ref type="bibr" target="#b10">[11]</ref> upsample feature maps at varying resolutions to a resolution of <ref type="bibr">1 4</ref> , and reduce their channel dimensions to C using 1 ? 1 convolutions. The features are then concatenated and finally fused to predict the output of size H 4 ? W 4 ? n cls for the segmentation task, where n cls are the number of classes. The feature maps are then upsampled using interpolation to H ?W ? n cls which helps in producing a smoother estimation. Such a decoder design suffers from the loss of local information due to the smoothing effect of interpolation. Earlier works using CNN have used Feature Pyramid Network (FPN) <ref type="bibr" target="#b21">[22]</ref> architecture design to preserve the local details. We adopt a similar design and a decoder that iteratively fuses feature maps from the lowest resolution for MVTs.</p><p>Effectively, for encoder feature maps E 1 , E 2 , E 3 , and E 4 with resolution { 1 4 , 1 8 , 1 16 , 1 32 }, we iteratively perform the following operation</p><formula xml:id="formula_2">D i = Conv{Concat[Upsample(D i+1 ), E i ]} i = 1, 2, 3, 4 (1)</formula><p>This procedure produces a map feature map F out of size H 2 ? W 2 ? C which is fed to the Transbins module as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We select C = 128 as in <ref type="bibr" target="#b18">[19]</ref>. To upsample the feature maps we have used transposed convolution with kernel size k = 2 and stride s = 2.</p><p>Transbins: Adabins <ref type="bibr" target="#b18">[19]</ref> predicts adaptive bins and attentual maps, and fuse the later with the feature map from decoder F out . The motivation is to fuse global information in the attenual maps with the decoder features. We take advantage of the encoded global information in F out via our encoder to only predict bin widths from the full-scale VIT. To predict the distribution over the bins, we use 1 ? 1 convolution over F out followed by softmax to predict the output of size H 2 ? W 2 ? n bins . The final depth is predicted by linear combination over the bin centers as in <ref type="bibr" target="#b18">[19]</ref>, which is then upsampled using bilinear interpolation to predict the depth at full resolution. The effectiveness of our proposed Transbins approach against Adabins for bin widths prediction can be seen in <ref type="table">Table 4</ref> which reduces the RMSE error by 14.2 %.</p><p>Training loss: We train our network on a sum of scaled version of the Scale-Invariant (SI) loss introduced by Eigen et al.</p><p>[1] L SILog and Chamfer loss L Chamfer <ref type="bibr" target="#b18">[19]</ref>. L SILog reduces the difference between the predicted depth map and the ground truth depth map. L Chamfer encourages the bin centers to be close to the actual ground truth depth values and vice versa.</p><formula xml:id="formula_3">L total = L SILog + ?L Chamfer .<label>(2)</label></formula><p>Method RM SE ? Rel ? ? 1 ? ? 2 ? ? 3 ? Eigen et al. <ref type="bibr" target="#b0">[1]</ref> 0.641 0.158 0.769 0.95 0.988 DORN <ref type="bibr" target="#b1">[2]</ref> 0.509 0.115 0.828 0.965 0.992 Chen et al. <ref type="bibr" target="#b4">[5]</ref> 0.514 0.111 0.878 0.977 0.994 VNL <ref type="bibr" target="#b22">[23]</ref> 0.416 0.108 0.875 0.976 0.994 BTS <ref type="bibr" target="#b2">[3]</ref> 0.392 0.110 0.885 0.978 0.994 DAV <ref type="bibr" target="#b3">[4]</ref> 0.412 0.108 0.882 0.980 0.996 DPT-Hybrid <ref type="bibr" target="#b17">[18]</ref> 0.357 0.110 0.904 0.988 0.998 Adabins <ref type="bibr" target="#b18">[19]</ref> 0.364 0.103 0.903 0.984 0.997 Depthformer (ours) 0.345 0.100 0.911 0.988 0.997 <ref type="table">Table 1</ref>. Results on NYUV2 Dataset. The best results are in bold and second best are underlined. Our method outperforms the previous SoTA methodsin most of the metrics.</p><p>We have used ? = 0.1 as in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>Datasets: NYU Depth v2 <ref type="bibr" target="#b20">[21]</ref> contains indoor scenes dataset with 640 ? 480 resolution images and depth maps with the upper bound of 10 m. We have trained our network on a 24K subset of dataset with a random crop of 576 ? 448. We evaluate on the pre-defined center cropping by Eigen et al. <ref type="bibr" target="#b0">[1]</ref> on test set of 654 images. KITTI <ref type="bibr" target="#b19">[20]</ref> is an outdoor scenes dataset containing stereo images with 1241 ? 376 resolution and corresponding 3D laser scans of outdoor scenes. We train our network on a subset of around 26K images from the left view with a random crop of size 704 ? 352 and test on 697 images. To compare our performance, we evaluate on a predefined crop by Garg et al. <ref type="bibr" target="#b23">[24]</ref> on 697 test images with a maximum value of 80 m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics:</head><p>We use the standard five metrics used in earlier works <ref type="bibr" target="#b0">[1]</ref> to compare our method against SoTA. Given the predicted depth d p , and the ground truth depth d * p at a pixel p, and n denoting the total number of pixels in an image, the error metrics are defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root mean squared error (RMSE):</head><p>1 n n p=1 dp ? d * Implementation Details: To train our network, we use the AdamW optimizer <ref type="bibr" target="#b24">[25]</ref> with weight-decay 0.1. We follow the training methodology prescribed in <ref type="bibr" target="#b25">[26]</ref>, and use 1-cycle policy for the learning rate with max lr = 10 ?4 , linear warmup from (3/10)max lr to max lr for the first 50% of iterations, followed by cosine annealing to (3/10)max lr. The network has been trained on 4 V100 GPUs with a 32GB memory, with a batch size of 16 for both NYUV2 and KITTI. Comparison with State of the Art: <ref type="table">Table 1</ref> and 2 demonstrate the performance of our proposed method, Depthformer, with the previous SoTA methods. We consistently outperform  <ref type="bibr" target="#b18">[19]</ref> (c) DPT-Hybrid <ref type="bibr" target="#b17">[18]</ref> (d) Depthformer (Ours). Our method is able to predict a more accurate depth estimation owing to it's capabilities of capturing long range information.</p><p>Method  <ref type="table">Table 3</ref>. Performance of different state of the art multiscalevision transformers for monocular depth estimation on the benchmark NYUV2 dataset.</p><formula xml:id="formula_4">RM SE ? Rel ? ? 1 ? ? 2 ? ? 3 ?</formula><p>across all the metrics for both outdoor scenes dataset KITTI and indoor dataset NYUV2. Our method reduces the RM SE by 3.5% for NYUV2 against SoTA DPT-Hybrid <ref type="bibr" target="#b17">[18]</ref> and by 3.2% against Adabins <ref type="bibr" target="#b18">[19]</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, our method Depthformer predicts a more accurate depth image in comparision to SoTAs <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>.</p><p>Benchmarking of VITs on Monocular depth estimation: <ref type="table">Table 3</ref> shows the performance of various SoTA VITs for monocular depth estimation on the NYUV2 dataset. For a fair comparison, we have taken model variants with a similar number of trainable parameters. For this experiment, we have taken the decoder as in <ref type="bibr" target="#b10">[11]</ref>. All the VITs outperform the CNN-based Resnet-50 <ref type="bibr" target="#b26">[27]</ref>. MiT-B2 <ref type="bibr" target="#b10">[11]</ref> shows the best performance in comparison to other MVTs. This can be attributed to shared information across feature vectors as mentioned in Section 2. In this work, we initialise the encoder via pretrained weights from MiT-B4 <ref type="bibr" target="#b10">[11]</ref>. We hope this benchmark can be used as a baseline for future research in depth estimation.  <ref type="table">Table 4</ref>. Ablation study on NYUV2 dataset for different decoder designs in the proposed Depthformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ablation for Decoder architecture: <ref type="table">Table 4</ref> showcases the efficacy of our proposed decoder network. For all these experiments, we have taken the proposed encoding backbone. The baseline uses a decoder network as proposed in <ref type="bibr" target="#b10">[11]</ref>, which predicts the depth at a resolution of 1/4th scale and upsamples it using bilinear interpolation. We first deploy Global Average Pooling (GAP) on top of our decoder to predict adaptive bins. As shown in the table, adding GAP gives a significant improvement over the baseline. Next, we ablate on our decoder design using the adaptive binning strategy proposed by Adabins <ref type="bibr" target="#b18">[19]</ref> and our proposed Transbins. As shown, Transbins outperforms Adabins by a good margin which validates our claim that encoding global information during the initial layers is a better strategy in comparison to encoding the information at the final layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper introduces a multiscale vision transformer-based monocular depth estimation technique that achieves stateof-the-art results on KITTI and NYUV2 datasets. We also benchmark SoTA transformer architectures for monocular depth estimation task. We hope it motivates researchers to design new architectures to address task specific intricacies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of our proposed method, Depthformer : MVT produces feature maps of multiple resolutions hierarchically fused</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>p</head><label></label><figDesc>Threshold accuracy (? i ) : % of d p such that max i &lt; Thr, where Thr = 1.25, 1.25 2 , 1.25 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Visualisation of predicted Depth map for input image (a) for (b) Adabins</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results on KITTI Dataset. The best results are in bold and second best are underlined.</figDesc><table><row><cell>Eigen et al. [1]</cell><cell></cell><cell>6.307</cell><cell cols="3">0.203 0.702 0.898 0.967</cell></row><row><cell>Goddard et al. [6]</cell><cell></cell><cell>4.935</cell><cell cols="3">0.114 0.861 0.949 0.976</cell></row><row><cell>Gan et al. [9]</cell><cell></cell><cell>3.933</cell><cell cols="3">0.098 0.890 0.964 0.985</cell></row><row><cell>DORN [2]</cell><cell></cell><cell>2.727</cell><cell cols="3">0.072 0.932 0.984 0.994</cell></row><row><cell>Yin et al. [23]</cell><cell></cell><cell>3.258</cell><cell cols="3">0.072 0.938 0.990 0.998</cell></row><row><cell>BTS [3]</cell><cell></cell><cell>2.756</cell><cell cols="3">0.059 0.956 0.993 0.998</cell></row><row><cell>DPT-Hybrid [18]</cell><cell></cell><cell>2.573</cell><cell cols="3">0.062 0.959 0.995 0.999</cell></row><row><cell>Adabins [19]</cell><cell></cell><cell>2.360</cell><cell cols="3">0.058 0.964 0.995 0.999</cell></row><row><cell cols="2">Depthformer (ours)</cell><cell>2.285</cell><cell cols="3">0.058 0.967 0.996 0.999</cell></row><row><cell>Method</cell><cell></cell><cell>Reference</cell><cell cols="3">Param. (M) RM SE ? REL ?</cell></row><row><cell>ResNet-50 [27]</cell><cell></cell><cell>CVPR'16</cell><cell>23.5</cell><cell>0.510</cell><cell>0.152</cell></row><row><cell>PVTv1 [12]</cell><cell></cell><cell>ICCV'21</cell><cell>23.9</cell><cell>0.508</cell><cell>0.166</cell></row><row><cell>Swin-T [15]</cell><cell></cell><cell>ICCV'21</cell><cell>27.5</cell><cell>0.456</cell><cell>0.142</cell></row><row><cell cols="3">Twins-SVT-S [16] NeurIPS'21</cell><cell>23.5</cell><cell>0.443</cell><cell>0.141</cell></row><row><cell>MiT-B2 [11]</cell><cell cols="2">NeurIPS'21</cell><cell>24.2</cell><cell>0.394</cell><cell>0.118</cell></row><row><cell>MPVIT [17]</cell><cell></cell><cell>CVPR'22</cell><cell>22.6</cell><cell>0.403</cell><cell>0.120</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement:</head><p>We acknowledge National Supercomputing Mission (NSM) for providing computing resources of 'PARAM Siddhi-AI', under National PARAM Supercomputing Facility, C-DAC Pune, and supported by the Ministry of Electronics and Information Technology and Department of Science and Technology, Government of India. This work has also been partly supported by the funding received from DST through the IMPRINT program (IMP/2019/000250).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Depthmap Prediction From A Single Image Using A Multi-scale Deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Ordinal Regression Network For Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">From Big To small: Multi-scale Local Planar Guidance For Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Guiding Monocular Depth Estimation Using Depth-Attention Volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Structure-Aware Residual Pyramid Network For Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised Video Summarization With Adversarial LSTM Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Depth From Single Monocular Images Using Deep Convolutional Neural Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-Supervised Deep Learning For Monocular Depth Map Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Monocular Depth Estimation With Affinity, Vertical Pooling, And Label Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<editor>ECCV 2018, Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and Efficient Design For Semantic Segmentation With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid Vision Transformer: A Versatile Backbone For Dense Prediction Without Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Twins: Revisiting The Design Of Spatial Attention In Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MPViT: Multi-Path Vision Transformer For Dense Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Willette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2022</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vision Transformers For Dense Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AdaBins: Depth Estimation Using Adaptive Bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision Meets Robotics: The KITTI Dataset</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Indoor Segmentation And Support Inference From RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enforcing Geometric Constraints Of Virtual Normal For Depth Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised CNN For Single View Depth Estimation: Geometry To The Rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
