<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De Witt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
						</author>
						<title level="a" type="main">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint actionvalues conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) holds considerable promise to help address a variety of cooperative multi-agent problems, such as coordination of robot swarms <ref type="bibr" target="#b11">(H?ttenrauch et al., 2017)</ref> and autonomous cars <ref type="bibr" target="#b1">(Cao et al., 2012</ref>  In many such settings, partial observability and/or communication constraints necessitate the learning of decentralised policies, which condition only on the local actionobservation history of each agent. Decentralised policies also naturally attenuate the problem that joint action spaces grow exponentially with the number of agents, often rendering the application of traditional single-agent RL methods impractical.</p><p>Fortunately, decentralised policies can often be learned in a centralised fashion in a simulated or laboratory setting. This often grants access to additional state information, otherwise hidden from agents, and removes inter-agent communication constraints. The paradigm of centralised training with decentralised execution <ref type="bibr" target="#b19">(Oliehoek et al., 2008;</ref><ref type="bibr" target="#b14">Kraemer &amp; Banerjee, 2016)</ref> has recently attracted attention in the RL community <ref type="bibr" target="#b12">(Jorge et al., 2016;</ref><ref type="bibr" target="#b5">Foerster et al., 2018)</ref>. However, many challenges surrounding how to best exploit centralised training remain open.</p><p>One of these challenges is how to represent and use the action-value function that most RL methods learn. On the one hand, properly capturing the effects of the agents' actions requires a centralised action-value function Q tot that conditions on the global state and the joint action. On the other hand, such a function is difficult to learn when there are many agents and, even if it can be learned, offers no obvious way to extract decentralised policies that allow each agent to select only an individual action based on an individual observation.</p><p>arXiv:1803.11485v2 <ref type="bibr">[cs.</ref>LG] 6 Jun 2018</p><p>The simplest option is to forgo a centralised action-value function and let each agent a learn an individual action-value function Q a independently, as in independent Q-learning (IQL) <ref type="bibr" target="#b27">(Tan, 1993)</ref>. However, this approach cannot explicitly represent interactions between the agents and may not converge, as each agent's learning is confounded by the learning and exploration of others.</p><p>At the other extreme, we can learn a fully centralised stateaction value function Q tot and then use it to guide the optimisation of decentralised policies in an actor-critic framework, an approach taken by counterfactual multi-agent (COMA) policy gradients <ref type="bibr" target="#b5">(Foerster et al., 2018)</ref>, as well as work by <ref type="bibr" target="#b7">Gupta et al. (2017)</ref>. However, this requires onpolicy learning, which can be sample-inefficient, and training the fully centralised critic becomes impractical when there are more than a handful of agents.</p><p>In between these two extremes, we can learn a centralised but factored Q tot , an approach taken by value decomposition networks (VDN) <ref type="bibr" target="#b24">(Sunehag et al., 2017)</ref>. By representing Q tot as a sum of individual value functions Q a that condition only on individual observations and actions, a decentralised policy arises simply from each agent selecting actions greedily with respect to its Q a . However, VDN severely limits the complexity of centralised action-value functions that can be represented and ignores any extra state information available during training.</p><p>In this paper, we propose a new approach called QMIX which, like VDN, lies between the extremes of IQL and COMA, but can represent a much richer class of actionvalue functions. Key to our method is the insight that the full factorisation of VDN is not necessary to extract decentralised policies. Instead, we only need to ensure that a global argmax performed on Q tot yields the same result as a set of individual argmax operations performed on each Q a . To this end, it suffices to enforce a monotonicity constraint on the relationship between Q tot and each Q a :</p><formula xml:id="formula_0">?Q tot ?Q a ? 0, ?a.<label>(1)</label></formula><p>QMIX consists of agent networks representing each Q a , and a mixing network that combines them into Q tot , not as a simple sum as in VDN, but in a complex non-linear way that ensures consistency between the centralised and decentralised policies. At the same time, it enforces the constraint of (1) by restricting the mixing network to have positive weights. As a result, QMIX can represent complex centralised action-value functions with a factored representation that scales well in the number of agents and allows decentralised policies to be easily extracted via linear-time individual argmax operations.</p><p>We evaluate QMIX on a range of unit micromanagement tasks built in StarCraft II 1 . <ref type="bibr" target="#b29">(Vinyals et al., 2017)</ref>. Our experiments show that QMIX outperforms IQL and VDN, both in terms of absolute performance and learning speed. In particular, our method shows considerable performance gains on a task with heterogeneous agents. Moreover, our ablations show both the necessity of conditioning on the state information and the non-linear mixing of agent Q-values in order to achieve consistent performance across tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent work in multi-agent RL has started moving from tabular methods <ref type="bibr" target="#b31">(Yang &amp; Gu, 2004;</ref><ref type="bibr" target="#b0">Busoniu et al., 2008)</ref> to deep learning methods that can tackle high-dimensional state and action spaces <ref type="bibr" target="#b26">(Tampuu et al., 2017;</ref><ref type="bibr" target="#b5">Foerster et al., 2018;</ref><ref type="bibr" target="#b21">Peng et al., 2017)</ref>. In this paper, we focus on cooperative settings.</p><p>On the one hand, a natural approach to finding policies for a multi-agent system is to directly learn decentralised value functions or policies. Independent Q-learning <ref type="bibr" target="#b27">(Tan, 1993)</ref> trains independent action-value functions for each agent using Q-learning <ref type="bibr" target="#b30">(Watkins, 1989)</ref>. <ref type="bibr" target="#b26">(Tampuu et al., 2017)</ref> extend this approach to deep neural networks using DQN <ref type="bibr" target="#b17">(Mnih et al., 2015)</ref>. While trivially achieving decentralisation, these approaches are prone to instability arising from the non-stationarity of the environment induced by simultaneously learning and exploring agents. <ref type="bibr" target="#b20">Omidshafiei et al. (2017)</ref> and <ref type="bibr" target="#b4">Foerster et al. (2017)</ref> address learning stabilisation to some extent, but still learn decentralised value functions and do not allow for the inclusion of extra state information during training.</p><p>On the other hand, centralised learning of joint actions can naturally handle coordination problems and avoids nonstationarity, but is hard to scale, as the joint action space grows exponentially in the number of agents. Classical approaches to scalable centralised learning include coordination graphs <ref type="bibr" target="#b6">(Guestrin et al., 2002)</ref>, which exploit conditional independencies between agents by decomposing a global reward function into a sum of agent-local terms. Sparse cooperative Q-learning <ref type="bibr" target="#b13">(Kok &amp; Vlassis, 2006</ref>) is a tabular Q-learning algorithm that learns to coordinate the actions of a group of cooperative agents only in the states in which such coordination is necessary, encoding those dependencies in a coordination graph. These methods require the dependencies between agents to be pre-supplied, whereas we do not require such prior knowledge. Instead, we assume that each agent always contributes to the global reward, and learns the magnitude of its contribution in each state.</p><p>More recent approaches for centralised learning require even more communication during execution: CommNet <ref type="bibr" target="#b23">(Sukhbaatar et al., 2016)</ref> uses a centralised network architecture to exchange information between agents. BicNet <ref type="bibr" target="#b21">(Peng et al., 2017)</ref> uses bidirectional RNNs to exchange information between agents in an actor-critic setting. This approach additionally requires estimating individual agent rewards.</p><p>Some work has developed hybrid approaches that exploit the setting of centralised learning with fully decentralised execution. COMA <ref type="bibr" target="#b5">(Foerster et al., 2018)</ref> uses a centralised critic to train decentralised actors, estimating a counterfactual advantage function for each agent in order to address multi-agent credit assignment. Similarly, <ref type="bibr" target="#b7">Gupta et al. (2017)</ref> present a centralised actor-critic algorithm with per-agent critics, which scales better with the number of agents but mitigates the advantages of centralisation. <ref type="bibr" target="#b16">Lowe et al. (2017)</ref> learn a centralised critic for each agent and apply this to competitive games with continuous action spaces. These approaches use on-policy policy gradient learning, which can have poor sample efficiency and is prone to getting stuck in sub-optimal local minima. <ref type="bibr" target="#b24">Sunehag et al. (2017)</ref> propose value decomposition networks (VDN), which allow for centralised value-function learning with decentralised execution. Their algorithm decomposes a central state-action value function into a sum of individual agent terms. This corresponds to the use of a degenerate fully disconnected coordination graph. VDN does not make use of additional state information during training and can represent only a limited class of centralised action-value functions.</p><p>A number of papers have established unit micromanagement in StarCraft as a benchmark for deep multi-agent RL. <ref type="bibr" target="#b28">Usunier et al. (2017)</ref> present an algorithm using a centralised greedy MDP and first-order optimisation. <ref type="bibr" target="#b21">Peng et al. (2017)</ref> also evaluate their methods on StarCraft. However, neither requires decentralised execution. Similar to our setup is the work of <ref type="bibr" target="#b4">Foerster et al. (2017)</ref>, who evaluate replay stabilisation methods for IQL on combat scenarios with up to five agents. <ref type="bibr" target="#b5">Foerster et al. (2018)</ref> also uses this setting. In this paper, we construct unit micromanagement tasks in the StarCraft II Learning Environment (SC2LE) <ref type="bibr" target="#b29">(Vinyals et al., 2017)</ref> as opposed to StarCraft, because it is actively supported by the game developers and SC2LE offers a more stable testing environment.</p><p>QMIX relies on a neural network to transform the centralised state into the weights of another neural network, in a manner reminiscent of hypernetworks <ref type="bibr" target="#b8">(Ha et al., 2017)</ref>. This second neural network is constrained to be monotonic with respect to its inputs by keeping its weights positive. <ref type="bibr" target="#b3">Dugas et al. (2009)</ref> investigate such functional restrictions for neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>A fully cooperative multi-agent task can be described as a Dec-POMDP <ref type="bibr" target="#b18">(Oliehoek &amp; Amato, 2016)</ref> consisting of a tuple G = S, U, P, r, Z, O, n, ? . s ? S describes the true state of the environment. At each time step, each agent a ? A ? {1, ..., n} chooses an action u a ? U , forming a joint action u ? U ? U n . This causes a transition on the environment according to the state transition function</p><formula xml:id="formula_1">P (s |s, u) : S ? U ? S ? [0, 1]. All agents share the same reward function r(s, u) : S ? U ? R and ? ? [0, 1) is a discount factor.</formula><p>We consider a partially observable scenario in which each agent draws individual observations z ? Z according to</p><formula xml:id="formula_2">observation function O(s, a) : S ? A ? Z. Each agent has an action-observation history ? a ? T ? (Z ? U ) * , on which it conditions a stochastic policy ? a (u a |? a ) : T ? U ? [0, 1].</formula><p>The joint policy ? has a joint action-value function:</p><formula xml:id="formula_3">Q ? (s t , u t ) = E st+1:?,ut+1:? [R t |s t , u t ], where R t = ? i=0 ? i r t+i is the discounted return.</formula><p>Although training is centralised, execution is decentralised, i.e., the learning algorithm has access to all local actionobservation histories ? and global state s, but each agent's learnt policy can condition only on its own actionobservation history ? a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Q-Learning</head><p>Deep Q-learning represents the action-value function with a deep neural network parameterised by ?. Deep Q-networks (DQNs) <ref type="bibr" target="#b17">(Mnih et al., 2015)</ref> use a replay memory to store the transition tuple s, u, r, s , where the state s is observed after taking the action u in state s and receiving reward r. ? is learnt by sampling batches of b transitions from the replay memory and minimising the squared TD error:</p><formula xml:id="formula_4">L(?) = b i=1 y DQN i ? Q(s, u; ?) 2 ,<label>(2)</label></formula><p>where y DQN = r + ? max u Q(s , u ; ? ? ). ? ? are the parameters of a target network that are periodically copied from ? and kept constant for a number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Recurrent Q-Learning</head><p>In partially observable settings, agents can benefit from conditioning on their entire action-observation history. <ref type="bibr" target="#b9">Hausknecht &amp; Stone (2015)</ref> propose Deep Recurrent Qnetworks (DRQN) that make use of recurrent neural networks. Typically, gated architectures such as LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref> or GRU <ref type="bibr" target="#b2">(Chung et al., 2014)</ref> are used to facilitate learning over longer timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Independent Q-Learning</head><p>Perhaps the most commonly applied method in multi-agent learning is independent Q-learning (IQL) <ref type="bibr" target="#b27">(Tan, 1993)</ref>, which decomposes a multi-agent problem into a collection of simultaneous single-agent problems that share the same environment. This approach does not address the nonstationarity introduced due to the changing policies of the learning agents, and thus, unlike Q-learning, has no convergence guarantees even in the limit of infinite exploration. In practice, nevertheless, IQL commonly serves as a surprisingly strong benchmark even in mixed and competitive games <ref type="bibr" target="#b26">(Tampuu et al., 2017;</ref><ref type="bibr" target="#b15">Leibo et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Value Decomposition Networks</head><p>By contrast, value decomposition networks (VDNs) (Sunehag et al., 2017) aim to learn a joint action-value function Q tot (? , u), where ? ? T ? T n is a joint actionobservation history and u is a joint action. It represents Q tot as a sum of individual value functions Q a (? a , u a ; ? a ), one for each agent a, that condition only on individual action-observation histories:</p><formula xml:id="formula_5">Q tot (? , u) = n i=1 Q i (? i , u i ; ? i ).<label>(3)</label></formula><p>Strictly speaking, each Q a is a utility function <ref type="bibr" target="#b6">(Guestrin et al., 2002)</ref> and not a value function since by itself it does not estimate an expected return. However, for terminological simplicity we refer to both Q tot and Q a as value functions.</p><p>The loss function for VDN is equivalent to (2), where Q is replaced by Q tot . An advantage of this representation is that a decentralised policy arises simply from each agent performing greedy action selection with respect to its Q a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QMIX</head><p>In this section, we propose a new approach called QMIX which, like VDN, lies between the extremes of IQL and centralised Q-learning, but can represent a much richer class of action-value functions.</p><p>Key to our method is the insight that the full factorisation of VDN is not necessary in order to be able to extract decentralised policies that are fully consistent with their centralised counterpart. Instead, for consistency we only need to ensure that a global argmax performed on Q tot yields the same result as a set of individual argmax operations performed on each Q a :</p><formula xml:id="formula_6">argmax u Q tot (? , u) = ? ? ? argmax u 1 Q 1 (? 1 , u 1 ) . . . argmax u n Q n (? n , u n ) ? ? ? . (4)</formula><p>This allows each agent a to participate in a decentralised execution solely by choosing greedy actions with respect to its Q a . As a side effect, if (4) is satisfied, then taking the argmax of Q tot , required by off-policy learning updates, is trivially tractable.</p><p>VDN's representation is sufficient to satisfy (4). However, QMIX is based on the observation that this representation can be generalised to the larger family of monotonic functions that are also sufficient but not necessary to satisfy (4). Monotonicity can be enforced through a constraint on the relationship between Q tot and each Q a :</p><formula xml:id="formula_7">?Q tot ?Q a ? 0, ?a ? A.<label>(5)</label></formula><p>To enforce <ref type="formula" target="#formula_7">(5)</ref>, QMIX represents Q tot using an architecture consisting of agent networks, a mixing network, and a set of hypernetworks <ref type="bibr" target="#b8">(Ha et al., 2017)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the overall setup.</p><p>For each agent a, there is one agent network that represents its individual value function Q a (? a , u a ). We represent agent networks as DRQNs that receive the current individual observation o a t and the last action u a t?1 as input at each time step, as shown in <ref type="figure" target="#fig_1">Figure 2c</ref>.</p><p>The mixing network is a feed-forward neural network that takes the agent network outputs as input and mixes them monotonically, producing the values of Q tot , as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>. To enforce the monotonicity constraint of (5), the weights (but not the biases) of the mixing network are restricted to be non-negative. This allows the mixing network to approximate any monotonic function arbitrarily closely <ref type="bibr" target="#b3">(Dugas et al., 2009</ref>).</p><p>The weights of the mixing network are produced by separate hypernetworks. Each hypernetwork takes the state s as input and generates the weights of one layer of the mixing network. Each hypernetwork consists of a single linear layer, followed by an absolute activation function, to ensure that the mixing network weights are non-negative. The output of the hypernetwork is then a vector, which is reshaped into a matrix of appropriate size. The biases are produced in the same manner but are not restricted to being non-negative. The final bias is produced by a 2 layer hypernetwork with a ReLU non-linearity. <ref type="figure" target="#fig_1">Figure 2a</ref> illustrates the mixing network and the hypernetworks.</p><p>The state is used by the hypernetworks rather than being passed directly into the mixing network because Q tot is allowed to depend on the extra state information in nonmonotonic ways. Thus, it would be overly constraining to pass some function of s through the monotonic network alongside the per-agent values. Instead, the use of hypernetworks makes it possible to condition the weights of the monotonic network on s in an arbitrary way, thus integrating the full state s into the joint action-value estimates as flexibly as possible.</p><p>QMIX is trained end-to-end to minimise the following loss:</p><formula xml:id="formula_8">L(?) = b i=1 y tot i ? Q tot (? , u, s; ?) 2 ,<label>(6)</label></formula><p>where b is the batch size of transitions sampled from the replay buffer, y tot = r + ? max u Q tot (? , u , s ; ? ? ) and ? ? are the parameters of a target network as in DQN. <ref type="formula" target="#formula_8">(6)</ref> is analogous to the standard DQN loss of (2). Since (4) holds, we can perform the maximisation of Q tot in time linear in the number of agents (as opposed to scaling exponentially in the worst case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Representational Complexity</head><p>The value function class representable with QMIX includes any value function that can be factored into a non-linear monotonic combination of the agents' individual value functions in the fully observable setting. This expands upon the linear monotonic value functions that are representable by VDN. However, the constraint in (5) prevents QMIX from representing value functions that do not factorise in such a manner.</p><p>Intuitively, any value function for which an agent's best action depends on the actions of the other agents at the same time step will not factorise appropriately, and hence cannot be represented perfectly by QMIX. However, QMIX can approximate such value functions more accurately than VDN. Furthermore, it can take advantage of the extra state information available during training, which we show empirically. A more detailed discussion on the representation complexity is available in the supplementary materials.</p><formula xml:id="formula_9">Agent 2 A B Agent 1 A 7 7 B 7 7 State 2A Agent 2 A B Agent 1 A 0 1 B 1 8</formula><p>State 2B </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Two-Step Game</head><p>To illustrate the effects of representational complexity of VDN and QMIX, we devise a simple two-step cooperative matrix game for two agents.</p><p>At the first step, Agent 1 chooses which of the two matrix games to play in the next timestep. For the first time step, the actions of Agent 2 have no effect. In the second step, both agents choose an action and receive a global reward according to the payoff matrices depicted in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We train VDN and QMIX on this task for 5000 episodes and examine the final learned value functions in the limit of full exploration ( = 1). Full exploration ensures that each method is guaranteed to eventually explore all available game states, such that the representational capacity of the state-action value function approximation remains the only limitation. The full details of the architecture and hyperparameters used are provided in the supplementary material. it to accurately represent the joint-action value function whereas VDN cannot. This directly translates into VDN learning the suboptimal strategy of selecting Action A at the first step and receiving a reward of 7, whereas QMIX recovers the optimal strategy from its learnt joint-action values and receives a reward of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setup</head><p>In this section, we describe the decentralised StarCraft II micromanagement problems to which we apply QMIX and the ablations we consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Decentralised StarCraft II Micromanagement</head><p>Real-time strategy (RTS) games have recently emerged as challenging benchmarks for the RL community. StarCraft, in particular, offers a great opportunity to tackle competitive and cooperative multi-agent problems. Units in StarCraft have a rich set of complex micro-actions that allow the learning of complex interactions between collaborating agents. Previous work <ref type="bibr" target="#b28">(Usunier et al., 2017;</ref><ref type="bibr" target="#b5">Foerster et al., 2018;</ref><ref type="bibr" target="#b21">Peng et al., 2017)</ref> applied RL to the original version of Star-Craft: BW, which made use of the standard API or related wrappers <ref type="bibr" target="#b25">(Synnaeve et al., 2016)</ref>. We perform our experiments on the StarCraft II Learning Environment (SC2LE) <ref type="bibr" target="#b29">(Vinyals et al., 2017)</ref>, which is based on the second version of the game. Because it is supported by the developers of the game, SC2LE mitigates many of the practical difficulties in using StarCraft as an RL platform, such as the dependence on complicated APIs and external emulation software.</p><p>In this work, we focus on the decentralised micromanagement problem in StarCraft II, in which each of the learning agents controls an individual army unit. We consider combat scenarios where two groups of identical units are placed symmetrically on the map. This facilitates the decentralisation of the problem and prohibits the usage of the attack-move macroactions that are integrated into the game. Furthermore, we disable the following unit behaviour when idle: responding to enemy fire and attacking enemies if they are in range. By doing so, we force the agents to explore in order to find the optimal combat strategy themselves, rather than relying on built-in StarCraft II utilities.</p><p>Partial observability is achieved by the introduction of unit sight range, which restricts the agents from receiving information about allied or enemy units that are out of range. Moreover, agents can only observe others if they are alive and cannot distinguish between units that are dead or out of range.</p><p>At each time step, the agents receive a joint reward equal to the total damage dealt on the enemy units. In addition, agents receive a bonus of 10 points after killing each opponent, and 200 points after killing all opponents. These rewards are all normalised to ensure the maximum cumulative reward achievable in an episode is 20.</p><p>The full details of the environmental setup, architecture and training are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations</head><p>We perform ablation experiments in order to investigate the influence of the inclusion of extra state information and the necessity of non-linear transformations in the mixing network.</p><p>First, we analyse the significance of extra state information on the mixing network by comparing against QMIX without hypernetworks. Thus, the weights and biases of the mixing network are learned in the standard way, without conditioning on the state. We refer to this method as QMIX-NS. We take the absolute value of the weights in order to enforce the monotonicity constraint.</p><p>Second, we investigate the necessity of non-linear mixing by removing the hidden layer of the mixing network. This method can be thought of as an extension of VDN that uses  the state s to perform a weighted sum over Q a values. We call this method QMIX-Lin.</p><p>Third, we investigate the significance of utilising the state s in comparison to the non-linear mixing. To do this we extend VDN by adding a state-dependent term to the sum of the agent's Q-Values. This state-dependent term is produced by a network with a single hidden layer of 32 units and a ReLU non-linearity, taking in the state s as input (the same as the hypernetwork producing the final bias in QMIX). We refer to this method as VDN-S.</p><p>We also show the performance of a non-learning heuristicbased algorithm with full observability, where each agent attacks the closest enemy and continues attacking the same target until the unit dies. Afterwards, the agent starts attacking the nearest enemy and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>In order to evaluate each method's performance, we adopt the following evaluation procedure: for each run of a method, we pause training every 100 episodes and run 20 independent episodes with each agent performing greedy decentralised action selection. The percentage of these episodes in which the method defeats all enemy units within the time limit is referred to as the test win rate.</p><p>Figures 3 and 4 plot the mean test win rate across 20 runs for each method on selected maps, together with 95% confidence intervals. The graphs for all methods on all maps are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Main Results</head><p>In all scenarios, IQL fails to learn a policy that consistently defeats the enemy. In addition, the training is highly unstable due to the non-stationarity of the environment which arises due to the other agents changing their behaviour during training.</p><p>The benefits of learning the joint action-value function can be demonstrated by VDN's superior performance over IQL in all scenarios. VDN is able to more consistently learn basic coordinated behaviour, in the form of focus firing which allows it to win the majority of its encounters on the 5m and 8m maps. On the 8m map, this simple strategy is sufficient for good performance, as evidenced by the extremely high win rate of the heuristic-based algorithm, and explains the performance parity with QMIX. However, on the 3m task, which requires more fine-grained control, it is unable to learn to consistently defeat the enemy.</p><p>QMIX is noticeably the strongest performer on all of the maps, in particular on the maps with hetergenous agent types. The largest performance gap can be seen in the  3s 5z and 1c 3s 5z maps, where VDN is unable to reach the performance of the simple heuristic. The superior representational capacity of QMIX combined with the state information presents a clear benefit over a more restricted linear decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Ablation Results</head><p>Our additional ablation experiments reveal that QMIX outperforms, or is competitive with, all of its ablations discussed in Section 6.2. <ref type="figure" target="#fig_5">Figure 4a</ref> shows that non-linear value function factorisation is not always required on a map with homogeneous agent types. However, the additional complexity introduced through the extra hidden layer does not slow down learning. In contrast, <ref type="figure" target="#fig_5">Figures 4b and 4c</ref> show that on a map with heterogeneous agent types a combination of both central state information and non-linear value function factorisation is required to achieve good performance. QMIX-NS performs on par or slightly better than VDN in both scenarios, which suggests that a non-linear decomposition is not always beneficial when not conditioning on the central state in complex scenarios. Additionally, the performance of VDN-S compared to QMIX-Lin shows the necessity of allowing a non-linear mixing in order to fully leverage central state information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Learned Policies</head><p>We examine the learned behaviours of the policies in order to better understand the differences between the strategies learnt by the different methods. On the 8m scenario, both QMIX and VDN learn the particularly sophisticated strategy of first positioning the units into a semicircle in order to fire at the incoming enemy units from the sides (as opposed to just head on). On the 2s 3z scenario, VDN first runs left and then attacks the enemy once they are in range with no regards to positioning or unit match-ups. QMIX, on the other hand learns to position the Stalkers so that the enemy Zealots cannot attack them. This is especially important since Zealots counter Stalkers. QMIX achieves this by having the allied Zealots first block off and then attack the enemy Zealots (whilst the Stalkers fire from a safe distance), before moving on to the enemy Stalkers. The same behaviour is observed in the 3s 5z scenario for QMIX. VDN-S does not learn to protect the Stalkers from the Zealots, and first positions the units around their starting location and then attacks the enemy as they move in.</p><p>The initial hump in the performance of both VDN and IQL is due to both methods initially learning the simple strategy of just attacking the first visible enemy (which is quite successful as shown by the heuristic). However, due to exploratory learning behaviour, they also attempt to move around (instead of just firing), which results in the rapid decline in performance. IQL is unable to then recover the initial strategy, whereas VDN learns how to combine small movements and firing together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper presented QMIX, a deep multi-agent RL method that allows end-to-end learning of decentralised policies in a centralised setting and makes efficient use of extra state information. QMIX allows the learning of a rich joint actionvalue function, which admits tractable decompositions into per-agent action-value functions. This is achieved by imposing a monotonicity constraint on the mixing network.</p><p>Our results in decentralised unit micromanagement tasks in StarCraft II show that QMIX improves the final performance over other value-based multi-agent methods that employ less sophisticated joint state-value function factorisation, as well as independent Q-learning.</p><p>In the near future, we aim to conduct additional experiments to compare the methods across tasks with a larger number and greater diversity of units. In the longer term, we aim to complement QMIX with more coordinated exploration schemes for settings with many learning agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. QMIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Representational Complexity</head><p>The value function class representable with QMIX includes any value function that can be factored into a non-linear monotonic combination of the agents' individual value functions in the fully observable setting.</p><p>This follows since the mixing network is a universal function approximator of monotonic functions <ref type="bibr" target="#b3">(Dugas et al., 2009)</ref>, and hence can represent any value function that factors into a non-linear monotonic combination of the agent's individual value functions. Additionally, we require that the agent's individual value functions order the values of the actions appropriately. By this we mean that Q a is such that Q a (s t , u a ) &gt; Q a (s t , u a ) ?? Q tot (s t , (u ?a , u a )) &gt; Q tot (s t , (u ?a , u a )), i.e., they can represent a function that respects the ordering of the agent's actions in the joint-action value function. Since the agents' networks are universal function approximators <ref type="bibr" target="#b22">(Pinkus, 1999)</ref>, they can represent such a Q a . Hence QMIX is able to represent any value function that factors into a non-linear monotonic combination of the agent's individual value functions.</p><p>In a Dec-POMDP, QMIX cannot necessarily represent the value function. This is because each agent's observations are no longer the full state, and thus they might not be able to distinguish the true state given their local observations. If the agent's value function ordering is then wrong, i.e., Q a (? a , u) &gt; Q a (? a , u ) when Q tot (s t , (u ?a , u)) &lt; Q tot (s t , (u ?a , u )), then the mixing network would be unable to correctly represent Q tot given the monotonicity constraints.</p><p>QMIX expands upon the linear monotonic value functions that are representable by VDN. <ref type="table" target="#tab_4">Table 3a</ref> gives an example of a monotonic value function for the simple case of a two-agent matrix game. Note that VDN is unable to represent this simple monotonic value function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent 2 A B</head><p>Agent However, the constraint in (5) prevents QMIX from representing value functions that do not factorise in such a manner. A simple example of such a value function for a two-agent matrix game is given in <ref type="table" target="#tab_4">Table 3b</ref>. Intuitively, any value function for which an agent's best action depends on the actions of the other agents at the same time step will not factorise appropriately, and hence cannot be represented perfectly by QMIX.</p><formula xml:id="formula_10">1 A 0 1 B 1 8 (a) Agent 2 A B Agent 1 A 2 1 B 1 8 (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two</head><p>Step Game</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Architecture and Training</head><p>The architecture of all agent networks is a DQN with a single hidden layer comprised of 64 units with a ReLU nonlinearity. Each agent performs independent greedy action selection, with = 1. We set ? = 0.99. The replay buffer consists of the last 500 episodes, from which we uniformly sample a batch of size 32 for training. The target network is updated every 100 episodes. The learning rate for RMSprop is set to 5 ? 10 ?4 . We train for 10k timesteps. The size of the mixing network is 8 units. All agent networks share parameters, thus the agent id is concatenated onto each agent's observations. We do not pass the last action taken to the agent as input. Each agent receives the full state as input.</p><p>Each state is one-hot encoded. The starting state for the first timestep is State 1. If Agent 1 takes Action A, it transitions to State 2 (whose payoff matrix is all 7s). If agent 1 takes Action B in the first timestep, it transitions to State 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Learned Value Functions</head><p>The learned value functions for the different methods on the Two Step Game are shown in <ref type="table">Tables 4 and 5</ref>  <ref type="figure" target="#fig_6">Figure 5</ref> shows the loss for the different methods. <ref type="table" target="#tab_6">Table 6</ref> shows the final testing reward for each method.  The global state, which is hidden from agents, is a vector comprised of features of units from the entire map. It does not contain the absolute distances between agents and stores only the coordinates of units relative to the centre of the map. In addition, the global state includes the health, shield and cooldown of all units. <ref type="bibr">3</ref> In addition, the global state contains the last actions taken by all allied agents. Marines, Stalkers, Zealots, and Colossi have 45, 80, 100, and 200 hit points, respectively. In addition, Stalkers, Zealots, and Colossi have 80, 50, and 150 shield points, respectively. All features, whether in local observations or global state, are normalised by their maximum values. For all unit types, the agent sight range and shooting ranges are set to 9 and 6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Architecture and Training</head><p>The architecture of all agent networks is a DRQN with a recurrent layer comprised of a GRU with a 64-dimensional hidden state, with a fully-connected layer before and after. Exploration is performed during training using independent -greedy action selection, where each agent a performs -greedy action selection over its own Q a . Throughout the training, we anneal linearly from 1.0 to 0.05 over 50k time steps and keep it constant for the rest of the learning. We set ? = 0.99 for all experiments. The replay buffer contains the most recent 5000 episodes. We sample batches of 32 episodes uniformly from the replay buffer and train on fully unrolled episodes. The target networks are updated after every 200 training episodes.</p><p>To speed up the learning, we share the parameters of the agent networks across all agents. Because of this, a one-hot encoding of the agent id is concatenated onto each agent's observations. All neural networks are trained using RMSprop 4 with learning rate 5 ? 10 ?4 .</p><p>During training and testing, we restrict each episode to have a length of 60 time steps for 3m and 5m maps, 120 time steps for 8m and 2s 3z maps, 150 for 3s 5z and 200 for 1c 3s 5z. If both armies are alive at the end of the episode, we count it as a loss. The episode terminates after one army has been defeated, or the time limit has been reached.</p><p>The mixing network consists of a single hidden layer of 32 units, utilising an ELU non-linearity. The hypernetworks are then sized to produce weights of appropriate size. The hypernetwork producing the final bias of the mixing network consists of a single hidden layer of 32 units with a ReLU non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. StarCraft II Results</head><p>The results for all six methods and the heuristic-based algorithm on the six maps. 3m 5m 8m 2s 3z 3s 5z 1c 3s 5z 76 60 95 82 45 70 <ref type="table">Table 7</ref>. The Test Win Rate % of the heuristic-based algorithm on the six maps.</p><p>2 unit type is only included in the 2s 3z, 3s 5z and 1c 3s 5z maps. 3 A unit's cooldown is the time it must wait before firing again. Shields act as additional forms of hit points and are lost first. In contrast to health, shields regenerate over time after absorbing damage. <ref type="bibr">4</ref> We set ? = 0.99 and do not use weight decay or momentum.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Decentralised unit micromanagement in StarCraft II, where each learning agent controls an individual unit. The goal is to coordinate behaviour across agents to defeat all enemy units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Mixing network structure. In red are the hypernetworks that produce the weights and biases for mixing network layers shown in blue. (b) The overall QMIX architecture. (c) Agent network structure. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Win rates for IQL, VDN, and QMIX on six different combat maps. The performance of the heuristic-based algorithm is shown as a dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Win rates for QMIX and ablations on 3m, 2s 3z and 3s 5z maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Loss for all six methods on the Two Step Game. The mean and 95% confidence interval is shown across 30 independent runs.QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning IQL VDN VDN-S QMIX QMIX-Lin QMIX-NS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Win rates for IQL, VDN, and QMIX on six different combat maps. The performance of the heuristic-based algorithm is shown as a dashed line. Win rates for QMIX and ablations on six different combat maps. The performance of the heuristic-based algorithm is shown as a dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Equal contribution 1 University of Oxford, Oxford, United Kingdom 2 Russian-Armenian University, Yerevan, Armenia. Correspondence to: Tabish Rashid &lt;tabish.rashid@cs.ox.ac.uk&gt;, Mikayel Samvelyan &lt;mikayel@samvelyan.com&gt;. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).</figDesc><table /><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Payoff matrices of the two-step game after the Agent 1 chose the first action. Action A takes the agents to State 2A and action B takes them to State 2B.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ,Table 2 .</head><label>22</label><figDesc>Qtot on the two-step game for (a) VDN and (b) QMIX.</figDesc><table><row><cell>capacity allows</cell></row></table><note>which shows the learned values for Q tot , demon- strates that QMIX's higher representational</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>At the beginning of each episode, the enemy units are ordered to attack the allies. We compare our results on a set of maps where each unit group consists of 3</figDesc><table><row><cell>Marines (3m), 5 Marines (5m), 8 Marines (8m), 2 Stalkers</cell></row><row><cell>and 3 Zealots (2s 3z), 3 Stalkers and 5 Zealots (3s 5z), or 1</cell></row><row><cell>Colossus, 3 Stalkers and 5 Zealots (1c 3s 5z).</cell></row><row><cell>Similar to the work of Foerster et al. (2018), the action</cell></row><row><cell>space of agents consists of the following set of discrete</cell></row><row><cell>actions: move[direction], attack[enemy id],</cell></row><row><cell>stop, and noop. Agents can only move in four directions:</cell></row><row><cell>north, south, east, or west. A unit is allowed to perform the</cell></row><row><cell>attack[enemy id] action only if the enemy is within</cell></row><row><cell>its shooting range.</cell></row></table><note>The units of the first, allied, group are controlled by the decentralised agents. The en- emy units are controlled by a built-in StarCraft II AI, which makes use of handcrafted heuristics. The initial placement of units within the groups varies across episodes. The diffi- culty of the computer AI controlling the enemy units is set to medium.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>(a) An example of a monotonic payoff matrix, (b) a non-monotonic payoff matrix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The final Test Reward acheived.The local observations of individual agents are drawn within their field of view, which encompasses the circular area of the map surrounding units and has a radius equal to the sight range. Each agent receives as input a vector consisting of the following features for all units in its field of view (both allied and enemy): distance, relative x, relative y and unit type. 2</figDesc><table><row><cell>C. StarCraft II Setup</cell></row><row><cell>C.1. Environment Features</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">StarCraft and StarCraft II are trademarks of Blizzard Entertainment TM .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713). It was also supported by the Oxford-Google DeepMind Graduate Scholarship, the UK EPSRC CDT in Autonomous Intelligent Machines and Systems, Chevening Scholarship, Luys Scholarship and an EPSRC grant (EP/M508111/1, EP/N509711/1). This work is linked to and partly funded by the project Free the Drones (FreeD) under the Innovation Fund Denmark and Microsoft. The experiments were made possible by a generous equipment grant from NVIDIA.</p><p>We would like to thank Frans Oliehoek and Wendelin Boehmer for helpful comments and discussion. We also thank Oriol Vinyals, Kevin Calderone, and the rest of the SC2LE team at DeepMind and Blizzard for their work on the interface.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey of Multiagent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Overview of Recent Progress in the Study of Distributed Multiagent Coordination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="427" to="438" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating functional knowledge in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Blisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1239" to="1262" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 34th International Conference on Machine Learning</title>
		<meeting>The 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1146" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiagent Planning with Factored MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1523" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cooperative Multi-agent Control Using Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous Agents and Multiagent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Recurrent Q-Learning for Partially Observable MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Guided Deep Reinforcement Learning for Swarm Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?ttenrauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>?o?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AA-MAS 2017 Autonomous Robots and Multirobot Systems (ARMS) Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to play guess who? and inventing a grounded language as a consequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?geb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gustavsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative Multiagent Reinforcement Learning by Payoff Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1789" to="1828" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning as a rehearsal for decentralized planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning in sequential social dilemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 16th Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6382" to="6393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A Concise Introduction to Decentralized POMDPs. SpringerBriefs in Intelligent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal and Approximate Q-value Functions for Decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T J</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Decentralized Multi-task Multi-Agent RL under Partial Observability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10069</idno>
		<title level="m">Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximation theory of the mlp model in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta numerica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Auvolat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torchcraft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00625</idno>
		<title level="m">Library for Machine Learning Research on Real-Time Strategy Games</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Calderone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Keet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brunasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Repp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starcraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04782</idno>
		<title level="m">A New Challenge for Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge England</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multiagent reinforcement learning for multi-robot systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
