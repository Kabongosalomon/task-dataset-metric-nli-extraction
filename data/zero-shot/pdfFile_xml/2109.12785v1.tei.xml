<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High Frame Rate Video Quality Assessment using VMAF and Entropic Differences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
							<email>:pavancm@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Birkbeck</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balu</forename><surname>Adsumilli</surname></persName>
							<email>badsumilli@google.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
							<email>bovik@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High Frame Rate Video Quality Assessment using VMAF and Entropic Differences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-high frame rate</term>
					<term>video quality assessment</term>
					<term>full reference</term>
					<term>entropy</term>
					<term>generalized Gaussian distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The popularity of streaming videos with live, highaction content has led to an increased interest in High Frame Rate (HFR) videos. In this work we address the problem of frame rate dependent Video Quality Assessment (VQA) when the videos to be compared have different frame rate and compression factor. The current VQA models such as VMAF have superior correlation with perceptual judgments when videos to be compared have same frame rates and contain conventional distortions such as compression, scaling etc. However this framework requires additional pre-processing step when videos with different frame rates need to be compared, which can potentially limit its overall performance. Recently, Generalized Entropic Difference (GREED) VQA model was proposed to account for artifacts that arise due to changes in frame rate, and showed superior performance on the LIVE-YT-HFR database which contains frame rate dependent artifacts such as judder, strobing etc. In this paper we propose a simple extension, where the features from VMAF and GREED are fused in order to exploit the advantages of both models. We show through various experiments that the proposed fusion framework results in more efficient features for predicting frame rate dependent video quality. We also evaluate the fused feature set on standard non-HFR VQA databases and obtain superior performance than both GREED and VMAF, indicating the combined feature set captures complimentary perceptual quality information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-The popularity of streaming videos with live, highaction content has led to an increased interest in High Frame Rate (HFR) videos. In this work we address the problem of frame rate dependent Video Quality Assessment (VQA) when the videos to be compared have different frame rate and compression factor. The current VQA models such as VMAF have superior correlation with perceptual judgments when videos to be compared have same frame rates and contain conventional distortions such as compression, scaling etc. However this framework requires additional pre-processing step when videos with different frame rates need to be compared, which can potentially limit its overall performance. Recently, Generalized Entropic Difference (GREED) VQA model was proposed to account for artifacts that arise due to changes in frame rate, and showed superior performance on the LIVE-YT-HFR database which contains frame rate dependent artifacts such as judder, strobing etc. In this paper we propose a simple extension, where the features from VMAF and GREED are fused in order to exploit the advantages of both models. We show through various experiments that the proposed fusion framework results in more efficient features for predicting frame rate dependent video quality. We also evaluate the fused feature set on standard non-HFR VQA databases and obtain superior performance than both GREED and VMAF, indicating the combined feature set captures complimentary perceptual quality information.</p><p>Index Terms-high frame rate, video quality assessment, full reference, entropy, generalized Gaussian distribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The explosion of internet usage has resulted in videos becoming more mainstream and forming an important component in day-to-day lives. With videos commanding a significant share in internet traffic, considerable effort has been made in improving the video characteristics in terms of dynamic range, spatial resolutions (4K/8K), immersive content (Virtual/Augmented Reality) and extending temporal resolutions by employing higher frame rate (HFR) videos. However, there has been much less emphasis placed on increasing frame rates, with many existing display devices and content compatible only upto 60 frames per second (fps).</p><p>With the recent surge in streaming content pertaining to sports, high motion and live action, there has been a renewed interest in HFR videos. It is a common belief that using more number of frames results in smoother variation of motion content, thus higher frame rates results in better quality videos. However, there have been very few works which comprehensively evaluate these notions. Existing legacy Video Quality Assessment (VQA) databases such as LIVE-VQA <ref type="bibr" target="#b0">[1]</ref>, CSIQ-VQA <ref type="bibr" target="#b1">[2]</ref> only contain videos which have reference and distorted versions at the same frame rate. This restricts the analysis of distortions that arise due to frame rate changes. Recent publications of HFR-VQA databases such as Waterloo-HFR <ref type="bibr" target="#b2">[3]</ref>, BVI-HFR <ref type="bibr" target="#b3">[4]</ref> and LIVE-YT-HFR <ref type="bibr" target="#b4">[5]</ref> has rekindled interest in the problem of HFR-VQA.</p><p>Perceptual VQA models are an essential component of many video streaming applications such as YouTube, Netflix, Hulu etc. where video quality is objectively evaluated at a large scale. VQA models can be categorized into three divisions: Full Reference (FR) models which have access to pristine high quality reference <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>, Reduced Reference (RR) models which use limited reference information <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and No Reference (NR) models which use only the distorted version for quality prediction <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. In this paper we consider the problem of VQA when reference and distorted sequences can possibly have different frame rates, thus it falls under the category of FR and RR VQA methods.</p><p>Prior works addressing the problem of VQA in HFR domain are very limited. In <ref type="bibr" target="#b15">[16]</ref> a motion smoothness measure is introduced to quantify quality changes that occur due to frame rate variations. Zhang et al. <ref type="bibr" target="#b16">[17]</ref> proposed Frame Rate Quality Metric (FRQM) which uses differences between the wavelet coefficients of reference and distorted videos to measure quality, and achieved competitive performance on the BVI-HFR dataset. Although the above methods account for artifacts arising due to frame rate changes, their performance deteriorates in the presence of compression, thus limiting their applicability. In <ref type="bibr" target="#b17">[18]</ref> a model based on entropic differences in spatial and temporal band-pass domain was proposed to capture frame rate artifacts in the presence of compression, and achieved superior performance on the LIVE-YT-HFR database.</p><p>In this work we propose a simple fusion framework to combine features from VMAF <ref type="bibr" target="#b18">[19]</ref> and GREED <ref type="bibr" target="#b17">[18]</ref> models. This principle is motivated from <ref type="bibr" target="#b19">[20]</ref> where similar idea was presented to combine VMAF features with SpEED <ref type="bibr" target="#b20">[21]</ref> VQA model. VMAF has superior correlation with subjective judgments on existing FR-VQA databases. However VMAF performance is limited when used with videos of different frame rates. Since these videos contain dominant temporal artifacts, lower VMAF performance indicates that the temporal component in VMAF is insufficient to capture frame rate artifacts. On the other hand, GREED achieves good performance when used with videos of different frame rates, suggesting the superiority of temporal features present in GREED. Thus Copyright ? 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works fusing these features can result in a robust model that can perform well on both HFR and non-HFR VQA databases. The rest of the paper is organized as follows. In Section II we provide a brief background about GREED and VMAF, and discuss the proposed feature integration procedure. In Section III we report and analyze various experimental results, and provide some concluding remarks in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GREED Description</head><p>A brief description of GREED <ref type="bibr" target="#b17">[18]</ref> model and its components is provided in this section. GREED model is motivated from the statistical deviations observed between the distributions of band-pass coefficients of different frame rates. The video V (x, t) (x = (x, y) is subjected to temporal bandpass filtering by a bank of K 1D filters denoted by b k for</p><formula xml:id="formula_0">k ? {1, . . . K} B k (x, t) = V (x, t) * b k (t) ?k ? {1, . . . K},<label>(1)</label></formula><p>where B k denotes band-pass response of k th filter. The coefficients of B k show heavy tailed distribution which can be well modeled using Generalized Gaussian Distribution (GGD) which is given by</p><formula xml:id="formula_1">f (x; ?, ?) = ? 2??(1/?) exp ? |x| ? ? where ?(.)</formula><p>is the gamma function:</p><formula xml:id="formula_2">?(a) = ? 0 x a?1 e ?x dx.</formula><p>Here ? and ? are parameters of GGD. The band-pass coefficients are partitioned into non-overlapping patches of size ? M ? ? M , which are indexed by p ? {1, 2, . . . P }. Let B kpt denote the vector of band-pass coefficients at frame t, patch p and band-pass filter k. The band-pass coefficients B kpt are allowed to pass through a Gaussian channel to model perceptual imperfections such as neural noise <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref> resulting in a noisy versionB kpt which can also be approximately modeled using a GGD. The main idea here is to compare the entropy h(B kpt of reference and distorted versions for evaluating quality. The expression for GGD entropy of random variable X ? GGD(0, ?, ?) is given by:</p><formula xml:id="formula_3">h(X) = 1 ? ? log ? 2??(1/?) .<label>(2)</label></formula><p>h(B kpt ) is calculated using (2) for each patch by estimating parameters ? and ? using kurtosis matching procedure detailed in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>Temporal GREED: To make entropies more local and provide numerical stability to regions with small variances, h(B kpt ) is premultiplied by a scaling factor given as:</p><formula xml:id="formula_4">kpt = [log(1 + ? 2 (B kpt ))]h(B kpt )</formula><p>Subsampling a signal as well as compression leads to entropy modification. However the entropy change due to frame rate dominates that occurring due to compression as can be observed from <ref type="figure">Fig. 1</ref> where entropy values remain nearly same regardless of compression factor. This is referred to as frame rate bias of entropy and this makes entropy difference inadequate to account for compression distortions. To overcome this, reference video R is temporally downsampled to have the same frame rate of distorted video D resulting in another sequence called Pseudo Reference P R. Given these signals, Temporal GREED is defined as</p><formula xml:id="formula_5">TGREED kt = 1 P P p=1 1 + | D kpt ? P R kpt | R kpt + 1 P R kpt + 1 ? 1 (3)</formula><p>TGREED can be interpreted as a weighted entropic difference, where the ratio term which is dependent on frame rate of R and D acts as a weighting factor. From <ref type="figure">Fig. 1</ref> it can be observed that the entropy separation among various frame rates is dependent on the content. This makes the ratio term sensitive to content. The absolute difference term captures compression artifacts since P R is lossless version of D at the same frame rate.</p><p>Spatial GREED: : For capturing spatial distortions, each frame is subjected to spatial band-pass filtering by local Mean Subtracted (MS) filtering scheme similar to <ref type="bibr" target="#b20">[21]</ref>. Similar to temporal band-pass coefficients, MS coefficients also can be modeled using GGD and scaled entropy can be calculated as</p><formula xml:id="formula_6">? pt = [log(1 + ? 2 (? M S pt ))]h(? M S pt )</formula><p>where? M S pt denotes MS coefficients of frame t and patch p passed through neural noise. There does not exist any frame rate bias since spatial entropies were calculated using only single frames and are frame rate agnostic. Spatial GREED (SGREED) is defined as :</p><formula xml:id="formula_7">SGREED t = 1 P P p=1 |? D pt ? ? R pt |.<label>(4)</label></formula><p>B. VMAF description VMAF uses multiple elementary quality indices and fuses them using a Support Vector Regressor (SVR) <ref type="bibr" target="#b18">[19]</ref>. The first step in VMAF is extracting the following features : DLM <ref type="bibr" target="#b23">[24]</ref>, VIF <ref type="bibr" target="#b7">[8]</ref> and absolute luma differences between consecutive frames (Temporal Information -TI). The DLM feature captures detail losses while VIF measures image fidelity based on Calculate SGREED from <ref type="formula" target="#formula_7">(4)</ref> 7:</p><p>for each b k do 8:</p><p>Calculate TGREED k from <ref type="formula">(3)</ref> 9: end for 10: end for 11: Concatenate DLM, VIF, SGREED, TGREED to obtain GREED-VMAF features.</p><p>Natural Scene Statistics (NSS). VIF is computed at four scales resulting four VIF features. TI aims to capture the temporal artifacts which are quantified through luma differences. This results in VMAF containing six features overall. TI is the only feature in VMAF that accounts for temporal quality evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Integration</head><p>Although GREED achieves high correlations against human judgements on LIVE-YT-HFR database, the performance is sensitive to the differences between the frame rates of reference and distorted sequences, where lower frame rate videos have lower correlations when compared to high frame rate videos. This behavior can be observed in <ref type="table" target="#tab_3">Table II</ref>. Another shortcoming of GREED is that it underperforms in the presence of other types of distortions apart from frame rate and compression artifacts. This is evidenced in <ref type="table" target="#tab_3">Table III</ref> where GREED is evaluated on databases such as LIVE-VQA <ref type="bibr" target="#b0">[1]</ref>, CSIQ-VQA <ref type="bibr" target="#b1">[2]</ref> etc. VMAF also has a drawback in terms of capturing temporal distortions as TI feature is a basic measure and complex temporal artifacts such as ghosting, flickering etc. are not effectively represented by TI.</p><p>To overcome these limitations, we propose to combine the features from both GREED and VMAF to exploit the advantages offered by both models. Since GREED achieves superior performance for temporal artifacts, we replace the TI feature in VMAF by all GREED features. We retain the 5 spatial VMAF features (DLM and VIF from 4 scales) as they sufficiently capture spatial quality. GREED features are computed at 2 scales s = 4, 5 where entropic differences are computed after downscaling both spatial dimensions by 2 s times. For calculating TGREED Biorthogonal-2.2 bandpass filter with 3 levels of wavelet packet decomposition <ref type="bibr" target="#b24">[25]</ref> was used. Since low-pass subband is ignored, this results in 7 TGREED features for each scale from respective temporal subbands and a total of 14 TGREED features from both scales. Although spatial components of VMAF account for spatial distortions, including SGREED features was experimentally observed to further improve performance. Thus the proposed model GREED-VMAF consists a total of 21 features alto-  <ref type="bibr" target="#b5">[6]</ref> 0.5566 0.4042 0.5418 9.99 MS-SSIM <ref type="bibr" target="#b6">[7]</ref> 0.5742 0.4135 0.5512 10.01 ST-RRED <ref type="bibr" target="#b11">[12]</ref> 0.6394 0.4516 0.6073 9.58 SpEED <ref type="bibr" target="#b20">[21]</ref> 0.6051 0.4437 0.5206 10.28 VMAF <ref type="bibr" target="#b18">[19]</ref> 0.7782 0.5918 0.7419 8.10 GREED 0.8822 0.7046 0.8869 5.48 GREED-VMAF 0.8658 0.6840 0.8723 <ref type="bibr">5.89</ref> gether. The entire feature extraction procedure for GREED-VMAF is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Compared Methods : We choose 3 FR-IQA methods: PSNR, SSIM <ref type="bibr" target="#b5">[6]</ref> and MS-SSIM <ref type="bibr" target="#b6">[7]</ref>, and 3 FR-VQA models: ST-RRED <ref type="bibr" target="#b11">[12]</ref>, SpEED <ref type="bibr" target="#b20">[21]</ref> and VMAF <ref type="bibr" target="#b18">[19]</ref> for performance comparison. The above mentioned models require reference and distorted versions to have same frame rates, thus for the cases where the frame rates were different the distorted video was temporally upsampled to match the frame rate of reference video by frame duplication. Although the frame rates can be matched by temporally downsampling the reference video as well, we avoid this option as it can potentially introduce temporal artifacts in the reference video which is not desirable.</p><p>Evaluation Criteria : VQA models are compared in terms Spearman's rank order correlation coefficient (SROCC), Kendall's rank order correlation coefficient (KROCC), Pearson's linear correlation coefficient (PLCC), and root mean squared error (RMSE). PLCC and RMSE are calculated after passing the predicted scores through a four-parameter logistic non-linearity as described in <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_8">Q(x) = ? 2 + ? 1 ? ? 2 1 + exp ? x??3 |?4| .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Analysis on LIVE-YT-HFR Dataset</head><p>Since GREED-VMAF is a training based model, the LIVE-YT-HFR database is divided into 70%, 15% and 15% subsets corresponding to training, validation and testing respectively. The validation subset was used to determine hyperparameters of SVR. Additionally we ensured that there was no content overlap between the subsets. To avoid any performance bias towards the choice of training set, the experiment was repeated 200 times with random train-test splits and median performance is reported. Since only the spatial features from VMAF are employed in GREED-VMAF, and since VMAF framework requires the compared videos to have same frame rate, we match the frame rate by temporally subsampling the   <ref type="bibr" target="#b5">[6]</ref> 0.802 0.798 0.705 MS-SSIM <ref type="bibr" target="#b6">[7]</ref> 0.830 0.800 0.757 ST-RRED <ref type="bibr" target="#b11">[12]</ref> 0.826 0.882 0.813 SpEED <ref type="bibr" target="#b20">[21]</ref> 0.801 0.886 0.743 VMAF <ref type="bibr" target="#b18">[19]</ref> 0.794 0.897 0.618 GREED 0.750 0.863 0.780 GREED-VMAF 0.784 0.904 0.907 reference video (PR video) instead of temporally upsampling distorted version as we observed the spatial artifacts were better captured by employing this design. SVR with linear kernel was used to learn mapping from features to quality scores.</p><p>In <ref type="table" target="#tab_3">Table I</ref> the performance of GREED-VMAF against other models is compared. GREED and GREED-VMAF outperform other models by a large margin. In <ref type="table" target="#tab_3">Table II</ref> the LIVE-YT-HFR database is subdivided into sets containing videos of same frame rate, and the performance is individually analyzed on each frame rate. Here as well GREED and GREED-VMAF have superior correlation when compared to other models across each frame rate. Although we observe a small drop in performance of GREED-VMAF in <ref type="table" target="#tab_3">Table I</ref>, its performance is superior to GREED for individual frame rates as shown in <ref type="table" target="#tab_3">Table II</ref>, particularly in low frame rate sets such as 24 and 30 fps. This makes performance of GREED-VMAF be less sensitive to the choice of distorted frame rate and provide more stable quality predictions than GREED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison on other VQA Databases</head><p>In this subsection we evaluate the generalizability of GREED-VMAF features by comparing its performance on three popular VQA databases LIVE-VQA <ref type="bibr" target="#b0">[1]</ref>, LIVE-mobile <ref type="bibr" target="#b26">[27]</ref> and CSIQ-VQA <ref type="bibr" target="#b1">[2]</ref>. These databases contain both the reference and distorted videos at the same frame rate, thus the TGREED term in expression (3) will only depend on the absolute difference of scaled entropies since the ratio term simplifies to one. We divide the database into sets of 80% and 20% for training and testing respectively. We also ensured that there does not exist any overlap of contents between training and testing subsets. Further, to avoid any performance bias towards the choice of train-test contents, we repeat the above procedure for all possible train-test combinations, and the median SROCC performance is reported in <ref type="table" target="#tab_3">Table III</ref>. For these datasets, SVR with radial basis function (RBF) kernel was used to learn mapping from features to quality scores. From the Table it maybe observed that GREED-VMAF outperforms GREED across all databases indicating that the combined set of features are more generalizable than GREED. Notably GREED-VMAF outperforms VMAF and achieves top performance among the compared models on LIVE-mobile and CSIQ-VQA datasets. Since GREED-VMAF performance is higher than both GREED and VMAF, it can be inferred that the respective individual features capture complementary perceptual quality information, and thus fusing them leads to better correlation with subjective judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>We proposed a simple extension to existing VQA models GREED and VMAF by combining the spatial features of VMAF with spatio-temporal features of GREED. This models exploits the advantages of both the models: can be used when reference and distorted videos have different frame rates, with no additional temporal pre-processing and can effectively capture other artifacts apart from frame rate and compression artifacts. We conducted a holistic evaluation of the proposed model on LIVE-YT-HFR database and observed that GREED-VMAF provides better correlation with perceptual judgments when videos of fixed frame rates were analyzed. We also evaluated the generalizability of GREED-VMAF on multiple fixed frame rate VQA databases and observed to have superior performance when compared to both GREED and VMAF. As part of future work we wish to further evaluate the generalizability of GREED-VMAF on additional VQA datasets such as NFLX, EPFL-Polimi etc. which is not reported in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pavan C Madhusudana 1 , Neil Birkbeck 2 , Yilin Wang 2 , Balu Adsumilli 2 , Alan C. Bovik 1 1 The University of Texas at Austin, TX, USA. 2 Google, Mountain View, CA, USA E-mail : pavancm@utexas.edu, {birkbeck, yilin, badsumilli}@google.com, bovik@ece.utexas.edu</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 GREED-VMAF Algorithm Input: reference video R, distorted video D Output: GREED-VMAF features 1: Temporal subsample R to get P R 2: Calculate DLM(PR,D) 3: Calculate VIF(PR,D) -4 spatial scales 4: Scale S = {4, 5}, band-pass filter bank b k : k ? {1, . . . 7} 5: for each s ? S do</figDesc><table /><note>6:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON OF GREED AGAINST DIFFERENT FR ALGORITHMS ON THE LIVE-YT-HFR DATABASE. THE REPORTED NUMBERS ARE MEDIAN VALUES FROM 200 ITERATIONS OF RANDOMLY CHOSEN TRAIN-TEST SETS. IN EACH COLUMN FIRST AND SECOND BEST MODELS ARE BOLDFACED.</figDesc><table><row><cell></cell><cell>SROCC ?</cell><cell>KROCC ?</cell><cell>PLCC ?</cell><cell>RMSE ?</cell></row><row><cell>PSNR</cell><cell>0.7802</cell><cell>0.5934</cell><cell>0.7481</cell><cell>7.75</cell></row><row><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON OF GREED AGAINST VARIOUS FR METHODS FOR INDIVIDUAL FRAME RATES ON THE LIVE-YT-HFR DATABASE. THE REPORTED NUMBERS ARE MEDIAN VALUES OVER 200 ITERATIONS OF RANDOMLY CHOSEN TRAIN-TEST SETS. IN EACH COLUMN THE BEST AND SECOND BEST VALUES ARE MARKED IN BOLDFACE.</figDesc><table><row><cell></cell><cell cols="2">24 fps</cell><cell cols="2">30 fps</cell><cell cols="2">60 fps</cell><cell cols="2">82 fps</cell><cell cols="2">98 fps</cell><cell cols="2">120 fps</cell><cell cols="2">Overall</cell></row><row><cell></cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell></row><row><cell>PSNR</cell><cell>0.541</cell><cell>0.487</cell><cell>0.564</cell><cell>0.535</cell><cell>0.753</cell><cell>0.694</cell><cell>0.771</cell><cell>0.771</cell><cell>0.821</cell><cell>0.783</cell><cell>0.741</cell><cell>0.736</cell><cell>0.780</cell><cell>0.748</cell></row><row><cell>SSIM [6]</cell><cell>0.266</cell><cell>0.222</cell><cell>0.283</cell><cell>0.189</cell><cell>0.382</cell><cell>0.302</cell><cell>0.371</cell><cell>0.362</cell><cell>0.537</cell><cell>0.497</cell><cell>0.867</cell><cell>0.833</cell><cell>0.556</cell><cell>0.541</cell></row><row><cell>MS-SSIM [7]</cell><cell>0.305</cell><cell>0.260</cell><cell>0.296</cell><cell>0.238</cell><cell>0.416</cell><cell>0.338</cell><cell>0.439</cell><cell>0.393</cell><cell>0.578</cell><cell>0.561</cell><cell>0.706</cell><cell>0.696</cell><cell>0.574</cell><cell>0.551</cell></row><row><cell>ST-RRED [12]</cell><cell>0.305</cell><cell>0.275</cell><cell>0.296</cell><cell>0.206</cell><cell>0.612</cell><cell>0.613</cell><cell>0.584</cell><cell>0.513</cell><cell>0.650</cell><cell>0.604</cell><cell>0.755</cell><cell>0.696</cell><cell>0.639</cell><cell>0.607</cell></row><row><cell>SpEED [21]</cell><cell>0.432</cell><cell>0.273</cell><cell>0.410</cell><cell>0.233</cell><cell>0.439</cell><cell>0.292</cell><cell>0.546</cell><cell>0.390</cell><cell>0.578</cell><cell>0.471</cell><cell>0.758</cell><cell>0.739</cell><cell>0.605</cell><cell>0.520</cell></row><row><cell>VMAF [19]</cell><cell>0.250</cell><cell>0.368</cell><cell>0.362</cell><cell>0.471</cell><cell>0.630</cell><cell>0.680</cell><cell>0.734</cell><cell>0.793</cell><cell>0.860</cell><cell>0.868</cell><cell>0.818</cell><cell>0.816</cell><cell>0.779</cell><cell>0.742</cell></row><row><cell>GREED</cell><cell>0.727</cell><cell>0.822</cell><cell>0.702</cell><cell>0.843</cell><cell>0.732</cell><cell>0.840</cell><cell>0.818</cell><cell>0.896</cell><cell>0.864</cell><cell>0.891</cell><cell>0.888</cell><cell>0.895</cell><cell>0.882</cell><cell>0.887</cell></row><row><cell>GREED-VMAF</cell><cell>0.748</cell><cell>0.805</cell><cell>0.743</cell><cell>0.833</cell><cell>0.773</cell><cell>0.836</cell><cell>0.786</cell><cell>0.880</cell><cell>0.860</cell><cell>0.899</cell><cell>0.881</cell><cell>0.903</cell><cell>0.865</cell><cell>0.872</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III SROCC</head><label>III</label><figDesc>PERFORMANCE COMPARISON ON MULTIPLE VQA DATABASES. THE REPORTED NUMBERS ARE MEDIAN VALUES FROM EVERY POSSIBLE COMBINATION OF TRAIN-TEST SPLITS WITH 80% OF CONTENT USED FOR TRAINING. IN EACH COLUMN THE BEST AND SECOND BEST MODELS ARE</figDesc><table><row><cell></cell><cell cols="2">BOLDFACED.</cell><cell></cell></row><row><cell></cell><cell>LIVE-VQA</cell><cell>LIVE-mobile</cell><cell>CSIQ-VQA</cell></row><row><cell>PSNR</cell><cell>0.711</cell><cell>0.788</cell><cell>0.579</cell></row><row><cell>SSIM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vis3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13016</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of high frame rate video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study of high frame rate video formats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1499" to="1512" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Subjective and objective quality assessment of high frame rate videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11634</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conf. Signals Syst</title>
		<imprint>
			<date type="published" when="2003-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RRED indices: Reduced reference entropic differencing for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="517" to="526" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video quality assessment by reduced reference spatio-temporal entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="694" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Process. Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal motion smoothness and the impact of frame rate variation on video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A frame rate dependent video quality metric based on temporal wavelet decomposition and spatiotemporal pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="300" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">St-greed: Space-time generalized entropic differences for frame rate dependent video quality prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Toward a practical perceptual video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manohara</surname></persName>
		</author>
		<ptr target="http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatiotemporal feature integration and model fusion for full reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2256" to="2270" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SpEED-QA: Spatial efficient entropic differencing for image and video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1333" to="1337" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New results on the sum of two generalized Gaussian random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Alouini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conf. on Signal and Information Process</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exposing image splicing with inconsistent local noise variances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image quality assessment by separately evaluating detail losses and additive impairments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="935" to="949" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entropy-based algorithms for best basis selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Wickerhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="713" to="718" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective quality metrics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video quality assessment on mobile devices: Subjective, behavioral and objective studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">De</forename><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="652" to="671" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
