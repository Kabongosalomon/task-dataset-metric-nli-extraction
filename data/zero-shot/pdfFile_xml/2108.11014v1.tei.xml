<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iDARTS: Improving DARTS by Node Normalization and Decorrelation Discretization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Di</forename><forename type="middle">Huang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">iDARTS: Improving DARTS by Node Normalization and Decorrelation Discretization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Neural Architecture Search</term>
					<term>Differentiable Architecture Search</term>
					<term>AutoML</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable ARchiTecture Search (DARTS) uses a continuous relaxation of network representation and dramatically accelerates Neural Architecture Search (NAS) by almost thousands of times in GPU-day. However, the searching process of DARTS is unstable, which suffers severe degradation when training epochs become large, thus limiting its application. In this paper, we claim that this degradation issue is caused by the imbalanced norms between different nodes and the highly correlated outputs from various operations. We then propose an improved version of DARTS, namely iDARTS, to deal with the two problems. In the training phase, it introduces node normalization to maintain the norm balance. In the discretization phase, the continuous architecture is approximated based on the similarity between the outputs of the node and the decorrelated operations rather than the values of the architecture parameters. Extensive evaluation is conducted on CIFAR-10 and ImageNet, and the error rates of 2.25% and 24.7% are reported within 0.2 and 1.9 GPU-day for architecture search respectively, which shows its effectiveness. Additional analysis also reveals that iDARTS has the advantage in robustness and generalization over other DARTS-based counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Despite the great success of neural networks in a large number of areas, the design of neural architectures is still a tedious task which needs rich experience and repeated adjustment of human experts. Many efforts have been made to automate this Neural Architecture Search (NAS) process. Some studies <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> formulate it as a Reinforcement Learning (RL) problem. The agents build neural architectures based on specific search spaces, and their rewards are estimated in terms of corresponding accuracies. Some methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b10">[11]</ref> use Evolutionary Algorithms (EA) to search effective solutions and form the architectures by mutation and recombination of different populations. There are also some attempts to explore improvements with Sequential Model-based Optimization (SMBO) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, Bayesian optimization <ref type="bibr" target="#b13">[14]</ref>, and Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b14">[15]</ref>. These methods deliver better models than the handcrafted ones; however, they demand a huge amount of computational cost.</p><p>Recently, one-shot methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> have been investigated for fast NAS, where a single neural network is trained during the searching process, and the final architecture is derived as the solution to the specific optimization task. Differential ARchitecture Search (DARTS) <ref type="bibr" target="#b19">[20]</ref> is a particularly popular instance and it relaxes the search space to be continuous by introducing architecture parameters into the gradient descent procedure. DARTS has two main phases:</p><p>(1) in training, it simultaneously optimizes the architecture parameters and model weights, and (2) in discretization, it approximates the neural architecture according to the values of the architecture parameters. Benefiting from the relaxation on the discrete search space, DARTS reports comparable performance at a much higher speed. Unfortunately, it suffers severe degradation, i.e., when the number of searching epochs becomes larger or the search space changes <ref type="bibr" target="#b20">[21]</ref>, the skipconnect operation tends to dominate the final architecture. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, DARTS finds a sound architecture by the first 50 epochs in the original search space, but converges to a poor one (only skip-connects) with a dramatic error increase as the epoch number reaches 200 or in different search spaces.</p><p>To address the issue aforementioned, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> adopt early stop to terminate searching before degradation occurs. <ref type="bibr" target="#b23">[24]</ref> amends the approximation in updating architecture parameters by gradient descent. <ref type="bibr" target="#b24">[25]</ref> manually intervenes the architecture based on a set of pre-defined rules. Such strategies do alleviate the degradation issue to some extent, but they are criticized for obvious drawbacks. Early stop and manual intervention bring in more hyper-parameters, and it still remains an open question to achieve appropriate parameters, making them not easy to be generalized to other scenarios (e.g. different search spaces or tasks). In addition, due to the estimation of second-order gradients, approximation amendment incurs high computational cost and unexpected model instability, which impedes its further application.</p><p>In this paper, we claim that the degradation of DARTS is caused by the mechanisms in training and discretization. For the former, the norms of different nodes are usually not of the same scale, and this imbalance tends to highlight the importance of the nodes linked to the input. DARTS thus builds more skip-connects and converges to a shallow architecture. For the latter, as connections in the auxiliary one-shot model are highly redundant, the outputs of these operations are usually correlated. In this case, it is probable to select the operations that generate redundant feature maps but with larger architecture parameters. Besides, some operations with fewer parameters than convolutions, e.g., skip-connects and poolings, are prone to gain larger architecture weights in the gradient backward process <ref type="bibr" target="#b24">[25]</ref>. Both the facts lead to shallow architectures full of skip-connects.</p><p>Then, we propose an improved version of DARTS, namely iDARTS, which introduces node normalization to maintain the balance between norms of different nodes in training and discretizes the continuous architecture based on the similarity between the outputs of the cell and the decorrelated operations rather than the values of the architecture parameters. We conduct extensive evaluation on two popular benchmarks and achieve the competitive top-1 test errors of 2.25% and 24.7% with the searching time at 0.2 and 1.9 GPU-day on CIFAR-10 and ImageNet, respectively. Furthermore, we show the advantage of iDARTS in robustness and generalization for various search spaces and large training epochs by its superior performance in comparison with the major DARTS-based counterparts. The remainder of the paper is organized as follows. Section II introduces the methods related to NAS, in particular DARTS-based ones. In Section III, we discuss the limitations of the original DARTS and present the solutions to them in detail. The experimental results are described and analyzed in Section IV. Finally, we conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review the major NAS methods in the literature.</p><p>RL based methods represent the network architecture as a variable-length string or a direct acyclic graph. NAS is then regarded as a process of sequential decision where an agent is made to learn how to design the neural architecture. Various agent policies and reward functions are employed to achieve performance gains. NAS-RL <ref type="bibr" target="#b1">[2]</ref> attempts to model an RNN-based controller whose predictions are considered as actions used to construct the neural architecture. NASNet <ref type="bibr" target="#b3">[4]</ref> adopts the proxy dataset and a carefully designed search space to reduce the computational cost. MetaQNN <ref type="bibr" target="#b0">[1]</ref> and Block-QNN <ref type="bibr" target="#b2">[3]</ref> both apply the Q-learning paradigm with the epsilon-greedy exploration strategy while the latter chooses to construct the network in a block-wise manner. ENAS <ref type="bibr" target="#b25">[26]</ref> proposes a parameter sharing strategy among candidate architectures to accelerate the searching process.</p><p>On the other hand, some studies try to build neural networks based on EA to avoid human intervention as much as possible. Large-scale Evolution <ref type="bibr" target="#b4">[5]</ref> initializes a large population based on the simplest network structure, and the operations such as reproduction, mutation and selection are designed to obtain the best architecture. GeNet <ref type="bibr" target="#b5">[6]</ref> encodes the network architecture to a fixed-length binary string rather than the graph-based forms. During iteration, the modified evolutionary operations are conducted on those binary strings to generate new individuals, where the most competitive one is taken as the final result. Hierarchical-EAS <ref type="bibr" target="#b7">[8]</ref> presents a hierarchical genetic representation to imitate the human design procedure. Separable convolutions are involved to reduce the spatial resolution, keeping the structure consistency between normal and reduction cells. AmoebaNet <ref type="bibr" target="#b10">[11]</ref> introduces an age property in EA to favor younger architectures and simplifies mutations in the search space of NASNet, reaching comparable performance to RL-based methods with lower computational cost.</p><p>Other optimization techniques are also exploited for this issue. PNAS <ref type="bibr" target="#b12">[13]</ref> progressively searches for the complex block architecture based on the SMBO algorithm, while DPP-Net <ref type="bibr" target="#b11">[12]</ref> considers the QoS (Quality of Service) and hardware requirments in the objective function. NASBOT <ref type="bibr" target="#b13">[14]</ref> develops a distance metric via optimal transport and adopts Gaussian process based Bayesian Optimal (BO) for architecture search. DeepArchitect <ref type="bibr" target="#b14">[15]</ref> designs a tree-structured search space and traverses it by MCTS. Despite promising results, such solutions bear quite high computational cost (e.g. thousands of GPU-day to learn a classification model on CIFAR-10).</p><p>Recently, one-shot methods have received increasing attention, aiming to reduce calculation and accelerate searching. SNAS <ref type="bibr" target="#b16">[17]</ref>, DSNAS <ref type="bibr" target="#b26">[27]</ref> and PVLL-NAS <ref type="bibr" target="#b27">[28]</ref> apply reparameterization tricks to train neural operation and architecture distribution in the same round of back propagation. SMASH <ref type="bibr" target="#b15">[16]</ref> introduces HyperNet <ref type="bibr" target="#b28">[29]</ref> to generate weights for candidate architectures rather than training them from scratch, reaching a faster evaluating speed. DAS <ref type="bibr" target="#b17">[18]</ref> starts to relax the discrete neural network architecture to a contiguously differentiable form, searching for the best hyperparameters of convolution layers as well as the weights. MaskConnect <ref type="bibr" target="#b18">[19]</ref> explores the possibility to directly optimize the connectivity of modules with a modified version of gradient descent.</p><p>By combining contiguously differentiable architecture parameters and flexible connections, DARTS <ref type="bibr" target="#b19">[20]</ref> reduces the searching cost by a large margin. It introduces concrete architecture parameters to represent the dense connections within the blocks, and the architecture parameters and model weights can thus be optimized simultaneously. Following the gradient based paradigm of DARTS, a number of investigations advance it to better fit more practical situations. P-DARTS <ref type="bibr" target="#b24">[25]</ref> progressively narrows down the gap between the search space and the target space. PC-DARTS <ref type="bibr" target="#b29">[30]</ref> increases the efficiency of searching by sampling a small part of the super-net to reduce the redundancy. GDAS <ref type="bibr" target="#b30">[31]</ref> presents a differentiable architecture sampler to produce subgraphs in the direct acyclic graph during training and only the sampled subgraph is optimized in iteration, thus alleviating the searching cost. Gold-NAS <ref type="bibr" target="#b31">[32]</ref> and Proxyless-NAS <ref type="bibr" target="#b32">[33]</ref> take the resource constraints into consideration to deliver a better balance between computational cost and model accuracy.</p><p>Even though those methods make large progress to optimize DARTS, a severe problem remains unsolved. DARTS encounters a degradation issue that the model is not stable when searching epochs become larger or the search space changes, prone to converge to a shallow architecture full of skipconnect operations. <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> decide to terminate searching in advance based on the eigenvalues or the composition of the architecture. P-DARTS <ref type="bibr" target="#b24">[25]</ref> and Amended-DARTS <ref type="bibr" target="#b23">[24]</ref> intervene the gradients or the architectures according to predefined rules. However, the degradation issue is only alleviated rather than eliminated, leaving much room for improvement.</p><p>In this paper, we claim that the degradation issue comes from both the training and discretization phases, and the proposed iDARTS deals with it by node normalization and decorrelation discretization respectively. Thanks to these two strategies, we significantly ameliorate the accuracy and robust-ness of DARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>In DARTS, the architecture search is performed on a supernet. As shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, the super-net consists of a set of a pre-defined number of layers. Each layer has a normal cell for feature encoding or a reduction cell for feature downsampling. There are several nodes in a single cell, representing the mid-layer results. All these nodes are densely connected as shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. Each cell outputs the concatenation of the nodes. The edge between each pair of nodes is a mix of a set of candidate neural operators in the search space, i.e., skip-connects, poolings, or convolutions. The output of the edge between node pair (i, j) is treated as a softmax over all possible operations, which can be written as:</p><formula xml:id="formula_0">o (i,j) (x) = o?O exp(? (i,j) o ) o ?O exp(? (i,j) o ) o(x)<label>(1)</label></formula><p>where o denotes the operations in operation set O and ?</p><formula xml:id="formula_1">(i,j) o</formula><p>represents the architecture parameter for node pair (i, j) corresponding to operation o. The output of node j is the summarization of those of all its connected edges, written as:</p><formula xml:id="formula_2">y (j) (x) = i&lt;j? (i,j) (x)<label>(2)</label></formula><p>The architecture parameter ? and the model weight w are simultaneously optimized by bi-level optimization. When the searching process finishes, w is discarded and the architecture is discretized according to ?. This process is conducted based on the hypothesis that the operator with a larger value of ?</p><formula xml:id="formula_3">(i,j) o</formula><p>is more important to the mixed output. DARTS preserves the operator with the maximal value of the architecture parameters for each edge (i, j). This strategy is widely adopted in the DARTS-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Problems in DARTS</head><p>We claim that there are two major problems of DARTS in its training and discretization respectively, which trigger the degradation issue. In the former phase, DARTS directly optimizes the architecture parameter without considering the balance between norms of different nodes, incurring incorrect parameter updating. In the latter phase, DARTS neglects the correlation of candidate operations and approximates the continuous architecture only based on the unstable architecture parameter, thus failing to achieve the ideal architecture.</p><p>Imbalanced Norms. Given mixed operation? (i,j) (x) on edge (i, j) with a set of operations {o|o ? O} and the corresponding architecture parameters ?</p><formula xml:id="formula_4">(i,j) o</formula><p>, the updating process of ? (i,j) o in the first-order term can be written as: where S o is the weight of operation o(x) after the softmax function. In this form, we can find when o(x) approaches to 0, the gradient of its corresponding architecture parameter has a higher volatility. When o(x) approaches to? (i,j) (x), its corresponding update is close to 0, making it harder to tune</p><formula xml:id="formula_5">?L valid ?? (i,j) o = ?L valid ?? (i,j) (x) ? ?? (i,j) (x) ?? (i,j) o =S o ?L valid ?? (i,j) (x) ? (o(x) ?? (i,j) (x))<label>(3)</label></formula><formula xml:id="formula_6">? (i,j) o</formula><p>. DARTS tries to fix the operation norms in the training phase by adding a static batch normalization, i.e., batch normalization without learnable affine parameters. However, the skip-connect operation is neglected. For an arbitrary input x with the shape of (B, C, W, H), the norm of outputs from the operations excluding skip-connects is a constant C which equals to ? B ? C ? W ? H since the static batch normalization rescales the outputs to a distribution with zeromean and unit-variance. In the meantime, skip-connects return the original norm of this arbitrary input. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (b), in the search process of DARTS, the norms of inputs on different edges are usually not in the same scale, and this imbalance is further aggravated with more iterations, making the norm of the output from each node approach to 0. In this case, the mid-layer nodes become invalid and the ones linked to the input are highlighted. Therefore more skip-connects are selected and a shallow architecture is established.</p><p>Correlated Operations. In discretization, DARTS searches for a few operations whose synthesis well approximates to the super-net output. Recall that static batch normalization (denoted as f ) is universally used in super-net. Each convolutionbased operation is followed by a static batch normalization. The norms of their outputs are a constant C which only depends on the feature map size, batch size, and feature channels. We denote the output z from mixed operation? after a static batch normalization as z = f (?). Here, we take the single operation selection as an example. When the architecture is discretized, the selected operation o i and the edge output z is represented as z = f (o i ). The distance between the original output z and the approximated output z is formulated as:</p><formula xml:id="formula_7">||z ? z || 2 = |z| 2 + |z | 2 ? 2|z| ? |z | ? cos? i = 2C 2 ? 2C 2 ? cos? i (4)</formula><p>where ? i denotes the vectorial angle between z and z . In discretization, we need to minimize the gap between the supernet z and the approximated one z . We then denote the normalization of? as? =? ?? where ?? is the variance of?. It is also a distribution with zeromean and unit-variance as o i . There is only the difference of constant times between? and?, and due to the property of batch normalization, we have z = f (?) = f (?) =? and z = f (o i ) = o i . ? i can thus be written as</p><formula xml:id="formula_8">? i =arccos( z ? z |z ||z| ) = arccos( o i ?? |o i ||?| ) = arccos( o i ?? ??C 2 ) =arccos( o i ? n j=1 ? j o j ??C 2 ) =arccos( ? i ? C 2 + n j=0,j =i ? j ? &lt; o i , o j &gt; ??C 2 ) =arccos( ? i + n j=0,j =i ? j ? cos(? (i,j) ) ?? ) (5)</formula><p>where &lt; ? &gt; represents inner product and ? (i,j) denotes the vectorial angle between o i and o j . Notice ?? is a constant if training ends. We can find that ? i is not consistent with ? i if the correlation exists among operations (cos(? (i,j) ) is a nonzero value). When the similarity of the selected operation and the mixed ones increases, the discrepancy between the outputs before and after discretization decreases.</p><p>Since the connections in the auxiliary one-shot model are highly redundant, the operation outputs are indeed correlated as in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref>. It increases the risk in selecting the operations that produce redundant feature maps but with larger values of architecture parameters. Besides, as stated in the previous studies <ref type="bibr" target="#b24">[25]</ref>, some operations i.e. skip-connects and poolings are prone to gain larger architecture values in the gradient backward process, because they have fewer parameters than convolutions. Therefore, DARTS usually delivers shallow architectures full of skip-connects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Solutions</head><p>According to the analysis above, we propose two effective solutions to deal with the problems of imbalanced norms and correlated outputs in training and discretization respectively.</p><p>Node Normalization. As stated in Section III-B, we know that imbalanced norms exist in DARTS, leading to unstable updating of architecture parameters. Since the norms of the outputs from the operators excluding skip-connects are a constant C, we can introduce additional normalization for skip-connects to ensure the norm consistency among operators.</p><p>As <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates, we can apply pre-normalization or postnormalization on skip-connect. Post-normalization is an intuitive solution, and it directly adds a static batch normalization for c = 1 to C do <ref type="bibr">4:</ref> for n = 1 to N do <ref type="bibr">5:</ref> Calculate node output y</p><formula xml:id="formula_9">(n) c = i&lt;n? (i,n) in I val 6:</formula><p>Orthogonalize y Calculate the mean cosine similarity over all the cells as?</p><formula xml:id="formula_10">(n) k,j = 1 C C c=1 ? (n) c,j 11:</formula><p>Select the j (n) k -th operator in node n according to j</p><formula xml:id="formula_11">(n) k = argmax{j :? (n)</formula><p>k,j } and add it into S 12: end for 13: Return S.</p><p>after each skip-connect operation. For input x, its output is normalized tox with a constant norm C. However, it breaks the consistency between the output of skip-connects and the input of other operations on the same edge, which contravenes the design of skip-connects blocks.</p><p>The alternative is pre-normalization. When the summarization is finished and x is reached in each node, we add a static batch normalization to x. After that, the subsequent nodes take the normalizedx as input and the outputs of those operations including skip-connects are constrained to the same scale without extra computation burden. Finally, we employ prenormalization as our node normalization strategy in practice. It significantly improves the stability of the parameter updating  process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Normalization</head><p>Decorrelation Discretization. As in Section III-B, we clarify that there exist correlations among the operators. The value of ? cannot fully reflect the importance in such a situation. Therefore, the correlation among those operators should be taken into consideration for architecture discretization. Recall that we aim to preserve the operations whose synthesis is most similar to the original output of the supernet.</p><p>To this end, an intuitive solution is to project the outputs of the operators onto an orthogonal set using the Gram-Schmidt process, and the importance of the operators is decided by the projection weights on those orthogonal biases. But it is not deterministic since there exist multiple orthogonal bias sets in the same operation space. An alternative is to find which set of the operators can synthesize the output of the super-net to the most extent. Nevertheless, there are C K M sets for the edge with M candidate operators to test and it is unaffordable to go through all the possible combinations.</p><p>Instead, we propose a novel discretization strategy to approximate the optimal combination. We directly search for the operator o (i,j) with the maximum projection length on the synthesized output y (j) and then remove its projection component from y <ref type="bibr">(j)</ref> . In this case, the remainder? (j) is orthogonal to the selected operation o (i,j) . The orthonormalizing step can be illustrated as follows:</p><formula xml:id="formula_12">y (j) = y (j) ? &lt; o (i,j) , y (j) &gt; &lt; o (i,j) , o (i,j) &gt; o (i,j)<label>(6)</label></formula><p>Since the output of each cell concatenates all the nodes, such dense connections also incur the redundancy in the cell output. We conduct the decorrelation step at both the node-level and cell-level. Firstly, we recursively select the operation with the highest similarity to the node outputs and decorrelate it from the node outputs until the number of predecessors for each node is satisfied. Then, we decorrelate each selected operation from the outputs of all the nodes rather than the single node it belongs to. The discretization details are illustrated in Algorithm 1. Benefiting from decorrelation discretization, the proper operators synthesize the supernet to the most extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Settings</head><p>We launch extensive experiments on CIFAR-10 <ref type="bibr" target="#b33">[34]</ref> and ImageNet <ref type="bibr" target="#b34">[35]</ref>. CIFAR-10 has 60K images of the resolution at 32 ? 32, equally distributed over 10 classes. We adopt the standard split where 50K images are used for training and the rest for testing. ImageNet contains around 1.3M images belonging to 1,000 classes, with 1.2M images for training and 50K images for validation. We follow the general setting to resize all the images to 224?224 for both training and testing.</p><p>We employ the same search space (denoted as S1) as in DARTS, i.e. 8 different candidate operations, including 3?3 and 5?5 separable convolutions, 3?3 and 5?5 dilated separable convolutions, 3?3 max pooling, 3?3 average pooling, identity, and zero. For searching on CIFAR-10, we use the same one-shot model as DARTS where 8 cells with 16 initial channels are trained. We take SGD for optimization with initial learning rate at 0.025 and cosine annealing to 0.001. The momentum is 0.9 and the weight decay is 3?10 ?4 . The epoch number is set at 50 and the batch size at 64. Following <ref type="bibr" target="#b29">[30]</ref>, we freeze the architecture parameters in the first 15 epochs. We randomly select 5,000 images from the training set for discretization.</p><p>For directly searching on ImageNet, following previous studies <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b29">[30]</ref>, we reduce the resolution of the input image from 224?224 to 28?28 with three stacked convolution layers of stride 2. The one-shot model also consists of 8 cells with 16 initial channels as on CIFAR-10. We select 5% data from the training set to update model weights and another 5% to update architecture parameters. The epoch number is set at 60 for further convergence. Batch size is set at 128 for both training and validation. SGD is applied for optimization with the initial learning rate at 0.2. We freeze the architecture parameters in the first 35 epochs. For architecture parameters, the weight decay is set at 0.001 and the learning rate is 0.006. A set with 10,000 images are randomly sampled from the training set for discretization.</p><p>For both the datasets, the weights of the one-shot model and the architecture parameters are updated alternatively. We determine the final structure based on the proposed discretization strategy.</p><p>The searching process takes 0.2 GPU-day on CIFAR-10 and 1.9 GPU-day on ImageNet with a single Nvidia Tesla V100. The architectures achieved on CIFAR-10 and ImageNet are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>For training on CIFAR-10, we use the network consisting of 20 cells with 36 initial channels. We adopt two training protocols for comprehensive comparison with the state-of-thearts. The first protocol is the same as DARTS. The training lasts 600 epochs and the hyper-parameters are the same as those in DARTS. The second protocol follows DARTS+ <ref type="bibr" target="#b21">[22]</ref>, where the number of the training epochs is set at 2,000 for better convergence and the weight decay is set at 5?10 ?4 . We also conduct data enhancement including cutout, path dropout and auxiliary towers as in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>. The training time lasts 0.9 day for 600 epochs and 3 days for 2,000 epochs with a single Nvidia Tesla V100.</p><p>For training on ImageNet, we consider the mobile setting where the input image size is 224 ? 224 and the number of multiply-add operations is less than 600M. We follow DARTS and set the number of cells at 14 with 48 initial channels. The model is trained for 250 epochs with the batch size of 1,024. We also take SGD as the optimizer with initial learning rate at 0.4 (cosine annealing to 0), momentum at 0.9 and weight decay at 3 ? 10 ?5 . We exploit learning rate warmup, label smoothing, and auxiliary loss tower as in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>. The training phase lasts for 3.8 days with 3 Nvidia Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on CIFAR-10</head><p>Comparison to State-of-the-arts. A number of state-ofthe-art methods are considered for comparison, including DARTS-based methods and classical handcrafted architectures. To reduce the randomness in the initialization and optimization procedure, we repeat the training process 4 times. The best, the mean and the variance of the 4 runs are reported as recent studies do. The search cost is also reported as in most DARTS-based methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>, which denotes the training processof the one-shot model.</p><p>From <ref type="table" target="#tab_1">Table I</ref>, we can see that iDARTS delivers a very competitive accuracy with the top-1 error of 2.45% in 600 epochs and 2.38% in 2,000 epochs, and the model is built in 0.2 GPU-day for architecture search. We notice that DARTS (1st-order) and iDARTS adopt the same bi-level optimization algorithm, but due to node normalization and decorrelation discretization, iDARTS significantly reduces the top-1 error of DARTS from 3.00% to 2.45% without introducing any hyper-parameters or amending the gradient-descent direction. iDARTS outperforms all the counterparts except <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b21">[22]</ref>. It should be noted that Proxyless-NAS <ref type="bibr" target="#b32">[33]</ref> makes use of much more parameters and consumes much higher computational cost (4 GPU-day). Although DARTS+ <ref type="bibr" target="#b21">[22]</ref> reaches a comparable performance with iDARTS, its early-stop criterion needs to be empirically decided, which is not always guaranteed, in   <ref type="bibr" target="#b10">[11]</ref> 3.34 ? 0.06 3.2 3,150 Evolution AmoebaNet-B + cutout <ref type="bibr" target="#b10">[11]</ref> 2.55 ? 0.05 2.8 3,150 Evolution Hierarchical Evolution <ref type="bibr" target="#b7">[8]</ref> 3.75 ? 0.12 15.7 300 Evolution PNAS <ref type="bibr" target="#b12">[13]</ref> 3.41 ? 0.09 3.2 225 SMBO ENAS + cutout <ref type="bibr" target="#b25">[26]</ref> 2.89 4.6 0.45 ? RL NAONet-WS <ref type="bibr" target="#b37">[38]</ref> 3.53 2.5 0.3 ? NAO DARTS (1st order) + cutout <ref type="bibr" target="#b19">[20]</ref> 3.00 ? 0.14 3.3 0.4 ? Gradient-based DARTS (2nd order) + cutout <ref type="bibr" target="#b19">[20]</ref> 2.76 ? 0.09 3.3 1 ? Gradient-based SNAS (mild)+ cutout <ref type="bibr" target="#b16">[17]</ref> 2.98 2.9 1.5 Gradient-based ProxylessNAS + cutout <ref type="bibr" target="#b32">[33]</ref> 2.08 5.7 4 ? Gradient-based P-DARTS + cutout <ref type="bibr" target="#b24">[25]</ref> 2.50 3.4 0.3 Gradient-based BayesNAS + cutout <ref type="bibr" target="#b38">[39]</ref> 2.81 ? 0.04 3.4 0.2 Gradient-based PC-DARTS + cutout <ref type="bibr" target="#b29">[30]</ref> 2.57 ? 0.07 3.6 0.1 Gradient-based DARTS+ (Rule 1) + cutout + ME <ref type="bibr" target="#b21">[22]</ref> 2.50 ? 0.11 3.7 0.4 * Gradient-based DARTS+ (Rule 2) + cutout + ME <ref type="bibr" target="#b21">[22]</ref> 2.37 ? 0.13 4.3 0.6 * Gradient-based Amended-DARTS + cutout <ref type="bibr" target="#b23">[24]</ref> 2.60 ? 0.15 3.6 1.1 Gradient-based SDARTS-RS + cutout <ref type="bibr" target="#b39">[40]</ref> 2.67 ? 0.03 3.4 0.4 Gradient-based DARTS+PT + cutout <ref type="bibr" target="#b35">[36]</ref> 2.61 ? 0.08 3.0 0.8 Gradient-based R-DARTS (L2) + cutout <ref type="bibr" target="#b22">[23]</ref> 2.95 ? 0.  <ref type="bibr" target="#b35">[36]</ref> and iDARTS modify the value-based discretization strategy. iDARTS employs more efficient discretization and additional norm constraints, and this solution reaches higher performance with much lower searching cost.</p><p>Ablation Study. We investigate the impact of the proposed node normalization and decorrelation discretization solutions on CIFAR-10. We take DARTS-1st as the baseline for its efficiency. We re-implement DARTS and discretize the architecture by the value-based strategy and the decorrelation discretization in the same run for fair comparison. The training epoch is extended to 2,000 for better convergence. Results are shown in <ref type="table" target="#tab_1">Table II</ref>. <ref type="table" target="#tab_1">Table II</ref>, when only decorrelation discretization is used in DARTS, the error is largely reduced to 2.57?0.05%, which highlights its effectiveness. When node normalization is then added to build iDARTS, the result is optimized to 2.38?0.10%, proving its necessity. It should be noted that it does not make much sense to only use node normalization in DARTS (the error is 2.90?0.10%), as we cannot deliver a sound architecture without correctly approximating the continuous one. To sum up, node normalization and decorrelation discretization improve DARTS from different aspects and their combination reaches the best performance. Besides, node normalization merely introduces a static batch normalization to each cell and decorrelation discretization is applied only once in the last epoch. The overall search cost is still 0.2 GPU-day on a single Tesla V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As in</head><p>Evaluation on Generalization. To evaluate the generalization ability of iDARTS, additional experiments are conducted in two more settings.</p><p>Two additional search spaces are considered. One is S2 which contains none, max pooling 3?3, average pooling 3?3, skip connection and separate convolution 3?3. S3 is smaller, which only contains three operations, namely 3?3 separable convolution, identity and zero. We search for full models on all the search spaces. The learned architectures are shown in <ref type="figure">Fig. 6</ref>. We take DARTS, DARTS+, and PC-DARTS as counterparts in comparison. We use the official implementations of DARTS and PC-DARTS. DARTS+ does not release the code and we apply the two early-stop strategies in the official DARTS implementation, generating the architectures according to the paper. To be specific, the search process of DARTS+ is terminated when there exist more than 2 skip-connect operations (R1) or the architecture becomes stable for more than 10 epochs (R2). In the searching and retraining part, all the hyper-parameters are strictly set following the original settings for fair comparison. We also re-implement DARTS+ in the original search space on CIFAR-10, and the results are 2.54?0.01% and 2.40?0.02% with 2,000 epochs respectively, which are comparable to their results reported in the original paper (2.50?0.01% and 2.37?0.13%). All the architectures are re-trained with 600 epochs as in DARTS rather than 2,000 epochs. The results are shown in <ref type="table" target="#tab_1">Table III</ref>.</p><p>As illustrated in <ref type="table" target="#tab_1">Table III</ref>, we can see that iDARTS achieves the most stable performance on all the spaces, demonstrating its strong generalization ability due to node normalization and decorrelation discretization. On the other side, it is worth noting that the early stop strategies in DARTS+ are not well generalized to S2 and S3, since the criteria need to be carefully adjusted when the search space is changed. PC-DARTS reaches comparable results on S1 and S3, but fails on S2. Most operators in S2 are highly correlated and partial connections in PC-DARTS tend to be disturbed by such correlations, thus resulting in degraded performance.</p><p>Validation of Robustness. To validate the robustness of iDARTS, we take DARTS (1st-order) as the counterpart since they share the same optimization procedure. Recall that their major differences lie in training and discretization. In training, iDARTS uses node normalization to keep the balance between the norms of different nodes, and in discretization, it approximates the architecture based on the similarity between operations and outputs rather than the values of architecture parameters.</p><p>We extend the number of search epochs to 200 and retrain the discreted architecture in the searching process. The architectures approximated by iDARTS are illustrated in <ref type="figure">Fig.  9</ref>. The averaged zero-ratio in the architecture parameters and the accuracy are displayed in <ref type="figure">Fig. 7</ref>.</p><p>From <ref type="figure">Fig. 7</ref>, we can see that the average weight of zero operations over 14 edges increases to an abnormal value (0.98) at 200 epochs. This phenomenon also appears in <ref type="bibr" target="#b23">[24]</ref>. The retraining accuracy drops consistently when epochs become larger.</p><p>When node normalization is adopted, the average weight of zero operations slightly increases in terms of epoch, and it stops in 0.18, which is a much more reasonable value. Besides, benefiting from decorrelation discretization, the retraining accuracy of iDARTS is more stable. The highest accuracy of 97.6% is achieved at 120 epochs and a slight drop occurs when the searching process goes to more epochs, finally reaching an accuracy of 97.3% at 200 epochs. This slight accuracy drop is caused by the difference in data distribution between the training and validation sets. Ideally, the alternate updating on both the datasets drives (w, ?) to converge towards the global optimal (w * , ? * ). However, if the number of epochs is set at a large value (e.g. 200), w tends to overfit to the suboptimal values w ? train on the training set, which then affects the updating process of ?, leading to unstable results (i.e. zero-ratio starts to increase). When comparing DARTS and iDARTS, DARTS degrades at the beginning while iDARTS delivers a much more stable performance during iteration due to node normalization and decorrelation discretization. We visualize those architectures given by DARTS and iDARTS in <ref type="figure" target="#fig_0">Fig. 10</ref> and <ref type="figure">Fig. 9</ref> in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on ImageNet</head><p>We further evaluate the architectures reached by iDARTS on ImageNet. For comparison, two architectures are taken, which are searched with 4 nodes in a single cell on CIFAR-10 and ImageNet respectively. The architectures are demonstrated in <ref type="figure" target="#fig_5">Fig. 5</ref>. Results are shown in <ref type="table" target="#tab_1">Table IV</ref>.</p><p>As in <ref type="table" target="#tab_1">Table IV</ref>, we can find that both the architectures learned on CIFAR-10 and ImageNet achieve comparable performance with the state-of-the-art manual or RL methods. The architecture learned on CIFAR-10 reaches a top-1 error of 25.2%. When directly searching on ImageNet, the top-1 error furtherly decreases to 24.7%, demonstrating its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose an improved version of DARTS, namely iDARTS, to address the architecture degradation issue. Our motivation lies in that DARTS-based approaches neglect the imbalanced norms between different nodes and the high correlation between operations. We then introduce the node normalization and decorrelation discretization strategies to solve such problems. Our approach delivers better performance with stronger generalization ability as well as stability.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>arXiv:2108.11014v1 [cs.CV] 25 Aug 2021 Demonstration of the degradation issue in DARTS. (a) is the selected architecture by DARTS in the original search space when the epoch number is set at 50; (b) and (c) are the ones selected by with first-order (DARTS-1st) and second-order (DARTS-2nd) gradient estimation respectively in a small search space; and (d) is the result when the epoch number is set at 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Visualization of DARTS: (a) the super-net stacked by several normal and reduction cells, and the inputs of each cell come from the previous two layers; and (b) the dense connections within a normal cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of the imbalance of nodes and the correlation between operations in a cell of DARTS: (a) the dense connection within a single cell; (b) the curves of norms of the nodes in training (normalized by the norm of Node c k?2 ); and (c) the correlation matrix between operations of Node 3 before discretization.Algorithm 1 Decorrelation Discretization for iDARTSInput:I val : validation set; C: cell number; N : node number; K: number of predecessors for each node. Output:S: selected operator set. 1: Initialize: S = [] 2: for k = 1 to N ? K do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of candidate solutions to imbalanced norms. (a) and (b) are pre-normalization and postnormalization to skip-connect operations. The gray blocks denote the original input for each cell, the orange blocks denote additional normalization, the blue blocks denote the operations in DARTS, and the red blocks denote the normalized results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Reduction cell searched on ImageNet Visualization of the normal cell (a) (c) and the reduction cell (b) (d) learned on CIFAR-10 and ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 Fig. 6 :</head><label>106</label><figDesc>Normal cell searched in S3 on CIFAR-Reduction cell searched in S3 on CIFAR-Normal and reduction cells learned on CIFAR-10 in different settings. (a) and (b) are the normal and reduction cells using the original model in S2. (c) and (d) are the normal and reduction cells with the small model in S3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :Fig. 8 :Fig. 9 :</head><label>789</label><figDesc>Curves in terms of zero-ratio and accuracy of DARTS and iDARTS in the searching process when the epoch number increases to 200. Normal and reduction cells given by iDARTS and DARTS on CIFAR-10 w/ or w/o NN and DD in S1. Reduction cell at epoch 200 Normal and reduction cells given by iDARTS on CIFAR-10 in S1 when searching for 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Normal and reduction cells given by DARTS on CIFAR-10 in S1 when searching for 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparison to the state-of-the-art methods on the CIFAR-10 dataset (ME: training with 2,000 epochs; ?: including searching and training; ?: the re-implemented results; and *: evaluation on Nvidia Tesla V100).</figDesc><table><row><cell>Architecture DenseNet [37] NASNet-A + cutout [4] AmoebaNet-A + cutout</cell><cell>Test Err. (%) 3.46 2.65</cell><cell>Params (M) 25.6 3.3</cell><cell>Search Cost (GPU-days) -1,800</cell><cell>Search Method Manual RL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Ablation studies on the CIFAR-10 dataset (NN: Node Normalization, DD: Decorrelation Discretization).</figDesc><table><row><cell>Improvements NN DD ? ? ? ?</cell><cell>Test Err (%) 2.81 ? 0.07 2.57 ? 0.10 2.90 ? 0.10 2.38 ? 0.10</cell><cell>Params (M) 2.8 2.9 2.0 3.6</cell><cell>Multiply-Add (M) 464.8 471.9 331.1 575.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Comparison of DARTS-based methods in different settings on the CIFAR-10 dataset. R1 and R2 denote Criterion 1 and Criterion 1* in DARTS+ respectively.</figDesc><table><row><cell>Architecture Baseline DARTS (1st) DARTS (2nd) DARTS+ (R1) DARTS+ (R2) PC-DARTS iDARTS</cell><cell>S1 3.68?0.09 Err (%) 3.00?0.14 2.76?0.09 2.58?0.05% 2.46?0.02% 2.57?0.07 2.45 ?0.05</cell><cell>Para (M) 1.8 2.9 3.3 3.3 3.6 3.6 3.6</cell><cell>Err (%) 3.09?0.09 2.83 ? 0.14 2.82 ? 0.04 2.94 ? 0.04 2.96 ? 0.10 3.22 ? 0.07 2.50 ? 0.02</cell><cell>S2</cell><cell>Params (M) 2.3 4.4 4.5 4.4 2.2 3.9 3.4</cell><cell>S3 2.81?0.11 Err (%) 5.52 ? 0.25 3.28 ? 0.11 5.52 ? 0.25 3.04 ? 0.05 2.60 ? 0.16 2.53 ? 0.03</cell><cell>Para (M) 3.1 1.6 1.9 1.6 3.9 3.6 3.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison to the state-of-the-art methods on the ImageNet dataset (ME: training with 800 epochs; and *: evaluation on Nvidia Tesla V100).</figDesc><table><row><cell>Architecture Inception-v1 [41] MobileNet-v2 (1.4?) [42] ShuffleNet-v2 (1?) [43] NASNet-A [4] AmoebaNet-A [11] PNAS [13] MnasNet-92 [44] EfficientNet-B0 [45] DARTS (2nd order) [20] SNAS (mild) [17] ProxylessNAS [33] P-DARTS [25] BayesNAS [39] ASAP [46] XNAS [47] PC-DARTS [30] DARTS+ (ImageNet) + ME [22] Amended-DARTS [24] iDARTS (CIFAR-10) (S1) iDARTS (ImageNet) (S1)</cell><cell>Test Error Top-1 Top-5 30.2 10.1 25.3 -26.4 10.2 26.0 8.4 25.5 8.0 25.8 8.1 25.2 8.0 23.7 6.8 26.7 8.7 27.3 9.2 24.9 7.5 24.4 7.4 26.5 8.9 24.4 -24.0 -25.1 7.8 23.9 7.4 24.3 7.4 25.2 7.9 24.7 7.7</cell><cell>Params (M) 6.6 6.9 5 5.3 5.1 5.1 4.4 5.4 4.7 4.3 7.1 4.9 3.9 -5.2 5.3 5.1 5.5 5.1 5.1</cell><cell>Multiply-Add (M) 1,448 585 524 564 555 588 388 390 574 522 465 557 --600 586 582 590 578 568</cell><cell>Search Cost (GPU-day) ---1,800 3,150 225 --4.0 1.5 8.3 0.3 0.2 0.2 0.3 0.1 6.8* 1.1 0.2* 1.9*</cell><cell>Search Method Manual Manual Manual RL Evolution SMBO RL RL Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based Gradient-based</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Practical blockwise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Genetic CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A genetic programming approach to designing convolutional neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5369" to="5373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning architecture search by neuro-cell-based evolution with function-preserving mutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="243" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dpp-net: Deviceaware progressive search for pareto-optimal neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference and Workshop on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeparchitect: Automatically designing and training deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno>abs/1704.08792</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SMASH: oneshot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differentiable neural network architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maskconnect: Connectivity learning by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DARTS+: improved differentiable architecture search with early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stabilizing DARTS with amended gradient estimation on architectural parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DSNAS: direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>pp. 12 081-12 089. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search in A proxy validation loss landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5853" to="5862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PC-DARTS: partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four GPU hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">GOLD-NAS: gradual, one-level, differentiable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno>abs/2007.03331, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking architecture selection in differentiable nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference and Workshop on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbation-based regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1554" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shufflenet V2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ASAP: architecture search, anneal and prune</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno>abs/1904.04123</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">XNAS: neural architecture search with expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference and Workshop on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1975" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">More architectures mentioned in Section IV-B are visualized. The architectures in ablation study and robustness validation are illustrated in Fig. 8, Fig. 9 and Fig. 10 respectively. NN denotes node normalization and DD denotes decorrelation discretization</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
