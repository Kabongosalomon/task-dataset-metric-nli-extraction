<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crossover Learning for Fast Online Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<postCode>PCG 3</postCode>
									<settlement>Tencent</settlement>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<postCode>PCG 3</postCode>
									<settlement>Tencent</settlement>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<postCode>PCG 3</postCode>
									<settlement>Tencent</settlement>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Crossover Learning for Fast Online Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling temporal visual context across frames is critical for video instance segmentation (VIS) and other video understanding tasks. In this paper, we propose a fast online VIS model named CrossVIS. For temporal information modeling in VIS, we present a novel crossover learning scheme that uses the instance feature in the current frame to pixel-wisely localize the same instance in other frames. Different from previous schemes, crossover learning does not require any additional network parameters for feature enhancement. By integrating with the instance segmentation loss, crossover learning enables efficient crossframe instance-to-pixel relation learning and brings costfree improvement during inference. Besides, a global balanced instance embedding branch is proposed for more accurate and more stable online instance association. We conduct extensive experiments on three challenging VIS benchmarks, i.e., YouTube-VIS-2019, OVIS, and YouTube-VIS-2021 to evaluate our methods. To our knowledge, CrossVIS achieves state-of-the-art performance among all online VIS methods and shows a decent trade-off between latency and accuracy. Code will be available to facilitate future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video instance segmentation (VIS) <ref type="bibr" target="#b71">[72]</ref> is an emerging task in computer vision that aims to perform per-pixel labeling of instances within video sequences. This task provides a natural understanding of the video scenes. Therefore achieving accurate, robust, and fast video instance segmentation in real-world scenarios will greatly stimulate the development of computer vision applications, e.g., autonomous driving, video surveillance, and video editing.</p><p>Recently, significant progress has been witnessed in still- image object detection and instance segmentation. However, extending these methods to VIS remains a challenging work. Similar to other video-based recognition tasks, such as video object segmentation (VOS) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33]</ref>, video object detection (VOD) <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b36">37]</ref> and multi-object tracking (MOT) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b74">75]</ref>, continuous video sequences always bring great challenges, e.g., a huge number of frames required to be fast recognized, heavy occlusion, object disappearing and unconventional object-to-camera poses <ref type="bibr" target="#b17">[18]</ref>. To conquer these challenges and obtain better performance on these video understanding tasks (VIS, VOS, VOD, and MOT), fully utilizing the temporal information among video frames is critical. Previous deep learning based methods on this topic are in four folds. (1) Pixel-level feature aggregation enhances pixels feature of the current frame using other frames, e.g., STM-VOS <ref type="bibr" target="#b45">[46]</ref> and STEm-Seg <ref type="bibr" target="#b0">[1]</ref> aggregates pixel-level space-time feature based on Non-local network <ref type="bibr" target="#b60">[61]</ref> and 3D convolution, respectively.</p><p>(2) Instance-level feature aggregation enhances region, proposal or instance features across frames, e.g., MaskProp <ref type="bibr" target="#b1">[2]</ref> propagates instance features using deformable convolution <ref type="bibr" target="#b14">[15]</ref> for VIS and SELSA <ref type="bibr" target="#b66">[67]</ref> fuses instance features using spectral clustering for VOD. (3) Associating instances using metric learning, e.g., MaskTrack R-CNN <ref type="bibr" target="#b71">[72]</ref> introduces an association head based on Mask R-CNN <ref type="bibr" target="#b24">[25]</ref> and SipMask-VIS <ref type="bibr" target="#b5">[6]</ref> adds an adjunctive association head based on FCOS <ref type="bibr" target="#b56">[57]</ref>. (4) Post-processing, e.g., Seq-NMS <ref type="bibr" target="#b23">[24]</ref> and ObjLink <ref type="bibr" target="#b47">[48]</ref> refine video object detection results based on dynamic programming and learnable object tubelet linking, respectively.</p><p>In this paper, we propose a new scheme for temporal information modeling termed crossover learning. The basic idea is to use the instance feature in the current frame to pixel-wisely localize the same instance in other frames. Different from previous pixel/instance-level feature aggregation methods, crossover learning does not require additional network blocks for feature alignment and fusion. It obtains temporal information enhanced features without increasing inference computation cost. Different from metric learning based instance associating methods that require additional metric learning losses, crossover learning is integrated with the instance segmentation loss. Besides, it enables efficient many-to-many relation learning across frames, i.e., the instance pixel features are enforced to be close to the pixels that belong to the same instance and far from pixels that belong to other instances and background. Different from the post-processing methods, crossover learning is end-to-end optimizable with back-propagation.</p><p>Since crossover learning is integrated with the instance segmentation loss, it is fully compatible with the other temporal information modeling strategies. In this paper, we further improve the instance association strategy by introducing a global balanced instance embedding learning network branch. Our main contributions are summarized as follows:</p><p>? We propose a novel crossover learning scheme that leverages the rich contextual information inherent in videos to strengthen the instance representation across video frames, and weaken the background and instance-irrelevant information in the meantime.</p><p>? We introduce a new global balanced instance embedding branch to tackle the association problem in video instance segmentation, which yields better and more stable results than previous pair-wise identity mapping approaches.</p><p>? We propose a fully convolutional online video instance segmentation model CrossVIS that achieves a promising result on three challenging VIS benchmarks, i.e., YouTube-VIS-2019, OVIS, and YouTube-VIS-2021. To our knowledge, CrossVIS achieves state-of-the-art performance among all online VIS methods and strikes a decent speed-accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Still-image Instance Segmentation. Instance segmentation is the task of detecting and segmenting each distinct object of interest in a given image. Many prior works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref> contribute a lot to the rapid developments in this field. Mask R-CNN <ref type="bibr" target="#b24">[25]</ref> adapts Faster R-CNN <ref type="bibr" target="#b51">[52]</ref> with a parallel mask head to predict instance masks, and leads the two-stage fashion for a long period of time. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref> promote Mask R-CNN and achieve better instance segmentation results. The success of these two-stage models partially is due to the feature alignment operation, i.e., RoIPool <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref> and RoIAlign <ref type="bibr" target="#b24">[25]</ref>. Recently, instance segmentation methods based on one-stage frameworks without explicit feature alignment operation begin to emerge <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. As a representative, the fully convolutional CondInst <ref type="bibr" target="#b55">[56]</ref> outperforms several state-of-the-art methods on the COCO dataset <ref type="bibr" target="#b39">[40]</ref>, which dynamically generates filters for mask head conditioned on instances. We build our framework on top of <ref type="bibr" target="#b55">[56]</ref> and extend it to the VIS task. Video Instance Segmentation (VIS). VIS requires classifying, segmenting, and tracking visual instances over all frames in a given video. With the introduction of YouTube-VIS-2019 dataset <ref type="bibr" target="#b71">[72]</ref>, tremendous progresses <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17]</ref> have been made in tackling this challenging task. As a representative method, MaskTrack R-CNN <ref type="bibr" target="#b71">[72]</ref> extends the two-stage instance segmentation model Mask R-CNN with a pair-wise identity branch to solve the instance association sub-task in VIS. SipMask-VIS <ref type="bibr" target="#b5">[6]</ref> follows the similar pipeline based on the one-stage FCOS <ref type="bibr" target="#b56">[57]</ref> and YOLACT <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> frameworks. <ref type="bibr" target="#b41">[42]</ref> separates all sub-tasks in VIS problem and designs specific networks for each of them, all networks are trained independently and combined during inference to generate the final predictions. MaskProp <ref type="bibr" target="#b1">[2]</ref> introduces a novel mask propagation branch on the multistage framework <ref type="bibr" target="#b8">[9]</ref> that propagates instance masks from one frame to another. As an offline method, MaskProp achieves accurate predictions but suffers from high latency.</p><p>[35] introduces a modified variational auto-encoder to solve the VIS task. STEm-Seg <ref type="bibr" target="#b0">[1]</ref> treats the video clip as 3D spatial-temporal volume and segments objects in a bottomup fashion. <ref type="bibr" target="#b29">[30]</ref> adopts recurrent graph neural networks for VIS task. CompFeat <ref type="bibr" target="#b19">[20]</ref> refines features at both framelevel and object-level with temporal and spatial context information. VisTR <ref type="bibr" target="#b63">[64]</ref> naturally adopts DETR <ref type="bibr" target="#b6">[7]</ref> for VIS task in a query-based end-to-end fashion.</p><p>Recently, more challenging benchmarks such as OVIS <ref type="bibr" target="#b50">[51]</ref> and YouTube-VIS-2021 <ref type="bibr" target="#b70">[71]</ref> are proposed to further promote the advancement of this field. CrossVIS is evaluated on three VIS benchmarks and shows competitive performances. We hope CrossVIS can serve as a strong baseline to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to leverage the rich contextual information across different video frames for a more robust instance representation in video instance segmentation (VIS). To this end, we take inspiration from <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b62">63]</ref> and propose CrossVIS (see <ref type="figure" target="#fig_1">Fig. 2</ref>) that consists of two key components tailor-made for VIS task: (1) the crossover learning scheme for more accurate video-based instance representation learning, and (2) the global balanced instance embedding branch for better online instance association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mask Generation for Still-image</head><p>For still-image instance segmentation, we leverage the dynamic conditional convolutions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">56]</ref>. Specifically, our method generates the instance mask M x,y at location (x, y) by convolving an instance-agnostic feature mapF x,y from the mask branch and a set of instance-specific dynamic filters ? x,y produced by the controller head. Formally:</p><formula xml:id="formula_0">F x,y = Concat F mask ; O x,y ,<label>(1)</label></formula><formula xml:id="formula_1">M x,y = MaskHead F x,y ; ? x,y ,<label>(2)</label></formula><p>whereF x,y is the combination of mask feature map F mask and relative coordinates O x,y . F mask is produced via the mask branch attached on FPN <ref type="bibr" target="#b37">[38]</ref> {P 3 , P 4 , P 5 } level features. The relative coordinates O x,y provide a strong localization cue for predicting the instance mask. The MaskHead consists of 3 conv-layers with dynamic filters ? x,y conditioned on the instance located at (x, y) as convolution kernels. The last layer has 1 output channel and uses sigmoid function for instance mask predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Crossover Learning</head><p>Intuition of Crossover Learning. Still-image instance segmentation needs two types of information <ref type="bibr" target="#b55">[56]</ref>: (1) appearance information to categorize objects, which is given by the dynamic filter ? x,y in our model; and (2) location information to distinguish multiple objects belonging to the same category, which is represented by the relative coordinates O x,y . In the aforementioned still-image instance segmentation model (see Sec. 3.1), for each instance, we have a one-to-one correspondence between the appearance information and location information: given a ? x,y , there exists one and only one O x,y as the corresponding location information belonging to the same instance. Meanwhile, the connection between different instances is isolated. However, in terms of the VIS task, given a sampled frame-pair from one video, the same instance may appear in different locations of two different sampled frames. Therefore it is possible to use the appearance information from one sampled frame to represent the same instance in two different sampled frames, guided by different location information. We can utilize the appearance information ? x,y (t) from one sampled frame t to incorporate the location information O x,y (t + ?) of the same instance in another sampled frame t + ?. By this kind of across frame mapping, we expect the learned instance appearance information can be enhanced and more robust, meanwhile, the background and instance-irrelevant information is weakened. Formulation of Crossover Learning. Specifically, for a given video, we denote a detected instance i at time t (or frame t) as:</p><formula xml:id="formula_2">I i (t) = (c i (t), ? x,y (t), e i (t)),<label>(3)</label></formula><p>where c i (t) is the instance category, ? x,y (t) is the dynamic filter for MaskHead, and e i (t) is the instance embedding for online association. Without loss of generality, we assume that an instance I i exists in both frame t (denoted as I i (t)) as well as frame t + ? (denoted as I i (t + ?)).</p><p>Within each frame, following the setup and notation in Sec. 3.1, at time t, the instance mask of I i (t) located at (x, y) can be represented as:</p><formula xml:id="formula_3">M x,y (t) = MaskHead F x,y (t); ? x,y (t) .<label>(4)</label></formula><p>At time t + ?, the instance move from location (x, y) to location (x , y ). So the instance mask of I i (t + ?) can be represented as:</p><formula xml:id="formula_4">M x ,y (t + ?) = MaskHead F x ,y (t + ?); ? x ,y (t + ?) ,<label>(5)</label></formula><p>Our crossover learning scheme establishes a connection between the dynamic filter from one frame and the mask feature map from another frame. Specifically, we expect the dynamic filter ? x,y (t) of I i (t) can produce the mask of I i (t + ?) by convolving its mask feature mapF x ,y (t + ?):</p><formula xml:id="formula_5">M ? x ,y (t + ?) = MaskHead F x ,y (t + ?); ? x,y (t) ,<label>(6)</label></formula><p>where M ? with a superscript "?" denotes the instance mask produced by crossover learning. Similarly, we expect the dynamic filter ? x ,y (t + ?) of I i (t + ?) can produce the mask of I i (t) by convolving its mask feature mapF x,y (t):</p><formula xml:id="formula_6">M ? x,y (t) = MaskHead F x,y (t); ? x ,y (t + ?) .<label>(7)</label></formula><p>Following <ref type="bibr" target="#b55">[56]</ref>, during training, the predicted instance</p><formula xml:id="formula_7">masks M x,y (t), M x ,y (t + ?), M ? x ,y (t + ?) and M ?</formula><p>x,y (t) are all optimized by the dice loss <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_8">L dice (M, M * ) = 1? 2 HW i M i M * i HW i (M i ) 2 + HW i (M * i ) 2 (8)</formula><p>where M is the predicted mask and M * is the ground truth mask, i denotes the i th pixel. During inference, the instance mask generation process keeps the same as <ref type="bibr" target="#b55">[56]</ref>, with no crossover involved.</p><p>Advantages of Crossover Learning. For a given instance I i (t), its appearance information ? x,y (t) can learn two kinds of representations: a within-frame one in frame t, and an across-frame one in frame t + ?. At time t + ?, the instance I i (t+?) may have a different appearance and be in -th sampled reference frames distribution -th sampled reference frames distribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shift</head><p>Shift is independent of sampled reference frames <ref type="figure">Figure 3</ref>. An illustration of pair-wise local embeddings ( <ref type="figure">Fig. 3</ref>, left) used in <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b5">6]</ref>, and the proposed instance proxies <ref type="figure">(Fig. 3,  right)</ref>. For pair-wise local embeddings, {eN (k)} is the set of all N instance embeddings from k-th sampled reference frames, while in the k -th sampling, the sampled instance identities will change to {e N (k )}, causing a distribution shift. Even if the same instance Ii is happened to be sampled in both k-th and k -th samplings, the corresponding embedding {ei(k)} may also shift to {ei(k )} due to occlusion, changing of background and scale variation, etc. In contrast, {wM } is a set of learnable instance-wise weights of the model and independent of sampled reference frames. Therefore {wM } produces a global, definite convergence status for instance embeddings.</p><p>a different context compared with the same instance I i (t) at time t. Meanwhile, the background may also changed. The crossover learning enables dynamic filter ? x,y (t) to identify the same instance representation at both time t and t + ?, regardless of the background and instance-irrelevant information. In this way, we can largely overcome the appearance inconsistency as well as background clutters problems in videos, leveraging the rich contextual information across video frames to get a more accurate and robust instance representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Global Balanced Embeddings for Instance Association</head><p>Another crucial sub-task in VIS is the instance association, i.e., learning instance embeddings where instances of the same identity are close to each other in the feature space, while instances that belong to different identities are far apart. These embeddings are used for online inference.</p><p>In previous tracking-by-detection VIS methods <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b5">6]</ref>, the instance embedding is trained in a pair-wise and local manner. Specifically, given a key frame at time t + ? and a reference frame at time t, assuming there is a detected candidate instance I i in the key frame as the training sample, and N already identified instances (given by ground truth label during training) in the reference frame as targets. Then, I i can only be assigned to one of the N identities if it is one of the already identified instances or a new identity if it is a new instance. The probability of assigning label n to I i is defined as:</p><formula xml:id="formula_9">p i (n) = ? ? ? ? ? ? ? ? ? ? ? ? ? exp (e i e n ) 1 + N j=1 exp (e i e j ) if n ? [1, N ], 1 1 + N j=1 exp (e i e j ) otherwise,<label>(9)</label></formula><p>where e i and e n denote the instance embedding of I i from the key frame and I n from the reference frame, respectively. p i (n) is optimized by cross-entropy loss:</p><formula xml:id="formula_10">L CE = ? log(p i (n)).<label>(10)</label></formula><p>However, this approach suffers from the following issues (see <ref type="figure">Fig 3, left)</ref>  <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b46">47]</ref>: the feature space where e i and {e N } := {e 1 , e 2 , . . . , e N } live in is defined by the sampled frames, and the decision boundary is closely related to the instance embeddings {e N } from the reference frame. Therefore, the optimization and instance association processes highly depend on stochastic frame sampling, which probably lead to unstable learning and slow convergence. We also observe a relatively large fluctuation in AP when using pair-wise embeddings (see ? AP in Tab. 7).</p><p>To remedy these problems and get a globally definite convergence status for instance embeddings, we train our model as a M -class classification problem where M equals to the number of all different identities in the whole training set. We then employ a set of learnable instance-wise weights {w M } := {w 1 , w 2 , . . . , w M } as proxies of instances (see <ref type="figure">Fig 3, right)</ref> to replace the embeddings of instances {e N } defined by the sampled frame pair directly <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b64">65]</ref>. In this way, the probability of assigning label n to I i is reformulated as:</p><formula xml:id="formula_11">p i (n) = exp (e i w n ) M j=1 exp (e i w j ) .<label>(11)</label></formula><p>p i (n) is also optimized by cross-entropy loss:</p><formula xml:id="formula_12">L CE = ? log(p i (n)).<label>(12)</label></formula><p>However, the M -class classification problem is hard to extend to large-scale datasets (e.g., M = 3, 774 for the YouTube-VIS-2019 training set) as all the negative classes participate in the loss computation, resulting in a large posneg samples imbalance issue. Moreover, the large gradient produced by these negative samples from the instance embedding branch dominates the learning process 1 , which can negatively affect the optimization of all sub-tasks. To remedy these problems, we adopt focal loss <ref type="bibr" target="#b38">[39]</ref> as the objective for our global instance embedding to balance the pos-neg samples as well as the learning of each sub-task:</p><formula xml:id="formula_13">p i (n) = ?(e i w n ) if I i = I n , 1 ? ?(e i w n ) otherwise,<label>(13)</label></formula><formula xml:id="formula_14">L id = L F ocal = ?? t (1 ? p i (n)) ? log(p i (n)),<label>(14)</label></formula><p>where ?(?) is the sigmoid function, ? t and ? follow the definition in <ref type="bibr" target="#b38">[39]</ref>. I i = I n means the two instances belong to the same identity. e i is generated by the proposed global balanced instance embedding branch which shares a common structure as the classification branches of <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Online Inference</head><p>We jointly train detection, segmentation, crossover learning and instance association tasks in an end-to-end manner. The multi-task loss for each sample is:</p><formula xml:id="formula_15">L = L det + L seg + L cross + L id .<label>(15)</label></formula><p>L det and L seg denote the object detection loss and stillimage instance segmentation loss in <ref type="bibr" target="#b55">[56]</ref>. L cross denotes the crossover learning loss:</p><formula xml:id="formula_16">L cross = L dice (M ? x,y (t), M * x,y (t)) + L dice (M ? x ,y (t + ?), M * x ,y (t + ?)),<label>(16)</label></formula><p>where L dice is formulated in Eq. <ref type="bibr" target="#b7">(8)</ref>. L id denotes the instance embedding loss defined in Eq. (13) &amp; Eq. <ref type="bibr" target="#b13">(14)</ref>. During inference, the testing video is processed by CrossVIS frame by frame in an online fashion. We follow the inference procedure described in <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We evaluate the proposed CrossVIS on three challenging video instance segmentation benchmarks, i.e., YouTube-VIS-2019 <ref type="bibr" target="#b71">[72]</ref>, OVIS <ref type="bibr" target="#b50">[51]</ref> and YouTube-VIS-2021 <ref type="bibr" target="#b70">[71]</ref>.</p><p>? YouTube-VIS-2019 is the first dataset for video instance segmentation, which has a 40-category label set,  <ref type="table">Table 1</ref>. Comparisons with some state-of-the-art VIS models on YouTube-VIS-2019 val set. The compared methods are listed roughly in the temporal order. " " under "Aug." indicates using multi-scale input frames during training. " " indicates using stronger data augmentation (e.g., random crop, higher resolution input, etc.) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> or additional data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. The FPS with superscript " ?" is not reported in <ref type="bibr" target="#b1">[2]</ref> and is estimated using its utilized components <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b2">3]</ref>. For the definition of online and offline, we follow <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>. Unless specified, AP and AR in this paper refer to the average precision and average recall defined in <ref type="bibr" target="#b71">[72]</ref>. Following previous works <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>, we report our results on the validation set to evaluate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Basic Setup. Similar to the setup of <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b5">6]</ref>, we initialize CrossVIS with corresponding CondInst instance segmentation model <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b37">38]</ref> pre-trained on COCO train2017 <ref type="bibr" target="#b39">[40]</ref> with 1? schedule. Then we train the CrossVIS on VIS datasets with 1? schedule. The pretrain procedure on COCO follows Detectron2 <ref type="bibr" target="#b67">[68]</ref> and AdelaiDet <ref type="bibr" target="#b54">[55]</ref>. 1? schedule on VIS datasets refers to 12 epoch <ref type="bibr" target="#b71">[72]</ref>. The learning rate is set to 0.005 initially following SipMask-VIS <ref type="bibr" target="#b5">[6]</ref> and reduced by a factor of 10 at epoch 9 and 11. Most FPS data 2 is measured with an RTX 2080</p><p>Ti GPU including the pre-and post-processing steps. We report the results using the median of 5 runs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref> due to the variance inherent in VIS models and datasets. Input Frame Size. For single-scale training, we resize the frame to 360 ? 640. For multi-scale training, we follow the setting in SipMask-VIS. During inference, we resize the frame to 360 ? 640. Main Results. For our main results, we evaluate the proposed CrossVIS on YouTube-VIS-2019, OVIS and YouTube-VIS-2021 datasets, respectively. Ablation Study. Our ablation study is conducted on the YouTube-VIS-2019 dataset using models with ResNet-50-FPN <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>Main Results on YouTube-VIS-2019 Dataset. We compare CrossVIS against some state-of-the-art methods in Tab. 1. The comparison is performed in terms of both accuracy and speed. (1) When using the single-scale training strategy, CrossVIS achieves 34.8 AP using ResNet-50 and 36.6 AP using ResNet-101, which is the best among all the online and near online methods in Tab. 1. CrossVIS also outperforms the recently proposed offline method VisTR.  <ref type="bibr" target="#b1">[2]</ref> is a state-of-the-art offline VIS approach that proposes a novel mask propagation mechanism in combination with Spatiotemporal Sampling Network <ref type="bibr" target="#b2">[3]</ref>, Hybrid Task Cascade mask head <ref type="bibr" target="#b8">[9]</ref>, High-Resolution Mask Refinement post-processing, longer training schedule and stronger data argumentation. MaskProp can achieve very high accuracy but suffer from low inference speed so it is far from real-time applications and online scenarios. Meanwhile, CrossVIS is designed to be an efficient online VIS model and focusing more on the speed-accuracy trade-off. Overall, the experiment results demonstrate the effectiveness of the proposed approach. Main Results on OVIS Dataset. OVIS is a much more challenging VIS benchmark than YouTube-VIS-2019 and all methods encounter a large performance degradation on this dataset. CrossVIS achieves 14.9 AP and 18.1 AP with the temporal feature calibration module proposed in <ref type="bibr" target="#b50">[51]</ref>, surpassing all methods investigated in <ref type="bibr" target="#b50">[51]</ref> under the same experimental conditions. We hope CrossVIS can serve as a strong baseline for this new and challenging benchmark.  Meanwhile, CrossVIS is 2.7 AP VIS better than CondInst-VIS under the same AP COCO mask . The above two observations prove that the improvement in AP VIS mainly comes from the proposed two modules instead of better pre-trained models or baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does the Efficiency of CrossVIS Simply Come from the Efficiency of CondInst?</head><p>The answer is no. We prove this in Tab. 5. In terms of the inference speed, CondInst is only 1.2 FPS faster than Mask R-CNN in instance segmentation task (similar conclusions are also reported in <ref type="bibr" target="#b55">[56]</ref>). Meanwhile, CrossVIS is 7.0 FPS faster than MaskTrack R-CNN in VIS task. This is mainly because: (1) crossover learning adds no extra parameters and can bring cost-free improvement during inference.   (2) When the sample time interval ? becomes larger, the scene and context become different and diverse between two frames in the sampled frame pair. The baseline without crossover learning cannot explicitly utilize the cross-frame information therefore only has limited improvement. However, crossover learning can benefit significantly from the larger ? and achieves up to 1.4 AP improvement compared to the baseline. (3) The proposed crossover learning scheme is quite insensitive to the variations of T . Overall, models trained with crossover scheme are ? 1 AP higher than baselines under a wide range of time intervals, i.e., from T = 3 to T = ? as shown in Tab. 6. These results prove the analysis in Sec. 3.2 that the crossover scheme can leverage the rich contextual information across video frames to get a more accurate and robust instance representation. Instance Association Embeddings. We study the instance association embeddings in Tab. 7. As expected in Sec. 3.3, (1) In terms of AP, the effect from "global" (using learnable {w M } instead of sampled {e N }) and "balanced" (us-  In terms of AP fluctuation, using the global embedding has a much smaller standard deviation ? AP than the pair-wise embedding regardless of the loss function, which indicates that the global embedding can produce a more definite convergence status and more stable results. Component-wise Analysis. We investigate the effects of crossover learning and global balanced embedding individually and simultaneously in Tab. 8. Using crossover learning and global balanced embedding individually can bring 1.0 AP and 1.4 AP improvement, respectively. In terms of AP, global balanced embedding is slightly higher. Meanwhile, crossover learning adapts CondInst naturally for VIS task during training and brings cost-free improvement during inference. Together, the two components bring 2.7 AP improvement, which is larger than 1.0 + 1.4 AP when used solely. Therefore the proposed two components are fully compatible with each other. They show synergy and their improvements are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce a novel VIS solution coined as CrossVIS, which performs the best among all online video instance segmentation methods in three challenging VIS benchmarks. Moreover, CrossVIS strikes a decent trade-off between latency and accuracy. We also show that the accuracy and efficiency of CrossVIS are not simply come from the instance segmentation framework but stems from the proposed design. Extensive study proves that  <ref type="figure">Figure 4</ref>. We compare some qualitative VIS results obtained by CrossVIS and MaskTrack R-CNN <ref type="bibr" target="#b71">[72]</ref>. Different color masks belong to different instances. crossover learning can bring cost-free improvement during inference, while the lightweight global balanced embedding can help stabilize the model performance. We believe that the proposed approach can serve as a strong baseline for further research on the VIS, and sheds light on other video analysis and video understanding tasks.</p><p>Qualitative Results. We compare some qualitative VIS results obtained by CrossVIS and MaskTrack R-CNN in <ref type="figure">Fig. 4</ref> and <ref type="figure">Fig. 5</ref>. Our CrossVIS segments and tracks object instances more robustly even if they are occluded or overlap with each other.  <ref type="figure">Figure 5</ref>. We compare some qualitative VIS results obtained by CrossVIS and MaskTrack R-CNN <ref type="bibr" target="#b71">[72]</ref>. Different color masks belong to different instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>CrossVIS can predict more accurate video instance segmentation results (bottom row) compared with the baseline model without crossover learning (top row). More qualitative comparisons on YouTube-VIS-2019 val set are available in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of CrossVIS in the training phase. Two frames at time t and t + ? are fed into an fully convolutional network (FCN) to generate dynamic filters ?x,y(t) &amp; ? x ,y (t + ?) and mask feature mapsFx,y(t) &amp;F x ,y (t + ?). Red lines indicate the dynamic filters and mask feature maps in frame t, blue lines indicate the same in frame t + ?. Solid lines indicate the still-image prediction process, dotted lines indicate the proposed crossover learning scheme. The four " " from top to bottom in the figure correspond to the mask generation process formulated in Eq. (4), Eq. (7), Eq. (6), and Eq. (5), respectively. Classification, localization as well as global balanced instance embedding branches are omitted in the figure for clarification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>YouTube-VIS-2021 dataset is an improved and augmented version of YouTube-VIS-2019 dataset, which has 8, 171 unique video instances and 232k highquality manual annotations (about 2? of YouTube-VIS-2019). There are 2, 985 training videos, 421 validation videos, and 453 test videos in this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Main Results on YouTube-VIS-2021 Dataset. YouTube-VIS-2021 dataset 3 is an improved and augmented version</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 )</head><label>2</label><figDesc>The global balanced embedding Crossover</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ing L F ocal instead of L CE ) are equally important and interdependent: Using L F ocal instead of L CE for pair-wise embedding can only bring 0.2 AP improvement, for large posneg imbalance do not exist in the pair-wise scheme. Using global instead of pair-wise embedding optimized by L CE can only bring 0.3 AP improvement, for there exists a large pos-neg imbalance issue. But together, global and balanced embedding can bring 1.7 AP improvement. So global and balanced are both indispensable for good performance. (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>4, 883 unique video instances and 131k high-quality manual annotations. There are 2, 238 training videos, 302 validation videos, and 343 test videos in it. ? OVIS dataset is a recently proposed very challenging VIS dataset with the philosophy of perceiving object occlusions in videos, which could reveal the complexity and the diversity of real-world scenes. OVIS consists of 296k high-quality instance masks (about 2? of YouTube-VIS-2019) and 5.80 instance per video</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Aug.</cell><cell>Type</cell><cell>FPS</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell>IoUTracker+ [72]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>-</cell><cell>23.6</cell><cell>39.2</cell><cell>25.5</cell><cell>26.2</cell><cell>30.9</cell></row><row><cell>OSMN [73]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>-</cell><cell>27.5</cell><cell>45.1</cell><cell>29.1</cell><cell>28.6</cell><cell>33.1</cell></row><row><cell>DeepSORT [66]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>-</cell><cell>26.1</cell><cell>42.9</cell><cell>26.1</cell><cell>27.8</cell><cell>31.3</cell></row><row><cell>FEELVOS [58]</cell><cell>ResNet-50</cell><cell></cell><cell>Offline</cell><cell>-</cell><cell>26.9</cell><cell>42.0</cell><cell>29.7</cell><cell>29.9</cell><cell>33.4</cell></row><row><cell>SeqTracker [72]</cell><cell>ResNet-50</cell><cell></cell><cell>Offline</cell><cell>-</cell><cell>27.5</cell><cell>45.7</cell><cell>28.7</cell><cell>29.7</cell><cell>32.5</cell></row><row><cell>MaskTrack R-CNN [72]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>32.8</cell><cell>30.3</cell><cell>51.1</cell><cell>32.6</cell><cell>31.0</cell><cell>35.5</cell></row><row><cell>MaskProp [2]</cell><cell>ResNet-50</cell><cell></cell><cell>Offline</cell><cell>&lt; 6.2  ?</cell><cell>40.0</cell><cell>-</cell><cell>42.9</cell><cell>-</cell><cell>-</cell></row><row><cell>SipMask-VIS [6]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>34.1</cell><cell>32.5</cell><cell>53.0</cell><cell>33.3</cell><cell>33.5</cell><cell>38.9</cell></row><row><cell>SipMask-VIS [6]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>34.1</cell><cell>33.7</cell><cell>54.1</cell><cell>35.8</cell><cell>35.4</cell><cell>40.1</cell></row><row><cell>STEm-Seg [1]</cell><cell>ResNet-50</cell><cell></cell><cell>Near Online</cell><cell>4.4</cell><cell>30.6</cell><cell>50.7</cell><cell>33.5</cell><cell>31.6</cell><cell>37.1</cell></row><row><cell>Johnander et al. [30]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>? 30</cell><cell>35.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CompFeat [20]</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>&lt; 32.8</cell><cell>35.3</cell><cell>56.0</cell><cell>38.6</cell><cell>33.1</cell><cell>40.3</cell></row><row><cell>VisTR [64]</cell><cell>ResNet-50</cell><cell></cell><cell>Offline</cell><cell>30.0</cell><cell>34.4</cell><cell>55.7</cell><cell>36.5</cell><cell>33.5</cell><cell>38.9</cell></row><row><cell>CrossVIS</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>39.8</cell><cell>34.8</cell><cell>54.6</cell><cell>37.9</cell><cell>34.0</cell><cell>39.0</cell></row><row><cell>CrossVIS</cell><cell>ResNet-50</cell><cell></cell><cell>Online</cell><cell>39.8</cell><cell>36.3</cell><cell>56.8</cell><cell>38.9</cell><cell>35.6</cell><cell>40.7</cell></row><row><cell>CrossVIS-Lite</cell><cell>DLA-34</cell><cell></cell><cell>Online</cell><cell>48.5</cell><cell>33.0</cell><cell>52.7</cell><cell>35.0</cell><cell>33.9</cell><cell>39.5</cell></row><row><cell>CrossVIS-Lite</cell><cell>DLA-34</cell><cell></cell><cell>Online</cell><cell>48.5</cell><cell>36.2</cell><cell>56.7</cell><cell>38.4</cell><cell>35.1</cell><cell>42.0</cell></row><row><cell>MaskTrack R-CNN [72]</cell><cell>ResNet-101</cell><cell></cell><cell>Online</cell><cell>28.6</cell><cell>31.9</cell><cell>53.7</cell><cell>32.3</cell><cell>32.5</cell><cell>37.7</cell></row><row><cell>MaskProp [2]</cell><cell>ResNet-101</cell><cell></cell><cell>Offline</cell><cell>&lt; 5.6  ?</cell><cell>42.5</cell><cell>-</cell><cell>45.6</cell><cell>-</cell><cell>-</cell></row><row><cell>STEm-Seg [1]</cell><cell>ResNet-101</cell><cell></cell><cell>Near Online</cell><cell>2.1</cell><cell>34.6</cell><cell>55.8</cell><cell>37.9</cell><cell>34.4</cell><cell>41.6</cell></row><row><cell>VisTR [64]</cell><cell>ResNet-101</cell><cell></cell><cell>Offline</cell><cell>27.7</cell><cell>35.3</cell><cell>57.0</cell><cell>36.2</cell><cell>34.3</cell><cell>40.4</cell></row><row><cell>CrossVIS</cell><cell>ResNet-101</cell><cell></cell><cell>Online</cell><cell>35.6</cell><cell>36.6</cell><cell>57.3</cell><cell>39.7</cell><cell>36.0</cell><cell>42.0</cell></row></table><note>(about 3.4? of YouTube-VIS-2019) from 25 seman- tic categories, where object occlusions usually occur. There are 607 training videos, 140 validation videos, and 154 test videos in this dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>2 )Table 2 .Table 3 .</head><label>223</label><figDesc>When using the multi-scale training strategy, CrossVIS achieves 36.3 AP and 39.8 FPS with ResNet-50, which outperforms SipMask-VIS, STEm-Seg and VisTR with the Comparisons with some VIS models on the recently proposed very challenging OVIS val set. We use ResNet-50 backbone and 1? schedule for all experiments. Comparisons with some VIS models on the recently proposed YouTube-VIS-2021 val set. We use ResNet-50 backbone and 1? schedule for all experiments.</figDesc><table><row><cell>Methods</cell><cell cols="2">Calibration [51] AP AP50 AP75</cell></row><row><cell>SipMask-VIS</cell><cell>10.3 25.4</cell><cell>7.8</cell></row><row><cell>MaskTrack R-CNN</cell><cell>10.9 26.0</cell><cell>8.1</cell></row><row><cell>STEm-Seg</cell><cell cols="2">13.8 32.1 11.9</cell></row><row><cell>CrossVIS</cell><cell cols="2">14.9 32.7 12.1</cell></row><row><cell>SipMask-VIS</cell><cell cols="2">14.3 29.9 12.5</cell></row><row><cell>MaskTrack R-CNN</cell><cell cols="2">15.4 33.9 13.1</cell></row><row><cell>CrossVIS</cell><cell cols="2">18.1 35.5 16.9</cell></row><row><cell>Methods</cell><cell cols="2">Aug. AP AP50 AP75 AR1 AR10</cell></row><row><cell>MaskTrack R-CNN</cell><cell cols="2">28.6 48.9 29.6 26.5 33.8</cell></row><row><cell>SipMask-VIS</cell><cell cols="2">31.7 52.5 34.0 30.8 37.8</cell></row><row><cell>CrossVIS</cell><cell cols="2">33.3 53.8 37.0 30.1 37.6</cell></row><row><cell>CrossVIS</cell><cell cols="2">34.2 54.4 37.9 30.4 38.2</cell></row></table><note>stronger ResNet-101 backbone. (3) Moreover, CrossVIS achieves the best speed-accuracy trade-off among all VIS approaches in Tab. 1. We also present a more efficient CrossVIS-Lite model with DLA-34 backbone, achieving 36.2 AP and 48.5 FPS, which shows a decent trade-off be- tween latency and accuracy. MaskProp</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparisons between CrossVIS and other baselines in terms of both AP VIS and AP COCO mask on YouTube-VIS-2019 val set. VIS under similar AP COCO mask .</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Sched. AP VIS AP COCO mask</cell></row><row><cell cols="2">MaskTrack R-CNN ResNet-50</cell><cell></cell><cell>30.3</cell><cell>34.7</cell></row><row><cell cols="2">MaskTrack R-CNN ResNet-101 CondInst-VIS ResNet-50</cell><cell>1?</cell><cell>31.9 32.1</cell><cell>35.9 35.7</cell></row><row><cell>CrossVIS</cell><cell>ResNet-50</cell><cell></cell><cell>34.8</cell><cell>35.7</cell></row><row><cell>Method</cell><cell cols="4">Backbone FPS (360 ? 640) AP VIS</cell></row><row><cell cols="2">Mask R-CNN [25] ResNet-50 CondInst [56]</cell><cell cols="2">41.6 42.8 (+1.2)</cell><cell>--</cell></row><row><cell cols="2">MaskTrack R-CNN ResNet-50 CrossVIS</cell><cell cols="2">32.8 39.8 (+7.0)</cell><cell>30.3 34.8</cell></row><row><cell cols="5">Table 5. Efficiency comparisons on YouTube-VIS-2019 val set.</cell></row><row><cell cols="5">of YouTube-VIS-2019 dataset. We evaluate the recently</cell></row><row><cell cols="5">proposed MaskTrack R-CNN and SipMask-VIS on this</cell></row><row><cell cols="5">dataset using official implementation for comparison. As</cell></row><row><cell cols="5">shown in Tab. 3, CrossVIS surpasses MaskTrack R-CNN</cell></row><row><cell cols="5">and SipMask-VIS by a large margin. We hope CrossVIS</cell></row><row><cell cols="5">can serve as a strong baseline for this new and challenging</cell></row><row><cell>benchmark.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.4. Ablation Study</cell><cell></cell><cell></cell></row><row><cell cols="5">Does Better VIS Results Simply Come from Better Still-</cell></row><row><cell cols="5">image Instance Segmentation Models? The answer is no.</cell></row><row><cell cols="5">We prove this in Tab. 4: (1) Compared with MaskTrack R-</cell></row><row><cell cols="5">CNN using ResNet-101 backbone, CrossVIS is 0.2 AP COCO mask</cell></row><row><cell cols="5">lower, which indicates that our pre-trained model is rela-</cell></row><row><cell cols="5">tively weaker in terms of still-image instance segmentation</cell></row><row><cell cols="5">on COCO. But for the VIS task, our model is 2.9 AP VIS</cell></row><row><cell cols="5">higher. (2) We implement a VIS baseline called CondInst-</cell></row><row><cell cols="5">VIS which replaces the Mask R-CNN part in MaskTrack R-</cell></row><row><cell cols="5">CNN by CondInst. Therefore the only differences between</cell></row><row><cell cols="5">CrossVIS and CondInst-VIS are the proposed crossover</cell></row><row><cell cols="5">learning scheme and global balanced instance embedding</cell></row><row><cell cols="5">branch. Compared with CondInst-VIS with ResNet-50 and</cell></row><row><cell cols="5">MaskTrack R-CNN with ResNet-101, we conclude that</cell></row><row><cell cols="2">they achieve similar AP</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Effect of crossover learning and sample time interval on AP. We randomly sample two frames at time t and t + ? respectively, where the sample time interval ? ? [?T, T ]. The "?" indicates the AP improvement of the model with crossover learning compared to the model without crossover learning under the same T . Study of instance association embeddings. To quantitate the fluctuation in results, we conduct 5 independent experiments for each configuration. We report the AP using the median of 5 runs. ?AP denotes the standard deviation of 5 runs. Here we investigate the effectiveness of the proposed crossover learning scheme in Sec. 3.2. During training, we randomly sample frame pairs with a sample time interval ? ? [?T, T ]. The results are shown in Tab. 6. We conclude that: (1) When the sample time interval ? is small, e.g., ? = [?1, 1], the crossover learning brings moderate improvement compared to the baseline. This makes sense because the sampled two frames are quite similar to each other when the ? is small. Under this circumstance, the crossover learning degenerates to na?ve still-image training.</figDesc><table><row><cell>Embedding</cell><cell>Loss</cell><cell>AP ? ?AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>Pair-wise</cell><cell>LCE</cell><cell>33.1 ? 0.78</cell><cell>51.9</cell><cell>34.9</cell></row><row><cell>Pair-wise</cell><cell>L F ocal</cell><cell>33.3 ? 0.72</cell><cell>52.1</cell><cell>35.0</cell></row><row><cell>Global</cell><cell>LCE</cell><cell>33.4 ? 0.27</cell><cell>53.9</cell><cell>35.7</cell></row><row><cell>Global</cell><cell>L F ocal</cell><cell>34.8 ? 0.25</cell><cell>54.6</cell><cell>37.9</cell></row><row><cell cols="5">branch adopts a lightweight fully convolutional design com-</cell></row><row><cell cols="5">pared to the fully connected design in MaskTrack R-CNN.</cell></row><row><cell cols="5">Therefore the efficiency of CrossVIS mainly comes from</cell></row><row><cell cols="5">the efficient design of crossover learning and global bal-</cell></row><row><cell cols="2">anced embedding.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Crossover Learning.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Impact of integrating CrossOver Learning (COL) and Global Balanced Embedding (GBE) into CondInst-VIS baseline.</figDesc><table><row><cell>Baseline</cell><cell>COL</cell><cell>GBE</cell><cell>AP</cell></row><row><cell></cell><cell></cell><cell></cell><cell>32.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>33.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>33.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>34.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the classification sub-task, the large amount of easy negative samples can be handled by focal loss<ref type="bibr" target="#b38">[39]</ref>. For regression and segmentation sub-tasks, only positive samples participate in training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please note that some FPS data (e.g.,<ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b29">30]</ref>) in Tab. 1 is measured using V100, which is slightly faster than measured using 2080 Ti.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This dataset is released in February, 2021. To our knowledge, there is no publicly available baseline result on this dataset by the time we submit this preprint to arXiv.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06218</idno>
		<title level="m">Yolact++: Better real-time instance segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">YOLACT: real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BlendMask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boundary-preserving mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Motchallenge: A benchmark for singlecamera multiple target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Temporal feature augmented network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual embedding learning for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03400</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-nms for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning video instance segmentation with recurrent graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03911</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Centermask : Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rog?rio Feris, and Linglin He. Video instance segmentation tracking with a modified vae architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual semantic fusion network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haosheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Spatiotemporal attention network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingmeng</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Video instance segmentation 2019: A winning approach for combined detection, segmentation, classification and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiple object tracking: A literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Quasi-dense instance similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06664</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Object detection in videos by high quality object linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Chunyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xinggang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Wenyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Wenjun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jingdong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<editor>Xiang Bai, Philip HS Torr, Serge Belongie, Alan Yuille, and Song Bai</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">AdelaiDet: A toolbox for instance-level recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://git.io/adelaidet" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">An empirical study of detection-based video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Youtubevis dataset 2021 version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/dataset/vis" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">A simple baseline for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
