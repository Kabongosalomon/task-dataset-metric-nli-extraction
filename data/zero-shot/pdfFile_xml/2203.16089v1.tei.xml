<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omni-DETR: Omni-Supervised Object Detection with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
							<email>zhaoweic@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<email>haoyng@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurumurthy</forename><surname>Swaminathan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>bschiel@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soattos@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><forename type="middle">Ai</forename><surname>Labs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>San</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename></persName>
						</author>
						<title level="a" type="main">Omni-DETR: Omni-Supervised Object Detection with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of omni-supervised object detection, which can use unlabeled, fully labeled and weakly labeled annotations, such as image tags, counts, points, etc., for object detection. This is enabled by a unified architecture, Omni-DETR, based on the recent progress on student-teacher framework and end-to-end transformer based object detection. Under this unified architecture, different types of weak labels can be leveraged to generate accurate pseudo labels, by a bipartite matching based filtering mechanism, for the model to learn. In the experiments, Omni-DETR has achieved state-of-the-art results on multiple datasets and settings. And we have found that weak annotations can help to improve detection performance and a mixture of them can achieve a better trade-off between annotation cost and accuracy than the standard complete annotation. These findings could encourage larger object detection datasets with mixture annotations. The code is available at https://github.com/amazonresearch/omni-detr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most of the successes of recent object detection are attributed to the large-scale well-established object detection datasets <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b37">39]</ref>, which have accurate and complete detection annotation (category and bounding box or segmentation mask) for every object of interest in an image. In general, complete and accurate detection annotation is very expensive. For example, complete annotation of a single image of MS-COCO <ref type="bibr" target="#b27">[29]</ref> takes about 346 seconds, 76.5 seconds on each category elimination and 269.5 seconds on accurate bounding box localization, on average <ref type="bibr" target="#b34">[36]</ref>  <ref type="bibr" target="#b0">1</ref> . Given this expensive cost, it is very difficult to scale up the data size. For example, OpenImages consisting of 9 million images <ref type="bibr" target="#b24">[26]</ref> used a combination of machine annotation and human verification to reduce the annotation cost. The ques-Work done during internship at Amazon. ? Corresponding author. <ref type="bibr" target="#b0">1</ref> Not accurate numbers from <ref type="bibr" target="#b27">[29]</ref> but rough estimation from <ref type="bibr" target="#b34">[36]</ref>.  tion is, do we need accurate and complete annotation which is expensive to achieve strong detection performances? There are many weaker forms of object annotation as shown in <ref type="figure" target="#fig_1">Figure 1</ref> (top), e.g., points, tags, counts, etc., but they are not well explored in the literature and the majority of the object detection frameworks are designed to be used with complete detection annotations. One of the main reasons for this is that using weaker forms of annotation has not shown promising results yet. For example, the performance of weakly supervised object detection (WSOD) <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b43">45]</ref> lags in performance compared to standard supervised detection using complete annotations. In addition, UFO 2 <ref type="bibr" target="#b34">[36]</ref>, as the first work in omni-supervised object detection (OSOD), has shown that using additional weak annotations only has marginal gains. In this paper, however, we will show that, weak annotation can help to improve detection performance and achieve better cost-accuracy trade-off.</p><p>Towards this, we propose a unified architecture for OSOD, Omni-DETR, which can work with different types of weak annotations, including image tags, object counts, points, loose bounding boxes without tags, etc., or a mixture of them. It is built on recent progresses on student-teacher based semi-supervised object detection (SSOD) <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b44">46]</ref> to better leverage the data even if it is unlabeled, and the end-to-end detection architecture of <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b51">53]</ref> with no heuristic detection procedures, like proposal detection, nonmaximum suppression, thresholding, etc. The weak ground truth labels are used to filter the teacher predictions to generate pseudo labels for the student to learn. We formulate the pseudo-label filtering as a bipartite matching problem between the sets of predictions and available weak ground truths, and propose a unified pseudo-label filtering strategy to accommodate any form of weak annotations.</p><p>Omni-DETR provides a unified framework to explore different weak forms of object annotations. With this framework, we have found that 1) weak annotations can bring additional gains even on a strong baseline; and 2) a mixture of weak and complete annotations can achieve better accuracycost trade-offs than just using complete annotations. As seen in <ref type="figure" target="#fig_1">Figure 1</ref> (bottom), our Omni-DETR achieves better results than a standard supervised and a stronger semisupervised detection baseline. In addition, some annotation forms are more suited than the others depending on the dataset characteristics. For example, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>, annotating accurate bounding boxes is difficult for Bees <ref type="bibr">[3]</ref> and CrowdHuman <ref type="bibr" target="#b38">[40]</ref> datasets, since objects are small and very crowded. However, it is easier to annotate with points for these datasets. Similarly, annotating accurate categories is difficult for Objects365 <ref type="bibr" target="#b37">[39]</ref> as there are too many categories (365), but annotating just the bounding boxes is relatively easy and cheap. Omni-DETR can accommodate all these different cases and help to reduce the cost of annotating such datasets, encouraging a larger scale of object detection datasets with mixed annotations.</p><p>Our contributions are summarized as: 1) a unified framework, Omni-DETR, that can accommodate various forms of object annotations or a mixture of them. 2) a novel and unified pseudo label filtering strategy, based on bipartite matching; 3) experimental findings that show weak annotations can provide additional gains and achieve better accuracycost trade-off than the standard full detection annotations; 4) the empirical exploration of optimal annotation mixtures for a fixed annotation budget, showing that the optimal mixture depends on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised Object Detection is a fundamental problem in computer vision <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b46">48]</ref>. Most detection frameworks can be categorized into two groups: twostage <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35]</ref> v.s. single-stage detectors <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b46">48]</ref>. These detectors usually have some heuristic steps, e.g., proposal detection, thresholding, non-maximum suppression, etc. More recently, <ref type="bibr" target="#b5">[7]</ref> proposed the DETR framework based on transformer <ref type="bibr" target="#b47">[49]</ref> for end-to-end detection. It formulates detection as a set-to-set prediction problem, eliminating some of the previous heuristics and enabling a simpler detection pipeline. The subsequent Deformable DETR <ref type="bibr" target="#b51">[53]</ref> improved on the slow training convergence of DETR and achieved better detection performances. Our Omni-DETR is also based on this end-to-end framework, which is now extended to support various forms of annotations.</p><p>Semi-Supervised Object Detection (SSOD) tries to improve detection performances by using additional unlabeled data <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b50">52]</ref>. A prevalent SSOD paradigm is to use a multi-stage self-training pipeline <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b48">50]</ref>: 1) train model on labeled data; 2) generate pseudo labels on unlabeled data; 3) retrain model on both labeled and pseudo-labeled data; 4) repeat this process if needed. Some recent works <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b44">46]</ref> have shown great progress by resorting to an online pipeline. <ref type="bibr" target="#b21">[23]</ref> leverages the consistency regularization between two different augmented views of a single image. <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b44">46]</ref> rely on a mean-teacher framework <ref type="bibr" target="#b45">[47]</ref>, where the teacher generates online pseudo labels for the student to learn. Omni-DETR is also based on this mean-teacher framework, but uses different weak annotations to generate accurate pseudo labels.</p><p>Weakly-Supervised Object Detection (WSOD) aims to reduce detection annotation efforts by leveraging cheaper weakly labeled data. Most works only use a single type of weak annotations, e.g. image-level tags <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b43">45]</ref> or instance-level points <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b31">33]</ref>, and usually formulate WSOD as a multiple instance learning (MIL) problem. This, however, has very limited success so far. Some recent works study weakly semi-supervised object detection (WS-SOD) <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15]</ref>, using a small amount of fully labeled data and a large amount of additional weakly labeled data. This has shown more promising results than WSOD. In general, different types of weak annotations require specific detection algorithms, e.g., <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref> for WSSOD with tags and <ref type="bibr" target="#b7">[9]</ref> for WSSOD with points. The proposed Omni-DETR is closely related to WSSOD but can accommodate various weak annotations instead of a single one.</p><p>Omni-Supervised Object Detection (OSOD) combines different forms of annotations to improve detection. It was first proposed in UFO 2 <ref type="bibr" target="#b34">[36]</ref>, which is based on the Faster R-CNN <ref type="bibr" target="#b33">[35]</ref> framework and formulates OSOD as a multitask learning problem. However, UFO 2 has only shown very marginal improvements for the addition of weak annotations, and suggested that, for a fixed annotation budget, the best choice is still full annotation. However, our experiments of Omni-DETR have the opposite observations: weak annotations are helpful and a mixture of annotations is a better solution than full annotation given a fixed annotation budget. <ref type="table">Table 1</ref> summarizes the related works of object detection using different types of annotations, and our Omni-DETR is a more universal framework on annotation formats than the previous works.</p><p>Object Detection Data Annotation is known to be an expensive and tedious task <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b37">39]</ref>, that requires annotators to choose the right category and localize the accurate bounding box for each object. For example, <ref type="bibr" target="#b42">[44]</ref> reports an average annotation time of 35 seconds for a high-quality bounding box. The total estimated time with associated categories per COCO image is 346 seconds <ref type="bibr" target="#b34">[36]</ref>. This high annotation cost prevents the detection dataset from being scaled up, in terms of the number of images, classes and objects. Several strategies have been used to reduce the cost. For example, Caltech Pedestrian <ref type="bibr" target="#b10">[12]</ref> interpolates annotations between two video frames, Open-Images <ref type="bibr" target="#b24">[26]</ref> uses machine prediction first and then human verification next, LVIS <ref type="bibr" target="#b17">[19]</ref> only annotates a few positive/negative classes for an image instead of complete category annotation, etc. Other approaches try to relax the accurate bounding box annotation, by proposing to use relatively loose bounding boxes <ref type="bibr" target="#b30">[32]</ref> or near-center points <ref type="bibr" target="#b31">[33]</ref>. In this work, using Omni-DETR, we empirically find that accurate and complete detection annotation is not the most economical, and a mixture of annotations can achieve a better trade-off between accuracy and cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Omni-DETR</head><p>We at first introduce the overall framework of Omni-DETR in this section and then the unified pseudo-label filtering for various weak annotations in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Omni-labels</head><p>Omni-DETR is a unified framework to combine fully and weakly labeled data. It assumes the availability of a fully labeled and a weakly labeled dataset. The fully la-</p><formula xml:id="formula_0">beled dataset D l = {(x l i , y l i )} N l i=1</formula><p>, where x l i is the i-th image and y l i = {(b i,j , c i,j ) ? R 4 ?{1, 2, ..., C}} Bi j=1 the corresponding label, composed by B i pairs of 1) four coordinate bounding boxes b i,j and 2) corresponding classes c i,j , assigns class label c i,j to the object localized by bounding </p><formula xml:id="formula_1">box b i,j . The weakly labeled data, D o = {(x o i , y o i )} N o i=1 , y o i</formula><p>can consist of any of the annotations introduced in the following. We omit the image index i in subsquent notations, for notational simplicity, and term the fully labeled dataset as labeled and the weakly labeled dataset as omni-labeled.</p><p>Omni-DETR supports any of the blow annotation forms y, or a mixture of them, as weak annotations for image x. None (None) y = ?. No annotation for image x. Tags (TagsU) y = {c j } M j=1 , which is a list of image-level classes <ref type="bibr" target="#b1">2</ref> . M is the number of tags. In the examples of <ref type="figure" target="#fig_1">Figure  1</ref> (top), M = 2, c 1 is "horses" and c 2 is "sheep". Tags with counts (TagsK), y = {(c j , n j )} M j=1 , where n j is the count number of objects of class c j . In <ref type="figure" target="#fig_1">Figure 1</ref> (top), c 1 is "horses" and n 1 = 1, while c 2 is "sheep" and n 2 = 3. Points without tags (PointsU), y = {p j ? R 2 } P j=1 , where p j is a point annotation for an object, e.g., the geometric center of the object or a random point inside the region of support of the object in the image, and P the number of points. In <ref type="figure" target="#fig_1">Figure 1</ref> (top), four point annotations identify four objects without class information. Points with tags (PointsK), y = {(p j , c j ) ? R 2 ? {1, 2, ..., C}} P j=1 . In addition to PointsU, the label of each point is also known. In <ref type="figure" target="#fig_1">Figure 1</ref> (top), the points and labels for three sheeps and a horse are annotated. Boxes without tags (BoxesU), y = {b j ? R 4 } B j=1 . The standard bounding box annotation but removing the class information. B is the number of boxes.</p><formula xml:id="formula_2">Extreme Clicking Box (BoxesEC), y = {b j ? R 4 } B j=1</formula><p>, where b j is a box derived from the annotation of extreme points of the object. This was introduced in <ref type="bibr" target="#b30">[32]</ref> and has much less annotation cost (5?) but only slightly worse quality than BoxesU annotation. <ref type="figure">Figure 3</ref> gives an overview of the Omni-DETR framework. Motivated by the recent successes of student-teacher frameworks for semi-supervised learning <ref type="bibr" target="#b39">[41]</ref> and SSOD <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b44">46]</ref>, our Omni-DETR is also composed of a student detection network F s (x; ? s ) and a teacher detection network F t (x; ? t ). For the omni-labeled data (x o , y o ) ? D o , two views of the image x o are generated by a strong and a weak augmentation, x o,s and x o,w , respectively. The weakly augmented view x o,w is forwarded through the teacher to produce the detection prediction? t = F t (x o,w ; ? t ), consisting of class prediction? cls and bounding box prediction y box . The predictions? t are then passed to a pseudo label filter T together with the available omni-labels y o . The filter generates the pseudo-labels? t = T (? t ; y o ), which are used to supervise the learning of the student on the strong augmentation x o,s . The pseudo-label filtering details will be discussed in Section 4. Here the weak/strong augmentation is only applied to teacher/student because the weak annotation can induce more accurate pseudo-labels for the teacher and the strong augmentation can make the learning of the student more challenging. For the labeled data (x l , y l ) ? D l , strong and weak augmentations are also generated, (x l,s , y l,s ) and (x l,w , y l,w ), and both are feedforwarded to the student network for learning only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified Framework</head><p>Only the student F s is optimized by standard SGD with the overall loss,</p><formula xml:id="formula_3">L s = i L(x l,s i , y l,s i ) + L(x l,w i , y l,w i ) + i L(x o,s i ,? t i ), (1) where L = ?L cls + ?L box<label>(2)</label></formula><p>is the weighted sum of classification loss L cls and bounding box regression loss L box , and ? and ? are the corresponding weights. The teacher F t is updated by the exponential moving average (EMA) from the student <ref type="bibr" target="#b45">[47]</ref>,</p><formula xml:id="formula_4">? t ? k? t + (1 ? k)? s ,<label>(3)</label></formula><p>where k is empirically set to a number close to 1, e.g., 0.9996. This EMA updated teacher can be seen as a temporal ensemble of student models along the training trajectories, which makes it more robust and able to generate more accurate pseudo-labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b20">22]</ref>. Note that this reduces to the Unbiased Teacher (UT) framework <ref type="bibr" target="#b29">[31]</ref> proposed for SSOD when no omni-labels are available, and only unlabeled data is used. It follows that UT is a baseline for Omni-DETR and the addition of any weak annotations should improve the accuracy of this SSOD baseline. This establishes a much stronger baseline than any previous weakly supervised object detection (WSOD) and weakly semi-supervised object detection (WSSOD) work <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b43">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detection Architecture</head><p>Although there is no constraint on which detector to use, DETR is chosen here because it has removed many heuristic procedures in the traditional detection frameworks <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35]</ref>. This is necessary for Omni-DETR since it needs to accommodate many different kinds of annotations. <ref type="figure">Figure 3</ref>. The framework of Omni-DETR, which is based on the student-teacher framework. The omni-label is used to filter the predictions of the teacher network, by a unified pseudo-label filter, to generate pseudo-labels for the student network to learn. The omni-label can be any annotation introduced in Section 3.1. DETR <ref type="bibr" target="#b5">[7]</ref> is a transformer <ref type="bibr" target="#b47">[49]</ref> based end-to-end object detection framework. In DETR, a standard CNN backbone is at first applied to a given image, and the output features are flattened and followed by an encoder transformer. In order to detect objects, the decoder transformer is applied by taking the object queries as input and cross-attending the encoded vision features, to generate the final object predictions with class and bounding box predictions? cls and y box . Then a set-to-set alignment is enabled by using Hungarian matching <ref type="bibr" target="#b23">[25]</ref> between the object predictions and the ground truth objects. After matching each hypothesis and ground truth, standard learning is used to optimize the classification task (with multi-class cross-entropy loss) and bounding box regression task (with generalized IoU and L1 loss). Due to the slow convergence of original DETR, we use Deformable DETR <ref type="bibr" target="#b51">[53]</ref> for faster convergence speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>The overall model is trained with two stages: 1) burn-in training of the student network alone on the labeled data; 2) student-teacher training on both labeled and omni-labeled data where the teacher model is initialized by duplicating the burn-in student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pseudo-label Filtering</head><p>As shown in <ref type="figure">Figure 3</ref>, the pseudo-label filter is a key component to leverage weak annotations in our Omni-DETR. It takes in both detection predictions and available omni-labels of an omni-labeled image, and then generates the pseudo-labels to supervise the learning of the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simple Pseudo-label Filtering</head><p>At first, we present some simple pseudo-label filtering approaches for different weak annotations. Object detectors usually output a vector of confidence scores s j ? [0, 1] C per detected bounding box b j . A popular approach to generate pseudo-labels is to simply threshold these scores. If only tag supervision (TagsU) is available, pseudo-labels can be generated by thresholding the confidence s cj j of the ground truth class c j . For a ground truth class if there is no prediction greater than the confidence threshold, the top-1 prediction is retrieved as the pseudo-label for that class. When count supervision is available (TagsK), with n j counts for each ground truth class c j , this can be extended to selecting the top n j predictions for class c j . When point supervision is available (PointsU), a similar strategy is to choose the predicted bounding boxes that contain the ground truth points. If additional tag supervision is available (PointsK), this can be extended to choosing candidates whose class prediction matches the point tag <ref type="bibr" target="#b34">[36]</ref>. However, these empirical filtering rules are specific to each type of weak supervision and do not provide a unified pseudo-labeling solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Unified Pseudo-label Filtering</head><p>Next, we introduce the proposed unified approach 3 . Formally, the filter is applied to? = T (?; y o ) where? = {? cls ,? box } is the prediction of the teacher network, wit? y cls and? box being the class and box predictions, respectively. Here, we define?</p><formula xml:id="formula_5">cls = [z 1 , ..., z K ] T ? R K?C and y box = [b 1 , ...,b K ] T ? R K?4 ,</formula><p>where z k is a vector of logits (the network output vector before the softmax),b k the associated bounding box prediction, and K the number of object queries. y o is the omni-label of section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">No Annotation</head><p>When no annotation is available (None), pseudo-labels are derived from confidence scores, as used in SSOD <ref type="bibr" target="#b29">[31]</ref>. Specifically,? cls is fed into a softmax layer, to produce [p 1 , ..., p K ] T ? R K?C , where p k is the probabilities over C class for query k. The predicted class of the k-th prediction is defined as? k = arg max c p c k and the confidence score as the associated probability s k = p? k k . The threshold ? is used to filter low-confidence predictions, by collecting the prediction index set I = {k|s k &gt; ?, k ? [1, K]}. With the bounding box predictions? box = [b 1 , ...,b K ] T ? R K?4 , the pseudo-labels are then defined as {(b k ,? k )|k ? I} whereb k is a pseudo box and? k its pseudo class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Weak Annotations</head><p>Motivated by DETR <ref type="bibr" target="#b5">[7]</ref>, we formulate the pseudo-label filtering problem as a bipartite matching problem, between the K teacher predictions and the available G ground truth omni-labels {g i } G i=1 (G &lt; K). Specifically, we search for a permutation? ? ? K of K elements such that ? = arg min</p><formula xml:id="formula_6">??? K K i L match (g i ,? ?(i) ),<label>(4)</label></formula><p>where L match (g i ,? ?(i) ) is an annotation-specific pair-wise matching cost between ground truth omni-label g i and 3 Although specific design is still needed for each weak annotation, here we use "unified" because the filtering of different weak annotations can be interpreted by a unified bipartite matching mechanism. teacher prediction? of index ?(i). The optimal assignment is enabled with Hungarian matching <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b23">25]</ref>, assigning pseudo-labels</p><formula xml:id="formula_7">{(b * i , c * i )} G i=1 . Here, b * i ? R 4</formula><p>is the pseudo bounding box and c * i the pseudo class, depending on the weak annotation type. Next, we present the specific L match (g i ,? ?(i) ) and (b * i , c * i ) for different annotations. TagsU When the image-level ground truth tags are available, y o = {c j } M j=1 , where M is the number of tags and c j is the j-th class, but the exact number of objects per class is not known, the matching of (4) is not directly applicable. To address this problem, the count n j of tag c j , is first predicted with,</p><formula xml:id="formula_8">n j = max(1, |{k|p cj k &gt; ?, k ? [1, K]}|),<label>(5)</label></formula><p>where p cj k is the probability of assigning the k-th prediction to class c j , and | ? | is the set cardinality. The predicted count is the number of predictions that pass the confidence threshold, if any, and set to one otherwise. This is because there is at least one object per ground truth tag. In order to accommodate the matching of (4) regarding G ground truth, we re-write the ground truth set as</p><formula xml:id="formula_9">{g i } = {c i } G i=1</formula><p>with G = M j n j and n j repetitions of each tag c j . Note that, for different i, c i could be the same if there are multiple objects per class. L match (g i ,? ?(i) ) in (4) is defined as</p><formula xml:id="formula_10">L t match (g i ,? ?(i) ) = 1 ? p ci ?(i) .<label>(6)</label></formula><p>After bipartite matching of (6), the pseudo-labels are</p><formula xml:id="formula_11">{(b? (i) , c i )} G i=1 , where?(i) ? {1, .</formula><p>.., K} is the matched index to the i-th ground truth omni-label.b? (i) is the predicted box and c i is the available ground truth class.</p><p>TagsK When the tags and their counts are known, y o = {(c j , n j )} M j=1 , where n j is the number of objects of class c j . There is no need to predict the counts anymore. The optimal matching can be computed with <ref type="bibr" target="#b4">(6)</ref>, to obtain the pseudo-labels</p><formula xml:id="formula_12">{(b? (i) , c i )} G i=1 . PointsU When points of objects are known, y o = {g i } = {p i ? R 2 } G i=1 ,</formula><p>where p i is a point and G points in total. The matching cost is defined as</p><formula xml:id="formula_13">L p match (g i ,? ?(i) ) = (d i,?(i) + e i,?(i) ) * ? i,?(i) ,<label>(7)</label></formula><p>where d i,?(i) is the L2 normalized distance, between the center of the predicted box and ground truth point, normalized to [0, 1] across K ? G distances by min-max normalization, and e i,?(i) = 1?s ?(i) , where s ?(i) is the confidence score of ?(i)-th prediction. Finally, ? i,?(i) is an indicator: ? i,?(i) = 1 if the i-th ground truth point is inside the ?(i)th predicted box, otherwise +?. This cost encourages the selected predicted box to cover the ground truth point with small geometric distance and high confidence. The pseudolabels {(b? (i) ,? i )} G i=1 are obtained by optimizing <ref type="formula" target="#formula_13">(7)</ref> via Hungarian matching.</p><p>PointsK When both the point and tag of an object are known, the ground truth is</p><formula xml:id="formula_14">y o = {g i } = {(p i , c i ) ? R 2 ? {1, 2, ..., C}} G i=1 .</formula><p>We combine <ref type="formula" target="#formula_10">(6)</ref> and <ref type="formula" target="#formula_13">(7)</ref> linearly as the overall matching cost,</p><formula xml:id="formula_15">L match (g i ,? ?(i) ) = ?L t match + (1 ? ?)L p match ,<label>(8)</label></formula><p>where ? is the trade-off coefficient, to obtain the pseudo-</p><formula xml:id="formula_16">labels {(b? (i) , c i )} G i=1</formula><p>. Boxes When the bounding boxes are known but without classes,</p><formula xml:id="formula_17">y o = {g i } = {b i ? R 4 } G i=1</formula><p>, we follow the bounding box cost definition of <ref type="bibr" target="#b5">[7]</ref>,</p><formula xml:id="formula_18">L b match (g i ,? ?(i) ) = ? iou L iou (g i ,b ?(i) )+? L1 ||g i ?b ?(i) || 1 ,<label>(9)</label></formula><p>where L iou is the generalized IoU loss <ref type="bibr" target="#b35">[37]</ref>, to obtain the pseudo-labels {(b i ,?? (i) )} G i=1 . Although BoxesEC and BoxesU have different box qualities, they are not differentiated by their matching costs here.</p><p>The discussion above unifies pseudo-label filtering for all weak annotations as a bipartite matching problem, performed by global optimization on a set-to-set matching problem. This will be shown, in experiments, to outperform the heuristic choice of Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Omni-DETR is extensively evaluated on different datasets and settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Datasets: MS-COCO <ref type="bibr" target="#b27">[29]</ref>, PASCAL VOC <ref type="bibr" target="#b11">[13]</ref>, Bees [3], CrowdHuman <ref type="bibr" target="#b38">[40]</ref> and Objects365 <ref type="bibr" target="#b37">[39]</ref> are used for evaluation. To evaluate and compare Omni-DETR to methods addressing different problems, we use the multiple experimental settings of <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b34">36]</ref>. (I) COCO-standard: we randomly sample {1, 2, 5, 10, 20, 30}% of data from COCO train2017 as the fully labeled training data and use the rest as the omni-labeled training data. (II) COCO-35to80: we use the COCO-35 (a.k.a. valminusminival), a subset of 35K images of COCO train2017 as the fully labeled data and COCO-80, the COCO train2014 of 80K images, as the omni-labeled data. (III) VOC-07to12: we use the VOC07 trainval as the fully labeled set and the VOC12 trainval as the omni-labeled set. On COCO, model performance is evaluated on the COCO val2017, and VOC07 test on VOC. Implementation details: For fair comparison, ResNet-50 pretrained on ImageNet <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b19">21]</ref> is used as the backbone. The confidence threshold ? = 0.7. For strong augmentation, following <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b51">53]</ref>, we apply random horizontal flipping, random resizing, random size cropping, color jittering, grayscale, Gaussian blur, and cutout patches. For weak augmentation, only random horizontal flipping is used. To mimic point annotations, we follow <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b34">36]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on Single Annotation</head><p>Under the setting of COCO-standard-10%, we first evaluate Omni-DETR for individual weak annotations in <ref type="table" target="#tab_1">Table  2</ref>, to study the effect of each weak annotation. The baseline is the standard supervised learning on 10% labeled data. A few observations are available. First, the additional 90% unlabeled data improves the baseline by 4.4% when using semi-supervised learning. Annotating extra weak labels always enhances the performances by 1.7 ? 4.4%. Second, among all annotation formats, PointsU has the smallest benefit and BoxesU the largest. Third, Extreme Clicking boxes (BoxesEC) is economical: only 0.3% worse than the highquality boxes of BoxesU but 5 times less costly. Fourth, count annotation provides a gain of 0.5% over tag annotation (TagsU v.s. TagsK). Fifth, adding tag information to points (PointsU v.s. PointsK), leads to 1.6% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the State-of-the-art</head><p>Omni-DETR is compared with previous works under different settings. In this section, "Supervised" is the supervised Deformable DETR baseline trained on the available fully-labeled data only. SSOD When no annotation is available, the Omni-DETR becomes a standard semi-supervised detector, which is compared to other SSOD methods in  <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b34">36]</ref>. It is worth noting, in <ref type="table">Table 5</ref>, that our model trained on 5% (10%) labeled data achieves 31.7 (35.9) mAP, which is higher than <ref type="bibr" target="#b12">[14]</ref> trained on 10% (20%) labeled data. In <ref type="table">Table 4</ref>, we improve the supervised baseline by 5.1% by using tags, whereas the improvement is 0.3% for UFO 2 . Our absolute improvement over UFO 2 is 10%. WSSOD with points When additional point with tag annotation is available for SSOD, the problem becomes WS-SOD with points <ref type="bibr" target="#b7">[9]</ref>. Omni-DETR is compared with Point DETR and UFO 2 , in <ref type="table">Table 6</ref> and 4 respectively. It can be observed in <ref type="table">Table 6</ref> that we outperform Point DETR by a significant margin (5 ? 7%). In <ref type="table">Table 4</ref> tions, and the rest on PointsK. As shown in <ref type="table">Table 7</ref>, Omni-DETR still has consistently significant gains over UFO 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>We ablate some key components of our Omni-DETR on COCO-standard-10% setting. Comparison with simple filters We compare the proposed unified pseudo-label filter with the simple filter of Section 4.1. <ref type="table">Table 8</ref> shows that the proposed unified filter is better than the simple and heuristic filter under various settings. This is because the matching in the unified filter is a global solution by Hungarian algorithm, instead of a heuristic one as in the simple filter. Confidence threshold The confidence threshold ? used in Section 4.2.1 and (5) determines the trade-off between the quality and quantity of the pseudo-labels. A larger ? leads to fewer examples passing the threshold but with high quality, but a smaller ? allows more examples passing but more likely false positives. The results of different values of ? (0.5 to 0.9) are reported in <ref type="table">Table 9</ref>. ? = 0.7 is the best. The effect of ? The hyperparameter ? of (8) balances the importance of positions and tag labels during the matching for point annotation. Its effect is evaluated in <ref type="table" target="#tab_3">Table 10</ref>, and ? = 0.5 is the best. Pseudo bounding box In Unbiased Teacher <ref type="bibr" target="#b29">[31]</ref>, pseudo bounding boxes are not used for learning from unlabeled data since the class confidence score does not reflect the goodness of the bounding box. However, we found that pseudo bounding boxes are useful and provide consistent improvement of 0.5 ? 1% in our experiments. One possible  reason for the improvement is the higher quality of Omni-DETR pseudo bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Budget-Aware Omni-Supervised Detection</head><p>We also empirically study the trade-off between annotation cost <ref type="bibr" target="#b3">5</ref> and accuracy of several annotation policies. Here, annotation policy refers to the strategy for mixing different annotation formats. Five diverse datasets with different characteristics are tested. The annotation cost, in seconds per image, for each type of annotation is shown in <ref type="table">Table 11</ref>, following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b42">44]</ref>. For each dataset, we attempt to identify the best annotation policy, given different budgets. SSOD is used as the baseline because, when the entire budget is used on standard full object annotation (bounding boxes and tags), the remaining data is considered unlabeled, which is the standard SSOD setup and widely adopted in practice. Next, we consider different weakly semi-supervised settings with 10% data fully labeled and the remaining 90% labeled with different weak annotations. Finally, two choices of mixture annotation are tested under three budgets per dataset, to show the omni-supervised results. We only tested the combination of weak annotations that are better than the SSOD baseline, and the combination ratio was decided manually so that the mixture and full annotations had similar costs, for a fair comparison. Please see more details in the supplementary.</p><p>The results are summarized in <ref type="figure" target="#fig_4">Figure 4</ref>. The grey lines and green dots are the SSOD and WSSOD results, respectively, whereas red/blue/purple lines are for OSOD variants. It can be found that the OSOD results are higher than the strong SSOD baseline in general. For a target accuracy, the mixture annotation strategy can help reduce cost, and for a given cost, SSOD can improve accuracy. For example, on Bees at the accuracy of 40% mAP, using mixture annotations of TagsK and PointsU can save the cost of about 15 hours from the standard detection annotation (25 hours v.s. 40 hours). On CrowdHuman, for the cost of about 330 hours OSOD improves the strong SSOD baseline mAP by ?4%. These findings support our claim that weak annotations are useful and can achieve a better cost-accuracy trade-off than standard detection annotation. <ref type="bibr" target="#b3">5</ref> Only human annotation costs are considered, and other costs are ignored if there is any. Some additional interesting observations are also available in <ref type="figure" target="#fig_4">Figure 4</ref>. First, the green upside-down triangles, rectangles and red/blue lines are all higher than the strong SSOD baseline. This suggests that annotating points (PointsU) and/or Extreme Clicking boxes (BoxesEC) is a better choice than standard complete annotation. Second, the green plus is above the reference on Bees and Crowd-Human, but not for the other three datasets, indicating that count annotation (TagsK) is useful for datasets with dense objects. Finally, weak annotations such as TagsU (green cross), TagsK (green plus) and PointsK (green triangle), are far below the SSOD baseline on datasets like VOC, COCO and Objects365. This suggests that tags are not a good annotation format for datasets with large number of classes, where annotating tags is expensive. In general, the optimal annotation choice is quite dataset-specific, depending on characteristics such as number of categories, number of objects per image, object size, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a unified framework for omnisupervised object detection, which can use different types of weak annotations. With this unified framework, we have found weak annotations are helpful and a mixture of them can achieve a better cost-accuracy trade-off. Limitations and Potential Negative Social Impact: It is unclear whether these findings of this paper are still consistent on larger datasets, since we have not explored dataset size beyond COCO (?120K images) yet. In addition, Omni-DETR could potentially increase the risk of improper use of detection systems, because it makes good detectors more accessible with fewer annotation efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Implementation Details</head><p>In this supplement, we show the details that are not presented in the main paper due to the page limitation. <ref type="table">Table 11</ref> In this section, we explain how the numbers in <ref type="table">Table 11</ref> are calculated. It is non-trivial to compute the labeling time for each type of annotation because it depends on several factors like the annotation tools or platforms, the quality requirement of the annotations, the crowdsourcing protocol used, etc. In our work, we mainly follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b42">44]</ref> for the calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Annotation Cost Calculation in</head><p>We denote the averaged number of categories per image as C avg , the averaged number of instances per image as I avg , and the overall number of categories for a dataset as C. We first list the statistics information for each dataset in <ref type="table" target="#tab_1">Table 12</ref>. Then we consider the labeling time calculation for each weak annotation. TagsU According to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">36]</ref>, collecting image-level class labels takes ? 1 second per category per image. Thus, the expected annotation time is equal to C on COCO, VOC, Objects365. On Bees or CrowdHuman, TagsU is not considered since they only have one category.</p><p>PointsU According to <ref type="bibr" target="#b1">[2]</ref>, it takes 0.9 seconds on average to annotate one point. Thus, the time is 0.9 ? I avg .</p><p>PointsK We follow the computation of <ref type="bibr" target="#b34">[36]</ref>. It takes 1 second to eliminate every non-existing class, and C ? C avg seconds in total. <ref type="bibr" target="#b34">[36]</ref> reports that annotators take a median of 2.4 seconds to click on the first instance of a class and 0.9 seconds for every additional instance. Thus the total labeling time is (C?C avg )+2.4?C avg +0.9?(I avg ?C avg ) on COCO, VOC, Objects365. On Bees or CrowdHuman, the time is equal to that of PointsU because since they only have one category.</p><p>TagsK It takes about 1 second to count a number <ref type="bibr" target="#b8">[10]</ref>. Thus, on COCO, VOC, Objects365, the estimated time A.2. Datasets and Splitting Details in Section 5.5</p><formula xml:id="formula_19">is (C ? C avg ) + 1.0 ? C avg + 1.0 ? (I avg ? C avg ) = C + I avg ? C avg .</formula><p>In the paper, for each dataset, <ref type="figure" target="#fig_4">Figure 4</ref> shows two different mixture policies, and three different budgets for each mixture policy. <ref type="table">Table 13</ref> reports the detailed mixture percentages and other information. The training set used to be split into labeled and omni-labeled data is presented as follows for each dataset. Bees [3] The total number of images is 3,596. Since Bees does not split the dataset officially, we randomly sample 80% images as the training set, after removing the broken images. The model is evaluated on the rest 20% data. CrowdHuman <ref type="bibr" target="#b38">[40]</ref> The official training set of 15,000 images is split by different percentages for the omnisupervision experiments. The model is evaluated on the official validation set. VOC <ref type="bibr" target="#b11">[13]</ref> We combine VOC07 trainval set and VOC12 trainval set as the training set with 22,136 images in total, which is used for the omni-supervision experiments. The model is evaluated on the VOC07 test set. COCO <ref type="bibr" target="#b27">[29]</ref> COCO train2017 set of 118,287 images is used as the training set for splitting. The model is evaluated on the COCO val2017 set. Objects365 <ref type="bibr" target="#b37">[39]</ref> To have faster experiments, 93,455 images are sampled from the Objects365 official training set as the training set for the omni-supervision experiments. In the process, since this dataset is long-tailed, we ensure that there is at least one image per category. Performance is evaluated on the official validation set.</p><p>The cost number (the second column from right) in <ref type="table">Table  13</ref> is computed by considering the mixture ratio, the dataset size and the cost per image in <ref type="table">Table 11</ref>. For example, for the first row of Bees, the cost is 25 = (3596 * 0.05 * 249.9 + 3596 * 0.8 * 6.1 + 3596 * 0.15 * 50)/3600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. The Simulation of Extreme Clicking Boxes</head><p>Because Extreme Clicking <ref type="bibr" target="#b30">[32]</ref> does not release the annotations except for VOC <ref type="bibr" target="#b11">[13]</ref>, we simulate the boxes generated by Extreme Clicking for the other four datasets in  <ref type="table">Table 13</ref>. The details of omni-supervision experiments in Section 5.5.</p><p>(a) Extreme Clicking on VOC. mean is 0.83; std is 0.15; second-order moment is 0.02.</p><p>(b) Simulated Extreme Clicking on COCO. mean is 0.82; std is 0.16; second-order moment is 0.02. <ref type="figure">Figure 5</ref>. The distribution of mIoU between the BoxesEC and ground truth.</p><p>our experiments. In detail, for each dataset, Gaussian noise is added to the ground truth bounding box coordinates, such that the distribution of mean Intersection over Union (mIoU) between the simulated boxes and ground truth boxes is close to the mIoU distribution between the given Extreme Clicking boxes and the ground truth boxes on Dataset Fully (%) Burn-In <ref type="table" target="#tab_1">Epochs Total Epochs   Bees   5  600  1000  10  400  1000  20  200  1000  30  100  1000   CrowdHuman   5  400  800  10  200  500  20  100  500  30  80  500   VOC   8  300  800  10  200  500  20  100  500  30  80  200   COCO   1  400  800  5  80  500  8  60  300  10  40  200  20  20  150  30  15  100   Objects365   8  130  300  10  100  200  20  50  200  25  40  200  30</ref> 30 200 VOC. The value of mIoU can be controlled by varying the covariance matrix of the Gaussian noise. <ref type="figure">Figure 5</ref> shows </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Other Implementation and Training Details</head><p>The number of epoch for Burn-In stage depends on the size of the labeled data in our experiments. The total epoch number is chosen until the training saturated. They are shown in <ref type="table" target="#tab_7">Table 14</ref>. All models of Deformable DETR are trained with total batch size of 16. For other hyperparameters, we mainly follow the settings of Unbiased Teacher <ref type="bibr" target="#b29">[31]</ref> and Deformable DETR <ref type="bibr" target="#b51">[53]</ref>. For example, in (2), weight ? = 2, ? = 5 to balance the classification loss (L cls ) and regression loss (L box ). L cls is the focal loss with default hyperparameters, and L box = 2L iou + 5L L1 combines generalized IoU loss and L1 loss. EMA smoothing constant k = 0.9996 in (3). Weights ? iou = 2, ? L1 = 5 in (9) for box matching. The object query number is K = 300 in Section 4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The top is the visualization of different forms of weak annotations, and the bottom is the trade-off comparison (accuracy v.s. annotation cost) of supervised/semi-supervised/omnisupervised detection (see Section 5.5 for more details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The potentially most suited annotation formats vary from dataset to dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy (mAP) and annotation cost trade-off. Grey lines are the SSOD baseline references. Green dots represent the WSSOD results with different weak labels. Red, blue and purple lines are the OSOD results for different mixture annotation choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the comparison of mIoU distribution of Extreme Clicking (left) and our simulation on COCO 10%Fully+90%BoxesEC setting (right) as an example. Their statistics comparison is: 1) Mean: 0.83 v.s. 0.82; Std: 0.15 v.s. 0.16; Second-order moment: 0.02 v.s. 0.02. These have shown our simulation is close to Extreme Clicking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The effects of different weak annotations on the baseline of 10% COCO-standard fully labeled data.</figDesc><table><row><cell>and randomly</cell></row></table><note>noise to their ground truth bounding box annotations, so that the resulting boxes have close distribution to that of Extreme Clicking on VOC. More details can be found in the supplementary. The detection performance is evaluated with the teacher model for all experiments. We use AP 50:95 , denoted as mAP, as the evaluation metric unless otherwise noted. The minimum size of image height and width is set to 600-pixels for faster experiments, except in experiments involving comparisons with other methods that use the stan- dard 800-pixel size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>We implemented the supervised Faster R-CNN and Deformable 1% 2% 5% 10% VOC Faster R-CNN [31, 35] 9.1 12.7 18.5 23.9 42.1 Faster R-CNN * 11.7 14.9 20.7 25.6 42.6 Deformable DETR * 11.0 14.7 23.7 29.2 46.2 STAC [42] 14.0 18.3 24.4 28.6 44.6 Unbiased Teacher [31] 20.8 24.3 28.3 31.5 48.7 Table 3. SSOD result comparison on COCO-standard and VOC-07to12. * indicates our implementation. WSSOD comparison with UFO 2 on COCO-35to80. The numbers in parentheses are gains over the supervised baseline (Faster R-CNN for UFO 2 but Deformable DETR for ours).Table 5. WSSOD with tags result comparison on COCO-standard.DETR trained only on the labeled data as the baselines. Omni-DETR achieves the best results on 5% and 10% of COCO, and VOC, and comparable results with the stateof-the-art on 1% and 2% of COCO 4 . Note that our Omni-DETR is not designed specifically for SSOD, but it still achieves competitive results. WSSOD with tags When additional tag annotation is available, SSOD becomes WSSOD with tags. We compare with the state-of-the-art methods, UFO 2<ref type="bibr" target="#b34">[36]</ref> and Fang et al.<ref type="bibr" target="#b12">[14]</ref>, on their settings. The results are reported inTable 4and 5, showing that our model consistently outperforms</figDesc><table><row><cell>Humble Teacher [46]</cell><cell cols="4">17.0 21.7 27.7 31.6 53.0</cell></row><row><cell>Omni-DETR</cell><cell cols="4">18.6 23.2 30.2 34.1 53.4</cell></row><row><cell cols="2">Supervised</cell><cell>+TagsU</cell><cell cols="2">+PointsK</cell></row><row><cell>UFO 2 [36]</cell><cell>29.1</cell><cell cols="3">29.4 (+0.3) 30.1 (+1.0)</cell></row><row><cell>Omni-DETR</cell><cell>34.3</cell><cell cols="3">39.4 (+5.1) 40.2 (+5.9)</cell></row><row><cell></cell><cell>1%</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell></row><row><cell>Supervised</cell><cell>11.0</cell><cell>23.7</cell><cell>29.2</cell><cell>33.6</cell></row><row><cell>Fang et al. [14]</cell><cell>18.4</cell><cell>27.4</cell><cell>31.3</cell><cell>35.0</cell></row><row><cell>Omni-DETR (ours)</cell><cell>20.1</cell><cell>31.7</cell><cell>35.9</cell><cell>38.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 10 .</head><label>10</label><figDesc>, when using points, Omni-DETR improves over the supervised baseline by 5.9% where UFO 2 improves by 1%, and our absolute gain over UFO 2 is 10.1%. OSOD In addition toTable 4, we also compare with UFO 2 on the X%B settings of<ref type="bibr" target="#b34">[36]</ref>, where X%B are different annotation policies using 10K images of COCO. Under a fixed budget, X% budget is spent on fully labeled annota-Table 6. WSSOD with points comparison on COCO-standard.Table 7. OSOD result comparison with UFO 2 on COCO. Effect of ? on COCO-standard-10%.</figDesc><table><row><cell></cell><cell></cell><cell>5%</cell><cell>10%</cell><cell cols="2">20%</cell><cell>30%</cell></row><row><cell>Supervised</cell><cell></cell><cell>23.7</cell><cell>29.2</cell><cell></cell><cell>33.6</cell><cell>35.2</cell></row><row><cell cols="2">Point DETR [9]</cell><cell>26.2</cell><cell>30.4</cell><cell></cell><cell>33.3</cell><cell>34.8</cell></row><row><cell cols="2">Omni-DETR (ours)</cell><cell>32.5</cell><cell>37.1</cell><cell></cell><cell>39.0</cell><cell>40.1</cell></row><row><cell></cell><cell></cell><cell>80%B</cell><cell cols="2">50%B</cell><cell>20%B</cell></row><row><cell cols="2">UFO 2 [36]</cell><cell>14.1</cell><cell>11.1</cell><cell></cell><cell>4.5</cell></row><row><cell cols="2">Omni-DETR</cell><cell>21.5</cell><cell>19.5</cell><cell></cell><cell>9.1</cell></row><row><cell cols="2">Simple Filtering</cell><cell></cell><cell cols="4">Unified Filtering</cell></row><row><cell cols="7">TagsU TagsK PointsU PoinsK TagsU TagsK PointsU PointsK</cell></row><row><cell>33.3 33.8</cell><cell>32.4</cell><cell>34.6</cell><cell cols="2">34.7 35.2</cell><cell>34.1</cell><cell>35.7</cell></row><row><cell cols="7">Table 8. Comparison with simple filters on COCO-standard-10%.</cell></row><row><cell></cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell>None</cell><cell cols="5">28.9 31.5 32.4 31.4 29.9</cell></row><row><cell cols="6">TagsU 31.1 34.1 34.7 33.9 33.1</cell></row><row><cell cols="6">Table 9. Effect of ? on COCO-standard-10%</cell></row><row><cell></cell><cell cols="2">0.00 0.25</cell><cell>0.5</cell><cell cols="3">0.75 1.00</cell></row><row><cell cols="7">PointsK 34.1 35.3 35.7 35.5 35.2</cell></row></table><note>4 [27] mentioned Unbiased Teacher is weaker for smaller batch size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 12 .</head><label>12</label><figDesc>Because this computation is derived from multi-class data domains [36], it is not applicable on Bees or CrowdHuman where they have only one category. For this reason, we simply estimate the TagsK cost of the singleclass dataset as k times of the PointsK cost. Here k is the averaged proportion of TagsK cost over PointsK cost on three multi-class datasets (VOC, COCO and Objects365), i.e., k = (21/22.9 + 84.2/88.7 + 375.8/381.7)/3 = 0.95, where these numbers are from Table 11 (the columns of TagsK and PointsK cost). Thus, the costs of TagsK on Bees and CrowdHuman are computed by 0.95 ? 6.4 = 6.1, 0.95 ? 20.4 = 19.4, respectively, where these numbers are from Table 11 (the columns of TagsK and PointsK cost).BoxesEC<ref type="bibr" target="#b30">[32]</ref> reports that it takes 7 seconds for one Extreme Clicking box, so the time is 7 ? I avg seconds. Dataset statistic. The information is provided by [3,<ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b42">44]</ref>.BoxesU Similarly, since annotating a high quality box needs 35 seconds<ref type="bibr" target="#b42">[44]</ref>, it takes 35?I avg seconds per image.Fully Following<ref type="bibr" target="#b34">[36]</ref>, the total time for full annotation is computed by (C ? C avg ) + 35 ? I avg on COCO, VOC, Objects365. On Bees and CrowdHuman, the time is equal to that of BoxesU because there is no category labeling.</figDesc><table><row><cell></cell><cell cols="5">COCO VOC Objects365 Bees CrowdHuman</cell></row><row><cell>C</cell><cell>80</cell><cell>20</cell><cell>365</cell><cell>1</cell><cell>1</cell></row><row><cell>Cavg</cell><cell>3.5</cell><cell>1.4</cell><cell>5</cell><cell>1</cell><cell>1</cell></row><row><cell>Iavg</cell><cell>7.7</cell><cell>2.4</cell><cell>15.8</cell><cell>7.14</cell><cell>22.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 14 .</head><label>14</label><figDesc>The epoch setting details of omni-supervision training</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We will also use "tag" to refer "class" interchangeably.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exponential moving average normalization for self-supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
	<note>Charless Fowlkes, Zhuowen Tu, and Stefano Soatto</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active learning with point supervision for cost-effective panicle detection in cereal crops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akshay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vikas Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seishi</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Points as queries: Weakly semi-supervised object detection by points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8823" to="8832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://www.blog.republicofmath.com/how-long-does-it-take-to-count-to-one-trillion/.11" />
		<title level="m">Counting</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wssod: A new pipeline for weakly-and semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11293</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Note-rcnn: Noise tolerant ensemble rcnn for semisupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9508" to="9517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient object annotation via speaking and pointing. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10759" to="10768" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking pseudo labels for semisupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00168</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training object class detectors with click supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6374" to="6383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ufo 2 : A unified framework towards omni-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6507</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Crowdsourcing annotations for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI workshop</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="176" to="191" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Humble teachers teach better students for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interactive self-training with mean teachers for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5941" to="5950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Instant-teaching: An end-to-end semi-supervised object detection framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
