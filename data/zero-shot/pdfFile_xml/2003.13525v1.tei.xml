<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving out-of-distribution generalization via multi-task self-supervised pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabela</forename><surname>Albuquerque</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRS-EMT</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Keskar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving out-of-distribution generalization via multi-task self-supervised pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-supervised learning</term>
					<term>out-of-distribution generalization</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised feature representations have been shown to be useful for supervised classification, few-shot learning, and adversarial robustness. We show that features obtained using self-supervised learning are comparable to, or better than, supervised learning for domain generalization in computer vision. We introduce a new self-supervised pretext task of predicting responses to Gabor filter banks and demonstrate that multi-task learning of compatible pretext tasks improves domain generalization performance as compared to training individual tasks alone. Features learnt through self-supervision obtain better generalization to unseen domains when compared to their supervised counterpart when there is a larger domain shift between training and test distributions and even show better localization ability for objects of interest. Selfsupervised feature representations can also be combined with other domain generalization methods to further boost performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods obtain impressive results on supervised learning benchmarks in computer vision, but struggle when tested on data distributions unseen during training time. This is not surprising since these models are optimized with empirical risk minimization (ERM) with the assumption that the examples from training and test sets are independently and identically drawn from the same distribution. However, machine learning models are often required to deal with a shift in data distribution or even with unseen distributions. Generalization to unseen distributions is important for building robust machine learning models. This problem is formally defined as the domain generalization problem, which aims to build models that can perform well on a target domain which is sampled from a different distribution as compared to the source domain distribution(s). Successfully solving the domain generalization problem requires learning domain-invariant feature representations that can generalize to unseen domains. Current approaches to solving the domain generalization problem in computer vision typically perform ERM on the source domains by training a feature extractor on all available data sources <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref> with or without additional strategies that enforce regularization on the feature extractor with an aim of improving generalization on the target domain. A majority of these methods start with a pretrained feature extractor on the ImageNet <ref type="bibr" target="#b8">[9]</ref> dataset, finetune the feature extractor on all-but-one datasets from a dataset collection containing much fewer samples such as VLCS <ref type="bibr" target="#b36">[37]</ref> and PACS <ref type="bibr" target="#b26">[27]</ref>, and evaluate the domain generalization performance on the held-out domain. VLCS consists of PASCAL VOC 2007, LabelMe, Caltech101, SUN datasets with a total of 10729 samples and PACS consists of Photos, Art Paintings, Cartoon, and Sketches datasets with a total of 9991 samples. These datasets, considered as unseen domains, present substantial similarity to ImageNet in that they contain images with very similar class labels. This makes the domain generalization problem easier. Moreover, supervised pretraining with ImageNet (or indeed any large scale supervised dataset) may lead to the network encoding strong class-discriminative biases for shapes <ref type="bibr" target="#b24">[25]</ref> and textures <ref type="bibr" target="#b18">[19]</ref> on the pretraining dataset that may not be useful (or even hinder) domain generalization on other domains.</p><p>An attractive alternative to using pretrained feature representations obtained from discriminative learning on datasets like ImageNet is to utilise unsupervised feature representation learning or Self-Supervised Learning (SSL). SSL aims to learn representations from unlabeled data by training feature encoders using pretext tasks-tasks that do not require per-sample human-annotated labels. For example, the Rotation task <ref type="bibr" target="#b20">[21]</ref> trains a neural network to predict the degree of rotation of an image. Feature representations obtained from SSL can come close to or even match <ref type="bibr" target="#b21">[22]</ref> the performance of supervised learning methods on tasks such as image classification, object detection, and semantic segmentation. These feature representations have also been shown to improve adversarial robustness and out-of-distribution detection for difficult, near-distribution examples <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this paper, we show that a feature extractor trained with SSL can match or exceed the performance of a fully-supervised feature extractor on the domain generalization task. Specifically, multi-task SSL-combined training of multiple self-supervision pretext tasks-is able to learn feature representations that are robust to out-of-domain samples. Experiments on PACS and VLCS dataset show that SSL perform substantially better than supervised learning on datasets such as LabelMe and Sketch that represent a significant domain shift from Ima-geNet. On these datasets, models finetuned from multi-task self-supervised feature representations are better at localizing objects from the class of interest, as compared to supervised learning. Moreover, our method can be combined with other domain generalization algorithms, like invariant risk minimization, to obtain further performance improvement. In summary, self-supervised learning has the potential to outperform fully supervised learning for training deep learning algorithms that adapt to out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Self-supervised learning, as a form of unsupervised learning, aims to train a feature encoder from unlabeled data such that the learnt encoder is transferable to other downstream tasks. The training process usually involves solving a "pretext" task with the purpose of learning good feature representations. Example pretext tasks include image inpainting <ref type="bibr" target="#b32">[33]</ref>, colorization <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, prediction of patch orderings <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref> or rotation degree <ref type="bibr" target="#b20">[21]</ref>. Some pretext tasks assign pseudo-labels to images by clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Other pretext tasks train the encoder to discriminate instances by forming contrastive loss functions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22]</ref>. Doersch and Zisserman <ref type="bibr" target="#b10">[11]</ref> show that combining multiple pretext tasks with an architecture that uses a lasso technique for factoring representations leads to performance improvement over single tasks on image classification, object detection, and depth prediction tasks. Moreover, deep encoders trained with SSL can improve robustness to adversarial or corrupted samples <ref type="bibr" target="#b22">[23]</ref> and improve few-shot learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Out-of-distribution generalization has been addressed by previous work under different settings. The domain adaptation literature focuses on strategies aimed at learning features capable of performing well under domain shift. Examples include Unsupervised Domain Adaptation <ref type="bibr" target="#b1">[2]</ref>, which assumes that unlabeled samples from target domain are available during training. The target data can be used, for example, to adapt the learnt features on the source domain to reduce the mismatch between source and target domains <ref type="bibr" target="#b17">[18]</ref>. A more general setting for out-of-distribution generalization consists of learning representations which are not adapted to a specific target domain. This is commonly referred as domain generalization and, in this case, no unlabeled target samples are assumed to be available at training time. Several recent efforts have addressed this problem by learning representations invariant to data distributions <ref type="bibr" target="#b29">[30]</ref>, incorporating domain shifts at training time <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>, or using data augmentation <ref type="bibr" target="#b37">[38]</ref>.</p><p>Recent work has adopted SSL to enforce the representation spaces learnt by neural networks to generalize to out-of-distribution data. The most pertinent related work for our paper is Carlucci et al. <ref type="bibr" target="#b2">[3]</ref> who combine a discriminatory loss for supervised learning with an auxiliary loss for solving jigsaw puzzles, an SSL task. Zhai et al. <ref type="bibr" target="#b40">[41]</ref> also study the impact of self-supervision on learning transferable features, focusing on the performance of individual SSL tasks on classification tasks that may not have the same label space. In this work, we show that carefully selected combination of self-supervised learning tasks trained with standard optimization techniques obtain comparable or better performance to supervised learning in the domain generalization setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>Let X and Y, represent the data and label space, respectively. A domain D is defined as a joint probability distribution over X ? Y. We consider a training set constructed by sampling pairs (x m , y m ) ? D Si from N different source domains D Si , and a test set (x m , y m ) sampled from a target domain D T distinct from all D Si , i = 1 : N . We are interested in learning representations that generalize to unseen target domains, while employing examples only from the source domains at training time. Specifically, we tackle the homogeneous domain generalization setting <ref type="bibr" target="#b28">[29]</ref>, where all the domains share the label space Y, i.e., the same classes are found across the source and target domains. We note that this problem is fundamentally different from the popular unsupervised domain adaptation setting <ref type="bibr" target="#b1">[2]</ref>, where the representation space is adapted to yield good performance for a specific target domain with unlabeled data sampled from this distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Supervised Learning for Domain Generalization</head><p>Our SSL approach for out-of-distribution generalization consists of two main steps: i) self-supervised pretraining, and ii) supervised fine-tuning. This setup differs from Carlucci et al. <ref type="bibr" target="#b2">[3]</ref> who finetune a representation learnt in a supervised manner using both supervised and self-supervised tasks simultaneously.</p><p>In our method, we use a feature extractor F with parameters ? is responsible for encoding the input image. We feed the encoded feature representation to a model T with parameters ? responsible for performing a specific self-supervised task. If K tasks are considered at training time, we use K task-specific modules denoted by T j , j = 1 : K, with parameters ? i . We perform preprocessing steps necessary for each task, encode the corresponding inputs, and feed the inputs to the corresponding task-specific module. We consider the K losses provided by each T j to update the feature extractor by using the average across losses provided by each task-specific module as loss function. When there is no tradeoff between optimizing the feature extractor for different tasks and the sample complexity for each task is reasonably similar, this approach is intuitively able to encode the input to an useful representation space for all tasks. Each T j is updated taking into account solely the loss corresponding to the j-th task <ref type="figure" target="#fig_0">(Figure 1</ref>-Left). In the case where different tasks are expected to converge at different rates, we sequentially train the feature extractor on different tasks, by fine-tuning the model obtained on one task using another task. After updating ? on the self-supervised tasks, we feed encoded input and outputs class probabilities for the downstream task to a model D with parameters ?. If N source domains are available at training, we find the optimal values of ? and ?, denoted by ? * and ? * respectively, by performing ERM over all source domains:</p><formula xml:id="formula_0">? * , ? * = arg min 1 N N j=1 (D(F (x i )), y i ).<label>(1)</label></formula><p>Note that ? is updated in both self-supervised and supervised fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pretext Tasks</head><p>We now describe the SSL pretext tasks employed in this paper, including a novel Gabor filter response reconstruction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gabor Filter Response Reconstruction:</head><p>A Gabor filter is a two-dimensional spatial linear filter which highlights lower-level features in an image such as edges in a specific direction and texture <ref type="bibr" target="#b16">[17]</ref>. Gabor filters are known to have similar properties as visual cortical cells of mammalian brain <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. We are specifically interested in designing an SSL task based on Gabor filters to leverage their ability to capture low-level visual information. We can combine this task with SSL approaches that try to capture low/mid-level visual information, such as rotation prediction <ref type="bibr" target="#b20">[21]</ref>, and high-level visual information, such as DeepCluster <ref type="bibr" target="#b3">[4]</ref>. Our proposed task is to train an encoder-decoder model given an input image, reconstruct the response of a Gabor filter bank considering seven distinct directions. We expect that, by learning to reconstruct the filter bank, the model will learn to capture the low-level features captured by the series of Gabor filters. In order to highlight the detected edges and to discourage the model from focusing on fine-grained information contained in the image, we subtract the original input from the filter bank response, convert it gray-scale, and apply a binary threshold on the intensity values of each pixel. The average pixel-wise binary cross-entropy between predicted and ground-truth filter responses is used as loss to update the parameters of the encoder-decoder model.</p><p>Rotation: Gidaris et al. <ref type="bibr" target="#b20">[21]</ref> proposed the rotation task which learns representations by training a model to predict the angle by which the input image is rotated. The authors argue that for a model to successfully learn to predict the angle of rotation, it needs to be able to capture information regarding pose, location, orientation, and the type of object present in the input image, as well as recognizing and localizing salient object parts in the image. Therefore, this task can be understood as taking into account both low-level features, such as orientation, as well as higher-level information, as the object type. The rotation task also forces the model to reduce the photographer bias <ref type="bibr" target="#b15">[16]</ref>, making it easier to transfer these features to real-world tasks. As in <ref type="bibr" target="#b20">[21]</ref>, we consider four rotation angles to be predicted: 0 o , 90 o , 180 o , and 270 o . DeepCluster: The DeepCluster task <ref type="bibr" target="#b3">[4]</ref> learns a feature representation by training a model to predict clustering assignments to each data point. At the beginning of each epoch, the training data is clustered in the current representation space using k-means and the labels are then re-assigned according to which cluster each data point belongs to. Convolutional layers implementing Sobel filters are employed in the model input in order to remove color information and encourage the model to capture features such as edges and shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Architecture Details</head><p>Following prior work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>, we utilize architectures based on AlexNet <ref type="bibr" target="#b25">[26]</ref> as the main backbone for all experiments. We replace instance normalization layers by batch normalization layers. When training a model with only Rotation and/or Gabor Reconstruction tasks, we decrease the number of filters on the first two convolutional blocks of AlexNet from 96 to 64 to match the architecture proposed by Gidaris et al. <ref type="bibr" target="#b20">[21]</ref>. For all tasks, we consider as representation the output of the last convolutional block, which outputs a tensor of shape (256, 6, 6), yielding a representation of size 9216 after flattening. Next, we describe the architecture details for the task-specific heads and for the downstream domain generalization task.</p><p>Rotation: For the Rotation task, the angle of rotation for the input image is predicted by a fully-connected (FC) architecture that follows the design of the classifier head from AlexNet:</p><formula xml:id="formula_1">Dropout(0.5) ? F C(9216, 4096) ? ReLU ? Dropout(0.5) ? F C(4096, 4096) ? ReLU ? F C(4096, 4).</formula><p>Gabor Reconstruction: For reconstructing the Gabor filter bank response using a 9216-size representation, we utilize as decoder an architecture designed to be a mirrored version of the AlexNet encoder. We replace the convolutional layers by transposed convolutions with the same parameters, except for the last convolutional layer, which maps the 64 channels of the input to a single-channel output, since we consider gray-scale filter responses.</p><p>DeepCluster: The task-specific head for DeepCluster was implemented following the design by Caron et al. <ref type="bibr" target="#b3">[4]</ref>, which is identical to the architecture for the rotation task head, except for the last FC layer that of size (4096, n c ), where n c corresponds to the number of clusters. We set n c to 10000, following <ref type="bibr" target="#b3">[4]</ref>. This layer is re-initialized at the beginning of each epoch, when the clusters assignments are recomputed.</p><p>Domain Generalization: Following <ref type="bibr" target="#b23">[24]</ref>, we employ a model composed by a single FC layer mapping the representation from 9216 to the number of classes specific to the domain generalization dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate that multi-task SSL is useful for achieving domain generalization, we perform four experiments which help answer the following questions: 1) How well does each model perform on each pretext task, and how is this performance affected by combining multiple tasks; 2) Are the representations learnt with SSL able to generalize to different domain shifts and which tasks are better suited for this goal; 3) Are the features learnt with SSL able to transfer across domains; and 4) What is the impact on out-of-distribution generalization when the sample diversity across the source domains is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pretext Tasks</head><p>In this set of experiments we evaluate the performance of individual tasks and of combinations of tasks. We combine tasks using two different approaches: 1) Average (AVG): The feature extractor parameters are updated with the aim of minimizing an average of the normalized losses provided by each task individually; 2) Fine-tuning (FT): The feature extractor is trained with one task until the task converges, then this task is dropped and a new task is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details:</head><p>We train each self-supervised model using the training partition of the ILSVRC 2012 datset and evaluate its performance on the validation partition to select hyperparameters. For all the tasks we use the Stochastic Gradient Descent (SGD) optimizer with Polyak's acceleration coefficient equal to 0.9. When training models with Rotation and Gabor Reconstruction tasks, we set the learning rate to 0.01, employ weight decay regularization with value 0.00005, and set the training budget to 20 epochs. The learning rate is decreased by a factor of 0.1 each 10 epochs. For DeepCluster, we perform experiments with the pretrained AlexNet released by the authors 3 and use the same hyperparameters-a learning rate of 0.05 and weight decay of 0.00001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretext Task Performance:</head><p>We report the performance of the models trained on individual tasks as well as results obtained by combining different tasks. For the Rotation task, we report the average accuracy on the validation set as metric after 20 epochs. <ref type="table">Table 1</ref> shows the performance of different models on S1: Performance of self-supervised models on pretext tasks. We measure accuracy for rotation task and reconstruction loss for the Gabor filter task. R: Rotation, G: Gabor, DC: DeepCluster. AVG: Models trained with average loss across tasks, FT: Models trained by sequentially finetuning tasks. the respective pretext tasks used at training time. The accuracy for Rotation decreases only slightly when Gabor filter response reconstruction task is added, indicating that there is no strong conflict between those two tasks. When finetuning the DeepCluster model on Rotation, we observe a large drop on accuracy, indicating that the features obtained with the DeepCluster task do not present a good initialization for Rotation. When the Gabor reconstruction task is included in training, the accuracy obtained on Rotation increases more than 7%, showing a synergy between the two tasks. Finally, the Gabor filter task is helped by adding the higher level tasks: Rotation and DeepCluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Generalization</head><p>We now describe the performance of feature representations obtained from the SSL tasks on VLCS and PACS, the two main domain generalization benchmarks. Each of these datasets are composed of four different datasets with the same classes. In all experiments, we fine-tune the self-supervised learnt representations using a leave-one-domain-out scheme, i.e. the model is fine-tuned on the training examples from three domains and the best performance in terms of accuracy obtained on the unseen remaining domain is reported.</p><p>To isolate the effect of the type of pretraining, we use the same architecture (AlexNet) for the feature extractor and the same architecture (a 1-layer neural network described in Section 3.4) for task head modules in all the evaluated strategies. We train all models for 100 epochs using SGD with learning rate equal to 0.001, Polyak's acceleration coefficient of 0.9, and weight decay regularization of 0.00005. We evaluate performance by computing the best accuracy achieved on the unseen target domain.</p><p>Baselines: The performance of the representations learnt by self-supervision is compared with two baselines: a randomly initialized model and a model pretrained using the full training partition of the ILSVRC 2012 (ImageNet) dataset. Previous work on domain generalization has shown that fine-tuning a pretrained model on ImageNet on all source domains is a strong baseline for comparing the capability of generalizing to unseen domains. This is primarily because most of the datasets considered as unseen domains actually have considerable similarity with ImageNet, in that they contain natural images with classes overlapping with ImageNet. We do not include comparisons with methods such as [30,28,3] which use different stopping criteria, architectures, or combinations of loss functions for evaluating domain generalization performance and are hence not directly comparable. A comparison with these methods is included in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PACS:</head><p>The PACS benchmark was proposed as a test bed for out-of-distribution generalization strategies that presents a high overall domain shift from Ima-geNet <ref type="bibr" target="#b26">[27]</ref>. PACS contains four domains: Photo, Art painting, Cartoon, and Sketch <ref type="figure" target="#fig_2">(Figure 3</ref>). Each dataset is divided into seven classes: dog, elephant, giraffe, guitar, horse, house, and person. In <ref type="table">Table 2</ref>, we show the performance of self-supervised learning methods, along with the baseline models obtained by supervised pretraining and random initialization. All single-and multi-task SSL approaches significantly outperform the randomly initialized baseline. As we combine multiple SSL tasks, the average performance for domain generalization improves. The SSL tasks complement each other, boosting the accuracy on the unseen domain by, for instance, 3.32% when  comparing Rotation with Rotation + Gabor + DeepCluster. Saliently, the combination of all three tasks surpass the performance of the supervised pretrained baseline by 2% on average and are better than the supervised pretrained baseline on Art painting, Cartoon, and Sketch domains. The combination of Rotation and DeepCluster also outperforms the supervised pretrained baseline on average. SSL obtains significant improvement over supervised pretraining on the Art painting, Cartoon, and Sketch domains, which represent a significant domain shift from natural images present in ImageNet. These results indicate that selfsupervised tasks are able to learn a feature representation that is more readily transferable across domains as compared strongly discriminative supervised learning on the same set of images. Interestingly, self-supervised pretraining shows the highest performance improvement (8.3% for R + G + DC) on the Sketch dataset over the supervised model. Images in the Sketch dataset contain, not surprisingly, simple sketches that lack texture and color ( <ref type="figure" target="#fig_2">Figure 3</ref>). The selfsupervised learning approach, containing tasks such as Rotation and Gabor filter reconstruction that focus on low-and mid-level features, may allow the model to capture information related to edges and shapes without capturing texture information. Note that the model pretrained with DeepCluster alone performs the worst on Sketch dataset, but the performance is recovered once Rotation and Gabor filter reconstruction tasks are included, confirming the importance of adding low-level tasks to pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLCS:</head><p>The VLCS benchmark <ref type="bibr" target="#b13">[14]</ref> contains natural images obtained from the PASCAL VOC <ref type="bibr" target="#b12">[13]</ref>, LabelMe <ref type="bibr" target="#b33">[34]</ref>, Caltech101 <ref type="bibr" target="#b14">[15]</ref>, and SUN09 <ref type="bibr" target="#b5">[6]</ref> datasets divided in five classes: bird, car, chair, dog, and person. Following convention, we split each dataset into training and validation sets that contain 80% and 20% of the data points, respectively.</p><p>In <ref type="table">Table 3</ref>, we summarize the results of single-task and multi-task selfsupervised pretraining strategies with the randomly initialized and ImageNetinitialized models. The average performance of best multi-task self-supervised model (R + DC) across datasets (69.78%) is significantly better than random initialization (59.86%) and almost matches the fully-supervised model (70.69%).</p><p>Saliently, 5 out of 6 SSL strategies beat the performance of the fully-supervised model on the LabelMe datset. As <ref type="figure">Figure 4</ref> shows, LabelMe represents a significant domain shift when compared with ImageNet; the objects are usually smaller in comparison to ImageNet and larger, distractor objects which do not belong to the class label are often present in the image. The supervised model slightly outperforms SSL on PASCAL VOC, Caltech101, and SUN09-datasets that are relatively more similar to ImageNet. Among the self-supervised tasks, R + DC obtain the best overall performance, followed by R + G + DC. Some individual tasks such as DC obtain better performance than multi-task models like R + G and R + G + DC. However, unlike the PACS dataset, multi-task SSL does not uniformly improve the performance over individual tasks.</p><p>Qualitative Differences: We perform a qualitative evaluation of the feature representations learnt by SSL and fully-supervised learning methods by visualizing the input regions that obtain the highest model activations for the predicted class, using the GradCAM heatmap method <ref type="bibr" target="#b34">[35]</ref>. Specifically, we consider the R + G + DC pretraining method which outperformed the supervised pretrained model on PACS and closely matched the performance on VLCS. <ref type="figure" target="#fig_3">Figure 5</ref> shows the performance on the PACS benchmark with Sketch as target domain. Regions more relevant for prediction are shown in red. Heatmaps corresponding to examples that were correctly classified by the self-supervised pretrained model and misclassified by the supervised baseline are shown along with the original input image. We observe that the multi-task self-supervised pretrained model is much better at focusing on parts of objects (such as heads and ears of animals, windows of houses), while ignoring the background. On the other hand, the supervised baseline considers larger portions of the input image for the prediction and frequently focuses on the background or distractor objects (e.g., the chair besides a person for the 'person' class).</p><p>We observe similar trends on the VLCS benchmark with LabelMe as the target domain ( <ref type="figure" target="#fig_4">Figure 6</ref>), which contains natural images. The SSL model is much better at localizing small objects corresponding to the class of interest, while ignoring the background and distractor objects, for classes such as bird, car, and person. In contrast, the supervised baseline is more distracted by surrounding objects in the LabelMe dataset, which contains significantly more contextual information than ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-domain Transfer</head><p>We also evaluate the performance of each representation on a cross-domain scenario where only one source domain is available at time, to investigate the case of low-data fine-tuning. For this purpose, we fine-tune each model with a training set composed of a single source domain and then evaluate the learnt representations on different domain. In the case of the VLCS benchmark, we perform experiments considering LabelMe as target and each remaining domain as source. Similarly, for the PACS benchmark, we use Sketch as target and each remaining  <ref type="figure" target="#fig_5">Figures 8 and 7</ref>. When the source domain datasets are similar to ImageNet (Caltech101 and Photo, for the VLCS and PACS benchmarks, respectively), the features learnt by models pretrained with self-supervised tasks yield better out-of-distribution generalization as compared to supervised learning. In other words, a neural network trained with roughly 1.2 million unlabeled images with self-supervised pretext tasks and finetuned with roughly 1500 labeled images obtains comparable or significantly better performance performance than a neural network with the same architecture trained on roughly 1.2 million labeled images and finetuned with approximately 1500  labeled images. These results also indicate that the self-supervision can be used to mitigate the effects caused by lack of visual diversity between the datasets employed in the pretraining and finetuning stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combination with Other Domain Generalization Methods</head><p>Finally, we study if a feature representation learnt with SSL can serve as good initialization for domain generalization methods that utilize different optimization techniques or loss functions to improve OOD performance. Specifically, we use Invariant Risk Minimization (IRM) <ref type="bibr" target="#b0">[1]</ref>, a recently proposed method with strong performance. For these experiments, we finetune the pretrained model using IRM in order to enforce learning a representation for which the best predictor is the same across all the training domains. We use the same hyperparameters as in <ref type="bibr">[</ref>   for the colored MNIST experiments 4 . A more extensive hyperparameter search is likely to improve performance across methods. We find that combining IRM with SSL yields better out-of-distribution performance as compared to supervised learning for both PACS and VLCS benchmarks on average <ref type="table">(Table 4</ref>). For the PACS benchmark, SSL improves the best target accuracy on 3 out of 4 domains. For the VLCS benchmark, SSL outperforms the performance on 2 target domains, including Caltech101. Note that in previous experiments <ref type="table">(Table 3)</ref> with ERM, supervised learning was superior to SSL on Caltech101. Overall, this experiment indicates that combining domain generalization strategies along with self-supervised pretraining can be an effective way to boost the out-of-distribution generalization capability of previously proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Self-supervised learning has emerged as a powerful framework for learning feature representation that can match the performance of supervised learning on problems like image classification and few-shot learning. Here we show that feature representations obtained from self-supervised learning, especially those ob-tained by combining multiple pretext tasks, are able to match or exceed the performance of fully-supervised feature extractors on the domain generalization task and even improve localization. Moreover, self-supervision can be combined with other techniques that aim to learn feature representations which are amenable to domain generalization. Future work in this area can explore the performance of contrastive pretext tasks on domain generalization and alternate optimization strategies for training multi-task self-supervised learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Comparing with domain generalization strategies: For the sake of completeness, we compare the performance obtained by our best self-supervised pretrained models with previous work on domain generalization that proposes strategies for out-of-distribution generalization. These methods train models on top of model weigths pretrained with ImageNet. Even though the performance is not directly comparable across the different methods due to large differences on the architecture and training budget, we believe this comparison is valuable to show the gap between self-supervision for representation learning (which is not specifically designed for out-of-distribution generalization) and strategies that aim to learn features robust to domain mismatches. We show in Tables S5 and S6 the results obtained by CIDDG <ref type="bibr" target="#b29">[30]</ref> and MLDG <ref type="bibr" target="#b27">[28]</ref> on the VLCS and PACS benchmarks, respectively. Since the performance of both methods is reported in the literature by computing the performance on the target domain achieved by the model with best accuracy on the source domains, we also show in both tables the results achieved by the best self-supervised strategies under the same criterion. These results are indicated in the tables by the symbol ?. In addition, we report the results obtained by the best self-supervised pretrained models on the target domain (indicated by ?), as well as the performance achieved by JiGen <ref type="bibr" target="#b2">[3]</ref>, which is also not directly comparable to the other results reported in the tables since the training stopping criterion was not specified. We also include in the tables the reported performance by the respective supervised baseline (denoted as DeepAll) for each method. Our best models are comparable to supervised methods trained with additional domain generalization techniques.  ?: 10 ?: 4 ?: 0.5 -? : 0.0</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of the training scheme. Left: Self-supervised pretraining with multiple tasks. The feature extractor is shared and is updated through the loss of all tasks. Right: Supervised finetuning for the domain generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Gabor filter response reconstruction task. Left: Prediction by a model trained with the Gabor filter response reconstruction task alone. Right: Prediction by a model simultaneously trained with DeepCluster, Rotation, and the Gabor filter response reconstruction task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Examples: ImageNet and PACS. Fig. 4: Examples: ImageNet and VLCS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>GradCam visualizations for examples correctly classified by a model pretrained using Rotation, Gabor reconstruction, and DeepCluster (R+G+DC) and misclassified by the supervised baseline. Regions more relevant for prediction are shown in red. Models trained with self-supervision show better localization performance. Both models were fine-tuned on the PACS benchmark using Photo, Art painting, and Cartoon as source domains. domain as source. Results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>GradCam visualizations for examples correctly classified by a model pretrained using Rotation, Gabor reconstruction, and DeepCluster (R+G+DC) and misclassified by the supervised baseline. Regions more relevant for prediction are shown in red. Models trained with self-supervision show better localization performance. Both models were fine-tuned on the VLCS benchmark using Pascal VOC, Caltech101, and SUN09 as source domains. Ra nd om Su pe rv ise d R R+ G DC +R +G Su pe rv ise d R R+ G DC +R +G Art to Sketch Ra nd om Su pe rv ise d R R+ G DC +R +G Cartoon to Sketch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Performance for PACS cross-domain transfer using Sketch as target. Selfsupervised learning with unlabeled images outperforms the supervised baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Performance for VLCS cross-domain transfer using LabelMe as target. Self-supervised learning with unlabeled images is comparable to, or outperforms, the supervised baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table S2 :</head><label>S2</label><figDesc>Domain generalization performance on the PACS benchmark. Multitask self-supervised learning outperforms supervised learning on PACS. Accuracy reported in percent. Bolded value indicates best model for the target domain.</figDesc><table><row><cell>Domain</cell><cell cols="2">Method R</cell><cell cols="3">G DC R+G R+DC R+G+DC Rand. Init. Supervised</cell></row><row><cell cols="2">P</cell><cell cols="2">80.96 77.66 79.88 82.28 85.99</cell><cell>84.31</cell><cell>70.12</cell><cell>87.19</cell></row><row><cell cols="2">A</cell><cell cols="2">54.20 47.71 54.74 56.01 62.65</cell><cell>61.67</cell><cell>45.21</cell><cell>61.67</cell></row><row><cell cols="2">C</cell><cell cols="2">65.10 58.62 62.29 65.61 62.97</cell><cell>67.41</cell><cell>53.58</cell><cell>64.85</cell></row><row><cell cols="2">S</cell><cell cols="2">63.76 55.61 44.18 60.45 60.73</cell><cell>63.91</cell><cell>53.50</cell><cell>55.61</cell></row><row><cell cols="2">Average</cell><cell cols="2">66.00 59.90 60.27 66.08 68.08</cell><cell>69.32</cell><cell>55.60</cell><cell>67.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table S3 :</head><label>S3</label><figDesc>Domain generalization performance on the VLCS benchmark. Multitask self-supervised learning performs comparably to supervised learning on VLCS. Accuracy reported in percent. Bolded value indicates best model for the target domain.</figDesc><table><row><cell>Domain</cell><cell cols="2">Method R</cell><cell cols="3">G DC R+G R+DC R+G+DC Rand. Init. Supervised</cell></row><row><cell cols="2">V</cell><cell cols="2">60.41 53.31 61.20 57.95 62.59</cell><cell>57.65</cell><cell>51.14</cell><cell>64.07</cell></row><row><cell cols="2">L</cell><cell cols="2">66.12 61.86 59.85 65.87 62.86</cell><cell>64.99</cell><cell>59.22</cell><cell>60.73</cell></row><row><cell cols="2">C</cell><cell cols="2">84.20 78.77 94.10 87.97 93.87</cell><cell>89.15</cell><cell>74.06</cell><cell>95.52</cell></row><row><cell cols="2">S</cell><cell cols="2">59.70 56.95 57.66 59.09 59.80</cell><cell>58.88</cell><cell>55.03</cell><cell>62.44</cell></row><row><cell cols="2">Average</cell><cell cols="2">67.60 62.73 68.20 67.72 69.78</cell><cell>67.67</cell><cell>59.86</cell><cell>70.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S4 :</head><label>S4</label><figDesc>Performance of domain generalization with Invariant Risk Minimization (IRM). Self-supervised learning obtains better performance than supervised learning when using IRM for domain generalization. Bolded value indicates best model for the target domain.</figDesc><table><row><cell>Domain</cell><cell cols="4">Method IRM-Supervised IRM-R+G+DC Domain</cell><cell cols="2">Method IRM-Supervised IRM-R+DC</cell></row><row><cell cols="2">P</cell><cell>79.76</cell><cell>77.31</cell><cell cols="2">V</cell><cell>63.18</cell><cell>59.33</cell></row><row><cell cols="2">A</cell><cell>54.05</cell><cell>59.67</cell><cell cols="2">L</cell><cell>59.10</cell><cell>62.11</cell></row><row><cell cols="2">C</cell><cell>61.43</cell><cell>63.78</cell><cell cols="2">C</cell><cell>87.74</cell><cell>91.51</cell></row><row><cell cols="2">S</cell><cell>46.50</cell><cell>62.66</cell><cell cols="2">S</cell><cell>61.01</cell><cell>60.91</cell></row><row><cell cols="2">Avg.</cell><cell>60.44</cell><cell>65.86</cell><cell cols="2">Avg.</cell><cell>67.76</cell><cell>68.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S5 :</head><label>S5</label><figDesc>Comparison with previously report domain generalization performance on the VLCS benchmark. CIDDG ? 62.71 61.28 85.73 59.33 67.26 CIDDG ? 64.38 63.06 88.83 62.10 69.72 DeepAll-JiGen 71.96 59.18 96.93 62.57 72.66 JiGen 70.62 60.90 96.93 64.30 73.19 R+DC ? 62.19 59.10 87.74 58.58 66.90 R+DC ? 62.59 62.86 93.87 59.80 69.78</figDesc><table><row><cell>V</cell><cell>L</cell><cell>C</cell><cell>S Average</cell></row><row><cell>DeepAll-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S6 :</head><label>S6</label><figDesc>Comparison with previously reported domain generalization performance on the PACS benchmark. MLDG ? 86.67 64.91 64.28 53.08 67.24 MLDG ? 88.00 66.23 66.88 58.96 70.01 DeepAll-JiGen 89.98 66.68 69.41 60.02 71.52 JiGen 89.00 67.63 71.71 65.18 73.38 R+G+DC ? 84.31 61.67 67.41 57.47 65.18 R+G+DC ? 84.31 61.67 67.41 63.91 69.32</figDesc><table><row><cell>P</cell><cell>A</cell><cell>C</cell><cell>S Average</cell></row><row><cell>DeepAll-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/deepcluster</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/facebookresearch/InvariantRiskMinimization</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Gabor filter bank hyperparameters:</head><p>We implement the Gabor filter bank using the OpenCV-Python toolbox get-GaborKernel function with the following hyperparameters:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2959" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-dimensional spectral analysis of cortical receptive field profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="847" to="856" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6447" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning by rotation feature decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10364" to="10374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gabor filters as texture discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="113" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1v4N2l0-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15637" to="15648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks: a new framework for modeling biological vision and brain information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of vision science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="417" to="446" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">When does self-supervision improve few-shot learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03560</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<title level="m">The visual task adaptation benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
