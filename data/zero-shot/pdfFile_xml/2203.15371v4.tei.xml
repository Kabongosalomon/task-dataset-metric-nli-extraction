<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">mc-BEiT: Multi-choice Discretization for Image BERT Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Li</surname></persName>
							<email>lixiaotong@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
							<email>kunyi@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Corresponding Author</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">mc-BEiT: Multi-choice Discretization for Image BERT Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-supervised Learning</term>
					<term>Vision Transformers</term>
					<term>Image BERT Pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image BERT pre-training with masked image modeling (MIM) becomes a popular practice to cope with self-supervised representation learning. A seminal work, BEiT, casts MIM as a classification task with a visual vocabulary, tokenizing the continuous visual signals into discrete vision tokens using a pre-learned dVAE. Despite a feasible solution, the improper discretization hinders further improvements of image pre-training. Since image discretization has no ground-truth answers, we believe that the masked patch should not be assigned with a unique token id even if a better "tokenizer" can be obtained. In this work, we introduce an improved BERT-style image pre-training method, namely mc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice training objectives. Specifically, the multi-choice supervision for the masked image patches is formed by the soft probability vectors of the discrete token ids, which are predicted by the off-the-shelf image "tokenizer" and further refined by high-level inter-patch perceptions resorting to the observation that similar patches should share their choices. Extensive experiments on classification, segmentation, and detection tasks demonstrate the superiority of our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning accuracy on ImageNet-1K classification, 49.2% AP b and 44.0% AP m of object detection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic segmentation, outperforming the competitive counterparts. The code will be available at https://github.com/lixiaotong97/mc-BEiT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised pre-training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref> is attracting emerging attention due to its effectiveness and flexibility in exploiting large-scale uncurated data, which demonstrates its superiority to supervised pre-training in a wide range of downstream applications, such as classification, detection, and segmentation, etc. Recently, the introduction of vision Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34]</ref> brings about a new revolution to self-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>. arXiv:2203.15371v4 [cs.CV] 28 Jul 2022 <ref type="figure">Fig. 1</ref>: The improper token ids for image discretization, where a better tokenizer <ref type="bibr" target="#b17">[18]</ref> is used here. We observe that semantically-similar patches might be allocated with different token ids while patches with different semantics might be allocated with the same token id, indicting that the hard-label classification with unique token ids in BEiT <ref type="bibr" target="#b0">[1]</ref> may hinder the pre-training performance. Inspired by the great success of BERT <ref type="bibr" target="#b13">[14]</ref> in natural language processing (NLP) tasks, masked image modeling (MIM) has been introduced for visual pre-training as a new pretext task. It is not trivial, because one key barrier lies in that the visual signal is continuous and cannot be properly classified as is done in masked language modeling (MLM) of BERT. A pioneer work, BEiT <ref type="bibr" target="#b0">[1]</ref>, tackles the challenge by "tokenizing" continuous visual signals into discrete vision tokens resorting to a pre-learned codebook <ref type="bibr" target="#b37">[38]</ref>, which plays the role of a pre-defined vocabulary in MLM. The pre-training objective is to predict the vision token id of the masked image patch based on its context and semantics.</p><p>Despite the impressive performances of BEiT on image pre-training, there remain some questions under-developed. (1) Does the masked patch prediction have ground-truth answers? Unlike the linguistic vocabulary which is naturally composed of discrete words, the image tokenizer is relatively subjective, i.e., there is no perfect answer to visual discretization and the tokenizer carries inevitable label noise even a better tokenizer is obtained in <ref type="bibr" target="#b17">[18]</ref>. For example, as shown in <ref type="figure">Fig. 1</ref>, patches of the dog and the shoe are discretized into the same vision token (#319) due to their similar pixel-level representations. (2) Should the masked patch be assigned a unique token id given a pre-learned tokenizer? Not really. As illustrated in <ref type="figure">Fig. 1</ref>, semantically-similar patches of the grass are discretized into different vision tokens, i.e., they are classified into distinct and unique ids in BEiT pre-training, neglecting their semantic relations.</p><p>Given the observation of the above two issues, we argue that performing MIM with a strict mapping between patch predictions and unique token ids by a hardlabel classification loss in BEiT limits the visual context capturing and the pretraining performance. To tackle the challenge, we introduce to effectively boost BERT-style image pre-training with eased and refined masked prediction targets, that is, multi-choice vision token ids. Rather than retraining the tokenizer with perceptual regularizations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b45">46]</ref>, we efficiently inject the semantic relations into off-the-shelf vision tokens without any extra computational overhead.</p><p>Specifically, to enable multi-choice answers for masked patches, we adopt the soft id probability vectors, rather than the hard predicted id over a pre-learned codebook, as the supervision signals for masked image modeling. Although the off-the-shelf image tokenizer <ref type="bibr" target="#b17">[18]</ref> can capture some local semantics with the training objectives of both pixel-level and perceptually-aware regularizations, it is proven to be still vulnerable to various low-level changes (see <ref type="figure">Fig. 1</ref>). Therefore, we introduce to refine the predicted soft id probabilities by inter-patch semantic similarities, which are estimated by the vision Transformers being trained. Under the observation that patches with similar high-level visual perceptions ought to share their predictions, we propagate the soft id probabilities of different patches in an image based on their semantic similarities and form ensembled learning targets for masked image patches (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The final training objective is formulated as a soft-label cross-entropy loss.</p><p>To fully evaluate our novel, flexible and effective method, we pre-train the vision Transformers with various scales on the widely-acknowledged ImageNet-1K <ref type="bibr" target="#b12">[13]</ref> dataset and fine-tune the pre-trained models on multiple downstream tasks, including image classification, instance/semantic segmentation, and object detection. The empirical results show that our method impressively outperforms supervised pre-training as well as recent self-supervised learning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b45">46]</ref>. Concretely, we achieve 84.1% top-1 accuracy on ImageNet-1K classification with a ViT-B model, outperforming the state-of-the-art iBOT <ref type="bibr" target="#b45">[46]</ref> by +0.3% with 800 fewer epochs. Regarding the transfer learning ability on different downstream tasks, our pre-trained ViT-B model achieves 49.2% AP b and 44.0% AP m of object detection and instance segmentation on COCO <ref type="bibr" target="#b30">[31]</ref>, 50.8% mIOU on downstream ADE20K <ref type="bibr" target="#b44">[45]</ref> semantic segmentation, outperforming all existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Self-supervised learning (SSL) has gained great popularity benefiting from its capability of exploiting the tremendous amounts of unlabeled data, which leverages input data itself as supervision. Substantial works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26]</ref> have shown that the pre-training can be beneficial for downstream tasks and enable faster training convergence, which shows its impressive potentials on various machine learning tasks, especially in the fields of natural language processing and computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BERT pre-training with masked language modeling</head><p>Self-supervised learning has been studied in NLP for decades. Masked language modeling (MLM) is firstly widely acknowledged because of BERT <ref type="bibr" target="#b13">[14]</ref>. BERT encourages bidirectional textual context understanding and adopts the masked language modeling approach for pre-training, which randomly masks 15% tokens and predicts the missing words as the target. After that, various MLM variants are proposed, e.g., GPT <ref type="bibr" target="#b2">[3]</ref>, XLM <ref type="bibr" target="#b27">[28]</ref>, and RoBERTa <ref type="bibr" target="#b32">[33]</ref>, etc. These MLM works achieve huge success and show impressive performances on various downstream tasks, which greatly advance the development of language pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised visual pre-training</head><p>In the past few years, various pretext tasks are designed for self-supervised visual pre-training. For example, earlier pretext-based works adopt the pseudo labels based on the attributes of images to learn the representation, such as image colorization <ref type="bibr" target="#b28">[29]</ref>, jigsaw puzzle <ref type="bibr" target="#b34">[35]</ref>, context prediction <ref type="bibr" target="#b14">[15]</ref>, and rotation prediction <ref type="bibr" target="#b20">[21]</ref>, etc. Besides these approaches, there are two mainstream paradigms, i.e., contrastive learning and masked image modeling approaches, which will be further analyzed in the following subsection.</p><p>Contrastive learning: Contrastive learning is an instance-level discriminative approach and has occupied a dominant position in visual pre-training. Contrastive learning methods, such as SimCLR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, MoCo <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>, and Swav <ref type="bibr" target="#b4">[5]</ref>, etc., typically rely on data augmentation to create the counterparts of the images and aim at learning such an embedding space, where similar sample pairs are close to each other while dissimilar ones are far apart. Swav <ref type="bibr" target="#b4">[5]</ref> proposes a cluster-based contrastive learning method to enforce consistency between cluster assignments under different augmentations. BYOL <ref type="bibr" target="#b21">[22]</ref> and SiamSim <ref type="bibr" target="#b10">[11]</ref> abandons the negative samples and avoids the collapse with either an additional momentum network or the stop-gradient operation. MoCov3 <ref type="bibr" target="#b11">[12]</ref> extends the contrastive learning framework for transformers and further promotes the development of self-supervised vision Transformers.</p><p>Masked Image Modeling: Motivated by the great success of BERT, masked image modeling (MIM) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b8">9]</ref> becomes a new trend in self-supervised visual pre-training, which randomly masks parts of images and reconstructs them based on the corrupted image. ViT <ref type="bibr" target="#b16">[17]</ref> attempts to adopt masked patch prediction for self-supervised learning. BEiT <ref type="bibr" target="#b0">[1]</ref> predicts the discrete tokens of masked token resorting to an off-the-shelf discrete VAE. Instead of discretizing the visual information, MAE <ref type="bibr" target="#b22">[23]</ref> and SimMIM <ref type="bibr" target="#b41">[42]</ref> propose to directly predict the pixel-level value as the reconstruction target. MaskFeat <ref type="bibr" target="#b39">[40]</ref> further exploits different supervision signals such as HOG feature to be the objective. iBOT <ref type="bibr" target="#b45">[46]</ref> performs masked prediction and adopts the teacher network as an online tokenizer to provide the supervision. PeCo <ref type="bibr" target="#b15">[16]</ref> further provides the evidence that the perceptually-aware tokenizer will provide better pre-training performance for the masked image modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image BERT Pre-training with Masked Image Modeling</head><p>The paradigm of mask-and-then-predict is first introduced in BERT pre-training <ref type="bibr" target="#b13">[14]</ref> of NLP tasks to encourage bidirectional context understanding of the textual signals. Recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref> reproduce the success of BERT by employing the proxy task of masked image modeling (MIM) on image pre-training of vision Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34]</ref>. MIM requires randomly masking a proportion of the image patches and then training the vision Transformer to recover the corrupted image via reasoning among the visual context. The pretext task of MIM enables a more fine-grained understanding of the local visual semantics compared to the contrastive counterparts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. Vision Transformers pre-trained with MIM objectives can be well transferred to a wide range of downstream tasks, i.e., classification, segmentation, and detection, after fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Masked Image Modeling as Single-choice Classification</head><p>Introducing the mask-and-then-predict paradigm into image pre-training is actually non-trivial, because the visual signals are continuous and cannot be predicted resorting to a well-defined vocabulary. A pioneering work, BEiT <ref type="bibr" target="#b0">[1]</ref>, tackles the challenge by casting masked patch prediction as a single-choice classification problem via discretizing the image into vision tokens with an off-the-shelf "tokenizer". The "tokenizer" can be a discrete auto-encoder <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b17">18]</ref> pre-learned towards the reconstruction objective.</p><p>Formally, given a raw image x ? R C?H?W , it is initially divided into N patches {x i } N i=1 and then mapped into compact patch embeddings. We denote the corrupted image asx, which is formed by masking part of the patches in x, and we denote the set of masked patch indices as M. We encode the image patch features f (x) ? R N ?D with high-level perceptions by feedingx into the vision Transformer. The patch features are further projected to the probabilities of the vision token ids using an MLP head which will be dropped for downstream tasks. We denote the probability vectors as q(f (x)) ? R N ?V where V is the length of the visual vocabulary defined by the pre-learned image "tokenizer".</p><p>To receive the answers for the masked image modeling, we discrete the raw image x into vision tokens {z i } N i=1 using the image "tokenizer", where z i ? R V . The assigned token id with the maximal probability in z i is termed as y i . The pretraining objective is formulated as a hard-label cross-entropy loss to encourage masked patch prediction with unique token ids as follow,</p><formula xml:id="formula_0">L mim (x) = E i?M [? log q (y i |f (x i ))].</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">mc-BEiT</head><p>BEiT provides inspiring insights of casting masked image modeling (MIM) as a classification problem to bridge the gap between discrete words in NLP tasks and continuous visual signals in computer vision tasks. However, as there are no perfect answers for visual discretization, performing a strict mapping between patch predictions and unique token ids as a single-choice classification problem is actually a sub-optimal solution for MIM pre-training. As illustrated in <ref type="figure">Fig. 1</ref>, there may exist multiple appropriate token ids for a certain patch, motivating us to boost the BEiT pre-training with multi-choice classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Masked Image Modeling as Multi-choice Classification</head><p>We introduce an improved BERT-style image pre-training with eased and refined masked prediction targets, i.e., multi-choice vision token ids, rather than a unique answer. All possible token ids in the visual vocabulary will be assigned possibilities to be chosen. To this end, we soften the training objective from the original hard-label cross-entropy loss to a soft-label cross-entropy loss with the multi-choice targets? ? R N ?V as follow,</p><formula xml:id="formula_1">L mc-mim (x) = E i?M ? V k=1? i,k log q (f (x i )) k ,<label>(2)</label></formula><p>where V k=1? i,k = 1, ?i and q (f (x i )) ? R V . We will go over how to produce such refined multi-choice answers for MIM pre-training in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-choice Visual Discretization</head><p>To produce multi-choice discretization without extra training stages or computational overhead, we attempt to exploit the predictions from the off-the-shelf image tokenizer. Given the discretization predictions z ? R N ?V from the image tokenizer, we estimate the soft probabilities p(z) ? R N ?V rather than using the unique predicted token id as done in the single-choice version. Specifically, the soft probability vector is obtained using a softmax operation, where a temperature coefficient ? is used to move between the sharpness (single-choice) and smoothness (multi-choice),</p><formula xml:id="formula_2">p(z i ) k = exp(z i,k /? ) V j=1 exp(z i,j /? ) .<label>(3)</label></formula><p>As discussed in the introduction section and illustrated in <ref type="figure">Fig 1,</ref> semanticallysimilar patches may be allocated with discrepant token ids and semanticallydissimilar patches may be allocated with the same token id due to their low-level similarities, indicating that the raw predictions from the off-the-shelf tokenizer are sub-optimal to fully represent the semantic relations among patches. The phenomenon motivates us to refine the predictions of the tokenizer with interpatch relations. The inter-patch relations can be estimated with their high-level perceptions, which are encoded by the in-training vision Transformer. Specifically, we calculate the cosine similarity between patch features to measure their affinities,</p><formula xml:id="formula_3">W (x i ) k = exp?f (x i ), f (x k )? N j=1 exp?f (x i ), f (x j )? ,<label>(4)</label></formula><p>where W (x) ? R N ?N and ??, ?? indicates the inner product between two feature vectors after ? 2 normalization. Based on the observation that perceptuallysimilar patches ought to share their choices, we propagate the soft probabilities p of different patches in an image x to form a refined target W (x)p(z) ? R N ?V . In this way, patches with similar high-level perceptions can provide complementary supervision signals for the masked patches. The overall objective of multi-choice image discretization is composed of the weighted sum of the aforementioned parts, where the semantic equilibrium coefficient ? is introduced to move between low-level semantics (directly predicted by the tokenizer) and high-level semantics (ensembled from the perceptuallysimilar patches). The former one adopts the eased supervision directly predicted from the tokenizer, while the latter one injects high-level perceptions by propagating among other semantically-similar patches, together forming the refined multi-choice targets? ? R N ?V as follow:</p><formula xml:id="formula_4">z = ?p(z) + (1 ? ?)W (x)p(z),<label>(5)</label></formula><p>which is further used as the objectives for masked patch predictions in Eq. (2). In our experiments, the images of 224?224 resolution are divided into 14?14 image sequences with 16?16 patch size. We use different architectures such as ViT-Base/16 and ViT-Large/16 for pre-training and the backbone implementation follows <ref type="bibr" target="#b16">[17]</ref> for fair comparisons. For the BERT-style visual pre-training, we randomly mask 75% patches for masked image modeling. Inspired by PeCo <ref type="bibr" target="#b15">[16]</ref>, we employ the off-the-shelf VQGAN of <ref type="bibr" target="#b17">[18]</ref> as a better tokenizer, which is pretrained on OpenImages <ref type="bibr" target="#b26">[27]</ref> with the vocabulary size of 8192. In our experiments, the semantic equilibrium coefficient ? is 0.8 and the temperature coefficient ? is 4.0 by default. The vision Transformers are pre-trained for 800 epochs on the widely-acknowledged ImageNet-1K <ref type="bibr" target="#b12">[13]</ref> dataset, which includes 1.28 million images. Note that the ground-truth labels are disabled for pre-training. We use 16 Nvidia A100 GPUs for pre-training and a batch size of 128 per GPU. We adopt simple image augmentation for pre-training, including random resized cropping and horizontal flipping. The detailed recipe of pre-training and finetuning is summarized in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Classification</head><p>For the ImageNet classification task, the fully-connected layer is employed as the classifier after the average pooling of the feature embeddings. We adopt top-1 accuracy after fine-tuning as the evaluation metric and we thoroughly compare our method with the supervised methods, i.e., ViT <ref type="bibr" target="#b16">[17]</ref>, DeiT <ref type="bibr" target="#b38">[39]</ref>, and recently published state-of-the-art self-supervised learning methods, i.e., MoCo v3 <ref type="bibr" target="#b11">[12]</ref>, DINO <ref type="bibr" target="#b5">[6]</ref>, BEiT <ref type="bibr" target="#b0">[1]</ref>, and iBOT <ref type="bibr" target="#b45">[46]</ref>. Besides, we also compare with the very recent pixel-level MIM methods, i.e., MAE <ref type="bibr" target="#b22">[23]</ref> and SimMIM <ref type="bibr" target="#b41">[42]</ref>. The experiment results are listed in Tab. 1. As observed from the results, the proposed method obtains 84.1% top-1 accuracy on ViT-B, outperforming the competing methods and achieving state-of-the-art performance. We can see that our mc-BEiT shows significant gains compared to the baseline BEiT, which verifies the effectiveness of our introduced multi-choice objectives. Concretely, our method outperforms the recent state-of-the-art method iBOT <ref type="bibr" target="#b45">[46]</ref> by +0.3% with the fewer 800 epochs pre-training. It is noted that iBOT adopts an extra teacher network and enables multi-crops for pre-training, showing lower efficiency than our method. Different training epochs and architectures: We also provide more comprehensive results of different training epochs and architectures in Tab. 2. From the table, we can see that our method can adapt well to different scales of vision tranformers, e.g., the mostly used ViT-B and ViT-L. It is worth noting that our method obtains a relatively high accuracy (already achieves the stateof-the-art performance) when pre-training for only 300 epochs. Moreover, the performance can be further improved with longer pre-training epochs, e.g., the accuracy reaches 84.1% pre-training for 800 epochs.</p><p>Convergence curve: <ref type="figure" target="#fig_1">In Fig 3,</ref> we further demonstrate the convergence curve of the supervised learning method and self-supervised learning methods, i.e., the baseline BEiT and our method, when fine-tuning ViT-B models. As shown in the figure, the proposed method achieves faster convergence as well as better performance than training DeiT from scratch <ref type="bibr" target="#b38">[39]</ref>. Meanwhile, our method obtains obvious and consistent performance gains compared to the baseline method BEiT, showing the superiority of the proposed multi-choice training objectives. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Object Detection and Instance Segmentation</head><p>For object detection and instance segmentation tasks, COCO <ref type="bibr" target="#b30">[31]</ref> benchmark is employed to validate the pre-training performances. We follow the implementation of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> and the model is trained for 25 epochs (we also provide another evaluation setting following iBOT <ref type="bibr" target="#b45">[46]</ref> in the Appendix). ViT-B is adopted as the backbone and Mask-RCNN <ref type="bibr" target="#b24">[25]</ref> is used as the task head. The evaluation metrics for objection detection and instance segmentation are bounding box AP and mask AP, respectively. As observed in Tab. 10, the BERT style pre-training shows superiority to supervised pre-training in terms of performances. Our method achieves 48.5% and 43.1% in AP b and AP m . Meanwhile, the proposed method outperforms the competitor BEiT with +0.9%/+0.9% gain in AP b and AP m . We also evaluate the performance after intermediate fine-tuning, the relative improvement is still obvious, i.e., +0.8%/+0.5% to BEiT. Our method even outperforms the recent pixel-level MIM method MAE and obtains better performances compared to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Semantic Segmentation</head><p>Semantic segmentation belongs to the pixel-level classification task and is often adopted to evaluate the pre-training performance on downstream tasks. Here we evaluate the performance on ADE20k <ref type="bibr" target="#b44">[45]</ref> benchmark and mean intersection over union (mIOU) averaged over all semantic categories is adopted as the evaluation metric. Following the common setting in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref>, ViT-B is adopted as the default backbone and UPerNet <ref type="bibr" target="#b40">[41]</ref> is used for semantic segmentation task head. Because the pre-training process does not introduce the instance discrimination, the performance can be further improved after intermediate fine-tuning on ImageNet-1K according to BEiT <ref type="bibr" target="#b0">[1]</ref>. Therefore we also compare the performance after intermediate fine-tuning. Tab. 4 shows that our method significantly improves the transferability of pre-trained models compared to the supervised learning, with +5.5% performance gain. It is also noticed that our method outperforms recent state-of-the-art self-supervised methods. It achieves better results as 47.0%/50.8% mIOU and improves +1.4%/3.1% gain to its pre-training only version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Study</head><p>In this section, we conduct an extensive ablation study of our method on ImageNet-1K. Considering the time expenditure, all ablation experiments are performed under 100-epoch pre-trained ViT-B/16 on ImageNet-1K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The temperature coefficient ?</head><p>The hyper-parameter of temperature coefficient ? is to scale the logits from the tokenizer, which moves between the sharpness (single-choice) and smoothness (multi-choice). We adopt the common values for temperature to ablate its effect. In general, the small temperature will sharp the probability distribution and the large one will smooth it conversely. When ? is extremely small, it is an approximate single-choice classification task. The ablation is shown in the <ref type="figure">Fig.  4(a)</ref>, where single-choice label indicates training with the strict mapping to the unique answer. From the result, we can observe that multi-choice vision token improves the BERT style pre-training performance and it behaves better when setting temperature factor at 4.0 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The semantic equilibrium coefficient ?</head><p>The semantic equilibrium coefficient ? is introduced to move between low-level semantics (directly predicted by the tokenizer) and high-level semantics (ensembled from the perceptually-similar patches). The ablation study is shown in <ref type="figure">Fig.  4(b)</ref>. When setting ? to 0, the objective relies on totally the inter-relationship guided objective and it achieves only 81.8% accuracy, which is because the inevitable noise of calculating patch similarity, especially in the early epochs, will cause collapse and degrade the pre-learning performance. As the coefficient goes larger, it shows consistent gains over baseline. When setting ? to 1.0, the objective comes only from the low-level signals of the tokenizer and the performance is still higher than baseline, which shows the superiority of multi-choice to singlechoice. As observed from the results, the semantic equilibrium coefficient is thus set to be 0.8 for better performances. <ref type="figure">Fig. 4</ref>: Ablation study on the hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Masking strategy</head><p>In the masked image modeling approach, the masking strategy determines the difficulty of inferring the missing patches. Tab. 3 shows the influence of different mask strategies, where Block and Random masking types and different mask ratios are conducted for ablation. It is observed from the results that the random masking strategy with 75% masking ratio makes the best performances, which is thus adopted as the default setting for pre-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Tokenizer</head><p>In the BERT-style visual pre-training, the tokenizer plays the role of a vocabulary in texts and is used to produce the discrete vision token as supervision.</p><p>As discussed in PeCo <ref type="bibr" target="#b15">[16]</ref>, perceptually-aware tokenizer may benefit the image BERT pre-training, so we introduce to use off-the-shelf VQGAN <ref type="bibr" target="#b17">[18]</ref> as a better tokenizer throughout our experiments. Besides, we would like to also verify the effectiveness of our multi-choice objectives on top of the vanilla BEiT. The influence of tokenizers is shown in Tab. 6. It is shown that adopting the VQGAN as the tokenizer brings better performance than DALL-E, which verify our observation that tokenizer with high semantics can indeed improve the pre-training performance. It also indicates that enhancing the semantic relation is beneficial to visual pre-training. Meanwhile, it is noticed that the relative improvement of our method is consistent regardless of different kinds of tokenizers, which demonstrates the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Visualization</head><p>Besides the quantitative experiment results, we further provide some visualizations in <ref type="figure" target="#fig_2">Fig. 5</ref> for better understanding the effects of our multi-choice answers. 75% patches of the images are randomly masked for prediction.</p><p>It can be observed from the blue box in <ref type="figure" target="#fig_2">Fig. 5(a)</ref>, the adjacent patches with similar semantics are still allocated with different vision token ids, indicating that the hard vision token id directly from the tokenizer neglects the semantic relations and is a sub-optimal objective. In contrast, the proposed eased and refined objective can provide diverse possible vision tokens for the prediction. As shown in our multi-choice token signals, the semantically-similar patches have the possibility to be allocated with the same vision token, which refines the objective with inter-patch perceptions. Furthermore, we randomly select a masked patch and shows the inter-patch perception relations (obtained from the patch feature similarity) learned by the pre-trained model in <ref type="figure" target="#fig_2">Fig. 5(c)</ref>. The similar patches can still be well estimated even under heavy random masking and the inter-patch relation shows higher responses, e.g, the skeleton of the car. It demonstrates that the informative semantic relations estimated by the intraining vision Transformer can properly enhance the multi-choice discretization for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose the mc-BEiT, i.e., multi-choice discretization for improving image BERT pre-training. Instead of adopting the unique label signals from the tokenizer, we introduce an eased and refined objective for providing multi-choice answers. Extensive experiments are conducted to evaluate the performances of our method. The empirical results show that mc-BEiT achieves the state-of-the-art performances on various tasks, such as image classification, semantic/instance segmentation, and objection detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>In the appendix, we provide the specific hyper-parameters of the experiments in our paper, including pre-training on ImageNet-1K and fine-tuning on different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Configuration for pre-training</head><p>The vision Transformers are pre-trained on the large-scale dataset ImageNet-1K <ref type="bibr" target="#b12">[13]</ref> and the configurations are summarized in Tab. 7. The implementation of the vision Transformers, i.e., ViT-Base/16 and ViT-Large/16, follows <ref type="bibr" target="#b38">[39]</ref> for fair comparisons and the training recipe is based on BEiT [1].  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object detection and instance segmentation</head><p>We adopt the implementation of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> to verify our performances of object detection and instance segmentation on COCO. Tab. 9 summarizes the configurations for fine-tuning on COCO. The training recipe of models with intermediate fine-tuning is the same as the pre-training only version. ViT-B <ref type="bibr" target="#b16">[17]</ref> is adopted as the backbone and Mask-RCNN <ref type="bibr" target="#b24">[25]</ref> is used as the task head.</p><p>Besides, we also provide another experiment result following the implementation in iBOT <ref type="bibr" target="#b45">[46]</ref> in Tab. 10. Because these experiments are not conducted on BEiT, we conduct the experiments following iBOT <ref type="bibr" target="#b45">[46]</ref> and the results of BEiT <ref type="bibr" target="#b0">[1]</ref> are based on our re-implementation. In order to adapt to the multi-scale strategy, we use absolute position embedding and interpolate it for different image resolutions. ViT-B <ref type="bibr" target="#b16">[17]</ref> is adopted as the backbone and Cascaded Mask-RCNN <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref> is used as the task head.  Semantic segmentation on ADE20K: For the semantic segmentation experiments on ADE20K <ref type="bibr" target="#b44">[45]</ref>, we follow the implementation of BEiT <ref type="bibr" target="#b0">[1]</ref> and adopt UperNet <ref type="bibr" target="#b40">[41]</ref> as the task layer. ViT-B <ref type="bibr" target="#b16">[17]</ref> is adopted as the default backbone and UPerNet <ref type="bibr" target="#b40">[41]</ref> is used as the task head. Tab. 11 summarizes the configurations for fine-tuning on ADE20k. Because the pre-training process does not introduce the instance discrimination, the performance can be further improved after intermediate fine-tuning on ImageNet-1K according to BEiT <ref type="bibr" target="#b0">[1]</ref>. we also </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The overview of the proposed method, mc-BEiT. We improve image BERT pre-training with multi-choice training objectives, which is composed of the soft probability vectors predicted by the off-the-shelf image "tokenizer" and further refined by high-level inter-patch perceptions. A proportion of image patches are randomly masked and then fed into the vision Transformer. The masked patch prediction is optimized towards eased and refined multi-choice token ids in the form of a soft-label cross-entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The convergence curves when fine-tuning ViT-B models on ImageNet-1K classification. The models are pre-trained by different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>The visualization is obtained using the off-the-shelf tokenizer and our pre-trained vision Transformer. The inter-patch perception relation is equipped with contour lines for better visual effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The top-1 fine-tuning accuracy of ImageNet-1K using ViT-Base and ViT-Large with different pre-training methods.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Pre-train Epoch</cell><cell>Acc (%)</cell></row><row><cell cols="3">Supervised Pre-training (training from scratch):</cell><cell></cell></row><row><cell>ViT-B/16 [17]</cell><cell>ICLR 2021</cell><cell>-</cell><cell>77.9</cell></row><row><cell>ViT-L/16 [17]</cell><cell>ICLR 2021</cell><cell>-</cell><cell>76.5</cell></row><row><cell>DeiT-B/16 [39]</cell><cell>ICML 2021</cell><cell>-</cell><cell>81.8</cell></row><row><cell cols="3">Self-supervised Pre-training using ViT-B/16:</cell><cell></cell></row><row><cell>MoCo v3 [12]</cell><cell>CVPR 2021</cell><cell>300</cell><cell>83.2</cell></row><row><cell>DINO [6]</cell><cell>ICCV 2021</cell><cell>300</cell><cell>82.8</cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>800</cell><cell>83.2</cell></row><row><cell>iBOT [46]</cell><cell>ICLR 2022</cell><cell>1600</cell><cell>83.8</cell></row><row><cell>MAE [23]</cell><cell>CVPR 2022</cell><cell>1600</cell><cell>83.6</cell></row><row><cell>SimMIM [42]</cell><cell>CVPR 2022</cell><cell>800</cell><cell>83.8</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>800</cell><cell>84.1</cell></row><row><cell cols="3">Self-supervised Pre-training using ViT-L/16:</cell><cell></cell></row><row><cell>MoCo v3 [12]</cell><cell>CVPR 2021</cell><cell>300</cell><cell>84.1</cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>800</cell><cell>85.2</cell></row><row><cell>MAE [23]</cell><cell>CVPR 2022</cell><cell>1600</cell><cell>85.9</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>800</cell><cell>85.6</cell></row><row><cell>5 Experiments</cell><cell></cell><cell></cell><cell></cell></row></table><note>5.1 Pre-Training Setup</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>Model Size</cell><cell>Pre-train Epoch</cell><cell>Acc (%)</cell></row><row><cell cols="3">Self-supervised Pre-training using ViT-B/16:</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>ViT-B</cell><cell>86M</cell><cell>100</cell><cell>83.3</cell></row><row><cell>Ours</cell><cell>ViT-B</cell><cell>86M</cell><cell>300</cell><cell>83.9</cell></row><row><cell>Ours</cell><cell>ViT-B</cell><cell>86M</cell><cell>800</cell><cell>84.1</cell></row><row><cell cols="3">Self-supervised Pre-training using ViT-L/16:</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>ViT-L</cell><cell>307M</cell><cell>300</cell><cell>85.2</cell></row><row><cell>Ours</cell><cell>ViT-L</cell><cell>307M</cell><cell>800</cell><cell>85.6</cell></row></table><note>The top-1 fine-tuning accuracy of ImageNet-1K using our mc-BEiT with different training epochs and backbone architectures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experiment results of object detection and instance segmentation on COCO. We follow the implementation of<ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> and the model is trained for 25 epochs. Intermediate fine-tuning denotes the model is further fine-tuned on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Object Det. AP b</cell><cell>Instance Seg. AP m</cell></row><row><cell>Supervised [39]</cell><cell>ICML 2021</cell><cell>46.5</cell><cell>41.7</cell></row><row><cell>MoCo v3 [12]</cell><cell>CVPR 2021</cell><cell>46.6</cell><cell>41.9</cell></row><row><cell>DINO [6]</cell><cell>ICCV 2021</cell><cell>47.6</cell><cell>42.3</cell></row><row><cell>MAE [23]</cell><cell>CVPR 2022</cell><cell>48.0</cell><cell>43.0</cell></row><row><cell>iBOT [46]</cell><cell>ICLR 2022</cell><cell>48.4</cell><cell>42.9</cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>47.6</cell><cell>42.2</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>48.5</cell><cell>43.1</cell></row><row><cell>+Intermediate Fine-tuning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>48.4</cell><cell>43.5</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>49.2</cell><cell>44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of semantic segmentation on ADE20K. Intermediate fine-tuning denotes the pre-trained model has been fine-tuned on ImageNet-1K classification.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>mIOU</cell></row><row><cell>Supervised [39]</cell><cell>ICML 2021</cell><cell>45.3</cell></row><row><cell>MoCo v3 [12]</cell><cell>CVPR 2021</cell><cell>47.2</cell></row><row><cell>DINO [6]</cell><cell>ICCV 2021</cell><cell>46.8</cell></row><row><cell>MAE [23]</cell><cell>CVPR 2022</cell><cell>48.1</cell></row><row><cell>iBOT [46]</cell><cell>ICLR 2022</cell><cell>50.0</cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>45.6</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>47.0</cell></row><row><cell>+Intermediate Fine-tuning</cell><cell></cell><cell></cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>47.7</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on different masking strategies.</figDesc><table><row><cell>Masking Strategy</cell><cell>Masking Ratio</cell><cell>Acc (%)</cell></row><row><cell>Block</cell><cell>45 %</cell><cell>83.2</cell></row><row><cell>Block</cell><cell>60 %</cell><cell>83.2</cell></row><row><cell>Block</cell><cell>75 %</cell><cell>82.8</cell></row><row><cell>Random</cell><cell>45 %</cell><cell>83.0</cell></row><row><cell>Random</cell><cell>60 %</cell><cell>83.1</cell></row><row><cell>Random</cell><cell>75 %</cell><cell>83.3</cell></row><row><cell>Random</cell><cell>90 %</cell><cell>83.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the different tokenizer.</figDesc><table><row><cell></cell><cell cols="2">Training Data</cell><cell cols="2">Top 1 Acc. (100 / 800 epochs)</cell></row><row><cell></cell><cell>Source</cell><cell>Scale</cell><cell>BEiT</cell><cell>Ours</cell></row><row><cell>DALL-E [38]</cell><cell>Private</cell><cell>250M</cell><cell>82.3 / 83.2</cell><cell>82.6 / 83.7</cell></row><row><cell>VQGAN [18]</cell><cell>OpenImage</cell><cell>9M</cell><cell>82.9 / 83.8</cell><cell>83.3 / 84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Configurations for pre-training. For the classification task, the fullyconnected layer is employed as the classifier after the average pooling of the feature embeddings. The fine-tuning configurations on ImageNet-1K for different backbone architectures are listed in Tab. 8.</figDesc><table><row><cell>Configuration</cell><cell>ViT-Base/16</cell><cell>ViT-Large/16</cell></row><row><cell>Layers</cell><cell>12</cell><cell>24</cell></row><row><cell>Hidden size</cell><cell>768</cell><cell>1024</cell></row><row><cell>FFN inner hidden size</cell><cell>3072</cell><cell>4096</cell></row><row><cell>Attention heads</cell><cell>12</cell><cell>16</cell></row><row><cell>Attention head size</cell><cell>64</cell><cell></cell></row><row><cell>Patch size</cell><cell cols="2">16 ? 16</cell></row><row><cell>Training epochs</cell><cell>800</cell><cell></cell></row><row><cell>Batch size</cell><cell>2048</cell><cell></cell></row><row><cell>Adam ?</cell><cell>1e-8</cell><cell></cell></row><row><cell>Adam ?</cell><cell cols="2">(0.9, 0.98)</cell></row><row><cell>Peak learning rate</cell><cell>1.5e-3</cell><cell></cell></row><row><cell>Minimal learning rate</cell><cell>1e-5</cell><cell></cell></row><row><cell>Learning rate schedule</cell><cell cols="2">Cosine</cell></row><row><cell>Warmup epochs</cell><cell>10</cell><cell></cell></row><row><cell>Gradient clipping</cell><cell>3.0</cell><cell>1.0</cell></row><row><cell>Dropout</cell><cell>None</cell><cell></cell></row><row><cell>Stoch. depth</cell><cell>0.1</cell><cell></cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell></cell></row><row><cell>Data Augment</cell><cell cols="2">RandomResizeAndCrop</cell></row><row><cell>Input resolution</cell><cell cols="2">224 ? 224</cell></row><row><cell cols="2">A.2 Configuration for fine-tuning</cell><cell></cell></row><row><cell cols="2">Classification task on ImageNet-1K</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Configurations for fine-tuning on ImageNet-1K.</figDesc><table><row><cell>Configuration</cell><cell>ViT-Base/16</cell><cell>ViT-Large/16</cell></row><row><cell>Peak learning rate</cell><cell cols="2">{2e-3,3e-3,4e-3,5e-3}</cell></row><row><cell>Fine-tuning epochs</cell><cell>100</cell><cell>50</cell></row><row><cell>Batch size</cell><cell>1024</cell><cell></cell></row><row><cell>Warmup epochs</cell><cell>20</cell><cell>5</cell></row><row><cell>Layer-wise learning rate decay</cell><cell>0.65</cell><cell>0.75</cell></row><row><cell>Adam ?</cell><cell>1e-8</cell><cell></cell></row><row><cell>Adam ?</cell><cell cols="2">(0.9, 0.999)</cell></row><row><cell>Minimal learning rate</cell><cell>1e-6</cell><cell></cell></row><row><cell>Learning rate schedule</cell><cell cols="2">Cosine</cell></row><row><cell>Repeated Aug</cell><cell>None</cell><cell></cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell></cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell></cell></row><row><cell>Stoch. depth</cell><cell>0.1</cell><cell></cell></row><row><cell>Dropout</cell><cell>None</cell><cell></cell></row><row><cell>Gradient clipping</cell><cell>None</cell><cell></cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell><cell></cell></row><row><cell>Input resolution</cell><cell cols="2">224 ? 224</cell></row><row><cell>Rand Augment</cell><cell>9/0.5</cell><cell></cell></row><row><cell>Mixup prob.</cell><cell>0.8</cell><cell></cell></row><row><cell>Cutmix prob.</cell><cell>1.0</cell><cell></cell></row><row><cell>Color jitter</cell><cell>0.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Configurations for fine-tuning on COCO.</figDesc><table><row><cell>Configuration</cell><cell>ViT-Base/16</cell></row><row><cell>Fine-tuning epochs</cell><cell>25</cell></row><row><cell>Peaking learning rate</cell><cell>8e-5</cell></row><row><cell>Learning rate decay</cell><cell>cosine</cell></row><row><cell>Adam ?</cell><cell>1e-8</cell></row><row><cell>Adam ?</cell><cell>(0.9, 0.999)</cell></row><row><cell>Dropout</cell><cell>None</cell></row><row><cell>Stoch. depth</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.1</cell></row><row><cell>Batch size</cell><cell>64</cell></row><row><cell>Input size</cell><cell>1024 ? 1024</cell></row><row><cell>Position embedding</cell><cell>Abs. + Rel.</cell></row><row><cell>Augmentation</cell><cell>LSJ(0.1, 2.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>We provide another experiment results of object detection and instance segmentation on COCO following the implementation of iBOT<ref type="bibr" target="#b45">[46]</ref>. Intermediate fine-tuning denotes the model is further fine-tuned on ImageNet-1K. Cascaded Mask R-CNN and 1? training schedule are adopted.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Object Det. AP b</cell><cell>Instance Seg. AP m</cell></row><row><cell>Supervised [39]</cell><cell>ICML 2021</cell><cell>47.9</cell><cell>42.9</cell></row><row><cell>MoCo v3 [12]</cell><cell>CVPR 2021</cell><cell>47.9</cell><cell>42.7</cell></row><row><cell>DINO [6]</cell><cell>ICCV 2021</cell><cell>50.1</cell><cell>43.4</cell></row><row><cell>iBOT [46]</cell><cell>ICLR 2022</cell><cell>51.2</cell><cell>44.2</cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>49.6</cell><cell>42.8</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>50.1</cell><cell>43.1</cell></row><row><cell>+Intermediate Fine-tuning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BEiT [1]</cell><cell>ICLR 2022</cell><cell>50.7</cell><cell>43.8</cell></row><row><cell>Ours</cell><cell>this paper</cell><cell>51.2</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Configurations for fine-tuning on ADE20k. Bilinear evaluate the performances after intermediate fine-tuning, where the pre-trained models have been fine-tuned on ImageNet-1K. For the models with intermediate fine-tuning, the training recipe is the same as the pre-training only version.</figDesc><table><row><cell>Configuration</cell><cell>ViT-Base/16</cell></row><row><cell>Peaking learning rate</cell><cell>8e-5</cell></row><row><cell>Fine-tuning steps</cell><cell>160000</cell></row><row><cell>Batch size</cell><cell>16</cell></row><row><cell>Adam ?</cell><cell>1e-8</cell></row><row><cell>Adam ?</cell><cell>(0.9, 0.999)</cell></row><row><cell>Layer-wise learning rate decay</cell><cell>0.9</cell></row><row><cell>Minimal learning rate</cell><cell>0</cell></row><row><cell>Learning rate schedule</cell><cell>Linear</cell></row><row><cell>Warmup steps</cell><cell>1500</cell></row><row><cell>Dropout</cell><cell>None</cell></row><row><cell>Stoch. depth</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.05</cell></row><row><cell>Input resolution</cell><cell>512?512</cell></row><row><cell>Position embedding</cell><cell>Relative</cell></row><row><cell>Position embedding interpolate</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the National Natural Science Foundation of China under Grant 62088102, and in part by the PKU-NTU Joint Research Institute (JRI) sponsored by a donation from the Ng Teng Fong Charitable Foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mc-BEiT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">BEit: BERT pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">VICReg: Variance-invariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: high quality object detection and instance segmentation. TPAMI pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1483" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03026</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL. pp</title>
		<imprint>
			<biblScope unit="page" from="4171" to="4186" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Peco: Perceptual codebook for bert pre-training of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unleashing vanilla vision transformer with masked image modeling for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02964</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12408</idno>
		<title level="m">Miles: Visual bert pre-training with injected language semantics for video-text retrieval</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<title level="m">Bootstrap your own latent-a new approach to self-supervised learning. NIPS pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Openimages: A public dataset for large-scale multilabel and multi-class image classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Benchmarking detection transfer learning with vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="mc" to="BEiT" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">Simmim: A simple framework for masked image modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09616</idno>
		<title level="m">Masked image modeling with denoising contrast</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5122" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Image BERT pre-training with online tokenizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
