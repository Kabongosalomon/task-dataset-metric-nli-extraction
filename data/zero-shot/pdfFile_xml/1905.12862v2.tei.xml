<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Fashion Recommendation: A Semantic Attribute Region Guided Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wu</surname></persName>
							<email>lewu@hfut.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<addrLine>4 WeBank</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
							<email>vincentz@webank.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Fashion Recommendation: A Semantic Attribute Region Guided Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In fashion recommender systems, each product usually consists of multiple semantic attributes (e.g., sleeves, collar, etc). When making cloth decisions, people usually show preferences for different semantic attributes (e.g., the clothes with v-neck collar). Nevertheless, most previous fashion recommendation models comprehend the clothing images with a global content representation and lack detailed understanding of users' semantic preferences, which usually leads to inferior recommendation performance. To bridge this gap, we propose a novel Semantic Attribute Explainable Recommender System (SAERS). Specifically, we first introduce a fine-grained interpretable semantic space. We then develop a Semantic Extraction Network (SEN) and Fine-grained Preferences Attention (FPA) module to project users and items into this space, respectively. With SAERS, we are capable of not only providing cloth recommendations for users, but also explaining the reason why we recommend the cloth through intuitive visual attribute semantic highlights in a personalized manner. Extensive experiments conducted on realworld datasets clearly demonstrate the effectiveness of our approach compared with the state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ubiquity of online fashion shopping has led to information explosion in the fashion industry. That imposes an increasing challenge for users who have to choose from a large number of available fashion products for satisfying personalized demands. Moreover, to promote profits growth, the fashion retailers have to understand the preferences of different customers and provide more intelligent recommendation services. However, unlike generic objects, clothes usually present significant variations of visual appearance, which has a vital impact on consumer decision. * Corresponding Author Along this line, some studies have enhanced the performance of fashion recommendation via various visual features. For example, ImageNet 1 pre-trained convolutional neural networks (CNNs) was adopted to extract visual representation <ref type="bibr" target="#b6">[McAuley et al., 2015;</ref><ref type="bibr" target="#b3">He and McAuley, 2016b;</ref><ref type="bibr" target="#b3">He et al., 2016b]</ref>. As these visual features were trained for the image classification task, the pre-trained CNNs could only capture category information. <ref type="bibr" target="#b8">Yu et al. [2018b]</ref> and  attempted to get a better understanding of clothing by utilizing aesthetics and style features, respectively. These methods have hammered at comprehending clothing from a holistic perspective via global categories, aesthetics and style information, while failing to dive deep into the details to explore clothing fine-grained semantic aspects.</p><p>However, when purchasing clothing products, it is intuitive that we often have preferences for detailed semantic attributes (such as neckline, heel height, skirt length) in addition to global impressions. For instance, <ref type="figure" target="#fig_0">Figure 1</ref> shows two users' purchase histories. The first user chooses three products in different categories with the same kind of neckline. Therefore, we can infer that she prefers V-neckline clothes. Meanwhile, the second user prefers the pointed toe incorporated with high-heeled shoes as evidenced by the frequent appearance of these two semantic attributes in her chosen products. In fact, clothing is not atomic and a piece of clothing usually consists of multiple attributes, such as neckline, sleeves length. In some cases, people choose a product because it contains the attributes they like. Therefore, the clothing semantic attribute can not only help us generate a comprehen-sive representation of items, but also make a deep understanding of user preferences. Unfortunately, there are still many unique challenges inherent in designing an effective solution to integrate semantic attribute information into fashion recommendation. On the one hand, it's difficult to obtain clothing semantic attributes features without the manual attribute annotation in the large-scale E-commerce data. On the other hand, the user preferences are sophisticated, while traditional methods usually transform the item image into a latent vector directly. These two aspects make it hard to generate explainable recommendations with current recommendation models.</p><p>To address the challenges mentioned above, in this paper, we propose a novel Semantic Attribute Explainable Recommender System (SAERS) for fashion recommendation. In SAERS, we introduce a fine-grained interpretable space named semantic attribute space, where each dimension corresponds to a semantic attribute. We project users and items into this space to capture the user's fine-grained preferences and generate explainable recommendations. Specifically, we firstly develop a Semantic Extraction Network (SEN), which is used to extract the region-specific attribute representations in a weakly-supervised manner. Through SEN, each item is projected to the semantic attribute space. Then, to capture the diversity of the semantic attribute preference among items, we design a Fine-grained Preferences Attention (FPA) module to automatically match the user preference for each attribute in semantic attribute space, and aggregate all attributes with different weights. Finally, we optimize the SAERS in Bayesian Personalized Rank (BPR) framework. Extensive experiments on the large-scale real-world Amazon dataset reveal that SAERS not only significantly outperforms several baselines on the visual recommendation task, but also provides interpretable insights by highlighting attribute semantic in a personalized manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generally, the related work can be grouped into the following three categories, i.e., visual-aware recommendation, attribute localization and explainable recommendation.</p><p>Visual-aware Recommendation. Visual-aware recommendation uses visual signals presented in the underlying data to model visual characteristics of items and user preferences. Some previous works <ref type="bibr" target="#b6">[McAuley et al., 2015;</ref><ref type="bibr" target="#b3">He and McAuley, 2016b;</ref><ref type="bibr" target="#b3">He and McAuley, 2016a;</ref><ref type="bibr" target="#b3">He et al., 2016b;</ref><ref type="bibr" target="#b4">Le et al., 2019]</ref> directly utilized the ImageNet pre-trained CNN to generate items' visual representation. To further explore the information contained in the product image,  extracted style features to represent item via eliminating the corresponding categorical information from the visual feature vector generated by CNN. <ref type="bibr" target="#b8">Yu et al. [2018b]</ref> leveraged an aesthetic network to obtain image aesthetic elements for fashion recommendation task. In <ref type="bibr" target="#b3">[Kang et al., 2017;</ref><ref type="bibr" target="#b5">Lei et al., 2016]</ref>, Siamese framework was utilized to obtain task-specific visual representation. This framework allowed CNN to be trained comparatively with two 'copies' and joined by a triplet loss function. Unfortunately, these methods focused on products' global characteristics and transferred item image into a fixed latent vector. However, the fine-grained attribute has been largely under-exploited.</p><p>Attribute Localization. In computer vision, the semantic attribute is a type of important auxiliary information and has been leveraged as a fine-grained representation for image understanding <ref type="bibr" target="#b3">Kovashka et al., 2012]</ref>.  <ref type="bibr">[2014]</ref> improved the interpretability of collaborative filtering models by utilizing explicit factor based matrix factorization (MF) as explainable components. They aligned each latent dimension in MF with a particular explicit product aspects/topics. In deep learning based methods, researchers discovered that the attention mechanism was capable of improving model explainability by automatically learning the importance of explicit features and also refined user/item embedding <ref type="bibr" target="#b2">Gao et al., 2019;</ref>.  proposed an attentive architecture with the supervision of user implicit feedback as well as textual reviews to capture users' visual preferences. These works attempted to make intuitional explanations for the recommendations but were limited on the item level. In this paper, we take a further step to explain the user preferences on the visual attribute level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SAERS: Semantic Attribute Explainable Recommender System</head><p>In this section, we introduce our proposed SAERS for addressing the personalized fashion recommendation problem. Most existing works represent users and items in a global visual space (as shown in <ref type="figure" target="#fig_1">Figure 2(a)</ref>), but the meaning of each dimension is unknown, reducing the interpretability of recommendations. To bridge this gap, SAERS utilizes a new semantic attribute visual space. In this space, each dimension represents an attribute, which corresponds to different regions of the clothing. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), items are split into several semantic attributes via Semantic Extraction Network, then projected into the semantic attribute visual space. After that, users are projected according to fine-grained preferences for clothing attributes. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, SAERS contains two main components, i.e., Semantic Extraction Network (SEN) and Fine-grained Preferences Attention (FPA). Specifically, with SEN, we first obtain the fashion item projections in the semantic feature space. Next, we design FPA to project users into the same semantic feature space. Then, we jointly learn the item representation in both global visual space and semantic attribute visual space under a pair-wise learning framework. Finally, with attribute preference inference, we can generate the explainable recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Projecting Item into Semantic Attribute Space</head><p>In this subsection, we investigate how to project items into semantic attribute space. First, we need to divide the item into several parts (semantic attributes), where each part corresponds to one dimension in the space. And then, we extract attribute representations for them. Whereas, most real-world E-commerce datasets lack attribute annotations to learn se-mantic attribute representation directly. To this end, we borrow a fine-grained labeled data and pre-train a Semantic Extraction Network (SEN), which is used to extract the regionspecific attribute representations and simultaneously locate and classify attributes.</p><p>Due to the lack of training data with bounding box annotations, we cannot leverage traditional object detection methods <ref type="bibr">[Girshick, 2015]</ref> to locate and extract attribute representations straightforwardly. Therefore, we utilize Gradientweighted Attribute Activation Maps (Grad-AAM) <ref type="bibr" target="#b6">[Selvaraju et al., 2017]</ref> to develop SEN in a weakly-supervised manner. Grad-AAM is capable of producing a coarse location highlighting according to the gradient information of target attribute in CNN. In SEN, we first train a classification CNN. Then, according to the results of CNN, we calculate the Grad-AAM to get attributes location. Finally, the region-specific attribute representations can be obtained by ROI pooling layer.</p><p>We combine the UT-Zap50K shoes 2 dataset and the Tianchi Apparel 3 dataset, which contains 50,025 shoe and over 180,000 apparel image-level attribute annotations respectively. As listed in <ref type="table">Table 1</ref>, each row represents an attribute. We employ 12 attributes that cover the various locations of the clothing, with each attribute is classified into several classes as shown after the colon. Then, the combined dataset is used to train a multi-task classification network. The network architecture can be arbitrary. In this work, ResNet-50 <ref type="bibr" target="#b3">[He et al., 2016a]</ref> is utilized. We use the following classifica-tion loss to train the network:</p><formula xml:id="formula_0">L C = ? N I=1 A a=1 log (p (? Ia |y Ia )) ,<label>(1)</label></formula><p>where? Ia represents the ground truth class of the a th attribute of the I th image. y Ia is the corresponding predicted result. N is the number of training examples and A is the number of attributes. The posterior probability estimates the probability of y Ia to be classified as? Ia . The classification loss is used to discover the most relevant regions of attributes. After the classification network has converged, we start to calculate the Grad-AAM M ac Grad-AAM for class c of an attribute a, where c is determined by the class that maximizes the classification confidence. We first compute the gradient of the score for class a c , y ac (before the softmax), with respect to feature maps F t of the last convolutional layer, i.e., ?y ac ?F t . These gradients flowing back are global-averaged-pooled to obtain the neuron importance weights ? ac t :</p><formula xml:id="formula_1">? ac t = global average pooling 1 Z m n ?y ac ?F t mn gradients via backpop .<label>(2)</label></formula><p>Here, the (m, n) indicates the spatial location of the t th channel feature map F t in the last convolutional layer. This weight ? ac t represents a partial linearization of the deep network downstream from F , and captures the 'importance' of feature map t for the target class a c .</p><p>We perform a weighted combination of forwarding activation maps, and follow it by a ReLU to obtain,</p><formula xml:id="formula_2">M ac Grad-AAM = ReLU t ? ac t F t linear combination .<label>(3)</label></formula><p>Actually, each element of M ac Grad-AAM indicates attribute class a c 's contribution of the activation. Meanwhile, these elements can be associated with the location of the input image. Consequently, by using M ac Grad-AAM , attribute location can be added to the network. In order to do so, all attribute locations are estimated with a simple hard threshold technique. As the implementation in <ref type="bibr" target="#b0">[Ak et al., 2018]</ref>, the pixel values that are above 20% of the maximum value in the generated map are segmented. This is followed by estimating a bounding box, that covers the largest connected region in the Grad-AAM. This step is repeated for each attribute. With attribute's bounding box and the last convolutional layer's feature map as input, then the ROI pooling layer <ref type="bibr">[Girshick, 2015]</ref> is used to generate region-specific attribute representation.</p><p>Accordingly, given a clothing image, we can obtain 12 attribute representation through the pre-trained SEN as introduced above. We use f k a to denote the k th attribute feature generated by SEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projecting User into Semantic Attribute Space</head><p>In this subsection, we describe how to project users into the semantic attribute space and capture fine-grained user preference on attributes. To get the final embedding of item i with respect to all the attribute features, our first intuition is to equally fuse all the features with an average pooling as follows:</p><formula xml:id="formula_3">f (i) = 1 A A k=1 E k f k a (i),<label>(4)</label></formula><p>where f k a (i) ? R m denotes the k th attribute feature of item i, E k ? R d?m is a matrix for transferring the k th attribute feature to lower dimension d. However, as discussed in the introduction, there may be a variety of clothing attributes that users are interested in. Different attributes may have different impacts on the candidate item i when considering whether user u will buy i or not. Therefore, we argue that the item embedding should be fused with adaptive weights to capture users' dynamic preference over attributes and enhance the interpretability of recommendations. Inspired by the recent success of attention mechanism , which allows different parts to contribute differently when compressing them to a single representation. We propose an attributelevel attention module, namely Fine-grained Preferences Attention (FPA). Specifically, as illustrated in the right upper part of <ref type="figure" target="#fig_2">Figure 3</ref>, we use a DNN D (in this work is a twolayer multilayer perceptron) to compute attention weight ? k ui with user embedding f (u) ? R d and item's k th transferred attribute feature E k f k a (i)'s concatenated vector as input:</p><formula xml:id="formula_4">? k ui = softmax D f (u), E k f k a (i) = exp D f (u), E k f k a (i) A k=1 exp (D (f (u), E k f k a (i)))</formula><p>.</p><p>The embedding of item i can thus be calculated as the weighted sum of its attribute features:</p><formula xml:id="formula_6">f (i) = A k=1 ? k ui E k f k a (i).<label>(6)</label></formula><p>Through this way, users are aligned in the semantic attribute space, and the preference towards each attribute can be captured.</p><p>Besides the semantic attributes, clothes usually have global characteristics information such as style, categories and aesthetics. Consequently, the embedding of item i can be calculated as the weighted sum of its attribute features plus the global characteristics feature:</p><formula xml:id="formula_7">f (i) = A k=1 ? k ui E k f k a (i) + f g (i),<label>(7)</label></formula><p>where f g (i) ? R d denotes the global characteristics feature. In this work, we leverage a Siamese CNN architecture based on the AlexNet [Krizhevsky et al., 2012] to obtain global characteristics feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Learning Stage</head><p>When making predictions, we feed the user embedding f (u) ? R d and the final item embedding f (i) ? R d into a function:</p><formula xml:id="formula_8">? ui = P (f (u), f (i)) ,<label>(8)</label></formula><p>where P is an arbitrary prediction function, such as the inner product between the corresponding embeddings, or a prediction neural network <ref type="bibr" target="#b3">[He et al., 2017]</ref>. Here, we choose the inner product? ui = f (u) ? f (i) as a specific implementation, as it gives us better training efficiency and avoids overfitting on our large-scale data. Since we address the fashion recommendation task from the ranking perspective, we employ a pairwise learning method to optimize model parameters. The assumption of pairwise learning is that the model could predict a higher score for an observed interaction that its unobserved counterparts. We adopt the Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b6">[Rendle et al., 2009]</ref> loss, which has been widely used in recommender systems:</p><formula xml:id="formula_9">L = (u,i,j)?D S ? ln ? (? ui ?? uj ) + ? ? 2 ,<label>(9)</label></formula><p>where ? is the sigmoid function and ? is the regularization hyper-parameter. ? includes all model parameters. D S is the training set, which consists of triples in the form (u, i, j). u denotes the user together with an interacted item i and a non-observed item j. In each iteration, we sample a user u, a positive item i ? I + u , and a negative item j ? I\I + u . Thus, the convolutional neural network ?, which is used to extract item's global characteristic feature, has two images: X i and X j to consider. Both CNNs ? (X i ) and ? (X j ) share the same weights. To optimize the above objective function, we employ the widely used Adam [Kingma and Ba, 2014] optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attribute Preference Inference</head><p>We project users and items into a new interpretable space. In this space, the user's preferences for attributes can be calculated, making it possible to generate personalized explainable recommendations. Specifically, when we recommend an item to a user, the user's preference towards each attribute of the item can be obtained according to the attention weight (Equation 5). The attribute with a large attention weight indicates the user's preference. Utilizing the SEN's location (Equation 3) and classification (Equation 1) ability, we can further go back to find out where the attribute is located and what the specific class is.</p><p>Accordingly, the explanation consists of three parts: (1) SAERS uses a bounding box to highlight which part of the product image the user might like. (2) SAERS provides which semantic attribute the highlighted part belongs to. (3) SAERS provides the possibility that the user likes the semantic attribute. We will demonstrate the explainable recommendation results in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments on a real-world dataset to verify the feasibility of our proposed framework. We first introduce the experimental setup, followed by the experiment results. We will also discuss the effectiveness of semantic attribute features and give a case study of SAERS's visual explainablility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset. We evaluate our methods on a real-world ecommerce dataset, i.e., Amazon Fashion. This dataset was introduced in <ref type="bibr" target="#b3">[He and McAuley, 2016b;</ref><ref type="bibr" target="#b3">Kang et al., 2017]</ref> and consist of reviews of clothing items crawled from Amazon.com. Amazon Fashion contains six representative fashion categories (men/women's tops, bottoms and shoes). We treat users' reviews as implicit feedback. There are 45,184 users, 166,270 items, and 358,003 records in total. The sparsity of the dataset is 99.9952%. In Amazon Fashion dataset, we discard inactive users with purchase records less than 5, similar as previous works <ref type="bibr" target="#b3">[He and McAuley, 2016b;</ref><ref type="bibr" target="#b3">Kang et al., 2017]</ref>. For each user, we randomly select one record for validation and another one for test.</p><p>Evaluation Protocol. We evaluate our method in two experiment scenarios: (1) In pair-wise recommendation, we use the Area Under the Roc Curve (AUC) <ref type="bibr" target="#b6">[Rendle et al., 2009]</ref> to measure the probability that a model will rank a randomly chosen positive instance higher than a randomly chosen negative one. (2) In list-wise top-N recommendation, the Normalized Discounted Cumulative Gain (NDCG@N) are calculated to evaluate the performance of the baselines and our model. In order to reduce the computational cost, we randomly select a user, and then randomly sample 500 unrated items at each time and combine them with the positive item the user likes in the ranking process. We repeated this procedure 10000 times and report the average ranking results .</p><p>Implementation Details. For all the models (except Random and PopRank), we tune hyper-parameters via grid search on the validation set, with a regularizer selected from [0, 0.0001, 0.001, 0.01, 0.1, 1], learning rate selected from [0.0001, 0.001, 0.01], and the latent feature dimension of <ref type="bibr">[10,</ref><ref type="bibr">30,</ref><ref type="bibr">50,</ref><ref type="bibr">100]</ref>. We use mini-batch size of 256 to train all the models until they converge. All experiments are trained with NVIDIA K80 graphics card and implemented by Tensorflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Methods</head><p>To demonstrate the effectiveness of SAERS, we compare it with the following alternative methods:</p><p>? Random: This baseline ranks items randomly for all users.</p><p>By definition, this method has AUC=0.5. ? PopRank: This is a nontrivial method that items are ranked in order of their popularity within the system.   ? BPR-MF <ref type="bibr" target="#b6">[Rendle et al., 2009]</ref>: This is a classical matrix factorization method based on pairwise personalized ranking for implicit feedback datasets. ? VBPR <ref type="bibr" target="#b3">[He and McAuley, 2016b]</ref>: One of the state-of-theart methods for visually-aware personalized ranking from implicit feedback. VBPR is a form of hybrid content-aware recommendation that makes use of pre-trained CNN features of product images. ? DeepStyle : A state-of-the-art method for visual recommendation, based on the learned style features and the BPR framework. The style features are obtained by subtracts categories information from visual features of items generated by CNN. <ref type="bibr" target="#b3">Zhang et al., 2017]</ref>: It is a state-of-the-art neural recommender system, which can leverage multimodal side information for Top-N recommendation. We only use image visual information in this baseline.</p><formula xml:id="formula_10">- - - - - -- - 0.75 -- - -- -- 0.70 - ? 0.65 - - - - - 0.60 - - - - - 0.55 - 0.50 i i n n - 0.45 d=l0 d=30 d=50 d=l00 d</formula><formula xml:id="formula_11">- - - -- 0.75 - - - - - - - - - 0.70 - - - - ? 0.65 - 0.60 - 0.55 - 0.50 - - 111 nn n, nn - 0.45 d=l0 d=30 d=50 d=l00 d</formula><formula xml:id="formula_12">? JRL [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Recommendation Performance</head><p>In this section, we evaluate our model for fashion recommendation. <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_5">Figure 5</ref> present the recommendation performance on AUC in All Items and Cold Items scenarios, respectively. For the latter, we seek to estimate relative preference scores among items that have never been observed at training time. <ref type="figure" target="#fig_6">Figure 6</ref> depicts the Top-N recommendation performance on two scenarios when embedding dimension d=100. <ref type="figure" target="#fig_7">Figure 7</ref> shows the AUC during training. The main findings are summarized as follows:</p><p>? We can find that our proposed SAERS achieves the best performance on AUC, with the improvement by up to 5.8% compared to the second-best method across all datasets, and 10.2% in cold-start scenarios. In <ref type="figure" target="#fig_6">Figure 6</ref>, we can also   find that our model outperforms the other baseline methods in Top-N recommendation at two scenarios. The result demonstrates the significant benefits of incorporating semantic attribute features in fashion recommendation. ? Our method surpasses the strongest collaborative filtering based method (BPR-MF) significantly, especially in Cold Items scenarios. This phenomenon is largely due to the fact that the dataset is particularly sparse. The sparsity of the dataset is 99.9952% and on average each item is only be purchased 2.15 times. This further illustrates the importance of using content-based recommendation methods and in-depth understanding of image information in fashion recommendation tasks. ? The performance of DeepStyle using style information does not work better than VBPR, by directly using CNN pre-trained features. DeepStyle assumes that style feature can be obtained by subtracting categorical information from visual features of items generated by CNN. However, in fashion recommendation tasks, there are few categories of clothing, but the style is indeed diverse. Therefore, the assumption could not work well. This reflects the difficulty of effectively incorporating content information into the recommender system. ? <ref type="figure" target="#fig_4">Figure 4</ref> and 5 show the sensitivity of the models to the embedding dimension d. It shows that SAERS is robust to hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Necessity of the Semantic Attribute</head><p>In this subsection, we discuss the necessity of using the semantic attribute features extracted by SEN. We construct three variant implements based on VBPR <ref type="bibr" target="#b3">[He and McAuley, 2016b]</ref> and SAERS to verify the effectiveness of incorporat-  As shown in <ref type="table" target="#tab_4">Table 2</ref>, the usage of SAF can increase the effect of VBPR and our model by 1.4% and 1.8%, respectively. Experimental results show our SAERS model, which captures both local attribute semantic information and global characteristic information, performs the best result on the Amazon dataset since these two kinds of information mutually enhance each other to a certain extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">User Attribute Interest Visualization</head><p>In order to better demonstrate the interpretability of our model, we randomly selected four users. As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, for each user, we present 3 logs from the purchase history as well as our recommendation result. The value in the red box indicates the attention weight. The greater weight demonstrates the higher personal preferences for this attribute. We show the user's favorite attribute calculated by the Fine-grained Preferences Attention (FPA) and locate it via Grad-AAM. (1) Attribute class, (2) attribute location and (3) attention weight are corresponding to three visual explanations we mentioned in Section 3.4. For example, SAERS recommends a T-shirt to user A and tells him that the reason for the recommendation is short sleeves and the position corresponding to the sleeve is highlighted. This is reasonable because the user has purchased three short sleeves clothing. For user C, SAERS recommends a shirt with V-neckline because it accurately captures the user's preference on this attribute. In the recommendation results, we can see that the neckline of this shirt has the largest attention weight, which accounts for 12.76%. This explains the reason we recommend this shirt: user C likes this kind of neckline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed SAERS, aiming at comprehending user preference in fine-grained attribute level and providing visual interpretability in fashion recommendation tasks. To achieve this goal, we first introduced a fine-grained interpretable space, namely, semantic attribute space. Then we developed a Semantic Extraction Network (SEN) and Fine-grained Preferences Attention (FPA) module to project </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of user preferences for semantic attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Difference between the conventional (a) Global Visual Space and our (b) Semantic Attribute Visual Space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The architecture for Semantic Attribute Explainable Recommender System (SAERS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>I</head><label></label><figDesc>c=J Random c=J PopRank c::::J BPR-MF c=JVBPR c=J DeepStyle c=JJRL c=J SAERS I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Overall performance of difference choices of embedding dimension d in All Items scenario. I c=J Random c=J PopRank c::::J BPR-MF c=JVBPR c=J DeepStyle c=JJRL c=J SAERS I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Overall performance of difference choices of embedding dimension d in Cold Items scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>NDCG@N in top-N recommendation for All Items and Cold Items scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Performance of training epochs (test set)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Examples of the visual explanations (The IDs of User A, B, C and D are A2ZEFKXUBN2IIS, A2OLJU8M0L4YZD, AIUZT213W0WDE and AQ6BC76U77C7X) users and items into this space, respectively. Finally, we jointly learned the item representation in both global visual space and semantic attribute space under a pair-wise learning framework. The experimental results on a large-scale realworld dataset clearly demonstrated the effectiveness and explanatory power of SAERS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Top high neck: ruffle semi-high, turtle,... collar: rib collar, puritan collar,... lapel: notched, shawl, collarless,... neckline: V, square, round,... sleeves length: sleeveless, cap, short,... body length: high waist, long, regular,... ankle, knee-high, mid-calf,... closure: lace-up, slip-on, zipper,... toe style: round, pointed, peep, open,...</figDesc><table><row><cell>Category</cell><cell>Attribute: Class</cell></row><row><cell>Bottom</cell><cell>skirt length: short, knee, midi, ankle,... trousers length: short, mid, 3/4, cropped,...</cell></row><row><cell></cell><cell>heel height: flat, 1 in-7/4 in, under 1 in,...</cell></row><row><cell>Shoes</cell><cell>boots height:</cell></row></table><note>Table 1: List of semantic attributes used in our method</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Improvement of using semantic attribute feature.ing the semantic attribute information.? SAFo: This is a model with Semantic Attribute Features only. Items are represented as the fusion of all semantic attributes representations, as shown in Equation 4. ? VBPR+SAF: Add the Semantic Attribute Features to the CNN features VBPR used as the item representation. ? SAERS-SAF: A variant of our model that uses only global characteristic features extract by Siamese CNN to represent items.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ 3 https://tianchi.aliyun.com/competition/entrance/231671/ information</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partially supported by grants from the National Key Research and Development Program of China (No. 2016YFB1000904) and the National Natural Science Foundation of China (Grants No. U1605251, 61727809,  61602147).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning attribute representations with localization for flexible fashion search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdollahi ; Ak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-Seng Chua. Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
	<note>SIGIR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="arXiv">arXiv:1801.10288</idno>
		<title level="m">Zheng Qin, and Hongyuan Zha. Visually explainable recommendation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2015. [He and McAuley, 2016a] Ruining He and Julian McAuley</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sherlock: sparse hierarchical embeddings for visually-aware one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcauley ; Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vbpr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Visual bayesian personalized ranking from implicit feedback</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR. Krizhevsky et al., 2012. Imagenet classification with deep convolutional neural networks. In NIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A hierarchical attention model for social contextual image recommendation</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from history and present: Next-item recommendation via discriminatively exploiting user behaviors</title>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1734" to="1743" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling the evolution of users&apos; preferences and social links in social networking services</title>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<editor>Krishna Kumar Singh and Yong Jae Lee</editor>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1458" to="1466" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple pairwise ranking with implicit feedback</title>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1727" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis</title>
		<idno type="arXiv">arXiv:1804.11192</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable recommendation: A survey and new perspectives</title>
		<editor>Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma</editor>
		<meeting><address><addrLine>Qingyao Ai</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SIGIR. Zhang et al., 2017] Yongfeng Zhang</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint representation learning for top-n recommendation with heterogeneous information sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce Croft ;</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3711" to="3717" />
		</imprint>
	</monogr>
	<note>CIKM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
