<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PolarDet: A Fast, More Precise Detector for Rotated Target in Aerial Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenshen</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjia</forename><surname>Bu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<email>pu@hikvision.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Bu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiliang</surname></persName>
						</author>
						<title level="a" type="main">PolarDet: A Fast, More Precise Detector for Rotated Target in Aerial Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fast and precise object detection for high-resolution aerial images has been a challenging task over the years. Due to the sharp variations on object scale, rotation, and aspect ratio, most existing methods are inefficient and imprecise. In this paper, we represent the oriented objects by polar method in polar coordinate and propose PolarDet, a fast and accurate one-stage object detector based on that representation. Our detector introduces a sub-pixel center semantic structure to further improve classifying veracity. PolarDet achieves nearly all SOTA performance in aerial object detection tasks with faster inference speed. In detail, our approach obtains the SOTA results on DOTA, UCAS-AOD, HRSC with 76.64% mAP, 97.01% mAP, and 90.46% mAP respectively. Most noticeably, our PolarDet gets the best performance and reaches the fastest speed(32fps) at the UCAS-AOD dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, object detectors <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b32">[33]</ref> based on convolutional neural network (CNN) have got many achievements in nature scene detection. Yet many shortages still exist when these methods straight transferred to aerial image. As shown in <ref type="figure">Fig.3</ref>, the horizontal bounding box, which is oversized, will cover the background and create wrong suppression. To avoid these problems, many detectors <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref> use five-parameter method with ? (x, y, w, h, ?) to express object orientation. However, due to the sharp change in rotation, one angle expression will cause many defects, such as precision decrease, angle boundary missing, and angle loss trap (as discussed in <ref type="bibr">3.4)</ref>. Also, directly regressing w and h will drop the network convergence performance. That is because of sharp variations on the object scale and aspect ratio. To increase the expression precision and avoid angle loss trap, eightparameter method (?x i , ?y i , (i = 1, 2, 3, 4)) is proposed by some detectors. But these methods still cannot resolve the decrease in network convergence performance.</p><p>In this paper, we propose a fast and more precise alternative polar method, called PolarDet. Based on the polar coordinate, we represent the target by multiple angles and shorter-polar diameter ratio. To be detailed, as shown in <ref type="figure" target="#fig_1">Fig.2</ref>, we represent target by ((x, y), (?x, ?y), ? p , s, r p , (p = 1, 2, 3, 4)). Concretely, targets will be described by a center point, offset, four polarangle, one shorter side, and four polar-ratio. They represent the center of the target, the offset of the target center, the angles between four polar diameters and the reference y-axis (will be explained in Section 3), the shorter one between the minimum bounding rectangle width and its height, the ratio between shorter side and polar diameter, respectively. The polar diameter here represents the Euclidean distance between the center point and corner. With the four angles prediction, our polar method can express orientation more precisely and avoid angle loss trap. With the relative polar diameter regression (shorter side and polar-ratio), our po- Existing shortages in current methods, (a)incorrect cover(same category), (b)incorrect cover(different category), (c)wrong suppression because of NMS lar method also can increase network convergence performance. We will discuss how it works and its advancement in Section 3.</p><p>Besides the polar method, we also introduce an improved center semantic Structure to enhance the classification capability of our network. Most importantly, our result is got from the single ResNet <ref type="bibr" target="#b11">[12]</ref> network as backbone and without FPN structure. With rarely extra parameters increasing, this achievement satisfies the need for speed and meets the requirement of accuracy at the same time.</p><p>This paper makes the following contributions:</p><p>(1) We propose a fast and more precise detector PolarDet, where we represent the targets by ((x, y), (?x, ?y), ? p , s, r p ).</p><p>This representation can resolve most of the defects that current methods face.</p><p>(2) We introduce an improved center semantic structure, which can enhance the precision of classification without adding much parametera.</p><p>(3) As shown in <ref type="figure" target="#fig_0">Fig.1</ref>, we achieve the SOTA results on both of the DOTA <ref type="bibr" target="#b40">[41]</ref> dataset, UCAS-AOD <ref type="bibr" target="#b19">[20]</ref> dataset, and HRSC2016 <ref type="bibr" target="#b28">[29]</ref> dataset. On DOTA dataset, we reach 76.64% mAP with ResNet-101 as backbone. On AOD and HRSC datasets, we attain 97.02% mAP and 90.46% mAP with ResNet-50 backbone respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We will show the detectors using horizontal methods in 2.1 and show the improved oriented methods in 2.2. Also, we will list the commonly used attention mechanism which is related to our center semantic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Horizontal Object Detectors</head><p>Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> first introduces horizontal anchor into target detection. FPN <ref type="bibr" target="#b23">[24]</ref>, Cascade R-CNN <ref type="bibr" target="#b2">[3]</ref>, and R-FCN <ref type="bibr" target="#b3">[4]</ref> achieve better performance based on horizontal anchor. SSD <ref type="bibr" target="#b27">[28]</ref>, YOLO <ref type="bibr" target="#b32">[33]</ref>[34] <ref type="bibr" target="#b34">[35]</ref> improve horizontal anchor strategy and increase detection speed. CornerNet <ref type="bibr" target="#b17">[18]</ref>, CenterNet <ref type="bibr" target="#b5">[6]</ref>, and ExtremeNet <ref type="bibr" target="#b51">[52]</ref> propose horizontal boundary embedding points prediction. CenterNet <ref type="bibr" target="#b50">[51]</ref> and FCOS <ref type="bibr" target="#b36">[37]</ref> regard target as point then generate horizontal bounding box.</p><p>These horizontal object detectors all face many problems shown in <ref type="figure">Fig.3</ref> because the sharp variations in aerial images and oriented detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Oriented Object Detectors</head><p>R-RPN <ref type="bibr" target="#b29">[30]</ref> directly uses rotated anchor to detect oriented target. R2CNN <ref type="bibr" target="#b14">[15]</ref> predicts horizontal and oriented prediction box based on horizontal anchor. ROI Transformer <ref type="bibr" target="#b4">[5]</ref>, SCRDet <ref type="bibr" target="#b46">[47]</ref>, R3Det <ref type="bibr" target="#b43">[44]</ref>, and SCRDet++ <ref type="bibr" target="#b45">[46]</ref> apply five-parameter method while Textbox++ <ref type="bibr" target="#b21">[22]</ref> and RS-Det <ref type="bibr" target="#b31">[32]</ref> apply eight-parameter method to represent oriented target. Gliding Vertex <ref type="bibr" target="#b42">[43]</ref> introduce vertex gliding to locate object.</p><p>However, these oriented object detectors still face angle boundary, angle-loss trap, regression fluctuation, and imprecise fitting problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Mechanism</head><p>STN <ref type="bibr" target="#b13">[14]</ref> and SENET <ref type="bibr" target="#b12">[13]</ref> introduce spatial and channel attention mechanism to extract key feature respectively. DANet <ref type="bibr" target="#b6">[7]</ref> connects spatial and channel semantic feature to distinguish target. PointRend <ref type="bibr" target="#b16">[17]</ref> utilizes boundary information in subpixel to enhance segmentation performance.</p><p>However, these method always add too much calculation into network by using convolution operation, which results in slow inference problem.</p><p>In this paper, we introduce the polar method to represent targets more precisely. Also, we raise improved center semantic structure to further classify and locate targets. <ref type="figure">Figure 4</ref>: The pipeline of our proposed method. Our Po-larDet mainly consists of five modules: basic backbone for feature extraction, 3x DeConv Modules for feature reconstruction, Polar method for regressing the size and angle of targets, Improved Center-Semantic Structure for decoding location and classification information which are hidden in sub-pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>In this section, we will expound the proposed method as the following order. First, we will explain the commonly used five-parameter and eight-parameter methods and their defects in Section 3.1. To resolve these defects, we propose PolarDet and describe its pipeline in Section 3.2. In our PolarDet, we propose a more precise method that represents the target in polar coordinate by multiple angles and shorter-polar diameter ratio. In Section 3.3 &amp; 3.4 &amp; 3.5, we will show the representation of target by our method and how it can resolve these defects. The representation consists of center point, polar angle, and polar diameter. At last, we introduce center semantic structure in Section 3.6 to increase classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Principle of Commonly used Representation</head><p>In orientated detection tasks, nearly all the methods use five-parameter or eight-parameter to represent rotate targets. As shown in <ref type="figure">Fig</ref>  However, these two methods may meet problems such as angle boundary, angle-loss trap, and convergence performance decrease. These problems will cause the decrease in dectection precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pipeline</head><p>As shown in <ref type="figure">Fig.4</ref> and <ref type="figure" target="#fig_3">Fig.5</ref>(a), PolarDet consists of the following five modules. The first is the feature extraction element, which is based on ResNet. In our method, we use ResNet18 in Ablation Study and ResNet101 in final testing. The following feature reconstruction increases the resolution to quarter than the input image. We use common deconvolution combined with the dcn module to expand the receptive field. From the last feature map, the polar method regresses five heads to represent the target. The regression contains Heatmaps, Offset, Polar-Angle, Shorter, and Polar-Ratio, which express the center of target, the offset of heatmap, angles between four polar diameters and the reference y-axis, the shorter one between the width and height of the target minimum bounding rectangle, the length ratio between shorter side and polar diameter, respectively. Also, we deploy the center semantic structure to enhance the accuracy of classification. This kind of light network design can maintain the inference by using interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Center Point</head><p>As discussed above, our method uses a center point to locate and classify targets. Different from CenterNet <ref type="bibr" target="#b50">[51]</ref>, we apply the center of minimum bounding rectangle for detecting rotate target or quadrilateral. But we still maintain the truth value of heatmap, which is actually a confidence map with value range from h ? [0, 1] C? H 4 ? W 4 . C is the number of dataset category.</p><p>In the training stage, we generate a truth-heatmap using the Gaussian kernel to train the confidence map. In detail, we first map targets onto a single point, which can be expressed as H c,y,x = 1. Then we use Gaussian kernel to endow value to the neighboring points followed by CornerNet, which can be expressed as</p><formula xml:id="formula_0">H c,y,x = exp ? (x?px) 2 +(y?py) 2 2? 2 p</formula><p>, where ? p follows the definition in CenterNet. Finally, as shown in formula.1, center focal loss <ref type="bibr" target="#b24">[25]</ref> is applied to guide the direction of regression:</p><formula xml:id="formula_1">L k = ?1 N xyc ? ? ? ? ? ? ? (1 ?? c,y,x ) ? log(? c,y,x ) if H c,y,x = 1 (1 ? H c,y,x ) ? (? c,y,x ) ? log(1 ?? c,y,x ) otherwise (1)</formula><p>where N is the number of objects in input image, ? and ? are hyper-parameters which are set to 2 and 4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Polar Angle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition</head><p>As shown in <ref type="figure" target="#fig_4">Fig.6</ref>, angles are based on the polar coordinate system. The definition of the positive axis is along with the image positive x-axis and the positive y-axis respectively. We regard the beginning angle 0 o coincide with the positive y-axis and increase the angle counterclockwise. In our method, we use four angles to describe the orientation of targets, which defined as (? 1 , ? 2 , ? 3 , ? 4 ). Making quadrilateral as an example, as the blue box shown in <ref type="figure" target="#fig_4">Fig.6</ref>, A, B, C, D express the four points of the quadrilateral. Under our polar coordinate system, we define (? 1 , ? 2 , ? 3 , ? 4 ) ranging from 0 to 2? which follow the radian. Also, these four angles increase counterclockwise, which means ? 1 will be the smallest angle while the ? 4 will be the biggest one. In our Polar Angle definition method, the range of angles makes them express targets flexibly. Moreover, the radian guarantees the stability of regression. In the training stage, we simply use L1-Loss as regression loss (as shown in formula.2).</p><formula xml:id="formula_2">L A = 1 N i m ? i ? A i<label>(2)</label></formula><p>B. Angle-Loss Trap Avoidance Many detectors that express the oriented target through just one angle may fall into the Angle-Loss Trap in a very high probability. As shown in 3.1, the angle of fiveparameter method ranges from ?90 o to 0 o . When this definition combined with radian strategy, the angle will only range from ?0.5? to 0, which is a relatively small value. As  <ref type="figure" target="#fig_5">Fig.8</ref>(a)(b)(c), the IOU highly relies on the precision of the angle when the aspect-ratio is relatively high. Under this situation, as shown in <ref type="figure" target="#fig_6">Fig.9</ref>, the IOU will drop a lot even the angle just misses a little. Worse, when the angle misses a little, the angle loss will be still very small because of the value range of rotation angle, which means dropping into the Angle-Loss Trap. For avoiding this trap, we propose the polar angle for substituting. As shown in <ref type="figure" target="#fig_5">Fig.8(c)</ref>, the polar angle method can express oriented target to the point. Also, the polar angle method can enhance angle convergence performance by the larger angle loss created by four angles expression.</p><p>C. Boundary Problem Solution Boundary problem is always existing trouble when using the five-parameter expression. As shown in <ref type="figure">Fig.7(a)</ref>, blue, green, red boxes represent the reference box, prediction box, gt box respectively. We can notice that the green box seems only to miss a little in angle prediction. However, the angle of gt is ?10 o and the angle of prediction is ?80 o , which misses a lot. Worse, as the figure shows, the width and height of the target are opposite with groundtruth, which also creates a very big loss. This situation leads the network to have to learn in the following steps. (1) first, the network needs to make the angle decrease to ?10 o as shown in step1. (2) then the network has to force the width into a smaller one while the height into a bigger one. These two steps will make the network unstable and decrease convergence performance. Our PolarDet based on four Polar Angles can easily resolve this problem. As shown in <ref type="figure">Fig.7(b)</ref>, all the points A, B, C, D in ground-truth are correspond to A , B , C , D in prediction respectively. Be- <ref type="figure">Figure 7</ref>: comparison in different angle definition: (a) prediction has to regress by step1 and step2 with OpenCV angle definition (b) regress directly using PolarDet angle definition cause we use a more precise angle range strategy described in <ref type="figure" target="#fig_4">Fig.6</ref>. All the predictions only miss a little compared with ground-truth. Also, the length will be close to groundtruth according to the polar diameter expression (discussing in 3.5). With these mentioned above, the PolarDet will converge to the truth quickly and won't be bothered with the boundary problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Polar Diameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition</head><p>In PolarDet, instead of using commonly used width and height to describe the size of target, we introduce a polar diameter expression. We introduce one length parameter and a pair of ratio parameters (s, r 1 , r 2 , r 3 , r 4 ) to express target. In detail, as shown in <ref type="figure" target="#fig_4">Fig.6</ref>, the expression consists of the following two parts: (1) the shorter side of the minimum bounding rectangle of target. (2) four ratios between shorter side and polar diameters. The polar diameters mean the distance between vertices and center point shown in 3.1. In the training stage, we still use L1-Loss for both of the expressions. These definitions and loss are shown directly in formula 3 4 5 6, where s means shorter side, M BR means the minimum bounding rectangle of target, r means ratio, D L2 means L2 distance, C means center point and V i means four vertices.  </p><formula xml:id="formula_3">s = min(w M BR , h M BR ) (3) r = s D L2 (C, V i ) (i = 1, 2, 3, 4)<label>(4)</label></formula><formula xml:id="formula_4">L s = 1 N i m |? i ? s i |<label>(5)</label></formula><formula xml:id="formula_5">L r = 1 N k 4 i m |r ik ? r ik |<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence Performance Increase</head><p>Whatever angle expression the common methods use, they often apply width and height to describe the size of the target. However, we notice that this kind of length expression often meets the decrease in network convergence performance. This decrease is commonly caused by sharp changes in the length regression, especially for tasks with various scales targets. For raising the performance of convergence, we propose the relative polar diameter expression which uses the shorter side and ratio discussed above. This strategy will decrease the prediction range and increase performance. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our expression has a better performance than the other methods including the ones using the Cartesian system and the ones using other expressions with the Polar system. These results all regard ResNet-18 as backbone and in the same hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Precise Quadrilateral Fitting</head><p>In oriented object detection like the aerial field, labels are often given as four detached points. Therefore, the groundtruth often manifests as quadrilateral.</p><p>As shown in <ref type="figure" target="#fig_5">Fig.8(d)</ref>, the red, blue, green box represents ground-truth, minimum bounding rectangle, polar angle with polar diameter expression. Easily, we can notice that just use minimum bounding rectangle (MBR) cannot cover target completely. Background information will also be included in MBR, which will confuse the network. However, when applying our method into expression, quadrilateral can be totally covered without introducing useless in- <ref type="table">Backbone   System  Expression  PL  BD  BR  GTF  SV  LV  SH  TC  BC  ST  SBF  RA  HA  SP  HC</ref>    <ref type="table">Table 2</ref>: AP and mAP (%) about applying different strategies of classification enhancement or not applying formation. We will show our impressive results in detecting orientated targets in the following experiments in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Improved Center-Semantic Structure</head><p>Clutter and complex background in aerial images will decrease classification performance and cause false positive detection. Inspired by PointRend <ref type="bibr" target="#b16">[17]</ref>, which introduces boundary information exists in sub-pixel, we propose that classification information also exists in sub-pixel. Therefore, our method introduces an improved center-semantic structure to optimize classification and verify our idea. As shown in <ref type="figure" target="#fig_3">Fig.5(a</ref> and multiple it with the predicted heatmap pixel-wisely in inference period.</p><p>In detail, we use one 1 ? 1 conv and four times bilinear upsample, one 1 ? 1 conv and two times bilinear upsample, only one 1 ? 1 conv to get the first, second, third pixel-interpolation modules. Then, we concat these three modules into M C? H 4 ? W 4 . Finally, we use one 3 ? 3 conv followed by 1 ? 1 conv to get merge layer M C? H</p><formula xml:id="formula_6">4 ? W 4 ,</formula><p>where each ground-truth will be expressed as a circle with the same diameter as in the confidence map. However, the value of the foreground will all be 1 while the background will be 0, which is different from the Gaussian value. In other words, by category classification heatmap</p><formula xml:id="formula_7">hc ? [0, 1] C? H 4 ? W 4 .</formula><p>In this structure, we apply two strategies to reduce the parameter. First, we use 1 ? 1 conv to change the number of channels into the number of class. Then, we introduce bilinear interpolation but not the commonly used deconvolution to upsample the resolution. With these, we can increase classification performance without increasing much parameter. Also, the following <ref type="table">Table 2</ref> shows that our center-semantic can get obvious improvement compared with baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We choose a wide range of different type datasets containing plentiful oriented targets, which are taken by satellite, drone, helicopter. The details are as follows.</p><p>A. DOTA DOTA <ref type="bibr" target="#b40">[41]</ref> is the largest dataset for oriented object detection which contains 2,806 aerial images and 15 categories with almost 200,000 instances. In the DOTA dataset, the training set, validation set, and test set account for 1/2, 1/6, 1/3 of the whole set, respectively.</p><p>We use the training set for training and validation set for evaluation in the ablation study while both the training and validation set for training and test set for submitting in the final test. Because the resolution of DOTA ranges from 800*800 to 30000*30000, we split the image into 1024*1024 patches with an overlap of 200 pixels, which is the same as the others like ROI Transformer <ref type="bibr" target="#b4">[5]</ref>. With this, we get about 14,000 patches and 19,000 patches in double tasks mentioned above. The model is trained by 360 epochs, and the learning rate drops from 1.25e-4 to 1.25e-6 in the 200 th epoch and 300 th epoch.</p><p>B. UCAS-AOD UCAS-AOD <ref type="bibr" target="#b19">[20]</ref> is a specialized dataset for remote sensing target detection. It contains 1,510 images with about 15,000 instances in two categories including plane and car. In line with SCRDet++ <ref type="bibr" target="#b45">[46]</ref>, we randomly select 1,110 for training and 400 for testing.</p><p>C. HRSC2016 HRSC2016 <ref type="bibr" target="#b28">[29]</ref> is another challenging dataset in the aerial field for oriented target detection. It contains 1,061 images and more than 20 categories of ships in various appearances. The image resolution is about 1500*900 not many ships in one image. Following ROI Transformer, we use the trainval set (617 images) for training and the test set (444 images) for testing, and we resize the image into both 1024*1024 and 800*800 resolution in the same method used in the DOTA dataset. We only use level one</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OBB (oriented bounding boxes)</head><p>Backbone <ref type="table" target="#tab_1">PL  BD  BR  GTF  SV  LV  SH  TC  BC  ST  SBF  RA  HA  SP  HC  mAP  Two-Stage Methods  FR-O [41]</ref> ResNet101 <ref type="bibr" target="#b11">[12]</ref> 79.09 69. <ref type="bibr" target="#b11">12</ref>    <ref type="table">Table 4</ref>: Results by mAP (%) on UCAS-AOD dataset.</p><p>to execute detection task like the other work such as ROI Transformer <ref type="bibr" target="#b4">[5]</ref> and P-RSDet <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The experiments on DOTA, UCAS-AOD, and HRSC2016 are implemented by Pytorch 1.0. We use 2?32GB NVIDIA Tesla V100 GPUs in the ablation study and 8?32GB NVIDIA Tesla V100 GPUs in final testing. We adopt ResNet-18 as backbone in all Ablation Studies and ResNet-101 in final testing. We set learning rate to 1.25e-4 then drop it by 10 times in 200 th , 300 th epoch in DOTA, UCAS-AOD, HRSC2016, and ICDAR2015 datasets. For all the datasets, the network is trained by Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with 128 batch size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Effect of Polar method. The Polar method involves two parts of target expression strategies, which are polar angle and polar diameter. We design different comparative experiments to prove the advancement of the Polar method as shown in <ref type="table" target="#tab_1">Table 1</ref>. We compare our Polar method with methods using different coordinate systems and different expression strategies. From the results, we can notice that directly express by polar angles and polar diameters can make a sense, but a little (about 0.34% increase). Also, we can see even adapt ratio as an expression bridge, the long side cannot achieve good performance but will hurt the stability of network (about 1.06% decrease), which is caused by sharp changes in targets side length. For example, if directly regress length, the prediction will range from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">300]</ref> in DOTA dataset. However, as shown in <ref type="figure" target="#fig_0">Fig.10</ref>, our method can handle the situation even with the high aspect-ratio or very clutter environment. With our PolarDet, the prediction range will drop to (0, ? 2], which is beneficial to regression. <ref type="figure" target="#fig_0">Figure 10</ref>: Merged Illustration of our proposed method PolarDet. We achieve amazing consequence when detecting Dense, Oriented, clutter, and high aspect-ratio targets in different datasets including DOTA, UCAS-AOD, and HRSC2016.</p><p>Moreover, we can notice from the <ref type="table" target="#tab_1">Table 1</ref> that our method nearly achieves big improvement in every category. Especially in BR, SV, LV, SH, SBF, and HC, we get 5%, 6%, 4%, 9%, 7%, 8% increase respectively, which prove the Polar method can resolve the sharp changes problems. Effect of Center-Semantic. Center-Semantic is a brandnew structure used to enhance the accuracy of classification. In our center-semantic, as shown in <ref type="figure" target="#fig_3">Fig.5(a)</ref>, we use pixel interpolation strategy to upsample and lightweight feature extraction to decode classification information. With these two features, center-semantic can achieve enhancement without adding many parameters. As shown in <ref type="table">Table  2</ref>, we compare the simple baseline, Polar method, and Polar method + Center-Semantic together to show the efficiency of center-semantic. Obviously, center-semantic gets increas compared with baseline and only the Polar method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the SOTA Methods</head><p>Our PolarDet is applied to DOTA, AOD, and HRSC datasets respectively.</p><p>Results on DOTA. As illustrated in <ref type="table" target="#tab_4">Table 3</ref>, we compare our results with the other state-of-the-art results on DOTA. The results of the DOTA shown here are obtained from the DOTA evaluation server. Model result is trained on the trainval set and evaluated on the test set. We only execute the OBB task, which is also DOTA official recommends. Our single-stage PolarDet finally achieves the best performance 76.64% with ResNet-101 and MS testing. This result has already exceeded all one-stage methods and equaled the SOTA two-stage methods. And when paying attention to inference speed, our PolarDet is much faster than the other one-stage methods with ResNet-50 backbone, which only consumes 40ms in inference. Most importantly, PolarDet reaches 75.02% mAP without any testing strategy in the mentioned condition. <ref type="figure" target="#fig_0">Fig.10</ref> shows the aerial images of clutter, complex, and huge scenes.</p><p>Results on UCAS-AOD. We also get the best performance on UCAS-AOD, where we get 97.02% with ResNet-50 backbone. <ref type="table">Table 4</ref> shows the competitive results.</p><p>Results on HRSC2016. We see HRSC2016 as a ship dataset regardless of Fine-grained image categorization. As shown in <ref type="table" target="#tab_6">Table 5</ref>, we achieve 90.13% mAP and 90.46% mAP on 800*800 and 1024*1024 resolution respectively, which are both SOTA results. Moreover, PolarDet is also the fastest detector with 25fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a polar coordinate-based onestage detector, PolarDet. PolarDet is a faster and more precise detector for oriented object detection. In our PolarDet, we propose polar method and center semantic structure to represent the target. The polar method redesigns the regression method based on polar angle and polar diameter expression. Furthermore, the center semantic structure enhances the accuracy of classification and location. All these methods gain a great improvement in performance. Extensive experiments on DOTA, UCAS-AOD, and HRSC2016 verify our approaches, where we achieve SOTA performance even compared with SOTA two-stage detectors.</p><p>Besides the oriented tasks and quadrilateral detection, our PolarDet can also detect polygon, concavity, and keypoint, where PolarDet just needs to add extra detection points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>mAP of different approaches of DOTA dataset. The proposed PolarDet outperforms nearly all the other algorithm and gets SOTA results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Based on polar coordinate, polar method represents target by ((x, y), (?x, ?y), ? p , s, r p ) Figure 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.5(b), the definition of five-parameter is based on OpenCV. (a) regard the point with the least y coordinate value as the vertex. (b) elicit a reference line from the vertex and coincide with the horizontal line. (c) rotate reference line counterclockwise until it covers the first side of the target, which is labeled as w(width) while the other side as h(height). (d) the center point is (x, y), and orientation is equal to the rotation angle of the reference line.As shown inFig.5(b), eight-parameter usually uses four boundary points to describe the rotating target. These four vertices are often represented as offsets from the center</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Left: Center-Semantic Head is composed of three Pixel Interpolation Module, where C1 means layer channel, C2 means class num, and F means pixel interpolation. Right: Five parameter and eight parameter methods for expressing targets point. Also, they are defined in counterclockwise order where the original point is the one with the least y coordinate value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Polar Angle representation method. OX and OY means the positive axis of the image coordinate system, blue box means target, orange box means the minimum bounding rectangle, dotted lines direct the vertices of the target shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>(a) and (b) show the IOU dropping a lot even with a small one-angle bias. (c) shows the superiority of our Angle Polar representation method. (d) shows different expression for quadrilateral or orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>The relationship between one-angle bias and IOU with different aspect-ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Cartesian Single Angle 89.79 71.73 25.27 52.93 51.59 75.54 78.00 90.75 63.77 78.19 61.48 67.08 61.04 69.28 42.99 65.29 Polar Direct 89.98 67.69 29.66 59.38 61.58 75.39 77.25 90.77 64.76 79.30 63.08 64.39 63.35 59.96 46.59 66.21 Polar Average 89.65 75.88 27.89 53.14 60.37 74.25 77.05 90.73 63.42 78.35 64.40 64.44 62.95 60.62 44.76 65.86 Polar Longer+Ratio 89.81 66.54 29.33 54.04 61.24 74.75 77.33 90.72 59.60 78.76 62.46 62.77 62.48 59.54 42.70 64.80 Polar Shorter+Ratio 89.93 76.56 34.65 59.84 67.06 78.28 86.12 90.81 66.13 80.10 70.58 60.72 64.64 60.55 53.37 69.29</figDesc><table><row><cell>mAP</cell></row><row><cell>ResNet-18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>AP and mAP (%) about using different kinds of target expression method 71.73 25.27 52.93 51.59 75.54 78.00 90.75 63.77 78.19 61.48 67.08 61.04 69.28 42.99 65.29 ? -89.93 76.56 34.65 59.84 67.06 78.28 86.12 90.81 66.13 80.10 70.58 60.72 64.64 60.55 53.37 69.29 ? ? 89.97 75.64 33.54 54.61 66.11 78.12 85.65 90.84 67.72 78.80 72.29 64.32 65.41 59.71 68.00 70.05</figDesc><table><row><cell cols="2">Backbone Polar-Method Center-Semantic</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>-</cell><cell>-</cell><cell>89.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>17.17 63.49 34.20 37.16 36.20 89.19 69.60 58.96 49.4 52.52 46.69 44.80 46.30 52.93 R 2 CNN [15] ResNet101 80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67 RRPN [30] ResNet101 88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01 ICN [1] ResNet101 81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20 68.20 RADet [21] ResNeXt101 [42] 79.45 76.99 48.05 65.83 65.46 74.40 68.86 89.70 78.14 74.97 49.92 64.63 66.14 71.58 62.16 69.09 RoI-Transformer [5] ResNet101 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 87.05 45.30 63.32 78.44 76.65 87.13 90.79 80.58 85.89 60.97 67.94 68.20 74.63 68.67 75.02 PolarDet-MS (ours) ResNet101 89.65 87.07 48.14 70.97 78.53 80.34 87.45 90.76 85.63 86.87 61.64 70.32 71.92 73.09 67.15 76.64</figDesc><table><row><cell>CAD-Net [48]</cell><cell>ResNet101</cell><cell>87.8</cell><cell>82.4</cell><cell>49.4</cell><cell>73.5</cell><cell>71.1</cell><cell>63.5</cell><cell>76.7</cell><cell>90.9</cell><cell>79.2</cell><cell>73.3</cell><cell>48.4</cell><cell>60.9</cell><cell>62.0</cell><cell>67.0</cell><cell>62.2</cell><cell>69.9</cell></row><row><cell>SCRDet [47]</cell><cell>ResNet101</cell><cell cols="16">89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61</cell></row><row><cell>FADet [20]</cell><cell>ResNet101</cell><cell cols="16">90.21 79.58 45.49 76.41 73.18 68.27 79.56 90.83 83.40 84.68 53.40 65.42 74.17 69.69 64.86 73.28</cell></row><row><cell>Gliding Vertex [43]</cell><cell>ResNet101</cell><cell cols="16">89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02</cell></row><row><cell>Mask OBB [39]</cell><cell>ResNeXt101</cell><cell cols="16">89.56 85.95 54.21 72.90 76.52 74.16 85.63 89.85 83.81 86.48 54.89 69.64 73.94 69.06 63.32 75.33</cell></row><row><cell>FFA [8]</cell><cell>ResNet101</cell><cell>90.1</cell><cell>82.7</cell><cell>54.2</cell><cell>75.2</cell><cell>71.0</cell><cell>79.9</cell><cell>83.5</cell><cell>90.7</cell><cell>83.9</cell><cell>84.6</cell><cell>61.2</cell><cell>68.0</cell><cell>70.7</cell><cell>76.0</cell><cell>63.7</cell><cell>75.7</cell></row><row><cell>APE [53]</cell><cell>ResNeXt-101</cell><cell cols="16">89.96 83.62 53.42 76.03 74.01 77.16 79.45 90.83 87.15 84.51 67.72 60.33 74.61 71.84 65.55 75.75</cell></row><row><cell>SCRDet++-MS [46]</cell><cell>ResNet101</cell><cell cols="16">90.05 84.39 55.44 73.99 77.54 71.11 86.05 90.67 87.32 87.08 69.62 68.90 73.74 71.29 65.08 76.81</cell></row><row><cell>Single-Stage Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IENet [26]</cell><cell>ResNet101</cell><cell cols="16">80.20 64.54 39.82 32.07 49.71 65.01 52.58 81.45 44.66 78.51 46.54 56.73 64.40 64.24 36.75 57.14</cell></row><row><cell>RetinaNet [25]</cell><cell>ResNet101</cell><cell cols="16">88.92 67.67 33.55 56.83 66.11 73.28 75.24 90.87 73.95 75.07 43.77 56.72 51.05 55.86 21.46 62.02</cell></row><row><cell>P-RSDet [50]</cell><cell>ResNet101</cell><cell cols="16">89.02 73.65 47.33 72.03 70.58 73.71 72.76 90.82 80.12 81.32 59.45 57.87 60.79 65.21 52.59 69.82</cell></row><row><cell>O 2 -DNet [40]</cell><cell cols="17">Hourglass104 [31] 89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04</cell></row><row><cell>R 3 Det [44]</cell><cell>ResNet152</cell><cell cols="16">89.24 80.81 51.11 65.62 70.67 76.03 78.32 90.83 84.89 84.42 65.10 57.18 68.10 68.98 60.88 72.81</cell></row><row><cell>RSDet [32]</cell><cell>ResNet152</cell><cell>90.1</cell><cell>82.0</cell><cell>53.8</cell><cell>68.5</cell><cell>70.2</cell><cell>78.7</cell><cell>73.6</cell><cell>91.2</cell><cell>87.1</cell><cell>84.7</cell><cell>64.3</cell><cell>68.2</cell><cell>66.1</cell><cell>69.3</cell><cell>63.7</cell><cell>74.1</cell></row><row><cell>SCRDet++ [46]</cell><cell>ResNet152</cell><cell cols="16">89.20 83.36 50.92 68.17 71.61 80.23 78.53 90.83 86.09 84.04 65.93 60.8 68.83 71.31 66.24 74.41</cell></row><row><cell>PolarDet (ours)</cell><cell>ResNet50</cell><cell>89.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">: AP and mAP (%) across categories of OBB task on DOTA.</cell></row><row><cell></cell><cell></cell><cell cols="2">FT indicates horizontal-flip MS indicates multi-scale</cell></row><row><cell>Method</cell><cell>Plane</cell><cell>Car</cell><cell>mAP</cell></row><row><cell>YOLOv2 [34]</cell><cell cols="3">96.60 79.20 87.90</cell></row><row><cell>R-DFPN [45]</cell><cell cols="3">95.90 82.50 89.20</cell></row><row><cell>DRBox [27]</cell><cell cols="3">94.90 85.00 89.95</cell></row><row><cell>S 2 ARN [2]</cell><cell cols="3">97.60 92.20 94.90</cell></row><row><cell cols="4">RetinaNet-H [44] 97.34 93.60 95.47</cell></row><row><cell>ICN [1]</cell><cell>-</cell><cell>-</cell><cell>95.67</cell></row><row><cell>FADet [20]</cell><cell cols="3">98.69 92.72 95.71</cell></row><row><cell>R 3 Det [44]</cell><cell cols="3">98.20 94.14 96.17</cell></row><row><cell>SCRDet++ [46]</cell><cell cols="3">98.93 94.97 96.95</cell></row><row><cell>PolarDet (Ours)</cell><cell cols="3">99.08 94.96 97.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results and speed comparison on HRSC2016.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>The research was supported by Hikvision Research Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single shot anchor refinement network for oriented object detection in optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="87150" to="87161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="294" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9799" to="9808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature-attentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Radet: Refine feature pyramid network and multi-layer attention network for arbitrary-oriented object detection of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">389</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Textboxes++: A singleshot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ienet: Interacting embranchment one stage anchor free detector for orientation aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ship rotated bounding box space for ship extraction from highresolution optical satellite images with complex backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1074" to="1078" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08299</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mask obb: A semantic attention-based mask oriented bounding box representation for multicategory object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10694</idno>
		<title level="m">Oriented objects as pairs of middle lines</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dota: A largescale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<title level="m">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scrdet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13316</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cad-net: A contextaware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Objects detection for remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adaptive period embedding for representing oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
