<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual Autoregressive Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
							<email>nicola.decao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
							<email>ledell@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
							<email>kpopat@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
							<email>artetxe@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Plekhanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<email>sriedel@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
							<email>fabiopetroni@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual Autoregressive Entity Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present mGENRE, a sequence-tosequence system for the Multilingual Entity Linking (MEL) problem-the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-toright, token-by-token in an autoregressive fashion.</p><p>The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name.</p><p>Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50% improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where mGENRE establishes new state-of-the-art results. Code and pretrained models at https://github.com/ facebookresearch/GENRE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity Linking <ref type="bibr">(EL, Hoffart et al., 2011;</ref><ref type="bibr">Dredze et al., 2010;</ref><ref type="bibr">Bunescu and Pa?ca, 2006;</ref><ref type="bibr">Cucerzan, 2007)</ref> is an important task in NLP, with plenty of applications in multiple domains, spanning Question Answering <ref type="bibr">(De Cao et al., 2019;</ref><ref type="bibr" target="#b15">Nie et al., 2019;</ref><ref type="bibr" target="#b0">Asai et al., 2020)</ref>, Dialogue <ref type="bibr" target="#b3">(Bordes et al., 2016;</ref><ref type="bibr" target="#b27">Wen et al., 2017;</ref><ref type="bibr" target="#b29">Williams et al., 2017;</ref><ref type="bibr">Chen et al., 2017;</ref><ref type="bibr">Curry et al., 2018)</ref>, Biomedical systems <ref type="bibr" target="#b7">(Leaman and Gonzalez, 2008;</ref><ref type="bibr" target="#b32">Zheng et al., 2015)</ref>, to name just a few. It consists of grounding entity mentions in unstructured texts to KB descriptors (e.g., Wikipedia articles).</p><p>The multilingual version of the EL problem has been for a long time tight to a purely cross-lingual formulation <ref type="bibr">(XEL, McNamee et al., 2011a;</ref><ref type="bibr" target="#b32">Ji et al., 2015)</ref>, where mentions expressed in one language are linked to a KB expressed in another (typically English). <ref type="bibr">Recently, Botha et al. (2020)</ref> made a step towards a more inherently multilingual formulation by defining a language-agnostic KB, obtained by grouping language-specific descriptors per entity. Such formulation has the power of considering entities that do not have an English descriptor (e.g., a Wikipedia article in English) but have one in some other languages.</p><p>A common design choice to most current solutions, regardless of the specific formulation, is to provide a unified entity representation, either by collating multilingual descriptors in a single vector or by defining a canonical language. For the common bi-encoder approach <ref type="bibr" target="#b30">(Wu et al., 2020;</ref><ref type="bibr">Botha et al., 2020)</ref>, this might be optimal. However, in the recently proposed GENRE model <ref type="bibr" target="#b17">(De Cao et al., 2021)</ref>, an autoregressive formulation to the EL problem leading to stronger performance and considerably smaller memory footprints than bi-encoder approaches on monolingual benchmarks, the representations to match against are entity names (i.e., strings) and it's unclear how to extend those beyond a monolingual setting.</p><p>In this context, we find that maintaining as much language information as possible, hence providing multiple representations per entity, helps due to the connections between source language and entity names in different languages. We additionally find  <ref type="figure">Figure 1</ref>: mGENRE: we use an autoregressive decoder to generate language IDs as well as entity names (i.e., Wikipedia titles). The combination of language ID and a entity name uniquely identify a Wikidata ID (with a Nto-1 mapping). We use Beam Search for efficient inference and we marginalize the probability scores for different languages to score entities. This example is a real output from our system. that using all available languages as targets and aggregating over the possible choices is an effective way to deal with a zero-shot setting where no training data is available for the source language.</p><p>Concretely, in this paper, we present mGENRE, the first multilingual EL system that exploits a sequence-to-sequence architecture to generate entity names in more than 100 languages left to right, token-by-token in an autoregressive fashion and conditioned on the context (see <ref type="figure">Figure 1</ref> for an outline of our system). While prior works use a single representation for each entity, we maintain entity names for as many languages as possible, which allows exploiting language connections between source input and target name. To summarize, this work makes the following contributions:</p><p>? Extend the catalog of entity names by considering all languages for each entry in the KB. Storing the multilingual names index is feasible and cheap (i.e., 2.2GB for ?89M names). ? Design a novel objective function that marginalizes over all languages to perform a prediction. This approach is particularly effective in dealing with languages not seen during fine-tuning (?50% improvements). ? Establish new state-of-the-art performance for the <ref type="bibr">Mewsli-9 (Botha et al., 2020)</ref>, TR2016 hard <ref type="bibr" target="#b23">(Tsai and Roth, 2016a)</ref> and TAC-KBP2015  MEL datasets. ? Present extensive analysis of modeling choices, including the usage of candidates from a mention table, frequency-bucketed evaluation, and performance on an heldout set including low-resource languages.</p><p>? Publicly release our best model, pre-trained as multilingual denoising auto-encoder using the BART objective  on large-scale monolingual corpora in 125 languages and fine-tuned to generate entity names given ?730M in-context Wikipedia hyperlinks in 105 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We first introduce Multilingual Entity Linking in Section 2.1 highlighting its difference with monolingual and cross-lingual linking. We address the MEL problem with a sequence-to-sequence model that generates textual entity identifiers (i.e., entity names). Our formulation generalizes the GENRE model by De <ref type="bibr" target="#b17">Cao et al. (2021)</ref> to a multilingual setting (mGENRE). Thus in Section 2.2 and 2.3, we discuss the GENRE model and how it ranks entities with Beam Search respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition</head><p>Multilingual Entity Linking (MEL, Botha et al., 2020) is the task of linking a given entity mention m in a given context c of language l ? L C to the corresponding entity e ? E in a multilingual Knowledge Base (KB). See <ref type="figure">Figure 1</ref> for an example: there are textual inputs with entity mentions (in bold) and we ask the model to predict the corresponding entities in the KB. A language-agnostic KB includes at least the name (but could include also descriptions, aliases, etc.) of each entity in one or more languages but there is no assumption about the relationship between these languages L KB and languages of the context L C . This is a generaliza-tion of both monolingual Entity Linking EL and cross-lingual EL <ref type="bibr">(XEL, McNamee et al., 2011a;</ref><ref type="bibr" target="#b32">Ji et al., 2015)</ref>. The latter considers contexts in different languages while mapping mentions to entities in a monolingual KB (e.g., English Wikipedia). Additionally, we assume that each e ? E has a unique textual identifier in at least a language. Concretely, in this work, we use Wikidata <ref type="bibr" target="#b26">(Vrande?i? and Kr?tzsch, 2014)</ref> as our KB. Each Wikidata item lists a set of Wikipedia pages in multiple languages linked to it and in any given language each page has a unique name (i.e., its title).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoregressive generation</head><p>GENRE ranks each e ? E by computing a score with an autoregressive formulation:</p><formula xml:id="formula_0">score ? (e|x) = p ? (y|x) = N i=1 p ? (y i |y &lt;i , x)</formula><p>where y is the sequence of N tokens in the identifier of e, x the input (i.e., the context c and mention m), and ? the parameters of the model. GENRE is based on finetuned BART architecture  and it is trained using a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing <ref type="bibr" target="#b20">(Sutskever et al., 2011</ref> and regularized with dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref> and label smoothing <ref type="bibr" target="#b22">(Szegedy et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ranking with Constrained Beam Search</head><p>At test time, it is prohibitively expensive to compute a score for every element in E and then sort them. Thus, GENRE exploits Beam Search <ref type="bibr">(BS, Sutskever et al., 2014)</ref>, an established approximate decoding strategy to navigate the search space efficiently. Instead of explicitly scoring all entities in E, search for the top-k entities in E using BS with k beams. BS only considers one step ahead during decoding (i.e., it generates the next token conditioned on the previous ones). Thus, GENRE employs a prefix tree (trie) to enable constrained beam search and then generate only valid entity identifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>To extend GENRE to a multilingual setting we need to define what are the unique identifiers of all entities in a language-agnostic fashion. This is not trivial since we rely on text representations that are by their nature grounded in some language. Concretely, for each entity e, we have a set of identifiers I e that consists of pairs l, n l e where l ? L KB indicates a language and n l e the name of the entity e in the language l. We extracted these identifiers from our KB-each Wikidata item has a set of Wikipedia pages in multiple languages linked to it, and in any given language, each page has a unique name. We identified 3 strategies to employ these identifiers: i) define a canonical textual identifier for each entity such that there is a 1-to-1 mapping between the two (i.e., for each entity, select a specific language for its name-see Section 3.1); ii) define a N-to-1 mapping between textual identifier and entities concatenating a language ID (e.g., a special token or the ISO 639-1 code 1 ) followed by its name in that languagealternatively concatenating its name first and then a language ID (see Section 3.2); iii) treat the selection of an identifier in a particular language as a latent variable (i.e., we let the model learn a conditional distribution of languages given the input and we marginalize over those-see Section 3.3). All of these strategies define a different way we compute the underlining likelihood of our model. In <ref type="figure">Figure 1</ref> we show an outline of mGENRE. The following subsections will present detailed discussions of the above 3 strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Canonical entity representation</head><p>Selecting a single textual identifier for each entity corresponds to choosing its name among all the available languages of that entity. We employ the same data-driven selection heuristic as in <ref type="bibr">Botha et al. (2020)</ref>: for each entity e we sort all its names n l e for each language l according to the number of mentions of e in documents of language l. Then we take the name n l e in the language l that has the most mentions of e. In case of a tie, we select the language that has the most number of mentions across all entities (i.e., the language for which we have more training data). Having a single identifier for each entity corresponds to having a 1to-1 mapping between strings and entities. Thus, score ? (e|x) = p ? (n e |x) where with n e we indicate the canonical name for e. We train to maximize the scores for all our training data. A downside of this strategy is that most of the time, the model cannot exploit the lexical overlap between the context and entity name since it has to translate it in the canonical one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multilingual entity representation</head><p>To accommodate the canonical representation issues, we can predict entity names in any language. Concatenating a language ID l and an entity name n l e in different orders induces two alternative factorizations. We train maximizing the scores for all our training data: score ? (e|x) = p ? (l|x) ? p ? (n l e |x, l) for 'lang+name' p ? (n l e |x) ? p ? (l|n l e , x) for 'name+lang'</p><p>(1)</p><p>The former corresponds to first predicting a distribution over languages and then predicting a title conditioning on the language l where the latter corresponds to the opposite. Predicting the language first conditions the generation to a smaller set earlier during beam search (i.e., all names in a specific language). However, it might exclude some targets from the search too early if the beam size is too small. Predicting the language last does not condition the generation of names in a particular language but it asks the model to disambiguate the language of the generated name whenever it is ambiguous (i.e., when the same name in different languages corresponds to possibly different entities). Only 1.65% of the entity names need to be disambiguated with the language. In practice, we observe no difference in performance between the two approaches. Both strategies define an N-to-1 mapping between textual identifiers and entities and then at test time we just use a lookup table to select the correct KB item. This N-to-1 mapping is an advantage compared to using canonical names because the model can predict in any available language and therefore exploit synergies between source and target language as well as avoiding translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Marginalization</head><p>Differently from the plain generation strategies described above, we can treat the textual identifiers as a latent variable and express score ? (e|x) as the probability of the entity name in all languages and marginalizing over them: score ? (e|x) = p ? (e|x) = l,n l e ?Ie p ? (n l e , l|x) .</p><p>Marginalization exposes the model to all representations in all languages of the same entity and it requires a minor modification of the training procedure. Unfortunately, because computing score ? (e|x) requires a sum over all languages, both training, and inference with marginalization are more expensive than with simple generation (scaling linearly with the number of languages). However, at least during inference, we can still apply BS to only marginalize using the top-k generations. For this reason, we test this training strategy only on few languages but we evaluate marginalization even when training with the other generation strategies described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Candidate selection</head><p>Modern EL systems that employ cross-encoding between context and entities usually do not score all entities in a KB as it is too computational expensive <ref type="bibr" target="#b30">(Wu et al., 2020)</ref>. Instead, they first apply candidate selection (with a less expensive method first or just a non-parametric mention table) to reduce the number of entities before scoring. In our formulation, there is no need to do that since mGENRE uses Beam Search to efficiently generate. However, using candidates might help, and therefore, we also experiment with that. Scoring all candidates might not be always possible (sometimes there are thousands of candidates for a mention) and especially when using an N-to-1 mapping between textual identifiers there will be names to rank in all languages available for each candidate. Thus, when we use candidates, it is to constrain BS steps further, rather than to rank all of them. Concretely, candidate selection is made with an alias table. Using the training data, we build a mention table where we record all entities indexed by the names used to refer to them. Additionally, we also use Wikipedia titles as additional mentions (useful for entities that never appear as links), redirects, Wikidata labels, and aliases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head><p>We use Wikidata <ref type="bibr" target="#b26">(Vrande?i? and Kr?tzsch, 2014)</ref> as our KB while exploiting the supervision signal from Wikipedia hyperlinks. For evaluation, we test our model on two established cross-lingual datasets, TR2016 hard and TAC-KBP2015 <ref type="bibr" target="#b23">Tsai and Roth, 2016a)</ref>, as well as the recently proposed Mewsli-9 MEL dataset <ref type="bibr">(Botha et al., 2020)</ref>. Additionally, we propose a novel setting extracted from Wikinews 2 where we train a model on a set of languages, and we test it on unseen ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Base: Wikidata</head><p>We use Wikidata as the target KB to link to filtering the same heuristic as Botha et al. (2020) (see Appedinx A for more details). Eventually, our entity set E contains 20,277,987 items (as a reference, English Wikipedia has just ?6M items). Using the corresponding Wikipedia titles as textual identifiers in all languages leads to a table of 53,849,351 entity names. We extended the identifiers including redirects which leads to a total of 89,270,463 entity names (see <ref type="table" target="#tab_4">Table 10</ref> in Appendix A for more <ref type="bibr">details)</ref>. Although large, the number of entity names is not a bottleneck as the generated prefix tree only occupies 2.2GB for storage. As a comparison the Botha et al.'s (2020) MEL systems need ?10 times more storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervision: Wikipedia</head><p>For all experiments, we do not train a model from scratch, but we fine-tune a multilingual language model trained on 125 languages (see Appendix A for more details on the pre-trained model). We exploit Wikipedia hyperlinks as the source of supervision for MEL. We used Wikipedia in 105 languages out of the &gt;300 available. These 105 are all the languages for which our model was pre-trained on that overlaps with the one available in Wikipedia (see full language list in <ref type="figure">Figure 2</ref> and more details in Appendix A). Eventually, we extracted a largescale dataset of 734,826,537 datapoints. For the plain generation strategy, we selected as the ground truth the name in the source language. When such entity name is not available we randomly select 5 alternative languages and we use all of them as datapoints. To enable model selection, we randomly selected 1k examples from each language to keep as a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>Mewsli-9 (Botha et al., 2020) contains 289,087 entity mentions appearing in 58,717 originally written news articles from Wikinews, linked to Wiki-Data. The corpus includes documents in 9 languages. 3 Differently from the cross-lingual setting, this is a truly multilingual dataset since 11% target entities in Mewsli-9 do not have an English Wikipedia page thus a XEL model would not link these.</p><p>TR2016 hard <ref type="bibr" target="#b23">(Tsai and Roth, 2016a</ref>) is a Wikipedia based cross-lingual dataset specifically constructed to contain difficult mention-entity pairs. Authors extracted Wikipedia hyperlinks for which the corresponding entity is not the most likely when using an alias table. Since we train on Wikipedia, to avoid an overlap with this test data, we removed all mentions from our training data that also appear in TR2016 hard . Note that this pruning strategy is more aggressive than <ref type="bibr" target="#b23">Tsai and Roth's (2016a)</ref> and Botha et al.'s (2020) strategies. <ref type="bibr" target="#b23">Tsai and Roth (2016a)</ref> assured to not have mention-entity pairs overlaps between training and test, but a mention (with a different entity) might appear in training. Botha et al.</p><p>(2020) 4 split at the page-level only, making sure to hold out all <ref type="bibr" target="#b23">Tsai and Roth (2016a)</ref> test pages (and their corresponding pages in other languages), but they trained on any mention-entity pair that could be extracted from their remaining training page partition (i.e., they have overlap between training and text entity-mention pairs). To compare with previous works <ref type="bibr" target="#b23">(Tsai and Roth, 2016a;</ref><ref type="bibr" target="#b25">Upadhyay et al., 2018;</ref><ref type="bibr">Botha et al., 2020)</ref> we only evaluate on German, Spanish, French and Italian (a total of 16,357 datapoints).</p><p>TAC-KBP2015 To evaluate our system on documents out of the Wikipedia domain, we experiment on the TAC-KBP2015 Tri-Lingual Entity Linking Track . To compare with previous works <ref type="bibr" target="#b23">(Tsai and Roth, 2016a;</ref><ref type="bibr" target="#b25">Upadhyay et al., 2018;</ref><ref type="bibr" target="#b18">Sil et al., 2018;</ref><ref type="bibr" target="#b33">Zhou et al., 2019)</ref>, we use only Spanish and Chinese (i.e., we do not evaluate in English). Following previous work, we only evaluate in-KB links <ref type="bibr" target="#b31">(Yamada et al., 2016;</ref><ref type="bibr">Ganea and Hofmann, 2017)</ref>, i.e, we do not evaluate on mentions that link to entities out of the KB. Previous works considered Freebase <ref type="bibr" target="#b2">(Bollacker et al., 2008)</ref> as KB, and thus we computed a mapping between Freebase ID and Wikidata ID. When we cannot solve the match, our system gets zero scores (i.e., it counts as a wrong prediction). TAC-KBP2015 contains 166 Chinese documents (84 news and 82 discussion forum articles) and 167 Spanish documents (84 news and 83 discussion forum articles) for a total of 12,853 mention-entity datapoints.  <ref type="figure">Figure 2</ref>: Accuracy of mGENRE on the 105 languages in our Wikipedia validation set. We also report the accuracy of the alias table and the log-training set sizes per each language (see <ref type="figure">Figure 7</ref> and <ref type="table" target="#tab_4">Table 11</ref> in Appendix B for a larger view and for precise values and language full names).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikinews</head><p>guages that are not in the Mewsli-9 language set. 5 <ref type="table" target="#tab_16">Table 7</ref> in Appendix A reports statistics of this dataset. Wikinews-7 is created in the same way as Mewsli-9, but we used our own implementation to extract data from raw dumps 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The main results of this work are reported in <ref type="table" target="#tab_4">Table 1</ref> for Mewsli-9, and in <ref type="table" target="#tab_5">Table 2</ref> for TR2016 hard , and TAC-KBP2015 respectively. Our mGENRE (trained with 'title+lang') outperforms all previous works in all those datasets. We show the accuracy of mGENRE on the 105 languages in our Wikipedia validation set against an alias table baseline in <ref type="figure">Figure 2</ref>. In <ref type="table" target="#tab_4">Table 11</ref> in Appendix A we report more details on the validation set results. In <ref type="table" target="#tab_12">Table 5</ref> we report some examples of correct and wrong predictions of our mGENRE on selected datapoints from Mewsli-9. Therefore these are the hardest languages to link. 7 When enabling candidate filtering to restrict the space for generation, we further improve error reduction to 13.6% and 21.0% for micro and macro average respectively. Marginalization reduces the error by the same amount as candidate filtering but combining search with candidates and marginalization leads to our best model: it improves error reduction to 14.5% and 23.0% on micro and macro average respectively. Our best model is also better than Model F + in English and on par with it in German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance evaluation</head><p>TR2016 hard and TAC-KBP2015 We compared our mGENRE against cross-lingual systems <ref type="bibr" target="#b23">(Tsai and Roth, 2016a;</ref><ref type="bibr" target="#b18">Sil et al., 2018;</ref><ref type="bibr" target="#b25">Upadhyay et al., 2018;</ref><ref type="bibr" target="#b33">Zhou et al., 2019)</ref>    and TAC-KBP2015 uses simplified Chinese. Many mentions in TAC-KBP2015 were not observed in Wikipedia, so the performance gain mostly comes from this but including the simplified and alternative Chinese names also played an important role (+5% comes from this alone). 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>By entity frequency  these cases. On very rare entities (i.e., the [1,10) bin) our model performs worse than Model F + . Note that Model F + was trained specifically to tackle those cases (e.g., with hard negatives and frequency-based mini-batches) whereas our model was not. We argue that similar strategies can be applied to mGENRE to improve performance on rare 0 2 0 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 <ref type="formula" target="#formula_1">2</ref>    entities, and we leave that to future work. The performance gap between Model F + and mGENRE on entities that appear more than 100 times in the training set is negligible.</p><p>By candidate frequency We additionally measure the accuracy on Mewsli-9 by the number of candidates retrieved from the alias table (details in <ref type="figure" target="#fig_0">Figure 3</ref>). When there are no candidates (?12k datapoints that is ?4% Mewsli-9) an alias table would automatically fail, but mGENRE uses the entire KB as candidates and has 63.9% accuracy. For datapoints with few candidates (e.g., less than 100), we could use mGENRE as a ranker and score all of the options without relying on constrained beam search. However, this approach would be computationally infeasible when there are no candidates (i.e., we use all the KB as candidates) or too many candidates (e.g., thousands). Constrained BS allows us to efficiently explore the space of entity names, whatever the number of candidates.  set of languages in train and test are disjoint). This zero-shot setting implies that no mention table is available during inference; hence we do not consider candidates for test mentions. We train our models on the nine Mewsli-9 languages and compare all strategies exposed in Section 3. To make our ablation study feasible, we restrict the training data to the first 1 million hyperlinks from Wikipedia abstracts. Results are reported in <ref type="table" target="#tab_9">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unseen Languages</head><p>Using our novel marginalization strategy that aggregates (both at training and inference time) over all seen languages to perform the linking brings an improvement of over 50% with respect to considering a single language. To deeper investigate the behaviour of the model in this setting, we compute the probability mass distribution over languages seen at training time for the first prediction (reported in <ref type="figure" target="#fig_1">Figure 4</ref>). When marginalization is enabled ( <ref type="figure" target="#fig_1">Figure 4b</ref>) the distribution is more spread across languages since the model is trained to use all of them. Hence the model can exploit connections between an unseen language and all seen languages for the linking process, which drastically increases the accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outcome</head><p>Wrong: from the context lawyer is potentially more appropriate than actor. This is the risk of not considering the full entity description (that might say that Marko is an actor and also the President of the World Mime Organization). Even if on average copying is an effective strategy, it does not always succeed (using the alias table on this example leads to a correct prediction).  <ref type="bibr" target="#b30">Wu et al. (2020)</ref> they rank entities with a dot-product between these representations. Model F + uses the description of entities as input to the entity encoder and title, document and mention (separated with special tokens) as inputs to the context encoder. Bi-encoders solutions may be memory inefficient since they require to keep in memory big matrices of embeddings, although memory-efficient dense retrieval has recently received attention <ref type="bibr" target="#b4">(Izacard et al., 2020;</ref><ref type="bibr">Min et al., 2021;</ref><ref type="bibr" target="#b17">Lewis et al., 2021)</ref>.</p><p>Another widely explored line of work is Cross-Language Entity Linking (XEL; <ref type="bibr" target="#b12">McNamee et al., 2011b;</ref><ref type="bibr">Cheng and Roth, 2013)</ref>. XEL considers contexts in different languages while mapping mentions to entities in a monolingual KB (e.g., English Wikipedia). <ref type="bibr" target="#b24">Tsai and Roth (2016b)</ref> used alignments between languages to train multilingual entity embeddings. They used candidate selection and then they re-rank them with an SVM using these embeddings as well as a set of features (based on the multilingual title, mention, and context tokens). <ref type="bibr" target="#b18">Sil et al. (2018)</ref> explored the use of more sophisticated neural models for XEL as well as <ref type="bibr" target="#b25">Upadhyay et al. (2018)</ref> who jointly modeled type information to boost performance. <ref type="bibr" target="#b33">Zhou et al. (2019)</ref> propose improvements to both entity candidate generation and disambiguation to make better use of the limited data in low-resource scenarios. Note that in this work we focus on multilingual EL, not crosslingual. XEL is limiting to a monolingual KB (usually English), where MEL is more general since it can link to entities that might not be necessary represented in the target monolingual KB but in any of the available languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we propose an autoregressive formulation to the multilingual entity linking problem. For a mention in a given language, our solution generates entity names left-to-right and token-bytoken. The resulting system, mGENRE, maintains entity names in as many languages as possible to exploit language connections and interactions between source mention context and target entity name. The constrained beam search decoding strategy enables fast search within a large set of entity names (e.g., the whole KB in multiple languages) with no need for large-scale dense indices. We additionally design a novel objective function that marginalizes over all available languages to perform a prediction. We empirically show that this strategy is really effective in dealing with languages for which no training data is available (i.e., 50% improvements for languages never seen during training). Overall, our experiments show that mGENRE achieves new state-of-the-art performance on three popular multilingual entity linking datasets. We used a pre-trained mBART ) model on 125 languages-see <ref type="figure">Figure 5</ref> for a visual overview of the overlap with these languages, Wikipedia and the languages used by <ref type="bibr">Botha et al. (2020)</ref>. mBART has 24 layers of hidden size is 1,024 and it has a total of 406M parameters. We pre-trained on an extended version of the cc100 <ref type="bibr" target="#b28">(Conneau et al., 2020;</ref><ref type="bibr" target="#b28">Wenzek et al., 2020)</ref> corpora available here 9 where we increased the number of common crawl snapshots for low resource languages from 12 to 60. The dataset has ?5TB of text. We pre-trained for 500k steps with max 1,024 tokens per GPU on a variable batch size (?3000). <ref type="figure">Figure 5</ref> shows a Venn diagram on the overlap of languages used during pre-training and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data for supervision</head><p>Wikidata Wikidata contains tens of millions of items but most of them are scholarly articles or they correspond to help and template pages in Wikipedia (i.e., not entities we want to retain) 10 . <ref type="bibr">Following (Botha et al., 2020)</ref>, we only keep Wikidata items that have an associated Wikipedia page in at least one language, independent of the languages we actually model. Moreover, we filter out items that are a subclass (P279) or instance of (P31) some Wikimedia organizational entities (e.g., help and template pages-see <ref type="table" target="#tab_15">Table 6</ref>).  the hyperlinks. We only keep unambiguous alignments since, when using Wikidata search (i.e., the third alignment strategy), the mapping could be ambiguous (e.g., multiple items may share the same labels and aliases).</p><p>In <ref type="table" target="#tab_4">Table 10</ref> we report some statistics of the training data extracted from Wikipedia. We use a standard Wikipedia extractor wikiextractor 11 by <ref type="bibr" target="#b1">Attardi (2015)</ref> and a redirect extractor 12 . We use both Wikipedia and Wikidata dumps from 2019-10-01. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Data for test</head><p>We use Wikinews (from 2019-10-01) to construct our unseen Wikinews-7 dataset. In <ref type="table" target="#tab_16">Table 7</ref> we report some statistic of our new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Training</head><p>We implemented, trained, and evaluate our model using the fariseq library <ref type="bibr" target="#b16">(Ott et al., 2019)</ref>. We trained mGENRE using Adam (Kingma and Ba, 2014) with a learning rate 10 ?4 , ? 1 = 0.9, ? 2 = 0.98, and with a linear warm-up for 5,000 steps followed by liner decay for maximum 2M steps. The objective is sequence-to-sequence categorical cross-entropy loss with 0.1 of label smoothing and 0.01 of weight decay. We used dropout probability of 0.1 and attention dropout of 0.1. We used max 3,072 tokens per GPU and variable batch size (?12,500). Training was done on 384 GPUs (Tesla V100 with 32GB of memory) and it completed in ?72h for a total of ?27,648 GPU hours or ?1,152 GPU days. Since TAC-KBP2015 contains noisy text (e.g., XML/HTML tags), we further fine-tune mGENRE for 2k steps on its training set when testing on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Inference</head><p>At test time, we use Constrained Beam Search with 10 beams, length penalty of 1, and maximum decoding steps of 32. We restrict the input sequence to be at most 128 tokens cutting the left, right, or both parts of the context around a mention. When employing marginalization, we normalize the log-probabilities by sequence length using log p(y|x)/L ? , where ? = 0.5 was tuned on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bin</head><p>Support Acc.   <ref type="table" target="#tab_18">Table 8</ref>. The accuracy of unseen mentions is 66.7% and increases up to 93.6% for mentions seen more than 10k times. For extremely common mentions (i.e., seen more than 1M times) the accuracy drops to 73.2%. These mentions correspond to entities that are harder to disambiguate (e.g., 'United States' appears 3.2M times but can be linked to the country as well as any sports team where the context refers to sports).</p><p>Unseen Languages Even though marginalization and canonical representation are the top-two systems in the unseen languages setting, they are not on seen languages. In <ref type="table" target="#tab_20">Table 9</ref> we report the results of all these strategies also on the seen languages (Mewsli-9 test set  ar de en es fa ja sr ta tr ar de en es fa ja sr ta tr 99.99 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.02 99.38 0.54 0.04 0.00 0.01 0.00 0.00 0.00 0.02 0.07 99.85 0.04 0.00 0.01 0.00 0.00 0.00 0.06 0.04 0.79 99.08 0.00 0.02 0.01 0.00 0.00 0.00 0.00 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 99.98 0.00 0.00 0.00 0.01 0.00 0.10 0.01 0.00 0.02 99.86 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 99.96 0.00 0.03 0.02 0.53 0.03 0.02 0.00 0.02 0.05 99.29   <ref type="figure">Figure 6</ref>: Distribution of languages on the top-1 prediction of two mGENRE models on Mewsli-9. Y-axis indicates the source language where X-axis indicates the language of the top-1 prediction. The models trained on those languages.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Results of mGENRE on Mewsli-9 by the number of retrieved candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Distribution of languages on the top-1 prediction of two mGENRE models on Wikinews-7 (unseen languages). Y-axis indicates the source (unseen at training time) language where X-axis indicates the language (seen at training time) of the first prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2103.12528v1 [cs.CL] 23 Mar 2021 [..] Es steht in Konkurrenz zum etablierten [START] GPS [END] -System der USA, soll aber mit den technischen Spezifikationen der Datenstrome des GPS-Systems kompatibel sein. [..] TRANSLATION: [..] It competes with the established [START] GPS [END] system in the USA, but should be compatible with the technical specifications of the data streams of the GPS system. [..]</figDesc><table><row><cell>INPUT: Autoregressive Transformer Decoder with prefix constrained Vocabulary Bidirectional Transformer Encoder</cell><cell>Globalse Navigationssate llitensystem Sistema di posizionamento globale Sistema de posicionament global Wikipedia-Wikidata mapping Q 1 7 9 4 3 5 Q 1 8 8 2 2 Q 1 8 8 2 2 Q 1 8 8 2 2 &gt;&gt; ca &gt;&gt; it posicionamento global &gt;&gt; es Sistema de &gt;&gt; de Global Positioning System Q 1 8 8 2 2 &gt;&gt; de</cell><cell>Sequence scores -1 . 27 -1 .1 7 -1 .1 0 -0 .9 6 -0 .0 9</cell><cell>Sequence scores Aggregating -0 .9 6 0 .6 1</cell><cell>(Global System) Q179435 (satellite system) navigation Positioning Q18822</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>-7 For the purpose of testing a model on languages unseen during training, we extract mention-entities pairs from Wikinews in 7 lan-</figDesc><table><row><cell>Accuracy</cell><cell>0.2 0.3 0.4 0.4 0.5 0.6 0.7 0.8 0.8 0.9 1.0</cell><cell>mg uz gu et eu sv kk sw af su br jv gn ky cy ga ur ca sk qu ja mk az gl la vi sa ka sr da lt eo no te bg sl ku fy ms nl fa th ro hu lv is id be de gd as ru hy fi hi uk it mr am ar sq cs tr hr ta ko ln my bs he zh kn mn or pt ne fr kg en xh pl es so sd pa ps wo el lo om ss ht tn km bn ml si ha tl bm ig lg yo ti ff mGENRE Alias Table log10(training size)</cell><cell>1 log 2 3 4 5 6 7 8 10 (training size) 9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>InTable 1we compare our mGENRE against the best model fromBotha et al. (2020)   (Model F + ) as well as with their alias table baseline. We report results from mGENRE with and without constraining the beam search to the top-k candidates from the table (see Section 3.4) as well as with and without marginalization (see Section 3.3). All of these alternatives outperform Model F + on both micro and macro average accuracy across the 9 languages. Our base model (without candidates or marginalization) has a 10.9% error reduction in micro average and 18.0% error reduction for macro average over all languages. The base model has no restrictions on candidates so it is effectively classifying among all the ?20M entities. The base model performs better than Model F + on each individual language except English and German. Note that these languages are the ones for which we have more training data (?134M and ?60M datapoints each) but also the languages that have the most entities/pages (?6.1M and ?2.4M).</figDesc><table><row><cell>Mewsli-9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Instead, using candidates gives +11% absolute accuracy on TAC-KBP2015 and +5% on TR2016 hard effectively making mGENRE state-of-the-art in both datasets. The role of candidates is very evident on TAC-KBP2015 where there is not much of a difference for Spanish but a +22% absolute accuracy for Chinese. TAC-KBP2015 comes with a training set and we used it to expand the candidate set. Additionally, we also included all simplified Chinese versions of the entity names because we used traditional Chinese in pre-training,Botha et al. (2020)    Ours Language AliasTable Model F + mGENRE + cand. + marg. + cand. + marg.</figDesc><table><row><cell>ar</cell><cell>89.0</cell><cell>92.0</cell><cell>94.7</cell><cell>94.8</cell><cell>95.3</cell><cell>95.4</cell></row><row><cell>de</cell><cell>86.0</cell><cell>92.0</cell><cell>91.5</cell><cell>91.8</cell><cell>91.8</cell><cell>92.0</cell></row><row><cell>en</cell><cell>79.0</cell><cell>87.0</cell><cell>86.7</cell><cell>87.1</cell><cell>87.0</cell><cell>87.2</cell></row><row><cell>es</cell><cell>82.0</cell><cell>89.0</cell><cell>90.0</cell><cell>90.1</cell><cell>90.1</cell><cell>90.1</cell></row><row><cell>fa</cell><cell>87.0</cell><cell>92.0</cell><cell>94.6</cell><cell>94.6</cell><cell>94.2</cell><cell>94.4</cell></row><row><cell>ja</cell><cell>82.0</cell><cell>88.0</cell><cell>89.9</cell><cell>91.1</cell><cell>90.2</cell><cell>91.4</cell></row><row><cell>sr</cell><cell>87.0</cell><cell>93.0</cell><cell>94.9</cell><cell>94.4</cell><cell>95.0</cell><cell>94.5</cell></row><row><cell>ta</cell><cell>79.0</cell><cell>88.0</cell><cell>92.9</cell><cell>93.3</cell><cell>93.1</cell><cell>93.8</cell></row><row><cell>tr</cell><cell>80.0</cell><cell>88.0</cell><cell>90.7</cell><cell>91.4</cell><cell>90.9</cell><cell>91.5</cell></row><row><cell>micro-avg</cell><cell>83.0</cell><cell>89.0</cell><cell>90.2</cell><cell>90.5</cell><cell>90.4</cell><cell>90.6</cell></row><row><cell>macro-avg</cell><cell>83.0</cell><cell>90.0</cell><cell>91.8</cell><cell>92.1</cell><cell>92.0</cell><cell>92.3</cell></row></table><note>and Model F + by Botha et al. (2020) in Table 2. Differently from Meswli-9, the base mGENRE model does not outperform pre- vious systems. Using marginalization brings mini- mal improvements.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on Mewsli-9 dataset. We report results of mGENRE (trained with 'title+lang') with and without top-k candidates from the table as well as with and without marginalization.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TAC-KBP2015</cell><cell></cell><cell></cell><cell cols="2">TR2016 hard</cell><cell></cell></row><row><cell>Method</cell><cell>es</cell><cell>zh</cell><cell>macro-avg</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>it</cell><cell>macro-avg</cell></row><row><cell>Tsai and Roth (2016a)</cell><cell cols="2">82.4 85.1</cell><cell>83.8</cell><cell cols="4">53.3 54.5 47.5 48.3</cell><cell>50.9</cell></row><row><cell>Sil et al. (2018)*</cell><cell cols="2">83.9 85.9</cell><cell>84.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Upadhyay et al. (2018)</cell><cell cols="2">84.4 86.0</cell><cell>85.2</cell><cell cols="4">55.2 56.8 51.0 52.3</cell><cell>53.8</cell></row><row><cell>Zhou et al. (2019)</cell><cell cols="2">82.9 85.5</cell><cell>84.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Botha et al. (2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">62.0 58.0 54.0 56.0</cell><cell>57.5</cell></row><row><cell>mGENRE</cell><cell cols="2">86.3 64.6</cell><cell>75.5</cell><cell cols="4">56.3 57.1 50.0 51.0</cell><cell>53.6</cell></row><row><cell>mGENRE + marg.</cell><cell cols="2">86.9 65.1</cell><cell>76.0</cell><cell cols="4">56.2 56.9 49.7 51.1</cell><cell>53.5</cell></row><row><cell>mGENRE + cand.</cell><cell cols="2">86.5 86.6</cell><cell>86.5</cell><cell cols="4">61.8 61.0 54.3 56.9</cell><cell>58.5</cell></row><row><cell cols="3">mGENRE + cand. + marg. 86.7 88.4</cell><cell>87.6</cell><cell cols="4">61.5 60.6 54.3 56.6</cell><cell>58.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Accuracy on TAC-KBP2015 Entity Linking dataset (only datapoints linked to FreeBase) and TR2016 hard of mGENRE (trained with 'title+lang') with and without top-k candidates from the table as well as with and without marginalization. *as reported by Upadhyay et al. (2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Botha et al. (2020)</cell><cell>mGENRE</cell></row><row><cell></cell><cell>Bin</cell><cell>Support</cell><cell cols="2">Acc. Support Acc.</cell></row><row><cell></cell><cell>[0, 1)</cell><cell>3,198</cell><cell>8.0</cell><cell>1,244 22.1</cell></row><row><cell></cell><cell>[1, 10)</cell><cell>6,564</cell><cell>58.0</cell><cell>5,777 47.3</cell></row><row><cell></cell><cell>[10, 100)</cell><cell>32,371</cell><cell>80.0</cell><cell>28,406 77.3</cell></row><row><cell></cell><cell>[100, 1k)</cell><cell>66,232</cell><cell>90.0</cell><cell>72,414 89.9</cell></row><row><cell></cell><cell>[1k, 10k)</cell><cell>78,519</cell><cell>93.0</cell><cell>84,790 93.2</cell></row><row><cell></cell><cell>[10k, +)</cell><cell>102,203</cell><cell>94.0</cell><cell>96,456 96.3</cell></row><row><cell></cell><cell>micro-avg</cell><cell>289,087</cell><cell cols="2">89.0 289,087 90.6</cell></row><row><cell>shows a break-</cell><cell>macro-avg</cell><cell>-</cell><cell>70.0</cell><cell>-71.0</cell></row><row><cell>down of Mewsli-9 accuracy by entity frequency</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in training for Botha et al.'s (2020) Model F + and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mGENRE. Interestingly, our mGENRE has much</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>higher accuracy (22% vs 8%) on unseen entities</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(i.e., the [0,1) bin). This is because our formula-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion can take advantage of copying names from the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>source, translating them or normalizing them. For</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>example, an unseen person name should likely be</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>linked to the entity with the same name. This is</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a powerful bias that gives the model advantage in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Results on the Mewsli-9 dataset, by entity frequency in training. The support is slightly different because training data differ (i.e., the set of languages from Wikipedia is different).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>mGENRE on the Wikinew-7 unseen lan- guages. Models are trained only on the Mewsli-9 lan- guages (1M datapoints per language). 'Can.' is canon- ical, 'N+L' is 'name+language' and 'L+N' is the oppo- site.M indicates marginalization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>.80 4.18 42.36 0.00 1.38 39.63 5.46 6.19 0.09 0.50 4.26 91.19 0.00 0.42 2.06 0.79 0.69 0.05 1.11 5.49 83.38 0.28 0.13 2.19 0.53 6.83 0.00 2.45 8.81 60.43 0.00 2.08 15.58 8.29 2.35 0.19 0.98 1.81 94.04 0.00 0.08 1.66 1.</figDesc><table><row><cell></cell><cell>ar de en es fa ja sr ta tr</cell></row><row><cell>cs fr it pl pt</cell><cell>0.00 013 0.11</cell></row><row><cell>ru</cell><cell>0.02 0.04 0.44 4.78 0.00 1.79 92.74 0.12 0.06</cell></row><row><cell>zh</cell><cell>0.47 0.00 1.16 1.42 0.11 94.89 1.05 0.42 0.47</cell></row><row><cell></cell><cell>(a) Lang+Name.</cell></row><row><cell></cell><cell>ar de en es fa ja sr ta tr</cell></row><row><cell>cs</cell><cell>8.75 12.44 10.94 8.86 5.44 21.83 8.44 19.69 3.60</cell></row><row><cell>fr</cell><cell>7.93 9.21 18.83 9.16 6.96 22.09 5.75 17.44 2.63</cell></row><row><cell>it</cell><cell>8.64 10.40 14.11 9.71 5.16 33.80 4.51 11.03 2.65</cell></row><row><cell>pl</cell><cell>7.88 12.46 24.70 7.84 5.22 19.59 6.46 13.06 2.78</cell></row><row><cell>pt</cell><cell>8.50 7.07 15.53 10.89 3.79 19.32 7.27 23.09 4.54</cell></row><row><cell>ru</cell><cell>7.66 6.91 14.81 7.56 5.15 26.09 7.05 20.62 4.15</cell></row><row><cell>zh</cell><cell>10.06 6.85 17.70 5.71 4.93 32.26 4.38 15.39 2.72</cell></row><row><cell></cell><cell>(b) Lang+Name M .</cell></row><row><cell>We use our Wikinews-7</cell><cell></cell></row><row><cell>dataset to evaluate mGENRE capabilities to deal</cell><cell></cell></row><row><cell>with languages not seen during training (i.e., the</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Input [ES] . . . Micha?lle Jean, gobernadora general de Canad?, ha emitido el mi?rcoles (13) un comunicado acerca del tiroteo ocurrido en el [START] Dawson College [END] de Montreal. . . . Translation [EN] . . . Micha?lle Jean, Governor General of Canada, issued a statement on Wednesday (13) about the shooting that occurred at [START] Dawson College [END] in Montreal. . . . Prediction 'Coll?ge Dawson ? fr': College in Montreal Q2983587 Outcome Correct: mGENRE copies and normalizes the college name even if it is not does not have an identifier in the source language (i.e., it predicts in French but the source is Spanish). Input [DE] . . . Etwa 47 Menschen sind bei den Protesten festgenommen worden, darunter Chas Booth, Mitglied der [START] schottischen Gr?nen [END] mit Sitz im Stadtrat von Edinburgh. . . . Translation [EN] . . . Around 47 people were arrested during the protests, including Chas Booth, a member of the [START] Scottish Greens [END] on the Edinburgh City Council. . . . even if the party is referred with its German alias, mGENRE predicts the identifier with its English name since the truth German Wikipedia page has the English name. Input [TR] . . . K?inat G?zeli yine Venezueladan [START] 2009 y?l? K?inat G?zellik Yar??mas? [END] 83 ?lkenin temsilcisiyle Bahamalarda yap?ld?. . . . Translation [EN] . . . Miss Universe is again from Venezuela [START] The Universe Beauty Contest of 2009 [END] was held in Bahamas with the representatives of 83 countries. . . . the model is conditioned early during beam search to start with '2009'. Thus, it does not effectively search sequences where the year is at the end missing the ground truth answer. Translation [EN] . . . [START] Marko Stojanovi? [END], President of the World Mime Organization, says: . . .</figDesc><table><row><cell>Prediction</cell><cell>'Scottish Green Party ? de': Scottish Green Party Q1256956</cell></row><row><cell cols="2">Outcome Correct: Prediction '2009 Eurovision ?ocuk?ark? Yar??mas? ? tr': Junior Eurovision Song Contest 2009 Q205038</cell></row><row><cell>Annotation</cell><cell>Miss Universe 2009 Q756701</cell></row><row><cell cols="2">Outcome Wrong: Prediction '????? ???j?????? sr' : Marko Stojanovi? (lawyer) Q12754975</cell></row><row><cell>Annotation</cell><cell>Marko Stojanovi? (actor) Q16099367</cell></row></table><note>Input [SR] . . . [START] ????? ???j?????? [END] , ?????????? ??????? ??????????j? ?????????????, ????:. . .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Examples of correct and wrong predictions of our mGENRE model on selected samples from Mewsli-9. With both correct and wrong predictions we highlight some specific behaviour of our model.</figDesc><table><row><cell>6 Related Work</cell></row><row><cell>The most related works to ours are De Cao et al.</cell></row><row><cell>(2021), that proposed to use an autoregressive lan-</cell></row><row><cell>guage model for monolingual EL, and Botha et al.</cell></row><row><cell>(2020) that proposes to extend the cross-lingual EL</cell></row><row><cell>task to multilingual EL with a language-agnostic</cell></row><row><cell>KB. We provide an outline of the GENRE model</cell></row><row><cell>proposed by De Cao et al. (2021) in Section 2.2</cell></row><row><cell>and 2.3. GENRE was applied not only to EL but</cell></row><row><cell>also for joint mention detection and entity linking</cell></row><row><cell>(still with an autoregressive formulation) as well</cell></row><row><cell>as to page-level document retrieval across multi-</cell></row></table><note>ple Knowledge Intensive Language Tasks (KILT; Petroni et al., 2021) i.e., fact-checking, open- domain question answering, slot filling, and dia- log. Botha et al.'s (2020) Model F + is a bi-encoder model: it is based on two BERT-based (Devlin et al., 2019) encoders that outputs vector repre- sentations for contet and entities. Similar to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 6 :</head><label>6</label><figDesc>Wikidata identifiers used for filtering out items from Botha et al. (2020).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Corpus statistics for the Wikinews unseen languages we use as an evaluation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: mGENRE results on Mewsli-9 dataset by</cell></row><row><cell>mention frequency in training.</cell></row><row><cell>B Additional results</cell></row><row><cell>B.1 Analysis</cell></row><row><cell>By mention frequency We show a breakdown</cell></row><row><cell>of the accuracy of mGENRE on Mewsli-9 by men-</cell></row><row><cell>tion frequency in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: mGENRE on the Mewsli-9. Models are</cell></row><row><cell>trained only on the Mewsli-9 languages (1M data-</cell></row><row><cell>points per language). 'Can.' is canonical, 'N+L' is</cell></row><row><cell>'name+language' and 'L+N' is the opposite. M indi-</cell></row><row><cell>cates marginalization.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Accuracy of mGENRE and alias table on the 105 languages in our Wikipedia validation set. We report also the log-training set sizes per each language. SeeTable 11for all precise values.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>mGENRE Alias Table log10(training size)</cell><cell></cell><cell></cell></row><row><cell>Afrikaans (af) Albanian (sq) Amharic (am) Arabic (ar) Armenian (hy) Assamese (as) Azerbaijani (az) Bambara (bm) Basque (eu) Belarusian (be) Bengali (bn) Bosnian (bs) Breton (br) Bulgarian (bg) Burmese (my) Catalan (ca) Chinese (zh) Croatian (hr) Czech (cs) Danish (da) Dutch (nl) English (en) Esperanto (eo) Estonian (et) Finnish (fi) French (fr) Frysk (fy) Fulah (ff) Gaelic, (gd) Galician (gl) Ganda (lg) Georgian (ka) German (de) Greek (el) Guarani (gn) Gujarati (gu) Haitian (ht) Hausa (ha) Hebrew (he) Hindi (hi) Hungarian (hu) Icelandic (is) Igbo (ig) Indonesian (id) Irish (ga) Italian (it) Japanese (ja) Javanese (jv) Kannada (kn) Kazakh (kk) Khmer (km) Kongo (kg) Korean (ko) Figure 7: Language Kurdish (ku)</cell><cell>Alias Table mGENRE 92.1 97.0 87.4 94.1 85.6 94.2 89.9 94.2 89.2 94.3 85.8 94.6 90.0 96.0 80.6 89.8 94.0 97.6 85.9 94.8 79.7 90.4 86.5 93.7 91.9 96.8 88.8 95.4 86.4 93.7 91.5 96.3 88.0 93.5 84.7 93.9 87.1 94.0 90.6 95.5 86.4 95.2 84.6 92.6 89.7 95.5 91.2 97.7 87.4 94.3 83.6 92.6 91.3 95.2 44.3 69.7 90.9 94.6 90.9 95.9 74.7 88.2 87.9 95.7 86.9 94.7 84.1 91.9 92.7 96.7 96.6 98.1 95.6 90.7 81.7 89.9 90.6 93.5 89.1 94.3 90.7 95.1 89.8 94.9 87.4 89.7 91.8 94.8 90.1 96.3 89.9 94.2 92.1 96.2 92.4 96.7 87.8 93.5 91.0 97.3 85.1 90.5 81.1 92.6 89.1 93.7 89.1 95.4</cell><cell>Support 1,089,581 978,394 75,575 10,308,074 3,082,000 61,209 1,562,968 2,191 4,305,648 2,459,794 960,484 1,916,515 1,255,295 4,655,641 98,992 14,790,419 17,262,417 4,223,179 12,173,376 5,621,483 25,002,389 134,477,329 5,570,306 4,700,888 8,390,037 59,006,932 1,206,432 912 180,186 4,709,070 2,476 1,369,094 60,638,345 3,310,875 89,593 402,483 677,064 19,929 9,947,354 1,040,288 10,138,904 772,213 4,702 7,882,254 435,135 39,382,886 45,957,053 718,589 227,731 1,564,344 73,950 3,733 8,309,492 244,779</cell><cell>2 (b) Sorted by training set size. 3 4 5 6 log 10 (training size) Alias Table mGENRE 86.1 96.6 86.8 91.5 88.0 95.8 85.3 95.0 84.4 93.7 89.5 95.5 Macedonian (mk) Language Kyrgyz (ky) Lao (lo) Latin (la) Latvian (lv) Lingala (ln) Lithuanian (lt) 88.4 96.1 Malagasy (mg) 94.4 98.8 Malay (ms) 89.0 95.2 Malayalam (ml) 75.8 90.0 Marathi (mr) 85.0 94.2 Mongolian (mn) 86.1 93.5 Nepali (ne) 83.9 92.8 Norwegian (no) 88.7 95.4 Oriya (or) 87.3 93.2 Oromo (om) 80.8 91.4 Panjabi (pa) 85.4 92.1 Pashto (ps) 80.0 92.1 Persian (fa) 87.6 95.2 Polish (pl) 82.3 92.4 Portuguese (pt) 85.9 93.0 Quechua (qu) 93.8 96.2 Romanian (ro) 88.8 95.2 Russian (ru) 85.7 94.5 Sanskrit (sa) 86.2 95.8 Serbian (sr) 85.7 95.5 Sindhi (sd) 80.7 92.1 Sinhala (si) 80.1 89.9 Slovak (sk) 88.0 96.3 Slovenian (sl) 87.0 95.4 Somali (so) 84.5 92.2 Spanish (es) 86.5 92.3 Sundanese (su) 93.6 96.8 Swahili (sw) 91.9 97.2 Swati (ss) 81.2 91.2 Swedish (sv) 90.9 97.5 Tagalog (tl) 83.4 89.9 Tamil (ta) 84.2 93.8 Telugu (te) 89.2 95.4 Thai (th) 92.3 95.2 Tigrinya (ti) 57.8 79.0 Tswana (tn) 89.5 90.6 Turkish (tr) 89.0 93.9 Ukrainian (uk) 85.8 94.3 Urdu (ur) 91.0 96.3 Uzbek (uz) 73.8 98.4 Vietnamese (vi) 91.8 95.8 Welsh (cy) 94.4 96.4 Wolof (wo) 78.0 91.9 Xhosa (xh) 73.9 92.6 Yoruba (yo) 75.6 87.9 micro-avg 86.5 93.8 macro-avg 86.6 93.9 total --</cell><cell>7 777,210,183 8 Support 271,335 16,173 1,986,307 1,522,814 15,518 3,512,764 2,035,348 857,000 3,190,700 712,869 355,536 208,847 151,958 10,234,086 79,261 7,153 145,204 46,987 5,567,774 25,817,929 20,625,904 247,508 6,974,837 35,783,391 73,380 7,012,202 33,990 90,866 4,014,344 3,754,135 53,132 37,749,593 598,878 693,049 4,344 39,409,278 562,526 1,110,037 841,549 2,190,249 696 4,896 5,657,757 16,360,016 1,142,953 764,566 10,015,209 1,254,901 7,257 14,163 88,032 --</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 11 :</head><label>11</label><figDesc>Accuracy of mGENRE and alias table on the 105 languages in our Wikipedia validation set. The support indicates how many datapoints where used to train where validation is done on 1,000 examples per language (less for Tigrinya and Fulah since we have less than a thousand hyperlinks).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.iso.org/standard/22109. html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.wikinews.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Arabic, English, Farsi, German, Japanese, Serbian, Spanish, Tamil, and Turkish.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Information provided by private correspondence with the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Chinese, Czech, French, Italian, Polish, Portuguese, and Russian. 6 Botha et al. (2020) did not release code for extracting Mewsli-9 from a Wikinews dump.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Even though there are many datapoints, disambiguating between more entities is harder. Moreover, low-resource languages should be harder as there are fewer training instances, but there are no such languages in the Mewsli-9 set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We speculate that including different version (e.g., different dialects for Arabic) of entity names could improve performance in all languages. Since this is not in the scope of this paper, we will leave it for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://github.com/attardi/ wikiextractor 12 https://code.google.com/archive/p/ wikipedia-redirect</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Authors thank Patrick Lewis, Aleksandra Piktus, for helpful discussions and technical support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 8</ref><p>: Distribution of languages on the top-1 prediction of mGENRE on Wikipedia heldout set. Y-axis indicates the source language where X-axis indicates the language of the top-1 prediction. The model is trained on all those languages. Clearly the model is biased to predict in the source language-note that we train in such a way-but there are some languages that are also used quite often (e.g., English). French and Russian are also often used from other languages.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giusepppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="https://github.com/attardi/wikiextractor" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A memory efficient baseline for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15156</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Overview of tac-kbp2015 tri-lingual entity discovery and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Banner: An executable survey of advances in biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>date: 04-01-2008 Through 08-01-2008</idno>
	</analytic>
	<monogr>
		<title level="m">13th Pacific Symposium on Biocomputing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="663" />
		</imprint>
	</monogr>
	<note>Pacific Symposium on Biocomputing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07033</idno>
		<title level="m">Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. Paq: 65 million probably-asked questions and what you can do with them</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crosslanguage entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="255" to="263" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslanguage entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="255" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumpei</forename><surname>Miyawaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Takahashi</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fajcik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Docekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Ondrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Smrz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okhonko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00133</idno>
		<title level="m">Yashar Mehdad, and Wen tau Yih. 2021. NeurIPS 2020 Efficien-tQA Competition: Systems, Analyses and Lessons Learned</title>
		<imprint>
			<publisher>Michael Schlichtkrull, Sonal Gupta</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1258</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2553" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">KILT: a Benchmark for Knowledge Intensive Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To appear at Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint multilingual supervision for cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2486" to="2495" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="677" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entity linking for biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Howsmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards zero-resource cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
