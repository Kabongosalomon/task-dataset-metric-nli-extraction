<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy-based Logic Explanations of Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Barbiero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>UK</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Ciravegna</surname></persName>
							<email>gabriele.ciravegna@unifi.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Universit? di Firenze (Italy)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Universit? di Siena (Italy)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universit? C?te d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Giannini</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universit? di Siena (Italy)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>UK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universit? di Siena (Italy)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universit? C?te d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Melacci</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universit? di Siena (Italy)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entropy-based Logic Explanations of Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (i.e. concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In this paper, we propose a novel end-to-end differentiable approach enabling the extraction of logic explanations from neural networks using the formalism of First-Order Logic. The method relies on an entropy-based criterion which automatically identifies the most relevant concepts. We consider four different case studies to demonstrate that: (i) this entropy-based criterion enables the distillation of concise logic explanations in safety-critical domains from clinical data to computer vision; (ii) the proposed approach outperforms state-of-the-art white-box models in terms of classification accuracy and matches black box performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Classification is the problem of identifying a set of categories an observation belongs to. We indicate with Y ? arXiv:2106.06804v4 [cs.AI]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The lack of transparency in the decision process of some machine learning models, such as neural networks, limits their application in many safety-critical domains <ref type="bibr">(EUGDPR 2017;</ref><ref type="bibr" target="#b19">Goddard 2017)</ref>. For this reason, explainable artificial intelligence (XAI) research has focused either on explaining black box decisions <ref type="bibr" target="#b75">(Zilke, Loza Menc?a, and Janssen 2016;</ref><ref type="bibr" target="#b73">Ying et al. 2019;</ref><ref type="bibr" target="#b8">Ciravegna et al. 2020a;</ref><ref type="bibr" target="#b3">Arrieta et al. 2020)</ref> or on developing machine learning models interpretable by design <ref type="bibr" target="#b61">(Schmidt and Lipson 2009;</ref><ref type="bibr" target="#b33">Letham et al. 2015;</ref><ref type="bibr" target="#b13">Cranmer et al. 2019;</ref><ref type="bibr" target="#b45">Molnar 2020)</ref>. However, while interpretable models engender trust in their predictions (Doshi-Velez and <ref type="bibr" target="#b16">Kim 2017</ref><ref type="bibr" target="#b17">Kim , 2018</ref><ref type="bibr" target="#b0">Ahmad, Eckert, and Teredesai 2018;</ref><ref type="bibr" target="#b57">Rudin et al. 2021)</ref>, black box models, such as neural networks, are the ones that provide state-of-the-art task performances <ref type="bibr" target="#b4">(Battaglia et al. 2018;</ref><ref type="bibr" target="#b15">Devlin et al. 2018;</ref><ref type="bibr" target="#b18">Dosovitskiy et al. 2020;</ref><ref type="bibr" target="#b72">Xie et al. 2020)</ref>. Research to address this imbalance is needed for the deployment of cutting-edge technologies.</p><p>Most techniques explaining black boxes focus on finding or ranking the most relevant features used by the model to make predictions <ref type="bibr" target="#b66">(Simonyan, Vedaldi, and Zisserman 2013;</ref><ref type="bibr" target="#b74">Zeiler and Fergus 2014;</ref><ref type="bibr" target="#b54">Ribeiro, Singh, and Guestrin 2016b;</ref><ref type="bibr" target="#b36">Lundberg and Lee 2017;</ref><ref type="bibr" target="#b62">Selvaraju et al. 2017)</ref>. Such featurescoring methods are very efficient and widely used, but they cannot explain how neural networks compose such features to make predictions <ref type="bibr" target="#b29">(Kindermans et al. 2019;</ref><ref type="bibr" target="#b28">Kim et al. 2018b;</ref><ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola 2018)</ref>. In addition, a key issue of most explaining methods is that explanations are given in terms of input features (e.g. pixel intensities) that do not correspond to high-level categories that humans can easily understand <ref type="bibr" target="#b28">(Kim et al. 2018b;</ref><ref type="bibr" target="#b68">Su, Vargas, and Sakurai 2019)</ref>. To overcome this issue, concept-based approaches have become increasingly popular as they provide explanations in terms of human-understandable categories (i.e. the concepts) rather than raw features <ref type="bibr" target="#b28">(Kim et al. 2018b;</ref><ref type="bibr" target="#b18">Ghorbani et al. 2019;</ref><ref type="bibr" target="#b30">Koh et al. 2020;</ref><ref type="bibr" target="#b7">Chen, Bei, and Rudin 2020)</ref>. However, fewer approaches are able to explain how such concepts are leveraged by the classifier and even fewer provide concise explanations whose validity can be assessed quantitatively <ref type="bibr" target="#b54">(Ribeiro, Singh, and Guestrin 2016b;</ref><ref type="bibr" target="#b21">Guidotti et al. 2018;</ref><ref type="bibr" target="#b14">Das and Rad 2020)</ref>.</p><p>Contributions. In this paper, we first propose an entropybased layer (Sec. 3.1) that enables the implementation of concept-based neural networks, providing First-Order Logic explanations <ref type="figure">(Fig. 1)</ref>. The proposed approach is not just a post-hoc method, but an explainable by design approach as it embeds additional constraints both in the architecture and in the learning process, to allow the emergence of simple logic explanations. This point of view is in contrast with posthoc methods, which generally do not impose any constraint on classifiers: After the training is completed, the post-hoc method kicks in. Second, we describe how to interpret the predictions of the proposed neural model to distill logic explanations for individual observations and for a whole target class (Sec. 3.3). We demonstrate how the proposed approach provides high-quality explanations according to six quantitative metrics while matching black-box and outperforming state-of-the-art white-box models (Sec. 4) in terms of classification accuracy on four case studies (Sec. 5). Finally, we share an implementation of the entropy layer, with extensive documentation and all the experiments in the public repository: https://github.com/pietrobarbiero/entropy-lens. <ref type="figure">Figure 1</ref>: The proposed pipeline on one example from the CUB dataset. The neural network f : C ? Y maps concepts onto target classes and provide concise logic explanations (yellow -arguments of predicates are dropped for simplicity) of its own decision process. When the input data is non-interpretable (as pixels intensities), a classifier g : X ? C maps inputs to concepts.</p><p>{0, 1} r the space of binary encoded targets in a problem with r categories. Concept-based classifiers f are a family of machine learning models predicting class memberships from the activation scores of k human-understandable categories, f : C ? Y , where C ? [0, 1] k (see <ref type="figure">Fig. 1</ref>). Concept-based classifiers improve human understanding as their input and output spaces consists of interpretable symbols. When observations are represented in terms of non-interpretable input features belonging to X ? R d (such as pixels intensities), a "concept decoder" g is used to map the input into a concept-based space, g : X ? C (see <ref type="figure">Fig. 1</ref>). Otherwise, they are simply rescaled from the unbounded space R d into the unit interval [0, 1] k , such that input features can be treated as logic predicates.</p><p>In the recent literature, the most similar method related to the proposed approach is the ? network proposed by <ref type="bibr">Ciravegna et al. (Ciravegna et al. 2020a,b)</ref>, an end-to-end differentiable concept-based classifier explaining its own decision process. The ? network leverages the intermediate symbolic layer whose output belongs to C to distill First-Order Logic formulas, representing the learned map from C to Y . The model consists of a sequence of fully connected layers with sigmoid activations only. An L1-regularization and a strong pruning strategy is applied to each layer of weights in order to allow the computation of logic formulas representing the activation of each node. Such constraints, however, limit the learning capacity of the network and impair the classification accuracy, making standard white-box models, such as decision trees, more attractive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entropy-based Logic Explanations of Neural Networks</head><p>The key contribution of this paper is a novel linear layer enabling entropy-based logic explanations of neural networks (see <ref type="figure" target="#fig_0">Fig. 2 and Fig. 3</ref>). The layer input belongs to the concept space C and the outcomes of the layer computations are: (i) the embeddings h i (as any linear layer), (ii) a truth table T i explaining how the network leveraged concepts to make predictions for the i-th target class. Each class of the problem requires an independent entropy-based layer, as emphasized by the superscript i. For ease of reading and without loss of generality, all the following descriptions concern inference for a single observation (corresponding to the concept tuple  c ? C) and a neural network f i predicting the class memberships for the i-th class of the problem. For multi-class problems, multiple "heads" of this layer are instantiated, with one "head" per target class (see Sec. 5), and the hidden layers of the class-specific networks could be eventually shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entropy-based linear layer</head><p>When humans compare a set of hypotheses outlining the same outcomes, they tend to have an implicit bias towards the simplest ones as outlined in philosophy <ref type="bibr" target="#b67">(Soklakov 2002;</ref><ref type="bibr" target="#b52">Rathmanner and Hutter 2011)</ref>, psychology <ref type="bibr" target="#b43">(Miller 1956;</ref><ref type="bibr" target="#b12">Cowan 2001)</ref>, and decision making <ref type="bibr" target="#b63">(Simon 1956</ref><ref type="bibr" target="#b64">(Simon , 1957</ref><ref type="bibr" target="#b65">(Simon , 1979</ref>. The proposed entropy-based approach encodes this inductive bias in an end-to-end differentiable model. The purpose of the entropy-based linear layer is to encourage the neural model to pick a limited subset of input concepts, allowing it to provide concise explanations of its predictions. The learnable parameters of the layer are the usual weight matrix W and bias vector b. In the following, the forward pass is described by the operations going from Eq. 1 to Eq.  <ref type="figure">Figure 3</ref>: A detailed view on one "head" of the entropybased linear layer for the 1-st class, emphasizing the role of the k-th input concept as example: (i) the scalar ? 1 k (Eq. 1) is computed from the set of weights connecting the k-th input concept to the output neurons of the entropy-based layer; (ii) the relative importance of each concept is summarized by the categorical distribution ? 1 (Eq. 2); (iii) rescaled relevance scores? 1 drop irrelevant input concepts out (Eq. 3); (iv) hidden states h 1 (Eq. 4) and Boolean-like concepts? 1 (Eq. 5) are provided as outputs of the entropy-based layer.</p><p>4 while the generation of the truth tables from which explanations are extracted is formalized by Eq. 5 and Eq. 6.</p><p>The relevance of each input concept can be summarized in a first approximation by a measure that depends on the values of the weights connecting such concept to the upper network. In the case of network f i (i.e. predicting the i-th class) and of the j-th input concept, we indicate with W i j the vector of weights departing from the j-th input (see <ref type="figure">Fig. 3</ref>), and we introduce</p><formula xml:id="formula_0">? i j = ||W i j || 1 .<label>(1)</label></formula><p>The higher ? i j , the higher the relevance of the concept j for the network f i . In the limit case (? i j ? 0) the model f i drops the j-th concept out. To select only few relevant concepts for each target class, concepts are set up to compete against each other. To this aim, the relative importance of each concept to the i-th class is summarized in the categorical distribution ? i , composed of coefficients ? i j ? [0, 1] (with j ? i j = 1), modeled by the softmax function:</p><formula xml:id="formula_1">? i j = e ? i j /? k l=1 e ? i l /?<label>(2)</label></formula><p>where ? ? R + is a user-defined temperature parameter to tune the softmax function. For a given set of ? i j , when using high temperature values (? ? ?) all concepts have nearly the same relevance. For low temperatures values (? ? 0), the probability of the most relevant concept tends to ? i j ? 1, while it becomes ? i k ? 0, k = j, for all other concepts. For further details on the impact of ? on the model predictions and explanations, see Appendix A.6. As the probability distribution ? i highlights the most relevant concepts, this information is directly fed back to the input, weighting concepts by the estimated importance. To avoid numerical cancellation due to values in ? i close to zero, especially when the input dimensionality is large, we replace ? i with its normalized instance? i , still in [0, 1] k , and each input sample c ? C is modulated by this estimated importance,</p><formula xml:id="formula_2">c i = c ? i with? i j = ? i j max u ? i u ,<label>(3)</label></formula><p>where denotes the Hadamard (element-wise) product. The highest value in? i is always 1 (i.e. max j? i j = 1) and it corresponds to the most relevant concept. The embeddings h i are computed as in any linear layer by means of the affine transformation:</p><formula xml:id="formula_3">h i = W ici + b i .<label>(4)</label></formula><p>Whenever? i j ? 0, the inputc i j ? 0. This means that the corresponding concept tends to be dropped out and the network f i will learn to predict the i-th class without relying on the j-th concept.</p><p>In order to get logic explanations, the proposed linear layer generates the truth table T i formally representing the behaviour of the neural network in terms of Boolean-like representations of the input concepts. In detail, we indicate withc the Boolean interpretation of the input tuple c ? C, while ? i ? {0, 1} k is the binary mask associated to? i . To encode the inductive human bias towards simple explanations <ref type="bibr" target="#b43">(Miller 1956;</ref><ref type="bibr" target="#b12">Cowan 2001;</ref><ref type="bibr" target="#b37">Ma, Husain, and Bays 2014)</ref>, the mask ? i is used to generate the binary concept tuple? i , dropping the least relevant concepts out of c,</p><formula xml:id="formula_4">c i = ?(c, ? i ) with ? i = I?i ? andc = I c? ,<label>(5)</label></formula><p>where I z? denotes the indicator function that is 1 for all the components of vector z being ? and 0 otherwise (considering the unbiased case, we set = 0.5). The function ? returns the vector with the components ofc that correspond to 1's in ? i (i.e. it sub-selects the data inc). As a results,? i belongs to a space? i of m i Boolean features, with m i &lt;k due to the effects of the subselection procedure.</p><p>The truth table T i is a particular way of representing the behaviour of network f i based on the outcomes of processing multiple input samples collected in a generic dataset C. As the truth table involves Boolean data, we denote wit? C i the set with the Boolean-like representations of the samples in C computed by ?, Eq. 5. We also introducef i (c) as the Boolean-like representation of the network output, f i (c) = I f i (c)? . The truth table T i is obtained by stacking data of? i into a 2D matrix? i (row-wise), and concatenating the result with the column vectorf i whose elements ar? f i (c), c ? C, that we summarize as</p><formula xml:id="formula_5">T i = ? i f i .<label>(6)</label></formula><p>To be precise, any T i is more like an empirical truth table than a classic one corresponding to an n-ary boolean function, indeed T i can have repeated rows and missing Boolean tuple entries. However, T i can be used to generate logic explanations in the same way, as we will explain in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss function</head><p>The entropy of the probability distribution ? i (Eq. 2),</p><formula xml:id="formula_6">H(? i ) = ? k j=1 ? i j log ? i j (7)</formula><p>is minimized when a single ? i j is one, thus representing the extreme case in which only one concept matters, while it is maximum when all concepts are equally important. When H is jointly minimized with the usual loss function for supervised learning L(f, y) (being y the target labels-we used the cross-entropy in our experiments), it allows the model to find a trade off between fitting quality and a parsimonious activation of the concepts, allowing each network f i to predict i-th class memberships using few relevant concepts only. Overall, the loss function to train the network f is defined as,</p><formula xml:id="formula_7">L(f, y, ? 1 , . . . , ? r ) = L(f, y) + ? r i=1 H(? i ),<label>(8)</label></formula><p>where ?&gt;0 is the hyperparameter used to balance the relative importance of low-entropy solutions in the loss function.</p><p>Higher values of ? lead to sparser configuration of ?, constraining the network to focus on a smaller set of concepts for each classification task (and vice versa), thus encoding the inductive human bias towards simple explanations <ref type="bibr" target="#b43">(Miller 1956;</ref><ref type="bibr" target="#b12">Cowan 2001;</ref><ref type="bibr" target="#b37">Ma, Husain, and Bays 2014)</ref>. For further details on the impact of ? on the model predictions and explanations, see Appendix A.6. It may be pointed out that a similar regularization effect could be achieved by simply minimizing the L 1 norm over ? i . However, as we observed in A.5, the L 1 loss does not sufficiently penalize the concept scores for those features which are uncorrelated with the predicted category. The Entropy loss, instead, correctly shrink to zero concept scores associated to uncorrelated features while the other remains close to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">First-order logic explanations</head><p>Any Boolean function can be converted into a logic formula in Disjunctive Normal Form (DNF) by means of its truth-table <ref type="bibr" target="#b42">(Mendelson 2009</ref>). Converting a truth table into a DNF formula provides an effective mechanism to extract logic rules of increasing complexity from individual observations to a whole class of samples. The following rule extraction mechanism is applied to any empirical truth table T i for each task i.</p><p>FOL extraction. Each row of the truth table T i can be partitioned into two parts that are a tuple of binary concept activations,q ?? i , and the outcome off i (q) ? {0, 1}. An example-level logic formula, consisting in a single minterm, can be trivially extracted from each row for whichf i (q) = 1, by simply connecting with the logic AND (?) the true concepts and negated instances of the false ones. The logic formula becomes human understandable whenever concepts appearing in such a formula are replaced with human-interpretable strings that represent their name (similar consideration holds forf i , in what follows). For example, the following logic formula ? i t ,</p><formula xml:id="formula_8">? i t = c 1 ? ?c 2 ? . . . ? c mi ,<label>(9)</label></formula><p>is the formula extracted from the t-th row of the table where, in the considered example, only the second concept is false, being c z the name of the z-th concept. Example-level formulas can be aggregated with the logic OR (?) to provide a class-level formula,</p><formula xml:id="formula_9">t?Si ? i t ,<label>(10)</label></formula><p>being S i the set of rows indices of T i for whichf i (q) = 1, i.e. it is the support off i . We define with ? i (?) the function that holds true whenever Eq. 10, evaluated on a given Boolean tuple?, is true. Due to the aforementioned definition of support, we get the following class-level First-Order Logic (FOL) explanation for all the concept tuples,</p><formula xml:id="formula_10">?? ?? i : ? i (?) ?f i (?).<label>(11)</label></formula><p>We note that in case of non-concept-like input features, we may still derive the FOL formula through the "concept decoder" function g (see Sec. 2),</p><formula xml:id="formula_11">?x ? X : ? i ?(g(x), ? i ) ?f i ?(g(x), ? i )<label>(12)</label></formula><p>An example of the above scheme for both example and class-level explanations is depicted on top-right of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Remarks. The aggregation of many example-level explanations may increase the length and the complexity of the FOL formula being extracted for a whole class. However, existing techniques as the Quine-McCluskey algorithm can be used to get compact and simplified equivalent FOL expressions <ref type="bibr" target="#b40">(McColl 1878;</ref><ref type="bibr" target="#b49">Quine 1952;</ref><ref type="bibr">Mc-Cluskey 1956)</ref>. For instance, the explanation (person ? nose) ? (?person ? nose) can be formally simplified in nose. Moreover, the Boolean interpretation of concept tuples may generate colliding representations for different samples. For instance, the Boolean representation of the two samples {(0.1, 0.7), (0.2, 0.9)} is the tuplec = (0, 1) for both of them. This means that their example-level explanations match as well. However, a concept can be eventually split into multiple finer grain concepts to avoid collisions. Finally, we mention that the number of samples for which any example-level formula holds (i.e. the support of the formula) is used as a measure of the explanation importance. In practice, example-level formulas are ranked by support and iteratively aggregated to extract class-level explanations, until the aggregation improves the accuracy of the explanation over a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>In order to provide explanations for a given black-box model, most methods focus on identifying or scoring the most relevant input features <ref type="bibr" target="#b66">(Simonyan, Vedaldi, and Zisserman 2013;</ref><ref type="bibr" target="#b74">Zeiler and Fergus 2014;</ref><ref type="bibr">Ribeiro, Singh, and Guestrin 2016b,a;</ref><ref type="bibr" target="#b36">Lundberg and Lee 2017;</ref><ref type="bibr" target="#b62">Selvaraju et al. 2017)</ref>. Feature scores are usually computed sample by sample (i.e. providing local explanations) analyzing the activation patterns in the hidden layers of neural networks <ref type="bibr" target="#b66">(Simonyan, Vedaldi, and Zisserman 2013;</ref><ref type="bibr" target="#b74">Zeiler and Fergus 2014;</ref><ref type="bibr" target="#b62">Selvaraju et al. 2017)</ref> or by following a modelagnostic approach <ref type="bibr" target="#b53">(Ribeiro, Singh, and Guestrin 2016a;</ref><ref type="bibr" target="#b36">Lundberg and Lee 2017)</ref>. To enhance human understanding of feature scoring methods, concept-based approaches have been effectively employed for identifying common activations patterns in the last nodes of neural networks corresponding to human categories <ref type="bibr" target="#b27">(Kim et al. 2018a;</ref><ref type="bibr" target="#b26">Kazhdan et al. 2020)</ref> or constraining the network to learn such concepts <ref type="bibr" target="#b7">(Chen, Bei, and Rudin 2020;</ref><ref type="bibr" target="#b30">Koh et al. 2020</ref>). Either way, feature-scoring methods are not able to explain how neural networks compose features to make predictions <ref type="bibr" target="#b29">(Kindermans et al. 2019;</ref><ref type="bibr" target="#b28">Kim et al. 2018b</ref>; Alvarez-Melis and Jaakkola 2018) and only a few of these approaches have been efficiently extended to provide explanations for a whole class (i.e. providing global explanations) <ref type="bibr" target="#b66">(Simonyan, Vedaldi, and Zisserman 2013;</ref><ref type="bibr" target="#b53">Ribeiro, Singh, and Guestrin 2016a)</ref>. By contrast, a variety of rule-based approaches have been proposed to provide concept-based explanations. Logic rules are used to explain how black boxes predict class memberships for indivudal samples <ref type="bibr" target="#b21">(Guidotti et al. 2018;</ref><ref type="bibr" target="#b55">Ribeiro, Singh, and Guestrin 2018)</ref>, or for a whole class <ref type="bibr" target="#b60">(Sato and Tsukimoto 2001;</ref><ref type="bibr" target="#b75">Zilke, Loza Menc?a, and Janssen 2016;</ref><ref type="bibr">Ciravegna et al. 2020a,b</ref>  <ref type="bibr" target="#b5">(Breiman et al. 1984;</ref><ref type="bibr" target="#b50">Quinlan 1986</ref><ref type="bibr" target="#b51">Quinlan , 2014</ref> and Decision Lists <ref type="bibr" target="#b56">(Rivest 1987;</ref><ref type="bibr" target="#b33">Letham et al. 2015;</ref><ref type="bibr" target="#b2">Angelino et al. 2018</ref>) were devised to be intrinsically interpretable. However, most of them struggle in solving complex classification problems. Logistic Regression, for instance, in its vanilla definition, can only recognize linear patterns, e.g. it cannot to solve the XOR problem <ref type="bibr" target="#b44">(Minsky and Papert 2017)</ref>. Further, only Decision Trees and Decision Lists provide explanations in the from of logic rules. Considering decision trees, each path may be seen as a human comprehensible decision rule when the height of the tree is reasonably contained. Another family of concept-based XAI methods is represented by rule-mining algorithms which became popular at the end of the last century <ref type="bibr" target="#b25">(Holte 1993;</ref><ref type="bibr" target="#b10">Cohen 1995)</ref>. Recent research has led to powerful rule-mining approaches as Bayesian Rule Lists (BRL) <ref type="bibr" target="#b33">(Letham et al. 2015)</ref>, where a set of rules is "pre-mined" using the frequent-pattern tree mining algorithm <ref type="bibr" target="#b22">(Han, Pei, and Yin 2000)</ref> and then the best rule set is identified with Bayesian statistics. In this paper, the proposed approach is compared with methods providing logic-based, global explanations. In particular, we selected one representative approach from different families of methods: Decision Trees 1 (white-box machine learning), BRL 2 (rule mining) and ? Networks 3 (explainable neural models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The quality of the explanations and the classification performance of the proposed approach are quantitatively assessed and compared to state-of-the-art white-box models. <ref type="figure">Figure 4</ref>: The four case studies show how the proposed Entropy-based networks (green) provide concise logic explanations (yellow) of their own decision process in different real-world contexts. When input features are noninterpretable, as pixel intensities, a "concept decoder" (ResNet10) maps images into concepts. Entropy-based networks then map concepts into target classes.</p><p>A visual sketch of each classification problem (described in detail in Sec. 5.1) and a selection of the logic formulas found by the proposed approach is reported in <ref type="figure">Fig. 4</ref>. Six quantitative metrics are defined and used to compare the proposed approach with state-of-the-art methods. Sec. 5.2 summarizes the main findings. Further details concerning the experiments are reported in the supplemental material A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification tasks and datasets</head><p>Four classification problems ranging from computer vision to medicine are considered. Computer vision datasets (e.g. CUB) are annotated with low-level concepts (e.g. bird attributes) used to train concept bottleneck pipelines <ref type="bibr" target="#b30">(Koh et al. 2020</ref>). In the other datasets, the input data is rescaled into a categorical space (R k ? C) suitable for conceptbased networks. Please notice that this preprocessing step is performed for all white-box models considered in the experiments for a fair comparison. Further descriptions of each dataset and links to all sources are reported in Appendix A.2. Will we recover from ICU? (MIMIC-II). The Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II, <ref type="bibr">(Saeed et al. 2011;</ref><ref type="bibr" target="#b20">Goldberger et al. 2000)</ref>) is a public- access intensive care unit (ICU) database consisting of 32,536 subjects (with 40,426 ICU admissions) admitted to different ICUs. The task consists in identifying recovering or dying patients after ICU admission. An end-to-end classifier f : C ? Y carries out the classification task.</p><p>What kind of democracy are we living in? (V-Dem).</p><p>Varieties of Democracy (V-Dem, <ref type="bibr" target="#b48">(Pemstein et al. 2018;</ref><ref type="bibr">Coppedge et al. 2021</ref>)) dataset contains a collection of indicators of latent regime characteristics over 202 countries from 1789 to 2020. The database include k 1 = 483 lowlevel indicators k 2 = 82 mid-level indices. The task consists in identifying electoral democracies from non-electoral ones. We indicate with C 1 , C 2 the spaces associated to the activations of the two levels of concepts. Classifiers f 1 and f 2 are trained to learn the map C 1 ? C 2 ? Y . Explanations are given for classifier f 2 in terms of concepts c 2 ? C 2 . What does parity mean? (MNIST Even/Odd). The Modified National Institute of Standards and Technology database (MNIST, (LeCun 1998)) contains a large collection of images representing handwritten digits. The task we consider here is slightly different from the common digitclassification. Assuming Y ? {0, 1} 2 , we are interested in determining if a digit is either odd or even, and explaining the assignment to one of these classes in terms of the digit labels (concepts in C). The mapping X ? C is provided by a Quantitative metrics. Measuring the classification quality is of crucial importance for models that are going to be applied in real-world environments. On the other hand, assessing the quality of the explanations is required for a safedeployment. In contrast with other kind of explanations, logic-based formulas can be evaluated quantitatively. Given a classification problem, first a set of rules are extracted for each target category from each considered model. Each explanation is then tested on an unseen set of test samples.</p><p>The results for each metric are reported in terms of mean and standard error, computed over a 5-fold cross validation <ref type="bibr" target="#b31">(Krzywinski and Altman 2013)</ref>. For each experiment and for each model model (f : C ? Y mapping concepts to target categories) six quantitative metrics are measured. (i) The MODEL ACCURACY measures how well the explainer identifies the target classes on unseen data (see <ref type="table" target="#tab_3">Table 1</ref>). (ii) The EXPLANATION ACCURACY measures how well the extracted logic formulas identifies the target classes ( <ref type="figure">Fig. 5</ref>). This metric is obtained as the average of the F1 scores computed for each class explanation. (iii) The COMPLEXITY OF AN EXPLANATION is computed by standardizing the explanations in DNF and then by counting the number of terms of the standardized formula ( <ref type="figure">Fig. 5)</ref>: the longer the formula, the harder the interpretation for a human being. (iv) The FI-DELITY OF AN EXPLANATION measures how well the extracted explanation matches the predictions obtained using the explainer <ref type="table" target="#tab_4">(Table 2</ref>). (v) The RULE EXTRACTION TIME measures the time required to obtain an explanation from scratch (see <ref type="figure">Fig. 6</ref>), computed as the sum of the time required to train the model and to extract the formula from a trained explainer. (vi) The CONSISTENCY OF AN EXPLANA-TION measures the average similarity of the extracted explanations over the 5-fold cross validation runs (see <ref type="table" target="#tab_5">Table 3)</ref>, computed by counting how many times the same concepts appear in a logic formula over different iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and discussion</head><p>Experiments show how entropy-based networks outperform state-of-the-art white box models such as BRL and decision trees 4 and interpretable neural models such as ? networks on challenging classification tasks <ref type="table" target="#tab_3">(Table 1)</ref>. Moreover, the entropy-based regularization and the adoption of a conceptbased neural network have minor affects on the classification accuracy of the explainer when compared to a standard black box neural network 5 directly working on the input data, and a Random Forest model applied on the concepts.At the same time, the logic explanations provided by entropy-based networks are better than ? networks and almost as accurate as the rules found by decision trees and BRL, while being far more concise, as demonstrated in <ref type="figure">Fig. 5</ref>. More precisely, logic explanations generated by the proposed approach represent non-dominated solutions <ref type="bibr" target="#b38">(Marler and Arora 2004)</ref> quantitatively measured in terms of complexity and classification error of the explanation. Furthermore, the time required to train entropy-based networks is only slightly  higher with respect to Decision Trees but is lower than ? Networks and BRL by one to three orders of magnitude ( <ref type="figure">Fig.  6</ref>), making it feasible for explaining also complex tasks. The fidelity <ref type="table" target="#tab_4">(Table 2</ref>) 6 of the formulas extracted by the entropybased network is always higher than 90% with the only exception of MIMIC. This means that almost any prediction made using the logic explanation matches the corresponding prediction made by the model, making the proposed approach very close to a white box model. The combination of these results empirically shows that our method represents a viable solution for a safe deployment of explainable cuttingedge models.</p><p>The reason why the proposed approach consistently outperform ? networks across all the key metrics (i.e. classification accuracy, explanation accuracy, and fidelity) can be explained observing how entropy-based networks are far less constrained than ? networks, both in the architecture (our approach does not apply weight pruning) and in the loss function (our approach applies a regularization on the distributions ? i and not on all weight matrices). Likewise, the main reason why the proposed approach provides a higher classification accuracy with respect to BRL and decision trees may lie in the smoothness of the decision functions of neural networks which tend to generalize better than rule-based methods, as already observed by Tavares et al. <ref type="bibr" target="#b69">(Tavares et al. 2020)</ref>. For each dataset, we report in the supplemental material (Appendix A.7) a few examples of logic explanations extracted by each method, as well as in <ref type="figure">Fig. 4</ref>. We mention that the proposed approach is the only matching the logically correct ground-truth explanation for the MNIST even/odd experiment, i.e. ?x,</p><formula xml:id="formula_12">isOdd(x) ? isOne(x)?isThree(x)?isFive(x)?isSeven(x)?isNine(x) and ?x, isEven(x) ? isZero(x) ? isTwo(x) ? isfour(x) ? isSix(x) ? isEight(x)</formula><p>, being ? the exclusive OR. In terms of formula consistency, we observe how BRL is the most consistent rule extractor, closely followed by the proposed approach <ref type="table" target="#tab_5">(Table 3)</ref>. <ref type="bibr">6</ref> We did not compute the fidelity of decision trees and BRL as they are trivially rule-based models.  <ref type="figure">Figure 6</ref>: Time required to train models and to extract the explanations. Our model compares favorably with the competitors, with the exception of Decision Trees. BRL is by one to three order of magnitude slower than our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work contributes to a safer adoption of some of the most powerful AI technologies, allowing deep neural networks to have a greater impact on society by making them explainable-by-design, thanks to an entropy-based approach that yields FOL-based explanations. Moreover, as the proposed approach provides logic explanations for how a model arrives at a decision, it can be effectively used to reverse engineer algorithms, processes, to find vulnerabilities, or to improve system design powered by deep learning models. From a scientific perspective, formal knowledge distillation from state-of-the-art networks may enable scientific discoveries or falsification of existing theories. However, the extraction of a FOL explanation requires symbolic input and output spaces. In some contexts, such as computer vision, the use of concept-based approaches may require additional annotations and attribute labels to get a consistent symbolic layer of concepts. Recent works on automatic concept extraction may alleviate the related costs, leading to more costeffective concept annotations <ref type="bibr" target="#b18">(Ghorbani et al. 2019;</ref><ref type="bibr" target="#b26">Kazhdan et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Software</head><p>In order to make the proposed approach accessible to the whole community, we released Anonymous (Anonymous 2021), a Python package 7 with an extensive documentation on methods and unit tests. The Python code and the scripts used for the experiments, including parameter values and documentation, is freely available under Apache 2.0 Public License from a GitHub repository.</p><p>The code library is designed with intuitive APIs requiring only a few lines of code to train and get explanations from the neural network as shown in the following code snippet 1.</p><p>1 import torch_explain as te 2 from torch_explain.logic import test_explanation 3 from torch_explain.logic.nn import explain_class 4 5 # XOR problem with additional features 6 x0 = torch.zeros( <ref type="formula" target="#formula_0">(4, 100)</ref> Listing 1: Example on how to use the APIs to implement the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Description</head><p>Will we recover from ICU? (MIMIC-II). The Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II, <ref type="bibr">(Saeed et al. 2011;</ref><ref type="bibr" target="#b20">Goldberger et al. 2000)</ref>) is a publicaccess intensive care unit (ICU) database consisting of 32,536 subjects (with 40,426 ICU admissions) admitted to different ICUs. The dataset contains detailed descriptions of a variety of clinical data classes: general, physiological, results of clinical laboratory tests, records of medications, fluid balance, and text reports of imaging studies (e.g. xray, CT, MRI, etc). In our experiments, we removed nonanonymous information, text-based features, time series inputs, and observations with missing data. We discretize continuous features into one-hot encoded categories. After such preprocessing step, we obtained an input space C composed of k = 90 key features. The task consists in identifying recovering or dying patients after ICU admission.</p><p>What kind of democracy are we living in? (V-Dem).</p><p>Varieties of Democracy (V-Dem, <ref type="bibr" target="#b48">(Pemstein et al. 2018;</ref><ref type="bibr">Coppedge et al. 2021)</ref>) is a dataset containing a collection of indicators of latent regime characteristics over 202 countries from 1789 to 2020. The database include k 1 = 483 lowlevel indicators (e.g. media bias, party ban, high-court independence, etc.), k 2 = 82 mid-level indices (e.g. freedom of expression, freedom of association, equality before the law, etc), and 5 high-level indices of democracy principles (i.e. electoral, liberal, participatory, deliberative, and egalitarian).</p><p>In the experiments a binary classification problem is considered to identify electoral democracies from non-electoral democracies. We indicate with C 1 and C 2 the spaces associated to the activations of the aforementioned two levels of concepts. Two classifiers f 1 and f 2 are trained to learn the map C 1 ? C 2 ? Y . Explanations are given for classifier f 2 in terms of concepts c 2 ? C 2 .</p><p>What does parity mean? (MNIST Even/Odd). The Modified National Institute of Standards and Technology database (MNIST, (LeCun 1998)) contains a large collection of images representing handwritten digits. The input space X ? R 28?28 is composed of 28x28 pixel images while the concept space C with k = 10 is represented by the label indicator for digits from 0 to 9. The task we consider here is slightly different from the common digit-classification. Assuming Y ? {0, 1} 2 , we are interested in determining if a digit is either odd or even, and explaining the assignment to one of these classes in terms of the digit labels (concepts in C). The mapping X ? C is provided by a ResNet10 classifier g <ref type="bibr" target="#b24">(He et al. 2016</ref>) trained from scratch. while the classifier f is used to learn both the final mapping and the explanation as a function C ? Y .</p><p>What kind of bird is that? (CUB). The Caltech-UCSD Birds-200-2011 dataset (CUB, <ref type="bibr">(Wah et al. 2011)</ref>) is a finegrained classification dataset. It includes 11,788 images rep- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental details</head><p>Batch gradient-descent and the Adam optimizer with decoupled weight decay <ref type="bibr" target="#b34">(Loshchilov and Hutter 2017)</ref> and learning rate set to 10 ?2 are used for the optimization of all neural models' parameters (Entropy-based Network and ? Network). An early stopping strategy is also applied: the model with the highest accuracy on the validation set is saved and restored before evaluating the test set.</p><p>With regard to the Entropy-based Network, Tab. 4 reports the hyperparameters employed to train the network in all experiments. All Entropy-based Networks feature ReLU activations and linear fully-connected layers (except for the first layer which is the Entropy Layer). A grid search crossvalidation strategy has been employed on the validation set to select hyperparameter values. The objective was to maximize at the same time both model and explanation accuracy. ? represents the trade-off parameter in Eq. 8 while ? is the temperature of Eq. 2.  Concerning the ? network in all experiments one network per class has been trained. They are composed of two hidden layer of 10 and 5 hidden neurons respectively. As indicated in the original paper, an l 1 weight regularization has been applied to all layers of the network. As in this work, <ref type="bibr">8</ref> A certain attribute is set as present only if it is also present in at least 50% of the images of the same class. Furthermore we only considered attributes present in at least 10 classes after this refinement. the contribute in the overall loss of the l 1 regularization is weighted by an hyperparameter ? = 10 ?4 . The maximum number of non-zero input weight (fan-in) is set to 3 in in MIMIC and V-Dem while for MNIST and CUB200 it is set to 4. In Ciravegna et al. <ref type="bibr" target="#b8">(Ciravegna et al. 2020a)</ref>, ? networks were devised to provide explanations of existing models; in this paper, however, we have shown how they can directly solve classification problems.</p><p>Decision Trees have been limited in their maximum height in all experiments to maintain the complexity of the rules at a comparable level w.r.t the other methods. More precisely the maximum height has been set to 5 in all binary classification tasks (MIMIC-II, V-Dem, MNIST) while we allowed a maximum height of 30 in the CUB experiment due to the high number of classes to predict (200).</p><p>BRL algorithms requires to first run the FP-growth algorithm <ref type="bibr" target="#b22">(Han, Pei, and Yin 2000)</ref> (an enhanced version of Apriori) to mine a first set of frequent rules. The hyperparameter used by FP-growth are: the minimum support in percentage of training samples for each rule (set to 10%), the minimum and the maximum number of features considered by each rule (respectively set to 1 and 2). Regarding the Bayesian selection of the best rules, the number of Markov chain Monte Carlo used for inference is set to 3, while 50000 iterations maximum are allowed. At last the expected length and width of the extracted rule list is set respectively to 3 and 1. These are the default values indicated in the BRL repository. Due to the computational complexity and the high number of hyperparameters, they have not been cross validated.</p><p>The code for the experiments is implemented in Python 3, relying upon open-source libraries <ref type="bibr" target="#b46">(Paszke et al. 2019;</ref><ref type="bibr" target="#b47">Pedregosa et al. 2011)</ref>. All the experiments have been run on the same machine: Intel ? Core? i7-10750H 6-Core Processor at 2.60 GHz equipped with 16 GiB RAM and NVIDIA GeForce RTX 2060 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Explainability metrics details</head><p>In the following, we report in tabular form the results concerning the explanation accuracy and the complexity of the rules <ref type="figure">(Fig. 5</ref>) and the extraction time ( <ref type="figure">Fig. 6</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Entropy and L1</head><p>This section presents additional experiments on a toy dataset showing (1) the advantage of using the entropy loss function in Eq. 7 w.r.t. the L1 loss (used by e.g. the ? network) and (2) the advantage in terms of explainability provided by the Entropy Layer w.r.t. a standard linear layer. Three neural models are compared:</p><p>? model A: a standard multi-layer perceptron using linear (fully connected) layers, using an L1 regularization in the loss function. ? model B: a multi-layer perceptron using the Entropy Layer as first layer and an L1 regularization in the loss function. ? model C: a multi-layer perceptron using the Entropy Layer as first layer and the entropy loss regularization (Eq. 7) in the loss function.</p><p>The dataset used for this experiment is shown in Table 8. The training set is composed of four Boolean features {x 1 , x 2 , x 3 , x 4 } and four Boolean target categories {y, ?y, z, ?z}. The target category y is the XOR of the features x 1 and x 2 , i.e. ?x : y = 1 ? x 1 ? x 2 . The target category z is the OR of the features x 3 and x 4 , i.e. ?x : z = 1 ? x 3 ? x 4 . The categories ?y and ?z are the complement of the categories y and z, respectively.</p><p>The neural networks used for these experiment are multilayer perceptrons with 2 hidden layers of 20 and 10 units with ReLu activation. Batch gradient-descent and the Adam optimizer with decoupled weight decay <ref type="bibr" target="#b34">(Loshchilov and Hutter 2017)</ref> and learning rate set to 10 ?4 are used for all neural models. The number of epochs is set to 18000 to ensure complete convergence (overfitting the training set), and the regularization coefficient is set to ? = 10 ?4 for both L1 and entropy losses. For the neural model using the entropy loss (model C), the temperature is set to ? = 0.3.</p><p>Once the networks have been trained, we extracted from each model a summary of the concept relevance for each target category. <ref type="figure">Figure 7</ref> shows the values of the weight matrix of the first hidden layer of the model A (not using the Entropy Layer). The L1 loss pruned some connections between input features and hidden neurons (h i ). However, it is not evident the relevance of each feature for each target class. <ref type="figure">Figure 8</ref> shows the matrix? of the concept scores provided by the Entropy layer of the model B (trained with the L1 loss). It can be observed how the matrix? offers a much better overview of the relevance of each feature for each target category. However, the L1 loss was not sufficient to make the model learn that e.g. the category y does not depend from the feature x 3 (recall that ?x : y = 1 ? x 1 ?x 2 ), x 1 x 2 x 3 x 4 y ?y z ?z 0 0 0 0 0 1 0 1 0</p><formula xml:id="formula_13">1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0</formula><p>as the score? y x3 ? 0.99. Finally, <ref type="figure" target="#fig_3">Figure 9</ref> shows the matrix ? of the concept scores provided by the Entropy layer of the model C (trained with the entropy loss in Eq. 7). The entropy loss was quite effective helping the neural network identify the most relevant input features for each task, discarding redundant input concepts. <ref type="figure">Figure 10</ref> shows the trained Entropy Network (model C) on the toy dataset as well as the resulting logic explanations inferred from the training set matching ground-truth logic formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Impact of hyperparameters on the logic explanations</head><p>We measured the impact of the hyperparameters on the quality of the explanations on the vDem dataset. <ref type="table" target="#tab_14">Table 9</ref> summarizes the average model accuracy, explanation accuracy, and explanation complexity obtained by running a 5-fold cross validation on the grid where ? ? [10 ?3 , 10 ?6 ] and ? ? <ref type="bibr">[4,</ref><ref type="bibr">6]</ref> for the vDem dataset. The reported variance is the standard error of the mean. All the other parameters of the network (number of layers, number of epochs, etc...) have been set as reported in the main experimental section. Overall, the variation (within the defined grid) of the hyperparameters produced some minor effects on the quality of the explanations in terms of accuracy and complexity.   Figure 10: Visualization of the Entropy-based Network (model C) trained on the toy dataset after 18000 epochs. The first layer of the network is the Entropy Layer. The gray-scale intensity of the input neurons represents the concept scores?. The darker the input neuron, the higher the score (? j i ? 1), the lighter the input neuron, the lower the score (? j i ? 0). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Logic formulas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BRL</head><p>black footed albatross ? (has back pattern striped ? has belly color black ? has bill shape hooked seabird ? ?has belly color white) ? (has back pattern striped ? ...</p><p>? Net black footed albatross ? (has bill shape hooked seabird ? ?has breast color white ? ?has size small 5 9 in ? ?has wing color grey) ? (has bill shape hooked seabird ? ...    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas</head><p>Black footed Albatross ? has bill length about the same as head ? has wing pattern solid ? ?has upper tail color grey ? ?has belly color white ? ?has wing shape roundedwings ? ?has bill color black Laysan Albatross ? has crown color white ? has wing pattern solid ? ?has under tail color white Sooty Albatross ? has upper tail color grey ? has size medium 9 16 in ? has bill color black ? ?has belly color white Groove billed Ani ? has breast color black ? has leg color black ? ?has bill shape allpurpose ? ?has bill length about the same as head ? ?has wing shape roundedwings Crested Auklet ? has nape color black ? ?has eye color black ? ?has belly color white Least Auklet ? has breast color black ? has breast color white ? ?has nape color white ? ?has size small 5 9 in Parakeet Auklet ? has size medium 9 16 in ? has primary color white ? has leg color grey Rhinoceros Auklet ? has size medium 9 16 in ? has leg color buff Brewer Blackbird ? has breast color black ? has wing shape roundedwings ? ?has bill length about the same as head ? ?has shape perchinglike Red winged Blackbird ? has belly color black ? has wing pattern multicolored ? ?has wing color white Rusty Blackbird ? has back color brown ? has belly color black ? ?has crown color brown Yellow headed Blackbird ? has forehead color yellow ? has primary color black Bobolink ? has belly color black ? ?has upper tail color grey ? ?has upper tail color black Indigo Bunting ? has forehead color blue ? has back pattern solid ? has wing pattern multicolored Lazuli Bunting ? has leg color black ? has bill color grey ? ?has under tail color white Painted Bunting ? has nape color blue ? has leg color grey ? has bill color grey Cardinal ? has forehead color red ? has wing shape roundedwings ? has wing pattern multicolored ? ?has nape color black Spotted Catbird ? has leg color grey ? ?has breast pattern solid ? ?has breast color black ? ?has belly color white ? ?has crown color black Gray Catbird ? has under tail color grey ? has belly color grey ? has crown color black ? ?has primary color black Yellow breasted Chat ? has primary color yellow ? has bill color black ? ?has back color grey ? ?has throat color grey ? ?has throat color black ? ?has nape color yellow ? ?has belly color white Eastern Towhee ? has breast color black ? has nape color black ? ?has belly color black ? ?has tail pattern multicolored ? ?has primary color white</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chuck will Widow</head><p>? has under tail color brown ? has belly color buff ? has crown color brown ? ?has bill shape allpurpose Brandt Cormorant ? has bill shape hooked seabird ? has breast color black ? ?has wing shape roundedwings Red faced Cormorant ? has belly color black ? ?has size small 5 9 in ? ?has bill color black Pelagic Cormorant ? has size medium 9 16 in ? has leg color black ? ?has bill shape hooked seabird ? ?has tail shape notched tail ? ?has belly color white ? ?has wing shape roundedwings Bronzed Cowbird ? has belly color black ? has shape perchinglike ? has wing pattern solid ? ?has bill shape allpurpose ? ?has underparts color yellow ? ?has bill length about the same as head</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shiny Cowbird</head><p>? has belly color black ? has shape perchinglike ? has wing pattern solid ? ?has wing shape roundedwings Brown Creeper ? has nape color buff ? ?has shape perchinglike Fish Crow ? has bill shape allpurpose ? has bill length about the same as head ? ?has under tail color grey ? ?has belly color white ? ?has shape perchinglike Black billed Cuckoo ? has leg color grey ? has crown color brown Mangrove Cuckoo ? has belly color buff ? has leg color grey ? ?has back color black Yellow billed Cuckoo ? has shape perchinglike ? has tail pattern solid ? has primary color white ? ?has bill color black Gray crowned Rosy Finch ? has under tail color black ? has crown color grey ? has wing pattern striped Purple Finch ? has forehead color red ? ?has wing shape roundedwings ? ?has belly pattern solid ? ?has bill color black Northern Flicker ? has belly color black ? has leg color grey ? ?has nape color black Acadian Flycatcher ? has breast color white ? has leg color black ? ?has under tail color white ? ?has bill color black Great Crested Flycatcher ? has tail pattern solid ? has primary color grey ? has wing pattern striped Least Flycatcher ? has tail shape notched tail ? has tail pattern solid ? ?has bill shape cone ? ?has underparts color black ? ?has back color brown ? ?has breast color yellow ? ?has throat color black ? ?has bill length about the same as head ? ?has primary color buff ? ?has leg color black Olive sided Flycatcher ? has belly color grey ? has belly color white Scissor tailed Flycatcher ? has forehead color white ? ?has under tail color white ? ?has shape perchinglike ? ?has tail pattern solid Vermilion Flycatcher ? has upper tail color black ? has wing shape roundedwings ? has leg color black ? ?has belly color white ? ?has back pattern striped ? ?has primary color black Yellow bellied Flycatcher ? has tail shape notched tail ? has wing pattern multicolored ? ?has wing shape roundedwings ? ?has primary color yellow ? ?has bill color black Frigatebird ? has underparts color black ? has underparts color white ? has head pattern plain ? ?has shape perchinglike Northern Fulmar ? has under tail color white ? has crown color white ? ?has upper tail color white Gadwall ? has under tail color black ? has size medium 9 16 in ? has bill color black ? ?has leg color grey ? ?has crown color black American Goldfinch ? has under tail color black ? has back pattern solid ? has wing pattern multicolored ? ?has belly color white ? ?has bill color black European Goldfinch ? has leg color buff ? has wing pattern multicolored ? ?has tail pattern solid Boat tailed Grackle ? has throat color black ? has wing shape roundedwings ? ?has bill length shorter than head ? ?has size small 5 9 in ? ?has size medium 9 16 in Eared Grebe ? has belly color grey ? has primary color black ? ?has tail pattern solid Horned Grebe ? has primary color black ? has bill color black ? ?has nape color black ? ?has size small 5 9 in ? ?has belly pattern solid Pied billed Grebe ? has under tail color brown ? has size medium 9 16 in Western Grebe ? has size medium 9 16 in ? has primary color white ? ?has throat color black ? ?has under tail color white Blue Grosbeak ? has under tail color black ? has bill color grey ? ?has tail pattern solid ? ?has crown color black Evening Grosbeak ? has nape color brown ? has tail pattern solid ? ?has nape color buff ? ?has back pattern solid Pine Grosbeak ? has under tail color grey ? has leg color black ? has wing pattern multicolored ? ?has back pattern solid </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas</head><p>Rose breasted Grosbeak ? has bill shape cone ? has wing shape roundedwings ? has primary color white ? ?has nape color buff Pigeon Guillemot ? has underparts color black ? has size medium 9 16 in ? ?has leg color black California Gull ? has under tail color black ? has wing pattern solid ? ?has back pattern solid Glaucous winged Gull ? has upper tail color white ? has under tail color grey Heermann Gull ? has nape color grey ? has crown color white ? ?has shape perchinglike Herring Gull ? has size medium 9 16 in ? has primary color grey ? has wing pattern solid ? ?has upper tail color grey ? ?has upper tail color black Ivory Gull ? has leg color black ? has bill color grey ? ?has shape perchinglike Ring billed Gull ? has under tail color white ? has bill color black ? ?has head pattern plain ? ?has forehead color black ? ?has shape perchinglike ? ?has wing pattern striped Slaty backed Gull ? has upperparts color black ? has forehead color white ? has size medium 9 16 in ? ?has upper tail color grey Western Gull ? has crown color white ? ?has shape perchinglike ? ?has back pattern solid Anna Hummingbird ? has size very small 3 5 in ? ?has breast color white ? ?has wing shape roundedwings Ruby throated Hummingbird ? has belly color white ? has leg color black ? ?has wing shape roundedwings ? ?has size small 5 9 in ? ?has back pattern solid Rufous Hummingbird ? has size very small 3 5 in ? has wing pattern multicolored ? ?has shape perchinglike Green Violetear ? has nape color blue ? ?has bill length shorter than head Long tailed Jaeger ? (has wing color grey ? has under tail color black ? ?has back color grey ? ?has bill length shorter than head) ? (has under tail color black ? ?has wing color black ? ?has back color grey ? ?has size small 5 9 in ? ?has primary color brown)</p><p>Pomarine Jaeger ? has size medium 9 16 in ? has leg color black ? has crown color black ? ?has breast color black ? ?has under tail color white Blue Jay ? has forehead color blue ? has under tail color black ? has leg color black Florida Jay ? has breast pattern multicolored ? has back pattern multicolored Green Jay ? has under tail color yellow ? has leg color grey ? ?has nape color grey ? ?has crown color black Dark eyed Junco ? has underparts color white ? has throat color grey Tropical Kingbird ? has forehead color grey ? has primary color yellow ? has bill color black ? ?has back pattern multicolored Gray Kingbird ? has forehead color grey ? ?has bill length shorter than head ? ?has under tail color black Belted Kingfisher ? has breast pattern multicolored ? has wing shape roundedwings ? ?has back color black ? ?has bill length shorter than head Green Kingfisher ? has throat color white ? has tail pattern solid ? ?has breast color white ? ?has belly pattern solid Pied Kingfisher ? has breast color black ? has wing shape roundedwings ? has leg color black ? ?has wing pattern solid ? ?has wing pattern striped ? ?has wing pattern multicolored Ringed Kingfisher ? has size small 5 9 in ? has primary color grey ? ?has nape color grey ? ?has wing shape roundedwings ? ?has wing pattern multicolored White breasted Kingfisher ? has crown color brown ? has wing pattern multicolored Red legged Kittiwake ? has wing color white ? has bill length shorter than head ? ?has tail shape notched tail ? ?has forehead color blue ? ?has forehead color grey ? ?has nape color brown ? ?has back pattern striped ? ?has tail pattern multicolored ? ?has crown color black Horned Puffin ? has throat color black ? has eye color black ? ?has breast color black ? ?has wing shape roundedwings ? ?has shape perchinglike Common Raven ? has wing shape roundedwings ? has size medium 9 16 in ? ?has bill shape hooked seabird ? ?has shape perchinglike White necked Raven ? has nape color white ? ?has throat color white ? ?has size small 5 9 in American Redstart ? has underparts color black ? has wing pattern multicolored ? ?has belly color black ? ?has leg color grey ? ?has crown color grey Geococcyx ? has nape color brown ? has leg color grey ? ?has primary color white Loggerhead Shrike ? has nape color grey ? has tail pattern multicolored ? ?has tail shape notched tail ? ?has breast color yellow ? ?has bill length about the same as head </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas</head><p>Great Grey Shrike ? has forehead color grey ? has wing shape roundedwings ? has wing pattern multicolored ? ?has upperparts color white ? ?has back pattern multicolored Baird Sparrow ? has back color brown ? has tail shape notched tail ? ?has wing shape roundedwings Black throated Sparrow ? has forehead color grey ? has belly color white ? ?has throat color white ? ?has wing pattern multicolored Brewer Sparrow ? has wing shape roundedwings ? has back pattern striped ? has primary color buff ? ?has under tail color brown ? ?has size very small 3 5 in ? ?has primary color brown ? ?has crown color black Chipping Sparrow ? has nape color grey ? has back pattern striped ? ?has upper tail color buff Clay colored Sparrow ? has throat color white ? has forehead color brown ? has primary color buff ? ?has nape color brown House Sparrow ? has back pattern striped ? has bill color black ? ?has breast color yellow ? ?has forehead color black ? ?has leg color grey Field Sparrow ? has belly color buff ? has wing pattern striped ? ?has leg color buff Fox Sparrow ? has breast pattern striped ? ?has back pattern solid ? ?has wing pattern striped Grasshopper Sparrow ? has under tail color buff ? has belly color buff ? has leg color buff Harris Sparrow ? has nape color buff ? has primary color white Henslow Sparrow ? has breast color black ? has leg color buff ? ?has primary color yellow Le Conte Sparrow ? has wing shape roundedwings ? has back pattern striped ? ?has back color brown ? ?has bill color black Lincoln Sparrow ? has size very small 3 5 in ? has wing pattern striped ? ?has belly pattern solid ? ?has crown color white Nelson Sharp tailed Sparrow ? has back pattern striped ? ?has nape color buff ? ?has size small 5 9 in ? ?has crown color black Savannah Sparrow ? has back pattern striped ? ?has back color buff ? ?has under tail color black ? ?has belly pattern solid ? ?has leg color black Seaside Sparrow ? has shape perchinglike ? has tail pattern solid ? ?has belly pattern solid ? ?has bill color black ? ?has wing pattern solid Song Sparrow ? has nape color buff ? has back pattern striped ? ?has forehead color black ? ?has primary color buff Tree Sparrow ? has tail shape notched tail ? has belly color white ? has back pattern striped ? ?has back color buff ? ?has under tail color brown Vesper Sparrow ? has breast color white ? has back pattern striped ? has leg color buff ? ?has under tail color buff White crowned Sparrow ? has forehead color black ? has nape color grey ? ?has leg color buff White throated Sparrow ? has forehead color yellow ? has primary color brown Cape Glossy Starling ? has nape color blue ? has wing pattern solid Bank Swallow ? has bill shape cone ? has breast color white ? has bill color black ? ?has forehead color blue ? ?has forehead color black ? ?has wing pattern solid Barn Swallow ? has back pattern solid ? has primary color black ? has bill color black ? ?has belly color black ? ?has shape perchinglike ? ?has leg color black Cliff Swallow ? has belly color buff ? has tail pattern solid ? ?has back pattern solid Tree Swallow ? has primary color white ? has bill color black ? ?has upperparts color black ? ?has size medium 9 16 in ? ?has primary color brown </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas</head><p>Scarlet Tanager ? has upperparts color black ? has forehead color red ? ?has bill length about the same as head ? ?has under tail color white Summer Tanager ? has tail shape notched tail ? has leg color grey ? ?has throat color white ? ?has forehead color black ? ?has primary color grey Artic Tern ? has head pattern capped ? has nape color black ? ?has bill shape dagger ? ?has upper tail color black Black Tern ? has belly color black ? ?has under tail color black ? ?has wing shape roundedwings Caspian Tern ? has head pattern capped ? has size medium 9 16 in ? has wing pattern solid ? ?has nape color black Common Tern ? has wing color grey ? has back color white ? ?has forehead color white Elegant Tern ? has forehead color white ? has size medium 9 16 in ? ?has head pattern plain Forsters Tern ? has head pattern capped ? has nape color black ? has bill color black ? ?has upper tail color black Least Tern ? has forehead color white ? has crown color black ? has wing pattern solid Green tailed Towhee ? has wing shape roundedwings ? has bill color grey ? ?has throat color yellow ? ?has nape color blue ? ?has nape color brown Brown Thrasher ? has nape color brown ? ?has forehead color brown ? ?has belly color yellow ? ?has leg color grey ? ?has wing pattern striped Sage Thrasher ? has wing pattern striped ? ?has eye color black Black capped Vireo ? has nape color black ? has size very small 3 5 in ? has leg color grey Blue headed Vireo ? has primary color grey ? has leg color grey ? has wing pattern striped Philadelphia Vireo ? has nape color grey ? has size very small 3 5 in ? has bill color grey Red eyed Vireo ? has upperparts color buff ? has forehead color grey Warbling Vireo ? has nape color grey ? has size very small 3 5 in ? has primary color buff ? ?has under tail color buff White eyed Vireo ? has tail shape notched tail ? has tail pattern multicolored ? ?has upperparts color black Yellow throated Vireo ? has nape color yellow ? has belly color white Bay breasted Warbler ? has wing shape roundedwings ? has back pattern striped ? has leg color grey Black and white Warbler ? has size very small 3 5 in ? has wing pattern striped ? ?has primary color buff Black throated Blue Warbler ? has primary color black ? has wing pattern multicolored ? ?has breast color yellow ? ?has under tail color black Blue winged Warbler ? has back color grey ? ?has nape color grey ? ?has back pattern striped ? ?has primary color grey ? ?has crown color black</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Canada Warbler</head><p>? has under tail color grey ? has nape color grey ? has belly color yellow ? ?has back pattern multicolored ? ?has primary color grey Cape May Warbler ? has back pattern striped ? has bill color black ? ?has belly color white ? ?has belly pattern solid Cerulean Warbler ? has nape color blue ? has size very small 3 5 in ? has shape perchinglike Chestnut sided Warbler ? has underparts color white ? has upper tail color grey ? ?has wing color white ? ?has bill length about the same as head ? ?has belly color grey ? ?has back pattern solid ? ?has primary color yellow Golden winged Warbler ? has forehead color yellow ? has wing pattern multicolored ? ?has primary color yellow Hooded Warbler ? has forehead color yellow ? has primary color yellow ? has leg color buff ? has crown color black Kentucky Warbler ? has size small 5 9 in ? has primary color yellow ? has leg color buff ? has crown color black Magnolia Warbler ? has forehead color grey ? has primary color black Mourning Warbler ? has forehead color grey ? has leg color buff </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas</head><p>Myrtle Warbler ? has leg color black ? has wing pattern striped ? ?has tail pattern solid Nashville Warbler ? has under tail color yellow ? has back pattern multicolored ? ?has crown color black Orange crowned Warbler ? has bill shape allpurpose ? has tail pattern multicolored ? ?has breast color yellow ? ?has primary color white ? ?has crown color grey ? ?has crown color black Palm Warbler ? has primary color yellow ? has wing pattern striped ? ?has belly color yellow Pine Warbler ? has forehead color yellow ? has under tail color grey ? has bill color grey Prairie Warbler ? has size very small 3 5 in ? has crown color yellow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prothonotary Warbler</head><p>? has under tail color black ? has tail pattern solid ? has leg color grey ? ?has upperparts color black Swainson Warbler ? has tail shape notched tail ? has bill length about the same as head ? ?has nape color grey ? ?has primary color black Tennessee Warbler ? has upper tail color grey ? has primary color yellow ? ?has breast color yellow Wilson Warbler ? has under tail color yellow ? has crown color black ? ?has leg color grey Worm eating Warbler ? has crown color yellow ? ?has primary color yellow ? ?has bill color black Yellow Warbler ? has under tail color yellow ? has wing pattern striped Northern Waterthrush ? has size small 5 9 in ? has tail pattern solid ? has leg color buff ? ?has breast color yellow ? ?has primary color grey Louisiana Waterthrush ? has breast pattern striped ? has wing pattern solid ? ?has nape color brown Bohemian Waxwing ? has upper tail color grey ? has wing pattern multicolored ? ?has under tail color grey Cedar Waxwing ? has nape color buff ? has wing pattern multicolored American Three toed Woodpecker ? has under tail color white ? has tail pattern solid ? has leg color grey Pileated Woodpecker ? has nape color white ? has leg color grey ? ?has primary color white Red bellied Woodpecker ? has forehead color red ? has wing pattern striped Red cockaded Woodpecker ? has head pattern capped ? has belly color black Red headed Woodpecker ? has forehead color red ? has back pattern solid ? has wing pattern multicolored Downy Woodpecker ? has under tail color white ? has back pattern multicolored ? ?has wing pattern multicolored Bewick Wren ? has under tail color brown ? has under tail color black Cactus Wren ? has nape color white ? ?has belly color white ? ?has back pattern solid Carolina Wren ? has breast color buff ? has bill color grey ? ?has leg color grey House Wren ? has breast color buff ? ?has forehead color black ? ?has wing shape roundedwings ? ?has leg color black ? ?has wing pattern solid Marsh Wren ? has nape color buff ? has belly color white ? has belly color buff Rock Wren ? has under tail color buff ? has size very small 3 5 in ? ?has crown color brown Winter Wren ? has breast pattern solid ? has breast color buff ? ?has belly pattern solid Common Yellowthroat ? has forehead color black ? has under tail color yellow ? ?has crown color black</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>For each class i, the network leverages one "head" of the entropy-based linear layer (green) as first layer, and it provides: the class membership predictions f i and the truth table T i (Eq. 6) to distill FOL explanations (yellow, top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ResNet10 classifier g (He et al. 2016) trained from scratch. while the classifier f learn both the final mapping and the explanation as a function C ? Y .What kind of bird is that? (CUB). The Caltech-UCSD Birds-200-2011 dataset (CUB,(Wah et al. 2011)) is a finegrained classification dataset. It includes 11,788 images representing r = 200 (Y = {0, 1} 200 ) different bird species. 312 binary attributes (concepts in C) describe visual characteristics (color, pattern, shape) of particular parts(beak,  wings, tail, etc.)  for each bird image. The mapping X ? C is performed with a ResNet10 model g trained from scratch while the classifier f learns the final function C ? Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Concept scores? for the Entropy layer (first layer of the network) trained by minimizing the entropy loss in Eq. 7 (model C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%). Left group, the compared white-box models. Right group, two black box models. We indicate in bold the best model in each group, with a star the best model overall. MIMIC-II 79.05 ? 1.35 77.53 ? 1.45 76.40 ? 1.22 77.19 ? 1.64 77.81 ? 2.45 78.88 ? 2.25 V-Dem 94.51 ? 0.48 85.61 ? 0.57 91.23 ? 0.75 89.77 ? 2.07 94.53 ? 1.17 93.08 ? 0.44 MNIST 99.81 ? 0.02 99.75 ? 0.01 99.80 ? 0.02 99.79 ? 0.03 99.72 ? 0.03 99.96 ? 0.01 CUB 92.95 ? 0.20 81.62 ? 1.17 90.79 ? 0.34 91.92 ? 0.27 93.10 ? 0.51 91.88 ? 0.36</figDesc><table><row><cell>Entropy net</cell><cell>Tree</cell><cell>BRL</cell><cell>? net</cell><cell>Neural Network Random Forest</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Out-of-distribution fidelity (%) II 79.11 ? 2.02 51.63 ? 6.67 V-Dem 90.90 ? 1.23 69.67 ? 10.43 MNIST 99.63 ? 0.00 65.68 ? 5.05 CUB 99.86 ? 0.01 77.34 ? 0.52</figDesc><table><row><cell>Entropy net</cell><cell>? net</cell></row><row><cell>MIMIC-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: Consistency (%)</cell><cell></cell></row><row><cell></cell><cell cols="2">Entropy net Tree</cell><cell>BRL</cell><cell>? net</cell></row><row><cell>MIMIC-II</cell><cell>28.75</cell><cell cols="2">40.49 30.48</cell><cell>27.62</cell></row><row><cell>V-Dem</cell><cell>46.25</cell><cell>72.00</cell><cell>73.33</cell><cell>38.00</cell></row><row><cell>MNIST</cell><cell>100.00</cell><cell>41.67</cell><cell cols="2">100.00 96.00</cell></row><row><cell>CUB</cell><cell>35.52</cell><cell>21.47</cell><cell>42.86</cell><cell>41.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>resenting r = 200 (Y = {0, 1} 200 ) different bird species. 312 binary attributes describe visual characteristics (color, pattern, shape) of particular parts(beak, wings, tail, etc.)   for each bird image. Attribute annotations, however, is quite noisy. For this reason, attributes are denoised by considering class-level annotations<ref type="bibr" target="#b30">(Koh et al. 2020)</ref> 8  . In the end, a total of 108 attributes (i.e. concepts with binary activations belonging to C) have been retained. The mapping X ? C from images to attribute concepts is performed again with a ResNet10 model g trained from scratch while the classifier f learns the final function C ? Y .</figDesc><table><row><cell cols="2">All datasets employed are freely available (only MIMIC-</cell></row><row><cell cols="2">II requires an online registration) and can be downloaded</cell></row><row><cell cols="2">from the following links:</cell></row><row><cell cols="2">MIMIC: https://archive.physionet.org/mimic2.</cell></row><row><cell>V-Dem:</cell><cell>https://www.v-dem.net/en/data/data/v-dem-</cell></row><row><cell>dataset-v111.</cell><cell></cell></row><row><cell cols="2">MNIST: http://yann.lecun.com/exdb/mnist.</cell></row><row><cell cols="2">CUB: http://www.vision.caltech.edu/visipedia/CUB-200-</cell></row><row><cell>2011.html.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Hyper parameters of entropy-based networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Explanation's accuracy (%) computed as the average of the F1 scores computed for each class. II 66.93 ? 2.14 69.15 ? 2.24 70.59 ? 2.17 49.51 ? 3.91 V-Dem 89.88 ? 0.50 85.45 ? 0.58 91.21 ? 0.75 67.08 ? 9.68 MNIST 99.62 ? 0.00 99.74 ? 0.01 99.79 ? 0.02 65.64 ? 5.05 CUB 95.24 ? 0.05 89.36 ? 0.92 96.02 ? 0.17 76.10 ? 0.56</figDesc><table><row><cell>Entropy net</cell><cell>Tree</cell><cell>BRL</cell><cell>? net</cell></row><row><cell>MIMIC-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Complexity computed as the number of terms in each minterm of the DNF rules.</figDesc><table><row><cell>Entropy net</cell><cell>Tree</cell><cell>BRL</cell><cell>? net</cell></row><row><cell cols="2">MIMIC-II 3.50 ? 0.88 66.60 ? 1.45 V-Dem 3.10 ? 0.51 30.20 ? 1.20</cell><cell>57.70 ? 35.58 145.70 ? 57.93</cell><cell>20.6 ? 5.36 5.40 ? 2.70</cell></row></table><note>MNIST 50.00 ? 0.00 47.50 ? 0.72 1352.30 ? 292.62 96.90 ? 10.01 CUB 3.74 ? 0.03 45.92 ? 1.16 8.87 ? 0.11 15.96 ? 0.96</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Rule extraction time (s) calculated as the time required to train the models and to extract the corresponding rules. II 23.08 ? 3.53 0.06 ? 0.00 440.24 ? 9.75 36.68 ? 6.10 V-Dem 59.90 ? 31.18 0.49 ? 0.07 22843.21 ? 194.49 103.78 ? 1.65 MNIST 138.32 ? 0.63 2.72 ? 0.02 2594.79 ? 177.34 385.57 ? 17.81 CUB 171.87 ? 1.95 8.10 ? 0.65 264678.29 ? 56521.40 3707.29 ? 1006.54</figDesc><table><row><cell>Entropy net</cell><cell>Tree</cell><cell>BRL</cell><cell>? net</cell></row><row><cell>MIMIC-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Toy dataset used to compare the Entropy Layer to a standard linear layer and the entropy loss to the L1 loss function.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Impact of the hyperparameters on the quality of the model and the explanations.</figDesc><table><row><cell>(?, ?)</cell><cell cols="3">Model accuracy Explanation accuracy Explanation complexity</cell></row><row><cell cols="2">(4, 10 ?6 ) 94.15 ? 0.53 (4, 10 ?5 ) 94.56 ? 0.28 (4, 10 ?4 ) 94.53 ? 0.42 (4, 10 ?3 ) 94.62 ? 0.45 (5, 10 ?6 ) 94.53 ? 0.65 (5, 10 ?5 ) 94.86 ? 0.23 (5, 10 ?4 ) 94.68 ? 0.48 (5, 10 ?3 ) 94.53 ? 0.58 (6, 10 ?6 ) 94.83 ? 0.49 (6, 10 ?5 ) 94.56 ? 0.51 (6, 10 ?4 ) 94.48 ? 0.73 (6, 10 ?3 ) 94.05 ? 0.58</cell><cell>89.68 ? 0.36 88.20 ? 1.05 87.73 ? 1.25 89.37 ? 0.73 89.91 ? 0.59 88.04 ? 1.29 88.20 ? 0.80 89.92 ? 0.39 85.53 ? 1.98 87.90 ? 0.98 87.22 ? 1.80 90.15 ? 0.68</cell><cell>1.70 ? 0.20 2.50 ? 0.76 3.10 ? 0.83 2.00 ? 0.16 2.50 ? 0.50 1.60 ? 0.19 3.25 ? 0.92 2.00 ? 0.22 3.30 ? 0.64 2.70 ? 0.58 1.80 ? 0.25 2.12 ? 0.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10</head><label>10</label><figDesc>reports a selection of the rule extracted by each method in all the experiments presented in the main paper. For all methods we report only the explanations of the first class for the first split of the Cross-validation. At last, for the Entropy-based method only, Tables 11, 12, 13, 14 resume the explanations of all classes in all experiments. For simplicity, in the following tables we dropped the universal quantifier for all formulas.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">L1  *  concept scores?</cell><cell></cell><cell>1.0</cell></row><row><cell>y</cell><cell>1.0000</cell><cell>0.9948</cell><cell>0.9890</cell><cell>0.9514</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell>?y</cell><cell>0.9991</cell><cell>1.0000</cell><cell>0.8371</cell><cell>0.8569</cell><cell>0.6</cell></row><row><cell>z</cell><cell>0.5491</cell><cell>0.5875</cell><cell>1.0000</cell><cell>0.9994</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell>?z</cell><cell>0.4210</cell><cell>0.3855</cell><cell>1.0000</cell><cell>0.9994</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row><row><cell></cell><cell>x1</cell><cell>x2</cell><cell>x3</cell><cell>x4</cell><cell></cell></row><row><cell cols="6">Figure 8: Concept scores? for the Entropy layer (first layer</cell></row><row><cell cols="6">of the network) trained by minimizing the L1 loss instead of</cell></row><row><cell cols="4">the entropy loss of Eq. 7 (model B).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Comparison of the formulas obtained in the first run of each experiment for all methods. We dropped the arguments in the logic predicates as well as the universal quantifiers for simplicity. Only the formula explaining the first class has been reported. Ellipses are used to truncate overly long formulas. II Entropy recover ? ?liver flg ? ?stroke flg ? ?mal flg DTree recover ? (age high &lt;0.5 ? mal flg &lt;0.5 ? stroke flg &lt;0.5 ? age normal &lt;0.5 ? iv day 1 normal &lt;0.5) ? (age high &lt;0.5 ? mal flg &lt;0.5 ? stroke flg &lt;0.5 ? age normal &lt;0.5 ? iv day 1 normal &gt;0.5) ? (age high &lt;0.5 ? ...BRL recover ? (age low ? sofa first low ? ?(mal flg ? ?weight first normal)) ? (age high ? ?service num normal ? ?(age low ? sofa first low) ? ?(chf flg ? ?day icu intime num high) ? ?(mal flg ? ?weight first normal) ? ?(stroke flg ? ... (v2xel frefair &lt;0.5 ? v2xdl delib &lt;0.5 ? v2x frassoc thick &lt;0.5) ? (v2xel frefair &lt;0.5 ? v2xdl delib &lt;0.5 ? v2x frassoc thick &gt;0.5 ? v2x freexp altinf &lt;0.5 ? v2xeg eqprotec &lt;0.5) ? ... BRL non electoral democracy ? ?v2x cspart ? ?v2x elecoff ? ?v2x frassoc thick ? ?v2x freexp altinf ? ?v2xcl rol ? (?v2x mpi ??v2xelfrefair) (one &lt;0.54 ? nine &lt;1.97?10 ? 5 ? three &lt;0.00 ? five &lt;0.09 ? seven &lt;0.20) ? (one &lt;0.54 ? nine &lt;1.97?10 ? 5 ? three &lt;0.00 ? five &gt;0.09 ? two &gt;0.97) ? (one &lt;0.54 ? nine &lt;1.97?10 ? 5 ? three &gt;0.00 ? two &lt;0.99 ? eight &gt;1.00) ? ... (four ? nine ? six ? three ? zero ? ?eight ? ?one ? ?seven) ? (four ? nine ? six ? two ? zero ? ?eight ? ?one ? ?seven) ? (eight ? six ? ?four ? ?nine ? ?seven ? ?three ? ?two) ? (eight ? six ?... CUB Entropy black footed albatross ? has bill length about the same as head ? has wing pattern solid ? ?has upper tail color grey ? ?has belly color white ? ?has wing shape roundedwings ? ?has bill color black DTree black footed albatross ? (has back pattern striped &lt;0.46 ? has back color buff &lt;0.69 ? has upper tail color white &lt;0.59 ? has under tail color buff &lt;0.82 ? has shape perchinglike &lt;0.66 ? ...</figDesc><table><row><cell>Dataset</cell><cell>Method Formulas</cell></row><row><cell>MIMIC-V-Dem MNIST</cell><cell>Entropy DTree non electoral democracy ? ? Net non electoral democracy ? ?v2xel frefair ? ?v2x elecoff ? ?v2x cspart ? ?v2xeg eqaccess ? ?v2xeg eqdr non electoral democracy ? ?v2xeg eqaccess ? (v2x egal ? ?v2x frassoc thick) ? (v2xeg eqdr ? ?v2x egal) ? (v2xel frefair ? ?v2x frassoc thick) ? (?v2x cspart ? ?v2x suffr) ? (?v2x frassoc thick ? ?v2x suffr) ? ... Entropy even ? (zero ? ?one ? ?two ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (two ? ?zero ? ?one ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (four ? ?zero ? ... DTree even ? BRL even ? (two ? ?one ? ?seven ? ?three ? ?(seven ? ?two)) ? (four ? ?five ? ?nine ? ?seven ? ?three ? ?(seven ? ?two) ? ?(two ? ?one)) ? (four ? ?five ? ?seven ? ?three ? ?(four ? ?nine) ? ?(seven ? ... ? Net even ?</cell></row></table><note>? Net recover ? (iv day 1 normal ? ?age high ? ?hour icu intime normal ? ?sofa first normal) ? (mal flg ? ?age high ? ?hour icu intime normal ? ?sofa first normal) ? ...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Formulas extracted from the MIMIC-II dataset. Formulas recover ? ?liver flg ? ?stroke flg ? ?mal flg non recover ? mal flg ? (age HIGH ? ?iv day 1 NORMAL)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Formulas extracted from the V-Dem dataset. Formulas non electoral democracy ? ?v2xel frefair ? ?v2x elecoff ? ?v2x cspart ? ?v2xeg eqaccess ? ?v2xeg eqdr electoral democracy ? v2xel frefair ? v2x elecoff ? v2x cspart</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Formulas extracted from the MNIST dataset.</figDesc><table><row><cell>Formulas</cell></row><row><cell>even ? (zero ? ?one ? ?two ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (two ? ?zero ? ?one ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (four ? ?zero ? ?one ? ?two ? ?three ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (six ? ?zero ? ?one ? ?two ? ?three ? ?four ? ?five ? ?seven ? ?eight ? ?nine) ? (eight ? ?zero ? ?one ? ?two ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?nine)</cell></row></table><note>odd ? (one ? ?zero ? ?two ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (three ? ?zero ? ?one ? ?two ? ?four ? ?five ? ?six ? ?seven ? ?eight ? ?nine) ? (five ? ?zero ? ?one ? ?two ? ?three ? ?four ? ?six ? ?seven ? ?eight ? ?nine) ? (seven ? ?zero ? ?one ? ?two ? ?three ? ?four ? ?five ? ?six ? ?eight ? ?nine) ? (nine ? ?zero ? ?one ? ?two ? ?three ? ?four ? ?five ? ?six ? ?seven ? ?eight)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Formulas extracted from the CUB dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell>continued from previous page</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell>continued from previous page</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 14</head><label>14</label><figDesc>continued from previous pageFormulasHorned Lark ? has primary color buff ? ?has under tail color black ? ?has wing shape roundedwings ? ?has back pattern solid ? ?has wing pattern striped Pacific Loon ? has size medium 9 16 in ? has leg color grey ? ?has belly pattern solid Mallard ? has breast color brown ? has wing pattern multicolored ? ?has forehead color yellow Western Meadowlark ? has belly color yellow ? has leg color buff ? has bill color grey Hooded Merganser ? has tail pattern solid ? has bill color black ? ?has eye color black Red breasted Merganser ? has forehead color black ? ?has belly color white ? ?has belly pattern solid ? ?has wing pattern striped Mockingbird ? has forehead color grey ? has wing shape roundedwings ? ?has upperparts color grey Nighthawk ? has breast color brown ? ?has underparts color brown ? ?has belly pattern solid White breasted Nuthatch ? has back pattern multicolored ? has tail pattern multicolored ? ?has nape color white ? ?has belly color yellow Baltimore Oriole ? has breast color yellow ? has under tail color yellow ? ?has wing shape roundedwings Hooded Oriole ? has breast color yellow ? has back pattern solid ? has tail pattern solid ? has wing pattern multicolored color yellow ? ?has belly color white Scott Oriole ? has under tail color yellow ? has wing pattern multicolored ? ?has back pattern solid ? ?has back pattern multicolored Ovenbird ? has breast color black ? has throat color white ? has wing pattern solid ? ?has leg color grey Brown Pelican ? has wing pattern solid ? ?has breast pattern solid ? ?has back pattern solid ? ?has primary color yellow Western Wood Pewee ? has tail pattern solid ? has bill color black ? has crown color grey ? ?has under tail color grey ? ?has wing shape roundedwings Sayornis ? has upper tail color brown ? has head pattern plain Whip poor Will ? has wing shape roundedwings ? ?has belly color white ? ?has shape perchinglike ? ?has leg color black ? ?has crown color brown ? ?has wing pattern solid</figDesc><table><row><cell>Clark Nutcracker ?has primary color yellow ?</cell><cell cols="2">has forehead color grey</cell><cell></cell><cell>?</cell><cell>has leg color grey</cell><cell>?</cell><cell>has wing pattern multicolored</cell><cell>?</cell></row><row><cell cols="4">Orchard Oriole ?has under tail White Pelican ?has size medium 9 16 in ? ?has shape perchinglike ? has leg color grey ? ? has crown color white ?</cell><cell cols="2">has crown color black ?has head pattern plain</cell><cell>?</cell><cell>?</cell><cell>has wing pattern multicolored ?has under tail color black</cell><cell>? ?</cell></row><row><cell cols="2">American Pipit ?has primary color brown ? has nape color buff</cell><cell>?</cell><cell cols="4">has wing shape roundedwings</cell><cell>?</cell><cell>?has belly pattern solid</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 continued</head><label>14</label><figDesc></figDesc><table /><note>from previous page</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 14</head><label>14</label><figDesc>continued from previous page</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 14</head><label>14</label><figDesc>continued from previous page</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://scikit-learn.org/stable/modules/tree.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The height of the tree is limited to obtain rules of comparable lengths. See supplementary materials A.3.5  In the case of MIMIC-II and V-Dem, this is a standard neural network with the same hyperparameters of the entropy-based one, but with a linear layer as first layer. In the case of MNIST and CUB, it is the g model directly predicting the final classes g : X ? Y .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Ben Day, Dobrik Georgiev, Dmitry Kazhdan, and Alberto Tonda for useful feedback and suggestions.</p><p>This work was partially supported by TAILOR and GODS-21 European Union's Horizon 2020 research and innovation programmes under GA No 952215 and 848077.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interpretable machine learning in healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teredesai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM international conference on bioinformatics, computational biology, and health informatics</title>
		<meeting>the 2018 ACM international conference on bioinformatics, computational biology, and health informatics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="559" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07538</idno>
		<title level="m">Towards robust interpretability with self-explaining neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Certifiably Optimal Rule Lists for Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Larus-Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01701.Anonymous.2021.Anonymous</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>D?az-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gil-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<title level="m">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Concept whitening for interpretable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="772" to="782" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human-driven FOL explanations of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ciravegna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melacci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence {IJCAI-PRICAI-20}, 2234-2240. International Joint Conferences on Artificial Intelligence Organization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Constraint-Based Approach to Learning and Explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ciravegna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3658" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast effective rule induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Coppedge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gerring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Knutsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Teorell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gastaldi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2021. V-Dem Codebook v11</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The magical number 4 in short-term memory: A reconsideration of mental storage capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="114" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05862</idno>
		<title level="m">Learning symbolic physics with graph networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Opportunities and challenges in explainable artificial intelligence (xai): A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11371</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Considerations for evaluation and generalization in interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable and interpretable models in computer vision and machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">General data protection regulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929.EUGDPR.2017.GDPR</idno>
		<idno>arXiv:1902.03129</idno>
	</analytic>
	<monogr>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards automatic concept-based explanations</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The EU General Data Protection Regulation (GDPR): European regulation that has a global impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goddard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Market Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="703" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<title level="m">PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Local rule-based explanations of black box decision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM sigmod record</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized additive models: some applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="371" to="386" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very simple classification rules perform well on most commonly used datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="90" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13233</idno>
		<title level="m">Now You See Me (CME): Concept-based Model Extraction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tcav: Relative concept importance testing with linear concept activation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The (un) reliability of saliency methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D?hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Concept bottleneck models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5338" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Error bars: the meaning of error bars is often misinterpreted, as is the statistical significance of their overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krzywinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="921" to="923" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1350" to="1371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intelligible models for classification and regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="150" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07874</idno>
		<title level="m">A unified approach to interpreting model predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Changing concepts of working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Bays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">347</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Survey of multi-objective optimization methods for engineering. Structural and multidisciplinary optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Marler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="369" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Minimization of Boolean functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mccluskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1417" to="1444" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The calculus of equivalent statements (third paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mccoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A statistical model for the analysis of ordinal level dependent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mckelvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zavoina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Sociology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Introduction to mathematical logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mendelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The magical number seven, plus or minus two: Some limits on our capacity for processing information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="81" to="97" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Perceptrons: An introduction to computational geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Papert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>Lulu. com</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The V-Dem measurement model: latent variable analysis for cross-national and cross-temporal expert-coded data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pemstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Marquardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzelgov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krusell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Miri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">V-Dem Working Paper</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The problem of simplifying truth functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Quine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American mathematical monthly</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="521" to="531" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">C4. 5: programs for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A philosophical treatise of universal induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rathmanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1076" to="1136" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should i trust you?</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Modelagnostic interpretability of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05386</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Anchors: Highprecision model-agnostic explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning decision lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="246" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11251</idno>
		<title level="m">Interpretable machine learning: Fundamental principles and 10 grand challenges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Reisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): a public-access intensive care unit database</title>
	</analytic>
	<monogr>
		<title level="j">Critical care medicine</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">952</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rule extraction from neural networks via decision tree induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tsukimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN&apos;01. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1870" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Distilling free-form natural laws from experimental data. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rational choice and the structure of the environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Models of man; social and rational</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Rational decision making in business organizations. The American economic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="493" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor as a formal basis for a physical theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Soklakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Physics Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="135" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05908</idno>
		<title level="m">Understanding boolean function learnability on deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint/>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9240</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">DeepRED -Rule Extraction from Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zilke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loza Menc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Janssen</surname></persName>
		</author>
		<idno>978-3-319-46307-0</idno>
	</analytic>
	<monogr>
		<title level="m">Discovery Science</title>
		<editor>Calders, T.</editor>
		<editor>Ceci, M.</editor>
		<editor>and Malerba, D.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="473" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
