<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Music Source Separation in the Waveform Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
							<email>defossez@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">INRIA ?cole Normale Sup?rieure PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
							<email>usunier@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
							<email>francis.bach@ens.fr</email>
							<affiliation key="aff3">
								<orgName type="department">INRIA ?cole Normale Sup?rieure</orgName>
								<orgName type="institution">PSL Research University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Music Source Separation in the Waveform Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song. Such components include voice, bass, drums and any other accompaniments. Contrarily to many audio synthesis tasks where the best performances are achieved by models that directly generate the waveform, the state-of-the-art in source separation for music is to compute masks on the magnitude spectrum. In this paper, we compare two waveform domain architectures. We first adapt Conv-Tasnet, initially developed for speech source separation, to the task of music source separation. While Conv-Tasnet beats many existing spectrogram-domain methods, it suffers from significant artifacts, as shown by human evaluations. We propose instead Demucs, a novel waveform-to-waveform model, with a U-Net structure and bidirectional LSTM. Experiments on the MusDB dataset show that, with proper data augmentation, Demucs beats all existing state-of-the-art architectures, including Conv-Tasnet, with 6.3 SDR on average, (and up to 6.8 with 150 extra training songs, even surpassing the IRM oracle for the bass source). Using recent development in model quantization, Demucs can be compressed down to 120MB without any loss of accuracy. We also provide human evaluations, showing that Demucs benefit from a large advantage in terms of the naturalness of the audio. However, it suffers from some bleeding, especially between the vocals and other source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cherry first noticed the "cocktail party effect" <ref type="bibr" target="#b0">[Cherry, 1953]</ref>: how the human brain is able to separate a single conversation out of a surrounding noise from a room full of people chatting. Bregman later tried to understand how the brain was able to analyse a complex auditory signal and segment it into higher level streams. His framework for auditory scene analysis <ref type="bibr" target="#b1">[Bregman, 1990]</ref> spawned its computational counterpart, trying to reproduce or model accomplishments of the brains with algorithmic means [Wang and Brown, 2006], in particular regarding source separation capabilities.</p><p>When producing music, recordings of individual instruments called stems are arranged together and mastered into the final song. The goal of source separation is to recover those individual stems from the mixed signal. Unlike the cocktail party problem, there is not a single source of interest to Preprint version.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Mel-spectrogram for a 0.8 seconds extract of the bass source from the track "Stich Up" of the MusDB test. From left to right: ground truth, Conv-Tasnet estimate and Demucs estimate. We observe that Conv-Tasnet missed one note entirely. differentiate from an unrelated background noise, but instead a wide variety of tones and timbres playing in a coordinated way. In the SiSec Mus evaluation campaign for music separation <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref>, those individual stems were grouped into 4 broad categories: (1) drums, (2) bass, (3) other, (4) vocals. Given a music track which is a mixture of these four sources, also called the mix, the goal is to generate four waveforms that correspond to each of the original sources. We consider here the case of supervised source separation, where the training data contain music tracks (i.e., mixtures), together with the ground truth waveform for each of the sources.</p><p>State-of-the-art approaches in music source separation still operate on the spectrograms generated by the short-time Fourier transform (STFT). They produce a mask on the magnitude spectrums for each frame and each source, and the output audio is generated by running an inverse STFT on the masked spectrograms reusing the input mixture phase <ref type="bibr" target="#b21">[Takahashi and Mitsufuji, 2017</ref><ref type="bibr" target="#b4">, Takahashi et al., 2018</ref><ref type="bibr" target="#b5">, Takahashi and Mitsufuji, 2020</ref>. Several architectures trained end-to-end to directly synthesize the waveforms have been proposed <ref type="bibr" target="#b6">[Llu?s et al., 2018</ref><ref type="bibr" target="#b7">, Jansson et al., 2017</ref>, but their performances are far below the state-of-the-art: in the last SiSec Mus evaluation campaign <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref>, the best model that directly predicts waveforms achieves an average signal-to-noise ratio (SDR) over all four sources of 3.2, against 5.3 for the best approach that predicts spectrograms masks (also see <ref type="table" target="#tab_0">Table 1</ref> in Section 6). An upper bound on the performance of all methods relying on masking spectrograms is given by the SDR obtained when using a mask computed using the ground truth sources spectrograms, for instance the Ideal Ratio Mask (IRM) or the Ideal Binary Mask (IBM) oracles. For speech source separation, <ref type="bibr" target="#b8">Luo and Mesgarani [2019]</ref> proposed Conv-Tasnet, a model that reuses the masking approach of spectrogram methods but learns the masks jointly with a convolutional front-end, operating directly in the waveform domain for both the inputs and outputs. Conv-Tasnet surpasses both the IRM and IBM oracles.</p><p>Our first contribution is to adapt the Conv-Tasnet architecture, originally designed for monophonic speech separation and audio sampled at 8 kHz, to the task of stereophonic music source separation for audio sampled at 44.1 kHz. While Conv-Tasnet separates with a high accuracy the different sources, we observed artifacts when listening to the generated audio: a constant broadband noise, hollow instruments attacks or even missing parts. They are especially noticeable on the drums and bass sources and we give one example on <ref type="figure">Figure 1</ref>, and measure it with human evaluation in Section 6.</p><p>To overcome the limitations of Conv-Tasnet, we introduce Demucs, a novel architecture for music source separation. Similarly to Conv-Tasnet, Demucs is a deep learning model that directly operates on the raw input waveform and generates a waveform for each source. Demucs is inspired by models for music synthesis rather than masking approaches. It is a U-net architecture with a convolutional encoder and a decoder based on wide transposed convolutions with large strides inspired by recent work on music synthesis <ref type="bibr" target="#b9">[D?fossez et al., 2018]</ref>. The other critical features of the approach are a bidirectional LSTM between the encoder and the decoder, increasing the number of channels exponentially with depth, gated linear units as activation function <ref type="bibr" target="#b10">[Dauphin et al., 2017]</ref> which also allow for masking, and a new initialization scheme.</p><p>We present experiments on the MusDB <ref type="bibr" target="#b11">[Rafii et al., 2017]</ref> benchmark. While Conv-Tasnet outperforms several existing spectrogram-domain methods, it does suffer from large audio artifacts as measured by human evaluations. On the other hand, with proper augmentation, the Demucs architecture surpasses all existing spectrogram or waveform domain architectures in terms of SDR <ref type="bibr" target="#b12">[Vincent et al., 2006]</ref>, with 6.3 points of SDR without extra training data (against 6.0 for the best existing method D3Net), and up to 6.8 with extra training data. In fact, for the bass source, Demucs is the first model to surpass the IRM oracle, with 7.6 SDR (against 7.1 for the IRM). We confirm the usefulness of pitch/tempo shift augmentation, as first noted by <ref type="bibr" target="#b13">Cohen-Hadria et al. [2019]</ref>, with a gain of 0.4 points of SDR, in particular for a model with a large number of parameters like Demucs, while it can be detrimental to Conv-TasNet.</p><p>We discuss in more detail the related work in the next Section. We then describe the original Conv-Tasnet model of <ref type="bibr" target="#b14">Luo and Mesgarani [2018]</ref> and its adaptation to music source separation. Our Demucs architecture is detailed in Section 4. We present the experimental protocol in Section 5, and the experimental results compared to the state-of-the-art in Section 6. Finally, we describe the results of the human evaluation and the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A first category of methods for supervised music source separation work on time-frequency representations. They predict a power spectrogram for each source and reuse the phase from the input mixture to synthesise individual waveforms. Traditional methods have mostly focused on blind (unsupervised) source separation. Non-negative matrix factorization techniques <ref type="bibr" target="#b15">[Smaragdis et al., 2014]</ref> model the power spectrum as a weighted sum of a learnt spectral dictionary, whose elements are grouped into individual sources. Independent component analysis <ref type="bibr" target="#b16">[Hyv?rinen et al., 2004]</ref> relies on independence assumptions and multiple microphones to separate the sources. Learning a soft/binary mask over power spectrograms has been done using either HMM-based prediction <ref type="bibr" target="#b17">[Roweis, 2001]</ref> or segmentation techniques <ref type="bibr" target="#b18">[Bach and Jordan, 2005]</ref>.</p><p>With the development of deep learning, fully supervised methods have gained momentum. Initial work was performed on speech source separation <ref type="bibr" target="#b19">[Grais et al., 2014]</ref>, followed by works on music using simple fully connected networks over few spectrogram frames <ref type="bibr" target="#b20">[Uhlich et al., 2015]</ref>, LSTMs <ref type="bibr" target="#b21">[Uhlich et al., 2017]</ref>, or multi scale convolutional/recurrent networks <ref type="bibr" target="#b22">[Liu and</ref><ref type="bibr">Yang, 2018, Takahashi and</ref><ref type="bibr" target="#b21">Mitsufuji, 2017]</ref>. <ref type="bibr" target="#b23">Nugraha et al. [2016]</ref> showed that Wiener filtering is an efficient post-processing step for spectrogram-based models and it is now used by all top performing models in this category. Those methods have performed the best during the last SiSec 2018 evaluation <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref> for source separation on the MusDB <ref type="bibr" target="#b11">[Rafii et al., 2017]</ref> dataset. After the evaluation, a reproducible baseline called Open Unmix has been released by <ref type="bibr" target="#b24">St?ter et al. [2019]</ref> and matches the top submissions trained only on MusDB. MMDenseLSTM, a model proposed by <ref type="bibr" target="#b4">Takahashi et al. [2018]</ref> and trained on 807 unreleased songs currently holds the absolute record of SDR in the SiSec campaign. More recently, <ref type="bibr" target="#b5">Takahashi and Mitsufuji [2020]</ref> improved the spectrogram-domain state-of-the-art with D3Net, which uses dilated convolutions with dense connection. <ref type="bibr" target="#b25">Hennequin et al. [2020]</ref> propose another U-Net architecture for spectrogram masking, called Spleeter, that is trained on the Bean dataset <ref type="bibr" target="#b26">Pr?tet et al. [2019]</ref>, composed of short extracts from nearly 25,000 songs. While Spleeter provides strong performance and has been widely adopted by the digital music industry, it is now outperformed by more advanced spectrogram domain architectures like D3Net, even though it is trained on much more data.</p><p>More recently, models operating in the waveform domain have been developed, so far with worse performance than those operating in the spectrogram domain. A convolutional network with a U-Net structure called Wave-U-Net was used first on spectrograms <ref type="bibr" target="#b7">[Jansson et al., 2017]</ref> and then adapted to the waveform domain <ref type="bibr" target="#b27">[Stoller et al., 2018]</ref>. Wave-U-Net was submitted to the SiSec 2018 evaluation campaign with a performance inferior to that of most spectrogram domain models by a large margin. A Wavenet-inspired, although using a regression loss and not auto-regressive, was first used for speech denoising <ref type="bibr" target="#b28">[Rethage et al., 2018]</ref> and then adapted to source separation <ref type="bibr" target="#b6">[Llu?s et al., 2018]</ref>. Our model significantly outperforms Wave-U-Net.Given that the Wavenet inspired model performed worse than Wave-U-Net, we did not consider it for comparison.</p><p>In the field of monophonic speech source separation, spectrogram masking methods have enjoyed good performance <ref type="bibr" target="#b29">[Kolbaek et al., 2017</ref><ref type="bibr" target="#b30">, Isik et al., 2016</ref>. <ref type="bibr" target="#b14">Luo and Mesgarani [2018]</ref> developed Tasnet, a waveform domain methods using masking over a learnable front-end obtained from a LSTM that reached the same accuracy. Improvements were obtained by <ref type="bibr" target="#b31">Wang et al. [2018]</ref> for spectrogram methods using the unfolding of a few iterations of a phase reconstruction algorithm in the training loss. In the mean time, <ref type="bibr" target="#b8">Luo and Mesgarani [2019]</ref> refined their approach, replacing the LSTM with a superposition of dilated convolutions, which improved the SDR and definitely surpassed spectrogram based approaches, including oracles that use the ground truth sources such as the ideal ratio mask (IRM) or the ideal binary mask (IBM). We show that, while surpassing many existing spectrogram domain methods, Conv-Tasnet does not benefit much from pitch/tempo shift augmentation, and we could only reach a SDR of 5.7 with it. Besides, its audio samples suffer from significant audio artifacts as measured by human evaluations. However, it has less contamination from other sources than Demucs. <ref type="bibr" target="#b32">[Luo et al., 2020]</ref> introduced the Dual-Path RNN architecture for speech separation. <ref type="bibr" target="#b33">Nachmani et al. [2020]</ref> improved on it, with strong results on the MusDB dataset (5.8 SDR). It is unclear whether it could benefit from pitch/tempo shift augmentation. <ref type="bibr" target="#b34">Samuel et al. [2020]</ref> also adapted the Conv-Tasnet architecture to the task of music source separation, using meta-learning (i.e., the weights for each source are generated from a meta neural network), multiresolution separation and auxilary losses. Their model matches the performance of our Conv-Tasnet baseline, but would likely suffer from the same limitation. Finally, <ref type="bibr" target="#b35">Lancaster and Souvira?-Labastie [2020]</ref> trained an original Tasnet model, with 30 times more training data than in the Musdb dataset. While this model achieves strong results, it fails to be competitive when considering with models like Conv-Tasnet or Demucs, especially considering the large amount of training data used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adapting Conv-Tasnet for music source separation</head><p>We describe in this section the Conv-Tasnet architecture of <ref type="bibr" target="#b14">Luo and Mesgarani [2018]</ref> and give the details of how we adapted the architecture to fit the setting of the MusDB dataset.</p><p>Overall framework Each source s is represented by a waveform x s ? R C,T where C is the number of channels (1 for mono, 2 for stereo) and T the number of samples of the waveform. The mixture (i.e., music track) is the sum of all sources x := S s=1 x s . We aim at training a model g parameterized by ?, such that g(</p><formula xml:id="formula_0">x) = (g s (x; ?)) S s=1 , where g s (x; ?) is the predicted waveform for source s given x, that minimizes min ? x?D S s=1 L(g s (x; ?), x s )<label>(1)</label></formula><p>for some dataset D and reconstruction error L. The original Conv-Tasnet was trained using a loss called scale-invariant source-to-noise ratio (SI-SNR), similar to the SDR loss described in Section 5. We instead use a simple L1 loss between the estimated and ground truth sources. We discuss in more details regression losses in the context of our Demucs architecture in Section 4.2.</p><p>The original Conv-Tasnet architecture Conv-Tasnet <ref type="bibr" target="#b14">[Luo and Mesgarani, 2018</ref>] is composed of a learnt front-end that transforms back and forth between the input monophonic mixture waveform sampled at 8 kHz and a 128 channels over-complete representation sampled at 1 kHz using a convolution as the encoder and a transposed convolution as the decoder, both with a kernel size of 16 and stride of 8. The high dimensional representation is masked through a separation network composed of stacked residual blocks. Each block is composed of a a 1x1 convolution, a PReLU <ref type="bibr" target="#b36">[He et al., 2015a]</ref> non linearity, a layer wise normalization over all channels jointly <ref type="bibr" target="#b37">[Ba et al., 2016]</ref>, a depth-wise separable convolution <ref type="bibr" target="#b38">[Chollet, 2017</ref><ref type="bibr" target="#b39">, Howard et al., 2017</ref> with a kernel size of 3, a stride of 1 and a dilation of 2 n mod N , with n the 0-based index of the block and N an hyper-parameter, and another PReLU and normalization. The output of each block participates to the final mask estimation through a skip connection, preceded by a 1x1 convolution. The original Conv-Tasnet counted 3 ? N blocks with N = 8. The mask is obtained summing the output of all blocks and then applying ReLU. The output of the encoder is multiplied by the mask and before going through the decoder.</p><p>Conv-Tasnet for music source separation We adapted their architecture to the task of stereophonic music source separation: the original Conv-Tasnet has a receptive field of 1.5 seconds for audio sampled at 8 kHz, we take N = 10 and increased the kernel size (resp. stride) of the encoder/decoder from 16 (resp. 8) to 20 (resp. 10), leading to the same receptive field at 44.1 kHz. We observed better results using 4 ? N blocks instead of 3 ? N and 256 channels for the encoder/decoder instead of 128. With those changes, Conv-Tasnet obtained state-of-the-art performance on the MusDB dataset, surpassing all known spectrogram based methods by a large margin as shown in Section 6.</p><p>Separating entire songs The original Conv-Tasnet model was designed for short sentences of a few seconds at most. When evaluating it on an entire track, we obtained the best performance by first  splitting the input track into chunks of 8 seconds each. We believe this is because of the global layer normalization. During training, only small audio extracts are given, so that a quiet part or a loud part would be scaled back to an average volume. However, when using entire songs as input, it will most likely contain both quiet and loud parts. The normalization will not map both to the same volume, leading to a difference between training and evaluation. We did not observe any side effects when going from one chunk to the next, so we did not look into fancier overlap-add methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Demucs Architecture</head><p>The architecture we propose, which we call Demucs, is described in the next few subsections, and the reconstruction loss is discussed in Section 4.2. Demucs takes a stereo mixture as input and outputs a stereo estimate for each source (C = 2). It is an encoder/decoder architecture composed of a convolutional encoder, a bidirectional LSTM, and a convolutional decoder, with the encoder and decoder linked with skip U-Net connections. Similarly to other work in generation in both image <ref type="bibr" target="#b40">[Karras et al., 2018</ref><ref type="bibr" target="#b41">[Karras et al., , 2017</ref> and sound <ref type="bibr" target="#b9">[D?fossez et al., 2018]</ref>, we do not use batch normalization <ref type="bibr" target="#b42">[Ioffe and Szegedy, 2015]</ref> as our early experiments showed that it was detrimental to the model performance. The overall architecture is depicted in <ref type="figure" target="#fig_1">Figure 2a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convolutional auto-encoder</head><p>Encoder The encoder is composed of L := 6 stacked convolutional blocks numbered from 1 to L. Each block i is composed of a convolution with kernel size K := 8, stride S := 4, C i?1 input channels, C i output channels and ReLU activation, followed by a convolution with kernel size 1, 2C i output channels and gated linear units (GLU) as activation function <ref type="bibr" target="#b10">[Dauphin et al., 2017]</ref>. Since GLUs halve the number of channels, the final output of block i has C i output channels. A block is described in <ref type="figure" target="#fig_1">Figure 2b</ref>. Convolutions with kernel width 1 increase the depth and expressivity of the model at low computational cost. As we show in our ablation study 6.2, the usage of GLU activations after these convolutions significantly boost performance. The number of channels in the input mixture is C 0 = C = 2, while we use C 1 := 64 as the number of output channels for the first encoder block. The number of channels is then doubled at each subsequent block, i.e., C i := 2C i?1 for i = 2..L, so the final number of channels is C L = 2048. We then use a bidirectional LSTM with 2 layers and a hidden size C L . The LSTM outputs 2C L channels per time position. We use a linear layer to take that number down to C L .</p><p>Decoder The decoder is mostly the inverse of the encoder. It is composed of L blocks numbered in reverse order from L to 1. The i-th blocks starts with a convolution with stride 1 and kernel width 3 to provide context about adjacent time steps, input/output channels C i and a ReLU activation. Finally, we use a transposed convolution with kernel width 8 and stride 4, C i?1 outputs and ReLU activation. The S sources are synthesized at the final layer only, after all decoder blocks. The final layer is linear with S ? C 0 output channels, one for each source (4 stereo channels in our case), without any additional activation function. Each of these channels directly generate the corresponding waveform.</p><p>U-network structure Similarly to Wave-U-Net <ref type="bibr" target="#b7">[Jansson et al., 2017]</ref>, there are skip connections between the encoder and decoder blocks with the same index, as originally proposed in U-networks <ref type="bibr" target="#b43">[Ronneberger et al., 2015]</ref>. While the main motivation comes from empirical performances, an advantage of the skip connections is to give a direct access to the original signal, and in particular allows to directly transfers the phase of the input signal to the output, as discussed in Section 4.2. Unlike Wave-U-Net, we use transposed convolutions rather than linear interpolation followed by a convolution with a stride of 1. For the same increase in the receptive field, transposed convolutions require 4 times less operations and memory. This limits the overall number of channels that can be used before running out of memory. As we observed that a large number of channels was key to obtaining good results, we favored the use of transposed convolutions, as explained in Section 6.</p><p>Motivation: synthesis vs masking The approach we follow uses the U-Network architecture <ref type="bibr" target="#b43">[Ronneberger et al., 2015</ref><ref type="bibr" target="#b27">, Stoller et al., 2018</ref><ref type="bibr" target="#b7">, Jansson et al., 2017</ref>, and builds on transposed convolutions with large number of channels and large strides (4) inspired by the approach to the synthesis of music notes of <ref type="bibr" target="#b9">D?fossez et al. [2018]</ref>. The U-Net skip connections and the gating performed by GLUs imply that this architecture is expressive enough to represent masks on a learnt representation of the input signal, in a similar fashion to Conv-Tasnet. The Demucs approach is then more expressive than Conv-Tasnet, and its main advantages are the multi-scale representations of the input and the non-linear transformations to and from the waveform domain.</p><p>Resampling We observed improved performance when performing upsampling of the input by a factor of 2, and downsampling of the output to recover the proper sample rate. We perform this operation as part of the end-to-end training loss, with a sinc resampling filter <ref type="bibr" target="#b44">Smith and Gossett [1984]</ref>. Note that the same positive impact of resampling has been noted for speech denoising <ref type="bibr" target="#b45">[Defossez et al., 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss function</head><p>For the reconstruction loss L(g s (x; ?), x s ) in (1), we either use the average mean square error or average absolute error between waveforms: for a waveform x s containing T samples and corresponding to source s, a predicted waveformx s and denoting with a subscript t the t-th sample of a waveform, we use one of L 1 or L 2 :</p><formula xml:id="formula_1">L 1 (x s , x s ) = 1 T T t=1 |x s,t ? x s,t | L 2 (x s , x s ) = 1 T T t=1 (x s,t ? x s,t ) 2 .<label>(2)</label></formula><p>In generative models for audio, direct reconstruction losses on waveforms can pose difficulties because they are sensitive to the initial phases of the signals: two signals whose only difference is a shift in the initial phase are perceptually the same, but can have arbitrarily high L 1 or L 2 losses. It can be a problem in pure generation tasks because the initial phase of the signal is unknown, and losses on power/magnitude spectrograms are alternative that do not suffer from this lack of specification of the output. Approaches that follow this line either generate spectrograms [e.g., , or use a loss that compares power spectrograms of target/generated waveforms <ref type="bibr" target="#b9">[D?fossez et al., 2018]</ref>.</p><p>The problem of invariance to a shift of phase is not as severe in source separation as it is in unconditional generation, because the model has access to the original phase of the signal. The pase can easily be recovered from the skip connections in U-net-style architectures for separation, and is directly used as input of the inverse STFT for methods that generate masks on power spectrograms. As such, losses such as L1/L2 are totally valid for source separation. Early experiments with an additional term including the loss of <ref type="bibr" target="#b9">D?fossez et al. [2018]</ref> did not suggest that it boosts performance, so we did not pursue this direction any further. Most our experiments use L1 loss, and the ablation study presented in Section 6.2 suggests that there is no significant difference between L1 and L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weight rescaling at initialization</head><p>The initialization of deep neural networks is known to have a critical impact on the overall performances <ref type="bibr" target="#b47">[Glorot and</ref><ref type="bibr">Bengio, 2010, He et al., 2015b]</ref>, up to the point that <ref type="bibr" target="#b49">Zhang et al. [2019]</ref> showed that with a different initialization called fixup, very deep residual networks and transformers can be trained without batch normalization. While Fixup is not designed for U-Net-style skip connections, we observed that the following different initialisation scheme had great positive impact on performances compared to the standard initialization of <ref type="bibr" target="#b48">He et al. [2015b]</ref> used in U-Networks.</p><p>Considering the so-called Kaiming initialization <ref type="bibr" target="#b48">[He et al., 2015b]</ref> as a baseline, let us look at a single convolution layer for which we denote w the weights after the first initialization. We take ? := std(w)/a, where a is a reference scale, and replace w by w = w/ ? ?. Since the original weights have element-wise order of magnitude (KC in ) ?1/2 where K is the kernel width and C in the number of output channels, it means that our initialization scheme produces weights of order of magnitude (KC in ) ?1/4 , together with a non-trivial scale. Based a search over the values [0.01, 0.05, 0.1], we select a = 0.1 for all the regular and transposed convolutions, see Section 6 for more details. We experimentally observed that on a randomly initialized model applied to an audio extract, it kept the standard deviation of the features along the layers of the same order of magnitude. Without initial rescaling, the output the last layer has a magnitude 20 times smaller than the first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The shift trick</head><p>A perfect source separation model is time equivariant, i.e. shifting the input mixture by X samples will shift the output Y by the exact same amount. Thanks to its dilated convolutions with a stride of 1, the mask predictor of Conv-Tasnet is naturally time equivariant and even if the encoder/decoder is not strictly equivariant, Conv-Tasnet still verifies this property experimentally <ref type="bibr" target="#b8">[Luo and Mesgarani, 2019]</ref>. Spectrogram based method will also verify approximately this property. Shifting the input by a small amount will only reflect in the phase of the spectrogram. As the mask is computed only from the magnitude, and the input mixture phase is reused, the output will naturally be shifted by the same amount. On the other hand, we noticed that our architecture did not naturally satisfy this property. We propose a simple workaround, the shift trick 1 , where we sample S random shifts of an input mixture x and average the predictions of our model for each, after having applied the opposite shift. This technique does not require changing the training procedure or network architecture. Using S = 10, we obtained a 0.3 SDR gain, see Section 6.2 for more details. It does make evaluation of the model S times slower, however, on a V100 GPU, separating 1 minute of audio at 44.1 kHz with Demucs takes only 0.8 second. With this technique, separation of 1 minute takes 8 seconds which is still more than 7 times faster than real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup 5.1 Evaluation framework</head><p>MusDB and additional data We use the MusDB dataset <ref type="bibr" target="#b11">[Rafii et al., 2017]</ref> , which is composed of 150 songs with full supervision in stereo and sampled at 44100Hz. For each song, we have the exact waveform of the drums, bass, other and vocals parts, i.e. each of the sources. The actual song, the mixture, is the sum of those four parts. The first 84 songs form the train set, the next 16 songs form the valid set (the exact split is defined in the musdb python package) while the remaining 50 are kept for the test set. We collected raw stems for 150 tracks, i.e., individual instrument recordings used in music production software to make a song. We manually assigned each instrument to one of the sources in MusDB. We call this extra supervised data the stem set. We also report the performances of Tasnet and Demucs trained using these 150 songs in addition to the 84 from MusDB, to anaylze the effect of adding more training data.</p><p>Source separation metrics Measurements of the performance of source separation models was developed by Vincent et al. for blind source separation <ref type="bibr" target="#b12">[Vincent et al., 2006]</ref> and reused for supervised source separation in the SiSec Mus evaluation campaign <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref>. Similarly to previous work <ref type="bibr" target="#b27">[Stoller et al., 2018</ref><ref type="bibr" target="#b21">, Takahashi and Mitsufuji, 2017</ref><ref type="bibr" target="#b4">, Takahashi et al., 2018</ref>, we focus on the SDR (Signal to Distortion Ratio) which measures the log ratio between the volume of the estimated source projection onto the ground truth, and the volume of what is left out of this projection, typically contamination by other sources or artifacts. Other metrics can be defined (SIR and SAR) and we present them in the supplementary material. We used the python package museval which provide a reference implementation for the SiSec Mus 2018 evaluation campaign. As done in the SiSec Mus competition, we report the median over all tracks of the median of the metric over each track computed using the museval package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare to several state-of-the-art baselines, both in the spectral and temporal domain. Open Unmix <ref type="bibr" target="#b24">[St?ter et al., 2019]</ref>, a 3-layer BiLSTM model with encoding and decoding fully connected layers on spectrogram frames. It was released by the organizers of the SiSec 2018 <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref> to act as a strong reproducible baseline and matches the performances of the best candidates trained only on MusDB. We also selected MMDenseLSTM <ref type="bibr" target="#b4">[Takahashi et al., 2018]</ref>, a multi-band dense net with LSTMs at different scales of the encoder and decoder. This model was submitted as TAK2 and trained with 804 extra labeled songs 2 .</p><p>We also compare to the more recent options like the D3Net <ref type="bibr" target="#b5">[Takahashi and Mitsufuji, 2020]</ref> model, which used dilated convolutions with dense connections, as well as the Spleeter model <ref type="bibr" target="#b25">[Hennequin et al., 2020]</ref>, a U-Net model trained on a very diverse dataset. MMDenseLSTM, D3Net, Spleeter, and Open Unmix use Wiener filtering <ref type="bibr" target="#b23">[Nugraha et al., 2016]</ref> as a last post processing step. We provide the metrics for the Ideal Ratio Mask oracle (IRM), which computes the best possible mask using the ground truth sources and is the topline of spectrogram based method <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref>.</p><p>The only waveform based method submitted to the SiSec 2018 evaluation campaign is Wave-U-Net <ref type="bibr" target="#b27">[Stoller et al., 2018]</ref> with the identifier STL2. We also compare to the more recent variant of DPRNN by <ref type="bibr" target="#b33">Nachmani et al. [2020]</ref>, Meta-Tasnet <ref type="bibr" target="#b34">[Samuel et al., 2020]</ref>, and Tasnet trained on extra data <ref type="bibr" target="#b35">[Lancaster and Souvira?-Labastie, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training procedure</head><p>Epoch definition and augmentations One epoch over the dataset is defined as a pass over all 11-second extracts with a stride of 1 second. We use a random time shift between 0 and 1 second and keep 10 seconds of audio from there as a training example. We perform the following data augmentation <ref type="bibr" target="#b21">[Uhlich et al., 2017]</ref>, also used by Open Unmix and MMDenseLSTM: shuffling sources within one batch to generate a new mix, randomly swapping channels, random scaling by a uniform factor between 0.25 and 1.25. We additionally multiply each source by ?1 <ref type="bibr" target="#b50">[Nachmani and Wolf, 2019]</ref>. Following <ref type="bibr" target="#b13">[Cohen-Hadria et al., 2019]</ref>, we also experiment with pitch/tempo shift. 20% of the time, we randomly change the pitch by -2, -1, 0, , +1, or +2 semitones, and the tempo by a factor taken uniformly between 0.88 and 1.12, using the Soundstretch utility 3 . When trained with extra data, we do not use pitch/tempo augmentation for Conv-Tasnet as it deteriorates its performance.</p><p>All Demucs models were trained over 360 epochs. Conv-Tasnet was trained for 360 epochs when trained only on MusDB and 240 when trained with extra data and using only 2-seconds audio extracts, due to its high memory usage.</p><p>Training setup and hyper-parameters The Demucs models are trained with 8 V100 GPUs with 16GB of RAM, while the Conv-Tasnet models were trained with 16 V100 with 32GB of RAM. We use a batch size of 64, the Adam [Kingma and <ref type="bibr" target="#b51">Ba, 2015]</ref> optimizer with a learning rate of 3e-4. For the Demucs model, we experimented with an initial number of channels in <ref type="bibr">[32,</ref><ref type="bibr">48,</ref><ref type="bibr">64]</ref>, with the best performance achieved for 64 channels. Based on early experiments, we set the initial weight rescaling reference level described in Section 4.3, to 0.1. We computed confidence intervals using 3 random seeds in <ref type="table" target="#tab_0">Table 1</ref>. For the ablation study on <ref type="table" target="#tab_4">Table 4</ref>, we provide metrics for a single run.</p><p>Quantization For quantization, we use the DiffQ method <ref type="bibr" target="#b52">[D?fossez et al., 2021]</ref>. DiffQ uses additive noise on the weights as a proxy for the true quantization error. The scale of the noise depends on learnable parameters that controls the number of bits used per group of 8 weight entries. Because  <ref type="bibr" target="#b3">[St?ter et al., 2018]</ref>. The All column reports the average over all sources. Demucs metrics are averaged over 3 runs, the confidence interval is the standard deviation over ? 3. In bold are the values that are statistically state-of-the-art either with or without extra training data.  this transformation is completely differentiable, one can simply use a model size penalty (expressed in MegaBytes), with a weight of 0.0003, added to the main reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>In this section, we first provide experimental results on the MusDB dataset for Conv-Tasnet and Demucs compared with state-of-the-art baselines. We then dive into the ablation study of Demucs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with baselines</head><p>We provide a comparison with the state-of-the-art baselines on <ref type="table" target="#tab_0">Table 1</ref>. The models on the top half were trained without any extra data while the lower half used unreleased training songs. As no previous work included confidence intervals, we considered the single metric provided by for the baselines as the exact estimate of their mean performance.</p><p>Quality of the separation We first observe that Demucs and Conv-Tasnet outperform previous waveform domain methods such as Wave-U-Net. Demucs surpasses the best spectrogram domain method, D3Net <ref type="bibr" target="#b5">[Takahashi and Mitsufuji, 2020]</ref>, when averaging over all sources, by 0.3 dB of SDR. When looking into more details, we see a clear advantage for spectrogram domain methods for the other and vocals sources. Without extra training data, D3Net also beats Demucs for the drums.</p><p>However, we will see in the next section that human evaluations show that spectrogram domain methods tend to deteriorate the attack of an instrument, so that it is still possible that this high SDR hides significant degradation of the attack of percussive sounds. As no audio samples were released for D3Net, this is impossible to verify.</p><p>When trained with extra training data, we still see a small advantage overall from Demucs over D3Net (0.1 point of SDR). Again we notice that the bass and drums sources are better handled in the temporal domain, while other and vocals are better handled in the spectrogram domain. Note that for the first time for music source separation, a temporal domain model beats the IRM oracle for the bass source, by 0.5 points of SDR.</p><p>An interesting experiment would be retrain D3Net with pitch/tempo augmentation and see if it benefits from it. Interestingly, the Conv-Tasnet model achieved the same SDR whether this augmentation was used or not, and the SDR even deteriorated when trained with extra tracks. For Demucs, which comparatively has more tunable weights (see <ref type="table" target="#tab_5">Table 5</ref>), the augmentation always improves performance, most likely by preventing some overfitting. We conjecture that pitch/tempo shift lead to deteriorated audio, but reduce overfitting. For Demucs, this is a clear win, while for smaller models, it might not be as beneficial.</p><p>Human evaluations We noticed strong artifacts on the audio separated by Conv-Tasnet, especially for the drums and bass sources: static noise between 1 and 2 kHz, hollow instrument attacks or missing notes as illustrated on <ref type="figure">Figure 1</ref>. In order to confirm this observation, we organized a mean opinion score survey. We separated 8 seconds extracts from all of the 50 test set tracks for Conv-Tasnet, Demucs and Open-Unmix. We asked 38 participants to rate 20 samples each, randomly taken from one of the 3 models or the ground truth. For each one, they were required to provide 2 ratings on a scale of 1 to 5. The first one evaluated the quality and absence of artifacts (1: many artifacts and distortion, content is hardly recognizable, 5: perfect quality, no artifacts) and the second one evaluated contamination by other sources (1: contamination if frequent and loud, 5: no contamination). We show the results on Tables 2 and 3.</p><p>Note that for this experiment, with used an early version of Demucs that was trained with 100 channels, and without pitch/tempo augmentation. We had trouble differentiating between the latest version of Demucs and this early version in an informal blind test performed by the authors, so that we believe the results presented here stay mostly valid for the best performing Demucs model presented on <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We confirmed that the presence of artifacts in the output of Conv-Tasnet degrades the user experience, with a rating of 2.85?.08 against 3.22 ? .09 for Demucs. On the other hand, Conv-Tasnet samples  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation study for Demucs</head><p>We provide an ablation study of the main design decisions for Demucs in <ref type="table" target="#tab_4">Table 4</ref>. Given the cost of training a single model, we did not compute confidence intervals for each variation.</p><p>We notice strong contributions for different design decisions we made. The L1 loss seems to be more robust than the MSE. The LSTM part plays a crucial role, and replacing it by an extra layer of convolution is not sufficient to recover the best possible SDR. The initial weight rescaling described in Section 4.3 provides significant gain, with an extra 0.4 dB for the SDR.</p><p>We introduced extra convolutions in the encoder and decoder, as described in Sections 4.1. The two proved useful, improving the expressivity of the model, especially when combined with GLU activation. Using a kernel size of 3 instead of 1 in the decoder further improves performance. We conjecture that the context from adjacent time steps helps the output of the transposed convolutions to be consistent through time and reduces potential artifacts arising from using a stride of 4.</p><p>The pitch/tempo augmentation gives one of the largest gain, with almost 0.5 dB extra for the SDR, which highlights the importance of strong data augmentation when training data is limited.</p><p>We observe that using the shift trick as described in Section 4 gives a gain of almost 0.3 SDR. We did not report the validation loss as we only use the stabilization when computing the SDR over the test set. We applied the randomized stabilization to Open-Unmix and Conv-Tasnet with no gain, since, as explained in Section 4.4, both are naturally equivariant with respect to initial time shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Quantization of Demucs</head><p>One of the main drawbacks of the Demucs model when compared to other architecture is its large model size, more than 1014MB, against 42MB for Conv-TasNet. On <ref type="table" target="#tab_5">Table 5</ref>, we propose two ways to reduce this issue, either by reducing the initial number of channels (32 or 48 channels), which will improve both the model size, as well as reduce the computational complexity of the model, or using the DiffQ quantization technique <ref type="bibr" target="#b52">[D?fossez et al., 2021]</ref>.</p><p>While it is possible to use 48 channels without suffering from a large loss in SDR, it seems getting to 32 channels will lead to a decrease of 0.2 dB in performance. On the other hand, quantization will reduce the model size down to 120MB without any loss of SDR. This is still more than the 42MB of Conv-Tasnet, but close to 10x improvement over the uncompressed baseline.</p><p>Note that the 120MB is achieved if each integer is exactly coded on the right number of bits. As this is currently not implemented in Python, we rely on gzip to approximately reach this ideal size. This leads to a small overhead, so that the download size for our quantized model is 150MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We experimented with two architectures for music source separation in the waveform domain: Demucs and Conv-Tasnet. We show that with proper data augmentation, Demucs surpasses all state-of-the-art architecture in the waveform or spectrogram domain by at least 0.3 dB of SDR. However, their is no clear winner between waveform and spectrogram domain models, as the former seems to dominate for the bass and drums sources, while the latter obtain the best performance on the vocals and other sources, as measured both by objective metrics and human evaluations. We conjecture that spectrogram domain models have an advantage when the content is mostly harmonic and fast changing, while for sources without harmonicity (drums) or with strong and emphasized attack regimes (bass), waveform domain will better preserve the structure of the music source.</p><p>In terms of training and architecture, we confirm the importance of using pitch/tempo shift augmentations (although the Conv-Tasnet architecture does not seem to benefit from it), as well as using LSTM for long range dependencies, and powerful encoding and decoding layers with 1x1 convolutions and GLU activations.</p><p>When trained with extra data, Demucs surpasses for the first time the IRM oracle for the bass source. On the other hand, Demucs still suffers from larger leakage than other architectures, specifically for the vocals and other sources, which we will try to reduce in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>= 64, Cout = 4 * 2) (a) Demucs architecture with the mixture waveform as input and the four sources estimates as output. Arrows represents U-Net connections. GLU(Conv1d(Cin, 2Cin, K = 3, S = 1)) Relu(ConvTr1d(Cin, Cout, K = 8, S = 4Detailed view of the layers Decoderi on the top and Encoderi on the bottom. Arrows represent connections to other parts of the model. For convolutions, Cin (resp Cout) is the number of input channels (resp output), K the kernel size and S the stride.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Demucs complete architecture on the left, with detailed representation of the encoder and decoder layers on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Conv-Tasnet and Demucs to state-of-the-art models that operate on the waveform (Wave-U-Net, Meta-Tasnet, DPRNN-like, Tasnet trained with extra data) and on spectrograms (Open-Unmix without extra data, D3Net, MMDenseLSTM with extra data) as well as the IRM oracle on the MusDB test set. The Extra? indicates the number of extra training songs used. We report the median over all tracks of the median SDR over each track, as done in the SiSec Mus evaluation campaign</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>?.02 7.58 ?.02 7.60 ?.13 4.69 ?.04 7.29 ?.06</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test SDR in dB</cell><cell></cell></row><row><cell>Architecture</cell><cell cols="2">Wav? Extra? All</cell><cell>Drums</cell><cell>Bass</cell><cell>Other</cell><cell>Vocals</cell></row><row><cell>IRM oracle</cell><cell>N/A</cell><cell>8.22</cell><cell>8.45</cell><cell>7.12</cell><cell>7.85</cell><cell>9.43</cell></row><row><cell>Wave-U-Net</cell><cell></cell><cell>3.23</cell><cell>4.22</cell><cell>3.21</cell><cell>2.25</cell><cell>3.25</cell></row><row><cell>Open-Unmix</cell><cell></cell><cell>5.33</cell><cell>5.73</cell><cell>5.23</cell><cell>4.02</cell><cell>6.32</cell></row><row><cell>Meta-Tasnet</cell><cell></cell><cell>5.52</cell><cell>5.91</cell><cell>5.58</cell><cell>4.19</cell><cell>6.40</cell></row><row><cell>Conv-Tasnet  ?</cell><cell></cell><cell cols="5">5.73 ?.10 6.02 ?.08 6.20 ?.15 4.27 ?.03 6.43 ?.16</cell></row><row><cell>DPRNN</cell><cell></cell><cell>5.82</cell><cell>6.15</cell><cell>5.88</cell><cell>4.32</cell><cell>6.92</cell></row><row><cell>D3Net</cell><cell></cell><cell>6.01</cell><cell>7.01</cell><cell>5.25</cell><cell>4.53</cell><cell>7.24</cell></row><row><cell>Demucs  ?</cell><cell></cell><cell cols="5">6.28 ?.03 6.86 ?.05 7.01 ?.19 4.42 ?.06 6.84 ?.10</cell></row><row><cell>Spleeter</cell><cell cols="2">? 25k  *  5.91</cell><cell>6.71</cell><cell>5.51</cell><cell>4.55</cell><cell>6.86</cell></row><row><cell>TasNet</cell><cell cols="2">? 2.5k 6.01</cell><cell>7.01</cell><cell>5.25</cell><cell>4.53</cell><cell>7.24</cell></row><row><cell>MMDenseLSTM</cell><cell>804</cell><cell>6.04</cell><cell>6.81</cell><cell>5.40</cell><cell>4.80</cell><cell>7.16</cell></row><row><cell>Conv-Tasnet  ? ?</cell><cell>150</cell><cell cols="5">6.32 ?.04 7.11 ?.13 7.00 ?.05 4.44?.03 6.74 ?.06</cell></row><row><cell>D3Net</cell><cell>1.5k</cell><cell>6.68</cell><cell>7.36</cell><cell>6.20</cell><cell>5.37</cell><cell>7.80</cell></row><row><cell>Demucs  ?</cell><cell>150</cell><cell>6.79</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*: each track is only 30 seconds, ?: from current work, ? ?: trained without pitch/tempo augmentation, as it deteriorates performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean Opinion Scores (MOS) evaluating the quality and absence of artifacts of the separated audio. 38 people rated 20 samples each, randomly sample from one of the 3 models or the ground truth. There is one sample per track in the MusDB test set and each is 8 seconds long. Ratings of 5 means that the quality is perfect (no artifacts). .46 ?.07 4.56 ?.13 4.25 ?.15 4.45 ?.13 4.64 ?.13 Open-Unmix 3.03 ?.09 3.10 ?.17 2.93 ?.20 3.09 ?0.16 3.00 ?.17 Demucs 3.22 ?.09 3.77 ?.15 3.26 ?.18 3.32 ?.15 2.55 ?.20 Conv-Tasnet 2.85 ?.08 3.39 ?.14 2.29 ?.15 3.18 ?.14 2.45 ?.16</figDesc><table><row><cell></cell><cell cols="3">Quality Mean Opinion Score</cell><cell></cell></row><row><cell>Architecture All</cell><cell>Drums</cell><cell>Bass</cell><cell>Other</cell><cell>Vocals</cell></row><row><cell>Ground truth 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mean Opinion Scores (MOS) evaluating contamination by other sources. 38 people rated 20 samples each, randomly sampled from one of the 3 models or the ground truth. There is one sample per track in the MusDB test set and each is 8 seconds long. Ratings of 5 means no contamination by other sources. .59 ?.07 4.44 ?.18 4.69 ?.09 4.46 ?.13 4.81 ?.11 Open-Unmix 3.27 ?.11 3.02 ?.19 4.00 ?.20 3.11 ?.21 2.91 ?.20 Demucs 3.30 ?.10 3.08 ?.21 3.93 ?.18 3.15 ?.19 3.02 ?.20 Conv-Tasnet 3.42 ?.09 3.37 ?.17 3.73 ?.18 3.46 ?.17 3.10 ?.17</figDesc><table><row><cell></cell><cell cols="3">Contamination Mean Opinion Score</cell><cell></cell></row><row><cell>Architecture All</cell><cell>Drums</cell><cell>Bass</cell><cell>Other</cell><cell>Vocals</cell></row><row><cell>Ground truth 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for the novel elements in our architecture described in Section 4. We use only the train set from MusDB and report best L1 loss over the valid set throughout training as well the SDR on the test set for the epoch that achieved this loss.</figDesc><table><row><cell>Valid set Test set</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Impact of the initial number of channels on the model size, and on the performance. We also report the results obtained from quantizing the model with DiffQ. The bottom part consist in the base Demucs and Conv-Tasnet models presented onTable 1. Unmix or Demucs, although by a small margin, with a rating of 3.42 ? .09 against 3.30 ? .10 for Demucs and 3.27 ? .11 for Open-Unmix.Training speed We measured the time taken to process a single batch of size 16 with 10 seconds of audio at 44.1kHz (the original Wave-U-Net being only trained on 22 kHz audio, we double the time for fairness), on a single GPU, ignoring data loading and using torch.cuda.synchronize to wait on all kernels to be completed. MMDenseLSTM does not provide a reference implementation. Wave-U-Net takes 1.2 seconds per batch, Open Unmix 0.2 seconds per batch and Demucs 1.4 seconds per batch. Conv-Tasnet cannot be trained with such a large sample size, however a single iteration over 2 seconds of audio with a batch size of 4 takes 0.7 seconds.</figDesc><table><row><cell>Valid set Test set</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Fabian-Robert St?ter is to be credited for the name.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Source: https://sisec18.unmix.app/#/methods/TAK2 3 https://www.surina.net/soundtouch/soundstretch.html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Results for all metrics</head><p>Reusing the notations from <ref type="bibr" target="#b12">Vincent et al. [2006]</ref>, let us take a source j ? 1, 2, 3, 4 and introduce P sj (resp P s ) the orthogonal projection on s j (resp on Span(s 1 , . . . , s 4 )). We then take with? j the estimate of source s j</p><p>We can now define various signal to noise ratio, expressed in decibels (dB): the source to distortion ratio SDR := 10 log 10 s target 2 e interf + e artif 2 , the source to interference ratio SIR := 10 log 10 s target 2 e interf 2 and the sources to artifacts ratio SAR := 10 log 10 s target + e interf 2 e artif 2 .</p><p>As explained in the main paper, extra invariants are added when using the museval package. We refer the reader to <ref type="bibr" target="#b12">Vincent et al. [2006]</ref> for more details. In the following, we only provide the metrics for some of the baselines. We refer the reader to the original papers for the results for the other baselines.</p><p>Test SIR in dB Architecture Wav? Extra? All Drums Bass Other Vocals</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustic Society of America</title>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auditory Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Computational Auditory Scene Analysis</title>
		<editor>DeLiang Wang and Guy J. Brown</editor>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno>1805.02410</idno>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA</title>
		<editor>Naoya Takahashi, Nabarun Goswami, and Yuki Mitsufuji</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Multi-scale multi-band densenets for audio source separation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">D3net: Densely connected multidilated densenet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end music source separation: is it possible in the waveform domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Llu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno>1810.12187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sing: Symbolto-instrument neural generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usunier</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The musdb18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>St?ter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving singing voice separation using deep u-net and wave-u-net with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Cohen-Hadria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Roebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, singlechannel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Static and dynamic source separation using nonnegative factorizations: A unified view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Fevotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Blind one-microphone speech separation: A spectral learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks for single channel source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Umut Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustic, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep neural network based instrument extraction from music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising auto-encoder with recurrent skip connections and residual regression for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multichannel music separation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Arie</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>24th European</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spleeter: a fast and efficient music source separation tool with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anis</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Moussallam</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.02154</idno>
		<ptr target="https://doi.org/10.21105/joss.02154.DeezerResearch" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">2154</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Singing voice separation: A study on training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Pr?tet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimena</forename><surname>Royo-Letelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vaglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 ieee international conference on acoustics, speech and signal processing (icassp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="506" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<idno>1806.03185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TASLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno>1607.02173</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno>1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for timedomain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7164" to="7175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning extractors for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="816" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A frugal approach to music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emery</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Souvira?-Labastie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A flexible sampling-rate conversion method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Gossett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1984" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="112" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Tacotron: Towards end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>1703.10135</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09321</idno>
		<title level="m">Fixup initialization: Residual learning without normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised singing voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>1904.06590</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Differentiable model compression via pseudo quantization noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09987</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<title level="m">Test SAR in dB Architecture Wav? Extra? All</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
