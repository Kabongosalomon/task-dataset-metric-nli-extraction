<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Modena and Reggio Emilia LORENZO BARALDI</orgName>
								<orgName type="institution" key="instit2">University of Modena and Reggio Emilia GIUSEPPE SERRA</orgName>
								<orgName type="institution" key="instit3">University of Udine RITA CUCCHIARA</orgName>
								<orgName type="institution" key="instit4">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>48</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Scene understanding</term>
					<term>Natural language generation</term>
					<term>Additional Key Words and Phrases: saliency, visual saliency prediction, image captioning, deep learning ACM Reference format: Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara 2018 Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention ACM Trans Multimedia Comput Commun Appl 14, 2, Article 48 (May 2018), 21 pages</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image captioning has been recently gaining a lot of attention thanks to the impressive achievements shown by deep captioning architectures, which combine Convolutional Neural Networks to extract image representations, and Recurrent Neural Networks to generate the corresponding captions. At the same time, a significant research effort has been dedicated to the development of saliency prediction models, which can predict human eye fixations. Even though saliency information could be useful to condition an image captioning architecture, by providing an indication of what is salient and what is not, research is still struggling to incorporate these two techniques. In this work, we propose an image captioning approach in which a generative recurrent neural network can focus on different parts of the input image during the generation of the caption, by exploiting the conditioning given by a saliency prediction model on which parts of the image are salient and which are contextual. We show, through extensive quantitative and qualitative experiments on large scale datasets, that our model achieves superior performances with respect to captioning baselines with and without saliency, and to different state of the art approaches combining saliency and captioning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A core problem in computer vision and artificial intelligence is that of building a system that can replicate the human ability of understanding a visual stimuli and describing it in natural language. Indeed, this kind of system would have a great impact on society, opening up to a new progress in human-machine interaction and collaboration. Recent advancements in computer vision and machine translation, together with the availability of large datasets, have made it possible to generate natural sentences describing images. In particular, deep image captioning architectures have shown impressive results in discovering the mapping between visual descriptors and words <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59]</ref>. They combine Convolutional Neural Networks (CNNs), to extract an image representation, and Recurrent Neural Networks (RNNs), to build the corresponding sentence.</p><p>While the progress of these techniques is encouraging, the human ability in the construction and formulation of a sentence is still far from being adequately emulated in today's image captioning systems. When humans describe a scene, they look at an object before naming it in a sentence <ref type="bibr" target="#b13">[14]</ref>, and they do not focus on each region with the same intensity, as selective mechanisms attract their gaze on saliency and relevant parts of the scene <ref type="bibr" target="#b42">[43]</ref>. Also, they care about the context using peripheral vision, so that the description of an image alludes not only to the main objects in the scene, and to how they relate to each other, but also to the context in which they are placed in the image.</p><p>An intensive research effort has been carried out in the computer vision community to predict where humans look in an image. This task, called saliency prediction, has been tacked in early works by defining hand-crafted features that capture low-level cues such as color and texture or higher-level concepts such as faces, people and text <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. Recently, with the advent of deep neural networks and large annotated datasets, saliency prediction techniques have obtained impressive results generating maps that are very close to the ones computed with eye-tracking devices <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Despite the encouraging progress in image captioning and visual saliency, and their close connections, these two fields of research have remained almost separate. In fact, only few attempts have been recently presented in this direction <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b51">52]</ref>. In particular, Sugano et al. <ref type="bibr" target="#b47">[48]</ref> presented a gaze-assisted attention mechanism for image caption based on human eye fixations (i.e. the static states of gaze upon a specific location). Although this strategy confirms the importance of using eye fixations, it requires gaze information from a human operator. Therefore, it can not be applied on general visual data archives, in which this information is missing. To overcome this limit, Tavakoli et al. <ref type="bibr" target="#b51">[52]</ref> presented an image captioning method based on saliency maps, which can be automatically predicted from the input image.</p><p>In this paper we present an approach which incorporates saliency prediction to effectively enhance the quality of image description. We propose a generative Recurrent Neural Network architecture which can focus on different regions of the input image by means of an attentive mechanism. This attentive behaviour, differently from previous works <ref type="bibr" target="#b55">[56]</ref>, is conditioned by two different attention paths: the former focused on salient spatial regions, predicted by a saliency model, and the latter focused on contextual regions, which are computed as well from saliency maps. Experimental results on five public image captioning datasets (SALICON, COCO, Flickr8k, Flickr30k and PASCAL-50S), demonstrate that our solution is able to properly exploit saliency cues. Also, we show that this is done without losing the key properties of the generated captions, such as their diversity and the vocabulary size. By visualizing the states of both attentive paths, we finally show that the trained model has learned to attend to both salient and contextual regions during the generation of the caption, and that attention focuses produced by the network effectively correspond, step by step, to generated words.</p><p>To sum up, our contributions are as follows. First, we show that saliency can enhance image description, as it provides an indication of what is salient and what is context. Second, we propose a model in which the classic machine attention approach is extended to incorporate two attentive paths, one for salient regions and one for context. These two paths cooperate together during the generation of the caption, and show to generate better captions according to automatic metrics, without loss of diversity and size of the dictionary. Third, we qualitatively show that the trained model has learned to attend to both salient and contextual regions in an appropriate way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review the literature related to saliency prediction and image captioning. We also report some recent works which investigate the contribution of saliency for generating natural language descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual saliency prediction</head><p>Saliency prediction has been extensively studied by the computer vision community and, in the last few years, has achieved a considerable improvement thanks to the large spread of deep neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>. However, a very large variety of models have been proposed before the advent of deep learning and almost each of them has been inspired by the seminal work of Itti and Koch <ref type="bibr" target="#b18">[19]</ref>, in which multi-scale low-level features extracted from the input image were linearly combined and then processed by a dynamic neural network with a winner-takes-all strategy. The same idea of properly combining different low-level features was also explored by Harel et al. <ref type="bibr" target="#b14">[15]</ref> who defined Markov chains over various image maps, and treated the equilibrium distribution over map locations as an activation. In addition to the exploitation of low-level features, several saliency models have also incorporated high-level concepts such as faces, people, and text <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b60">61]</ref>. In fact, Judd et al. <ref type="bibr" target="#b22">[23]</ref> highlighted that, when humans look at images, their gazes are attracted not only by low-level cues typical of the bottom-up attention, but also by top-down image semantics. To this end, they proposed a model in which low and medium level features were effectively combined, and exploited face and people detectors to capture important high-level concepts. Nonetheless, all these techniques have failed to effectively capture the wide variety of causes that contribute to define the visual saliency on images and, with the advent of deep learning, researchers have developed data-driven architectures capable of overcoming many of the limitations of hand-crafted models.</p><p>First attempts of computing saliency maps through a neural network lacked from the absence of sufficiently large training datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref>. Vig et al. <ref type="bibr" target="#b53">[54]</ref> proposed the first deep architecture for saliency, which was composed by only three convolutional layers. Afterwards, K?mmerer et al. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> based their models on two popular convolutional networks (AlexNet <ref type="bibr" target="#b26">[27]</ref> and VGG-19 <ref type="bibr" target="#b45">[46]</ref>) obtaining adequate results, despite the network parameters were not fine-tuned on a saliency dataset. Liu et al. <ref type="bibr" target="#b34">[35]</ref> tried to overcome the absence of large scale datasets by training their model on image patches centered on fixation and non-fixation locations, thus increasing the amount of training data.</p><p>With the arrival of the SALICON dataset <ref type="bibr" target="#b20">[21]</ref>, which is still the large publicly available dataset for saliency prediction, several deep architectures have moved beyond previous approaches bringing consistent performance advances. The starting point of all these architectures is a pre-trained Convolutional Neural Network (CNN), such as VGG-16 <ref type="bibr" target="#b45">[46]</ref>, GoogleNet <ref type="bibr" target="#b49">[50]</ref> and ResNet <ref type="bibr" target="#b15">[16]</ref>, to which different saliency-oriented components are added <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, together with different training strategies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In particular, Huang et al. <ref type="bibr" target="#b17">[18]</ref> compared three standard CNNs by applying them at two different image scales. In addition, they were the first to train the network using a saliency evaluation metric as loss function. Jetley et al. <ref type="bibr" target="#b19">[20]</ref> introduced a model which formulates a saliency map as generalized Bernoulli distribution. Moreover, they trained their network by using different loss functions which pair the softmax activation function with measures designed to compute distances between probability distributions. Tavakoli et al. <ref type="bibr" target="#b50">[51]</ref> investigated inter-image similarities to estimate the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. Kruthiventi et al. <ref type="bibr" target="#b27">[28]</ref>, instead, presented an unified framework to predict both eye fixations and salient objects. Another saliency prediction model was recently presented by Pan et al. <ref type="bibr" target="#b37">[38]</ref> who, following the large dissemination of Generative Adversarial Networks, trained their model by using adversarial examples. In particular, their architecture is composed by two agents: a generator which is responsible for generating the saliency map of a given image, and a discriminator which performs a binary classification task between generated and real saliency maps. Liu et al. <ref type="bibr" target="#b33">[34]</ref>, instead, proposed a model to learn long-term spatial interactions and scene contextual modulation to infer image saliency showing promising results, also thanks to the use of the powerful ResNet-50 architecture <ref type="bibr" target="#b15">[16]</ref>.</p><p>In contrast to all these works, we presented two different deep saliency architectures. The first one, called ML-Net <ref type="bibr" target="#b7">[8]</ref>, effectively combines features coming from different levels of a CNN and applies a matrix of learned weights to the predicted saliency map thus taking into account the center bias present in human eye fixations. The second one, called SAM <ref type="bibr" target="#b8">[9]</ref>, incorporates neural attentive mechanisms which focus on the most salient regions of the input image. The core component of the proposed model is an Attentive Convolutional LSTM that iteratively refines the predicted saliency map. Moreover, to tackle the human center bias, the network is able to learn multiple Gaussian prior maps without predefined information. Since this model achieved state of the art performances, being at the top of different saliency prediction benchmarks, we use it in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image captioning</head><p>Recently, the automatic description of images and video has been addressed by computer vision researchers with recurrent neural networks which, given a vectored description of the visual content, can naturally deal with sequences of words <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b54">55]</ref>. Before deep learning models, the generation of sentences was mainly tackled by identifying visual concepts, objects and attributes which were then combined into sentences using pre-defined templates <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>. Another strategy was that of posing the image captioning as a retrieval problem, where the closest annotated sentence in the training set was transferred to a test image, or where training captions were split into parts and then reassembled to form new sentences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref>. Obviously, all these approaches limited the variety of possible outputs and could not satisfy the richness of natural language. Recent captioning models, in fact, address the generation of sentences as a machine translation problem in which a visual representation of the image coming from a convolutional network is translated in a language counterpart through a recurrent neural network.</p><p>One of the first models based on this idea is that proposed by Karpathy et al. <ref type="bibr" target="#b23">[24]</ref> in which sentence snippets are aligned to the visual regions that they describe through a multimodal embedding. After that, these correspondences are treated as training data for a multimodal recurrent neural network which learns to generate the corresponding sentences. Vinyals et al. <ref type="bibr" target="#b54">[55]</ref>, instead, developed an end-to-end model trained to maximize the likelihood of the target sentence given the input image. Xu et al. <ref type="bibr" target="#b55">[56]</ref> introduced an approach to image captioning which incorporates a form of machine attention, by which a generative LSTM can focus on different regions of the image while generating the corresponding caption. They proposed two different versions of their model: the first one, called "Soft Attention", is trained in a deterministic manner using standard backpropagation techniques, while the second one, called "Hard Attention", is trained by maximizing a variational lower bound through the reinforcement learning paradigm.</p><p>Johnson et al. <ref type="bibr" target="#b21">[22]</ref> addressed the task of dense captioning, which jointly localizes and describes in natural language salient image regions. This task consists of generalizing the object detection problem when the descriptions consist of a single word, and the image captioning task when one predicted region covers the full image. You et al. <ref type="bibr" target="#b58">[59]</ref> proposed a semantic attention model in which, given an image, a convolutional neural network extracts top-down visual features and at the same time detects visual concepts such as regions, objects and attributes. The image features and the extracted visual concepts are combined through a recurrent neural network that finally generates the image caption. Differently from previous works which aim at predicting a single caption, Krause et al. <ref type="bibr" target="#b25">[26]</ref> introduced the generation of entire paragraphs for describing images. Finally, Shetty et al. <ref type="bibr" target="#b44">[45]</ref> employed adversarial training to change the training objective of the caption generator from reproducing ground-truth captions to generating a set of captions that is indistinguishable from human generated captions.</p><p>In this paper, we are interested in demonstrating the importance of using saliency along with contextual information during the generation of image descriptions. Our solution falls in the class of neural attentive captioning architectures and, in the experimental section, we compare it against a standard attentive model built upon the Soft Attention approach presented in <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual saliency and captioning</head><p>Only a few other previous works have investigated the contribution of human eye fixations to generate image descriptions. The first work that has explored this idea was that proposed in <ref type="bibr" target="#b47">[48]</ref> which presented an extension of a neural attentive captioning architecture. In particular, the proposed model incorporates human fixation points (obtained with eye-tracking devices) instead of computed saliency maps to generate image captions. This kind of strategy mainly suffers of the need of having both eye fixation and caption annotations. Currently, only the SALICON dataset <ref type="bibr" target="#b20">[21]</ref>, being a subset of the Microsoft COCO dataset <ref type="bibr" target="#b32">[33]</ref>, is available with both human descriptions and saliency maps.</p><p>Ramanishka et al. <ref type="bibr" target="#b40">[41]</ref>, instead, introduced an encoder-decoder captioning model in which spatiotemporal heatmaps are produced for predicted captions and arbitrary query sentences without explicit attention layers. They refer to these heatmaps as saliency maps, even though they are internal representations of the network, not related with human attention. Experiments showed that the gain in performance with respect to a standard captioning attentive model is not consistent, even though the computational overhead is lower.</p><p>A different approach, presented in <ref type="bibr" target="#b51">[52]</ref>, explores if image descriptions, by humans or models, agree with saliency and if saliency can benefit image captioning. To this end, they proposed a captioning model in which image features are boosted with the corresponding saliency map by exploiting a moving sliding window and mean pooling as aggregation strategies. Comparisons with respect to a no-saliency baseline did not show significant improvements (especially on the Microsoft COCO dataset).</p><p>In this paper, we instead aim at enhancing image captions by directly incorporating saliency maps in a neural attentive captioning architecture. Differently from previous models that exploit human fixation points, we obtain a more general architecture which can be potentially trained using any image captioning dataset, and can predict captions for any input image. In our model, the machine attentive process is split in two different and unrelated paths, one for salient regions and one for context. We demonstrate through extensive experiments that the incorporation of saliency and context can enhance image captioning on different state of art datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WHAT IS HIT BY SALIENCY?</head><p>Human gazes are attracted by both low-level cues such as color, contrast and texture, and highlevel concepts such as faces and text <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. Current state of the art saliency prediction methods, thanks to the use of deep networks and large-scale datasets, are able to effectively incorporate all these factors and predict saliency maps which are very close to those obtained from human eye fixations <ref type="bibr" target="#b8">[9]</ref>. In this section we qualitatively investigate which parts of an image are actually hit or ignored by saliency models, by jointly analyzing saliency and semantic segmentation maps. This will motivate the need of using saliency predictions as an additional conditioning for captioning models.</p><p>To compute saliency maps, we employ the approach in <ref type="bibr" target="#b8">[9]</ref>, which has shown good results on popular saliency benchmarks, such as the MIT Saliency <ref type="bibr" target="#b4">[5]</ref> and the SALICON dataset <ref type="bibr" target="#b20">[21]</ref>, and which also won the LSUN Challenge in 2017. It is worthwhile to mention, anyway, that the qualitative conclusions of this section can be applied to any state of the art saliency model.</p><p>Since semantic segmentations algorithms are not always completely accurate, we perform the analysis on three semantic segmentation datasets, in which regions have been segmented by human annotators: Pascal-Context <ref type="bibr" target="#b35">[36]</ref>, Cityscapes <ref type="bibr" target="#b6">[7]</ref> and the Look into Person (LIP) <ref type="bibr" target="#b12">[13]</ref> dataset. While the first one contains natural images without a specific target, the other two are focused, respectively, on urban streets and human body parts. In particular, the Pascal-Context provides additional annotations for the Pascal VOC 2010 dataset <ref type="bibr" target="#b9">[10]</ref> which contains 10, 103 training and validation images and 9, 637 testing images. It goes beyond the original Pascal semantic segmentation task by providing annotations for the whole scene, and images are annotated by using more than 400 different labels. The Cityscapes dataset, instead, is composed by a set of video sequences recorded in street scenes from 50 different cities. It provides high quality pixel-level annotations for 5, 000 frames and coarse annotations for 20, 000 frames. The dataset is annotated with 30 streetspecific classes, such as car, road, traffic sign, etc. Finally, the LIP dataset is focused on the semantic segmentation of people and provides more than 50, 000 images annotated with 19 semantic human part labels. Images contain person instances cropped from the Microsoft COCO dataset <ref type="bibr" target="#b32">[33]</ref> and split in training, validation and testing sets with 30, 462, 10, 000 and 10, 000 images respectively. For our analyses we only consider train and validation images for the Pascal-Context and LIP datasets, and the 5, 000 pixel-level annotated frames for the Cityscapes dataset. <ref type="figure" target="#fig_0">Figure 1</ref> shows, for some sample images, the predicted saliency map and the corresponding semantic segmentation on the three datasets.</p><p>We firstly investigate which are the most and the least salient classes for each dataset. Since there are semantic classes with a low number of occurrences with respect to the total number of images, we only consider relevant semantic classes (i.e. classes with at least N occurrences). Due to the different dataset sizes, we set N to 500 for the Pascal-Context and LIP datasets, and to 200 for the Cityscapes dataset. To collect the number of times that the predicted saliency hits a semantic class, we binarize each map by thresholding the values of its pixels. A low threshold value leads to a binarized map with dilated salient regions, while an high threshold creates small salient regions around the fixation points. For this reason, we use two different threshold values to analyze the most and the least salient classes. We choose a threshold near 0 to find the least salient classes for each dataset, and a value near 255 to find instead the most salient ones. <ref type="figure">Figures 2 and 3</ref> show the most and the least salient classes in terms of the percentage of times that saliency hits a region belonging to a class. As it can be seen, there are different distributions depending on the considered dataset. For example, for the Pascal-Context, the most salient classes are animals (such as cats, dogs and birds), people and vehicles (such as airplanes and cars), while the least salient ones result to be ceiling, floor and light. As for the Cityscapes dataset, cars are absolutely the most salient class with a 70% of times in which is hit by saliency. All other classes, instead, do not reach the 40%. On the LIP dataset, the most salient classes are all human body parts in the upper body, while the least salient ones are all in the lower body. As expected, people faces are those most hit by saliency with an absolute number of occurrences near to 90%. It can be observed as a general pattern that the most important or visible objects in the scene are hit by saliency, while objects in the background, and the context itself of the image are usually ignored. This leads to the hypothesis that both salient and non salient regions are important to generate the description of an image, given that we generally want the context to be included in the caption, and that the distinction between salient regions and context, given by a saliency prediction model, can improve captioning results. We also investigate the existence of a relation between the size of an object and its saliency values. In <ref type="figure" target="#fig_2">Figure 4</ref>, we plot the joint distribution of object sizes and saliency values on the three datasets, where the size of an object is simply computed as the number of its pixels normalized by the size of the image. As it can be seen, most of the low saliency instances are small; however, high saliency values concentrate on small objects as well as on large ones. In summary, there is not always a proportionality between the size of an object and its saliency, so the importance of an object can not be assessed by simply looking at its size. In the image captioning scenario that we want to tackle, larger objects correspond to larger activations in the last layers of a convolutional architecture, while smaller objects correspond to smaller activations. Since salient and non salient regions can have comparable activations, the supervision given by a saliency prediction model on whether a pixel belongs or not to a salient region can be beneficial during the generation of the caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SALIENCY AND CONTEXT AWARE ATTENTION</head><p>Following the qualitative findings of the previous section, we develop a model in which saliency is exploited to enhance image captioning. Here, a generative recurrent neural network is conditioned, step by step, on salient spatial regions, predicted by a saliency model, and on contextual features which account for the role on non-salient regions in the generation of the caption. In the following, we describe the overall model. An overview is presented in <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>Each input image I is firstly encoded through a Fully Convolutional Network, which provides a stack of high-level features on a spatial grid {a 1 , a 2 , ..., a L }, each corresponding to a spatial location of the image. At the same time, we extract a saliency map for the input image using the model in <ref type="bibr" target="#b8">[9]</ref>, and downscale it to fit the spatial size of the convolutional features, so to obtain a spatial grid {s 1 , s 2 , ..., s L } of salient regions, where s i ? [0, 1]. Correspondingly, we also define a spatial grid of contextual regions, {z 1 , z 2 , ..., z L } where z i = 1 ? s i . Under the model, visual features at different locations will be selected or inhibited according to their saliency value.</p><p>The generation of the caption is carried out word-by-word by feeding and sampling words from an LSTM layer, which, at every timestep, is conditioned on features extracted from the input image and on the saliency map. Formally, the behaviour of the generative LSTM is driven by the following equations:</p><formula xml:id="formula_0">i t = ? (W vivt + W wi w t + W hi h t ?1 + b i ) (1) f t = ? (W v fvt + W w f w t + W hf h t ?1 + b f ) (2) o t = ? (W vovt + W wo w t + W ho h t ?1 + b o )<label>(3)</label></formula><formula xml:id="formula_1">g t = ?(W v?vt + W w? w t + W h? h t ?1 + b ? ) (4) c t = f t ? c t ?1 + i t ? g t (5) h t = o t ? ?(c t )<label>(6)</label></formula><p>where, at each timestep,v t denotes the visual features extracted from I , by considering the map of salient regions {s i } i , and those of contextual regions {z i } i . w t is the input word, and h and c are respectively the internal state and the memory cell of the LSTM. ? denotes the element-wise Hadamard product, ? is the sigmoid function, ? is the hyperbolic tangent tanh, W * are learned weight matrices and b * are learned biases vectors.</p><p>To provide the generative network with visual features, we draw inspiration from the machine attention literature <ref type="bibr" target="#b55">[56]</ref> and compute the fixed-length feature vectorv t as a linear combination of spatial features {a 1 , a 2 , ..., a L } with time-varying weights ? t i , normalized over the spatial extent via a softmax operator:v</p><formula xml:id="formula_2">t = L i=1 ? t i a i ,<label>(7)</label></formula><formula xml:id="formula_3">? t i = exp (e t i ) L k=1 exp (e t k )</formula><p>.</p><p>At each timestep the attention mechanism selects a region of the image, based on the previous LSTM state, and feeds it to the LSTM, so that the generation of a word is conditioned on that specific region, instead of being driven by the entire image. Ideally, we want weights ? t i to be aware of the saliency and contextual value of location a i , and to be conditioned on the current status of the LSTM, which can be well encoded by its internal state h t . In this way, the generative network can focus on different locations of the input image according to their belonging to a salient or contextual region, and to the current generation state. Of course, simply multiplying attention weights with saliency values would result in a loss of context, which is fundamental for caption generation. We instead split attention weights e t i into two contributions, one for saliency and one for context regions, and employ two different fully connected networks to learn the two contributions ( <ref type="figure" target="#fig_3">Figure 5</ref>). Conceptually, this is equivalent to building two separate attention paths, one for salient regions and for contextual regions, which are merged to produce the final attention. Overall, the model obeys to the following equation:</p><formula xml:id="formula_5">e t i = s i ? e sal t i + z i ? e ct x t i<label>(9)</label></formula><p>where e sal t i and e ct x t i are, respectively, the attention weights for salient and context regions. Attention weights for saliency and context are computed as follows:</p><formula xml:id="formula_6">e sal t i = v T e,sal ? ?(W ae,sal ? a i + W he,sal ? h t ?1 ) (10) e ct x t i = v T e,ct x ? ?(W ae,ct x ? a i + W he,ct x ? h t ?1 )<label>(11)</label></formula><p>Notice that our model learns different weights for saliency and contextual regions, and combines them into a final attentive map in which the contributions of salient and non-salient regions are merged together. Similarly to the classical Soft Attention approach <ref type="bibr" target="#b55">[56]</ref>, the proposed generative LSTM can focus on every region of the image, but the attentive process is aware of the saliency of each location, so that the focus on salient and contextual regions is driven by the output of the saliency predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence generation</head><p>Words are encoded with one-hot vectors having size equal to that of the vocabulary, and are then projected into an embedding space via a learned linear transformation. Because sentences have different lengths, they are also marked with special begin-of-string and end-of-string tokens, to keep the model aware of the beginning and end of a particular sentence. Given an image and sentence (y 0 , y 1 , ..., y T ), encoded with one-hot vectors, the generative LSTM is conditioned step by step on the first t words of the caption, and is trained to produce the next word of the caption. The objective function which we optimize is the log-likelihood of correct words over the sequence</p><formula xml:id="formula_7">max w T t =1</formula><p>log Pr(y t |v t , y t ?1 , y t ?2 , ..., y 0 )</p><p>where w are all the parameters of the model. The probability of a word is modeled via a softmax layer applied on the output of the LSTM. To reduce the dimensionality, a linear embedding transformation is used to project one-hot word vectors into the input space of the LSTM and, viceversa, to project the output of the LSTM to the dictionary space.</p><formula xml:id="formula_9">Pr(y t |v t , y t ?1 , y t ?2 , ..., y 0 ) ? exp(y T t W p h t )<label>(13)</label></formula><p>where W p is a matrix for transforming the LSTM output space to the word space and h t is the output of the LSTM. At test time, the LSTM is given a begin-of-string tag as input for the first timestep, then the most probable word according to the predicted distribution is sampled and given as input for the next timestep, until an end-of-string tag is predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section we perform qualitative and quantitative experiments to validate the effectiveness of the proposed model with respect to different baselines and other saliency-boosted captioning methods. First, we describe datasets and metrics used to evaluate our solution and provide implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and metrics</head><p>To validate the effectiveness of the proposed Saliency and Context aware Attention, we perform experiments on five popular image captioning datasets: SALICON <ref type="bibr" target="#b20">[21]</ref>, Microsoft COCO <ref type="bibr" target="#b32">[33]</ref>, Flickr8k <ref type="bibr" target="#b16">[17]</ref>, Flickr30k <ref type="bibr" target="#b59">[60]</ref>, and PASCAL-50S <ref type="bibr" target="#b52">[53]</ref>.</p><p>Microsoft COCO is composed by more than 120, 000 images divided in training and validation sets, where each of them is provided with at least five sentences generated by using Amazon Mechanical Turk. SALICON is a subset of this one, created for the visual saliency prediction task. Since its images are from the Microsoft COCO dataset, at least five captions for each image are available. Overall, it contains 10, 000 training images, 5, 000 validation images and 5, 000 testing images where eye fixations for each image are simulated with mouse movements. In our experiments, we only use train and validation sets for both datasets. The Flickr8k and the Flickr30k datasets are composed by 8, 000 and 30, 000 images respectively. Both of them come with five annotated sentences for each image. In our experiments, we randomly choose 1, 000 validation images and 1, 000 test images for each of these two datasets. The PASCAL-50S dataset provides additional annotations for the UIUC PASCAL sentences <ref type="bibr" target="#b41">[42]</ref>. It is composed of 1, 000 images from the PASCAL-VOC dataset, each of them annotated with 50 human-written sentences, instead of 5 as in the original dataset. Due to the limited number of samples and for a fair comparison with other captioning methods, we first pre-train the model on the Microsoft COCO dataset, then we test it on the images of this dataset without a specific fine-tuning.</p><p>For evaluation, we employ four automatic metrics which are usually employed in image captioning: BLEU <ref type="bibr" target="#b39">[40]</ref>, ROUGE L <ref type="bibr" target="#b31">[32]</ref>, METEOR <ref type="bibr" target="#b1">[2]</ref> and CIDEr <ref type="bibr" target="#b52">[53]</ref>. BLEU is a modified form of precision between n-grams to compare a candidate translation against multiple reference translations. We evaluate our predictions with BLEU using mono-grams, bi-grams, three-grams and four-grams. ROUGE L computes an F-measure considering the longest co-occurring in sequence n-grams. ME-TEOR, instead, is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as stemming and synonymy matching, along with the standard exact word matching. CIDEr, finally, computes the average cosine similarity between n-grams found in the generated caption and those found in reference sentences, weighting them using TF-IDF. To ensure a fair evaluation, we use the Microsoft COCO evaluation toolkit 1 to compute all scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation details</head><p>Each image is encoded through a convolutional network, which computes a stack of high-level features. We employ the popular ResNet-50 <ref type="bibr" target="#b15">[16]</ref>, trained over the ImageNet dataset <ref type="bibr" target="#b43">[44]</ref>, to compute the feature maps over the input image. In particular, the ResNet-50 is composed by 49 convolutional layers, divided in 5 convolutional blocks, and 1 fully connected layer. Since we want to maintain the spatial dimensions, we extract the feature maps from the last convolutional layer and ignore the fully connected layer. The output of the ResNet model is a tensor with 2048 channels. To limit the number of feature maps and the number of learned parameters, we fed this tensor into another convolutional layer with 512 filters and a kernel size of 1, followed by a ReLU activation function. Differently from the weights of the ResNet-50 which are kept fixed, the weights of this last convolutional layer are initialized according to <ref type="bibr" target="#b11">[12]</ref> and fine-tuned over the considered datasets. In the LSTM, following the initialization proposed in <ref type="bibr" target="#b0">[1]</ref>, the weight matrices applied to the inputs are initialized by sampling each element from the Gaussian distribution of 0 mean and 0.01 2 variance, while the weight matrices applied to the internal states are initialized by using the orthogonal <ref type="table">Table 1</ref>. Image captioning results. The conditioning of saliency and context (Saliency+Context Attention) enhances the generation of the caption with respect to the traditional machine attention mechanism. Soft Attention here indicates our reimplementation of <ref type="bibr" target="#b55">[56]</ref>, using the same visual features of our model. initialization. The vectors v e,sal and v e,ct x as well as all bias vectors b * are instead initialized to zero.</p><p>To predict the saliency map for each input image, we exploit our Saliency Attentive Model (SAM) <ref type="bibr" target="#b8">[9]</ref> which is able to predict accurate saliency maps according to different saliency benchmarks. We note however, that we do not expect a significant performance variation when using other state of the art saliency methods.</p><p>As mentioned, we perform experiments over five different datasets. For the SALICON dataset, since its images have all the same size of 480 ? 640, we keep the original size of these images, thus obtaining L = 15 ? 20 = 300. For all other datasets, which are composed of images with different sizes, we set the input size to 480 ? 480 obtaining L = 15 ? 15 = 225. Since saliency maps are exploited inside the proposed saliency-context attention model, we resize the SALICON saliency maps to have a size of 15 ? 20 while, for all other datasets, we resize them to <ref type="bibr">15 ? 15.</ref> All experiments are performed by using the Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with Nestorov momentum <ref type="bibr" target="#b48">[49]</ref> using an initial learning rate of 0.001 and batch size 64. The hidden state dimension is set to 1024 while the embedding size to 512. For all datasets, we choose a vocabulary size equal to the number of words which appear at least 5 times in training and validation captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative results and comparisons with baselines</head><p>To assess the performance of our method, and to investigate the hypotheses behind it, we first compare with the classic Soft Attention approach, and we then build three baselines in which saliency is used to condition the generative process.</p><p>Soft Attention <ref type="bibr" target="#b55">[56]</ref>: The visual input to the LSTM is computed via the Soft Attention mechanism to attend at different locations of the image, without considering salient and non-salient regions. A single feed forward network is in charge of producing attention values, which can be obtained by Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention 48:13</p><p>replacing Eq. 9 with e t i = v T e ? ?(W ae ? a i + W he ? h t ?1 ).</p><p>This approach is equivalent to the one proposed in <ref type="bibr" target="#b55">[56]</ref>, although some implementation details are different. In order to achieve a fair evaluation, we use activations from the ResNet-50 model instead of the VGG-19, and we do not include the doubly stochastic regularization trick. For this reason, the numerical results that we report are not directly comparable with those in the original paper (ours are in general higher than the original ones). Saliency pooling: Visual features from the CNN are multiplied at each location by the corresponding saliency value, and then summed, without any attention mechanism. In this case the visual input of the LSTM is not time dependent, and salient regions are given more focus than non-salient ones. Comparing with Eq. 7, it can be seen as a variation of the Soft Attention in which the network always focuses on salient regions.</p><formula xml:id="formula_11">v t =v = L i=1 s i a i<label>(15)</label></formula><p>Attention on saliency: This is an extension of the Soft Attention approach in which saliency is used to modulate attention values at each location. The attention mechanism, therefore, is conditioned to attend salient regions with higher probability, and to ignore non-salient regions.</p><formula xml:id="formula_12">e t i = s i ? v T e ? ?(W ae ? a i + W he ? h t ?1 )<label>(16)</label></formula><p>Attention on saliency and context (with weight sharing): The attention mechanism is aware of salient and context regions, but weights used to compute the attentive scores of salient and context are shared, excluding the v T e vectors. Notice that, if those were shared too, this baseline would be equivalent to the Soft Attention one.</p><formula xml:id="formula_13">e t i = s i ? e sal t i + (1 ? s i ) ? e ct x t i<label>(17)</label></formula><formula xml:id="formula_14">e sal t i = v T e,sal ? ?(W ae ? a i + W he ? h t ?1 )<label>(18)</label></formula><p>e ct x t i = v T e,ct x ? ?(W ae ? a i + W he ? h t ?1 )</p><p>It is straightforward also to notice that our proposed approach is equivalent to the last baseline, without weight sharing.</p><p>In <ref type="table">Table 1</ref> we first compare the performance of our method with respect to the Soft Attention approach, to assess the superior performance of the proposal with respect to the published state of the art. We report results on all the datasets, both on validation and test sets, with respect to all the automatic metrics described in Section 5.1. As it can be seen, the proposed approach always overcomes by a significant margin the Soft Attention approach, thus experimentally confirming the benefit of having two separate attention paths, one for salient and one for non-salient regions, and the role of saliency as a conditioning for captioning. In particular, on the METEOR metric, the relative improvement ranges from <ref type="bibr" target="#b31">32</ref> In <ref type="table">Table 2</ref>, instead, we compare our approach with the three baselines that incorporate saliency. Firstly, it can be observed that the Saliency pooling baseline usually performs worse than the Soft Attention, thus demonstrating that always attending to salient locations is not sufficient to achieve good captioning results. When plugging in attention, like in the Saliency Attention baseline, numerical results are a bit higher, thanks to a time-dependent attention, but still far from the performance achieved by the complete model. It can also be noticed that, even though this baseline does not take into account the context, it sometimes achieves better results than the Soft Attention <ref type="table">Table 2</ref>. Comparison with image captioning with saliency baselines. While the use of machine attention strategies is beneficial (see Saliency pooling vs. Attention on Saliency), saliency and context are both important for captioning. The use of different attention paths for saliency and context also enhances the performance (see Saliency+Context Attention (with weight sharing) vs. Saliency+Context Attention). model (such as in the case of SALICON, with respect to the METEOR metric). Finally, we notice that the baseline with attention on saliency and context, and with weight sharing, is better than Saliency Attention, further confirming the benefit of including the context. Having two completely separated attention paths, such as in our model, is anyway important, as demonstrated by the numerical results of this last baseline with respect to those of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons with other saliency-boosted captioning models</head><p>We also compare to existing captioning models that incorporate saliency during the generation of image descriptions. In particular, we compare to the model proposed in <ref type="bibr" target="#b47">[48]</ref>, which exploited human fixation points, to the work by Tavakoli et al. <ref type="bibr" target="#b51">[52]</ref> which reports experiments on Microsoft COCO and on PASCAL-50S, and to the proposal by Ramanishka et al. <ref type="bibr" target="#b40">[41]</ref> which used convolutional activations as a proxy for saliency. <ref type="table" target="#tab_3">Table 3</ref> shows the results on the three considered datasets in term of BLEU@4, METEOR, ROUGE L and CIDEr. We compare our solutions to both versions of the model presented in <ref type="bibr" target="#b51">[52]</ref>. The GBVS version exploits saliency maps calculated by using a traditional bottom-up model <ref type="bibr" target="#b14">[15]</ref>, while the other one includes saliency maps extracted from a deep convolutional network <ref type="bibr" target="#b50">[51]</ref>.</p><p>Overall, results show that the proposed Saliency and Context Attention model can overcome the other methods on different metrics, thus confirming the strategy of including two attention paths. In particular, on the METEOR metric, we obtain a relative improvement of 4.57% on the SALICON dataset, 5.53% on the Microsoft COCO and 8.94% on the PASCAL-50S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis of generated captions</head><p>We further collect statistics on captions generated by our method and by the Soft Attention model, to quantitatively assess the quality of generated captions. Firstly, we define three metrics which evaluate the vocabulary size and the difference between the corpus of captions generated by the two models and the ground-truth:</p><p>? Vocabulary size: number of unique words generated in all captions; ? Percentage of novel sentences: percentage of generated sentences which are not seen in the training set; ? Percentage of different sentences: percentage of images which are described differently by the two models;</p><p>Then, we measure the diversity of the set of captions generated by each of the two models, via the following two metrics <ref type="bibr" target="#b44">[45]</ref>:</p><p>? Div-1: ratio of number of unique unigrams in a set of captions to the number of words in the same set. Higher is more diverse. ? Div-2: ratio of number of unique bigrams in a set of captions to the number of words in the same set. Higher is more diverse. In <ref type="table" target="#tab_4">Table 4</ref> we compare the set of captions generated by our model with that generated by the Soft Attention baseline. Although our model features a slight reduction of the vocabulary size on SALICON, COCO and PASCAL-50S, captions generated by the two models are very often different, thus confirming that the two approaches have learned different captioning models. Moreover, the diversity and the number of novel sentences of the Soft Attention approach are entirely preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis of attentive states</head><p>The selection of a location in our model is based on the competition between the saliency attentive path and the context attentive path (see Eq. 9). To investigate how the two paths interact and contribute to the generation of a word, in <ref type="figure" target="#fig_4">Figure 6</ref> we report, for several images from the Microsoft COCO dataset, the changes in attention weights between the two paths. Specifically, for each image we report the average of e sal t i and e ct x t i values at each timestep, along with a visualization of its saliency map. It is interesting to see how the model was able to correctly exploit the two attention paths for generating different parts of the caption, and how generated words correspond in most cases to the attended regions. For example, in the case of the first image ("a group of zebras graze in a grassy field"), the saliency attentive path is more active than the context path during the generation of words corresponding to the "group of zebras", which are captured by saliency. Instead, when the model has to describe the context ("in a grassy field"), the saliency attentive path has lower weights with respect to the context attentive path. The same can be observed for all the reported images; it can also be noticed that generated captions tend to describe both salient objects and the context, and that usually the salient part, which is also the most important, is described before the context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative results</head><p>Finally, in <ref type="figure">Figure 7</ref> we report some sample results on images taken from the Microsoft COCO dataset. For each image we report the corresponding saliency map, and captions generated by our model and by the Soft Attention baseline compared to the ground-truth. It can be seen that, on average, captions generated by our model are more consistent with the corresponding image and the human-generated caption, and that, as also observed in the previous section, salient parts are described as well as the context. The incorporation of saliency and context also help the model to avoid failures due to hallucination, such as in the case of the fourth image, in which the Soft Attention model predicts a remote control which is not depicted in the image. Other failure cases, which are avoided by our model, include the repetition of words (as in the fifth image) and the failure to describe the context (first image). We speculate that the presence of two separate attention paths, which the model has learned to attend during the generation of the caption, helps to avoid such failures more effectively than the classic machine attention approach. For completeness, some failure cases of the proposed model are reported in <ref type="figure">Figure 8</ref>. The majority of failures occurs when the salient regions of the image are not described in the corresponding ground-truth caption (as for example in the first row), thus causing a performance loss. Some problems arise also in presence of complex scenes (such as in the fourth image). However, we observe that the Soft Attention baseline fails as well to predict correct and complete captions in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed a novel image captioning architecture which extends the machine attention paradigm by creating two attentive paths conditioned on the output of a saliency prediction model. The Ground-truth: A group of people that are standing around giraffes. Saliency+Context Attention: A group of people standing around a giraffe. Without saliency: A group of people standing around a stage with a group of people.</p><p>Ground-truth: A group of people at the park with some flying kites. Saliency+Context Attention: A group of people flying kites in a park. Without saliency: A group of people standing on top of a lush green field.</p><p>Ground-truth: A man is looking into a home refrigerator. Saliency+Context Attention: A man is looking inside of a refrigerator. Without saliency: A man is making a refrigerator in a kitchen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth:</head><p>A women who is jumping on the bed. Saliency+Context Attention: A woman is jumping up in a bed. Without saliency: A woman is playing with a remote control.</p><p>Ground-truth: A man takes a profile picture of himself in a bathroom mirror. Saliency+Context Attention: A person taking a picture of himself in a bathroom. Without saliency: A bathroom with a sink and a sink.</p><p>Ground-truth: A double decker bus driving down a street. Saliency+Context Attention: A double decker bus driving down a street. Without saliency: A bus is parked on the side of the road.</p><p>Ground-truth: A teddy bear holding a cell phone in front of a window with a view of the city. Saliency+Context Attention: A teddy bear sitting on a chair next to a window. Without saliency: A brown dog is sitting on a laptop keyboard.</p><p>Ground-truth: A group of people riding down a snow covered slope. Saliency+Context Attention: A group of people riding skis down a snow covered slope. Without saliency: A group of people on skis in the snow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth: A laptop computer sitting on top of a table.</head><p>Saliency+Context Attention: A laptop computer sitting on a top of a desk. Without saliency: A desk with a laptop computer and a laptop.</p><p>Ground-truth: A person on a motorcycle riding on a mountain. Saliency+Context Attention: A person riding a motorcycle on a road. Without saliency: A man on a bike with a bike in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth:</head><p>A car is parked next to a parking meter. Saliency+Context Attention: A car is parked in the street next to a parking meter. Without saliency: A car parked next to a white fire hydrant.</p><p>Ground-truth: A plate of food and a cup of coffee. Saliency+Context Attention: A plate of food with a sandwich and a cup of coffee. Without saliency: A table with a variety of food on it. <ref type="figure">Fig. 7</ref>. Example results on the Microsoft COCO dataset <ref type="bibr" target="#b32">[33]</ref>.</p><p>first one is focused on salient regions, and the second on contextual regions: the overall model exploits the two paths during the generation of the caption, by giving more importance to salient or contextual regions as needed. The role of saliency with respect to context has been investigated by collecting statistics on semantic segmentation datasets, while the captioning model has been evaluated on large scale captioning datasets, using standard automatic metrics and by evaluating the diversity and the dictionary size of the generated corpora. Finally, the activations of the two attentive paths have been investigated, and we have shown that they correspond, word by word, to a focus on salient objects or on the context in the generated caption; moreover, we qualitatively Ground-truth: The yellow truck passes by two people on motorcycles from opposing directions. Saliency+Context Attention: A person on a motor bike in a city. Without saliency: A man in a red shirt on a horse.</p><p>Ground-truth: A cityscape that is seen from the other side of the river. Saliency+Context Attention: A large building with a large clock tower in the background. Without saliency: A large building with a large clock in the water.</p><p>Ground-truth: A large tree situated next to a large body of water. Saliency+Context Attention: A person is sitting under a red umbrella. Without saliency: A street sign with a large tree in the middle.</p><p>Ground-truth: A busted fire hydrant spewing water out onto a street. Saliency+Context Attention: A person standing in a front of a large cruise ship. Without saliency: A man is standing in a dock near a large truck.</p><p>Ground-truth: A small airplane flying over a field filled with people. Saliency+Context Attention: A group of people walking around a large jet. Without saliency: A large group of people standing on top of a lush green field.</p><p>Ground-truth: The view of city buildings is seen from the river. Saliency+Context Attention: A large clock tower towering over the water. Without saliency: A large building with a large clock tower in the water. <ref type="figure">Fig. 8</ref>. Failure cases on sample images of the Microsoft COCO dataset <ref type="bibr" target="#b32">[33]</ref>.</p><p>assessed the superiority of the captions generated by our method with respect to those generated by the Soft Attention approach. Although our focus has been that of demonstrating the effectiveness of saliency on captioning, rather than that of beating captioning approaches which rely on different cues, we point out that our method can be easily incorporated into those architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Ground-truth semantic segmentation and saliency predictions from our model<ref type="bibr" target="#b8">[9]</ref> on sample images from Pascal-Context<ref type="bibr" target="#b35">[36]</ref> (first row), Cityscapes<ref type="bibr" target="#b6">[7]</ref> (second row) and LIP<ref type="bibr" target="#b12">[13]</ref> (last row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Pascal-Context [36] c a r b u il d in g li c e n s e p la te p e rs o n b u s v e g e ta ti o Cityscapes [7] fa c e u p p e r-c lo th e s h a t h a ir s o c k s s k ir t d re s Most salient classes on Pascal-Context, Cityscapes and LIP datasets. Cityscapes [7] ri g h t-s h o e le ft -s h o e ri g h t-le g le ft -l e g p a n ts g lo v e ju m p s u it s Least salient classes on Pascal-Context, Cityscapes and LIP datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Distribution of object sizes and saliency values (best seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>PayingFig. 5 .</head><label>5</label><figDesc>More Attention to Saliency: Image Captioning with Saliency and Context Attention 48:9 Overview of the proposed model. Two different attention paths are built for salient regions and contextual regions, to help the model build captions which describe both components (best seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>PayingFig. 6 .</head><label>6</label><figDesc>More Attention to Saliency: Image Captioning with Saliency and Context Attention 48:17 Examples of attention weights changes between saliency and context along with the generation of captions (best seen in color). Images are from the Microsoft COCO dataset<ref type="bibr" target="#b32">[33]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with existing saliency-boosted captioning models.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="4">B@4 METEOR ROUGE L CIDEr</cell></row><row><cell>SALICON</cell><cell>Sugano et al. [48] Ours</cell><cell>24.5 26.9</cell><cell>21.9 22.9</cell><cell>52.4 50.4</cell><cell>63.8 73.3</cell></row><row><cell></cell><cell cols="2">Tavakoli et al. [52] (GBVS) 28.7</cell><cell>23.5</cell><cell>51.2</cell><cell>84.1</cell></row><row><cell>COCO</cell><cell cols="2">Tavakoli et al. [52] (iSEEL) 28.3</cell><cell>23.5</cell><cell>50.8</cell><cell>83.6</cell></row><row><cell></cell><cell>Ours</cell><cell>28.4</cell><cell>24.8</cell><cell>52.1</cell><cell>89.8</cell></row><row><cell>Flickr30k (Test)</cell><cell>Ramanishka et al. [41] Ours</cell><cell>-21.3</cell><cell>18.3 20.0</cell><cell>-45.2</cell><cell>-46.4</cell></row><row><cell></cell><cell cols="2">Tavakoli et al. [52] (GBVS) 40.0</cell><cell>30.2</cell><cell>63.5</cell><cell>61.5</cell></row><row><cell>PASCAL-50S</cell><cell cols="2">Tavakoli et al. [52] (iSEEL) 39.6</cell><cell>30.2</cell><cell>63.2</cell><cell>61.4</cell></row><row><cell></cell><cell>Ours</cell><cell>45.7</cell><cell>32.9</cell><cell>66.3</cell><cell>70.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Statistics on vocabulary size and diversity of the generated captions. Including saliency and context in two different machine attention paths (Saliency+Context attention) produces different captions with respect to the traditional machine attention approach (Soft Attention), while preserving almost the same diversity statistics.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Div-1</cell><cell>Div-2</cell><cell cols="2">Vocabulary % novel sent. % different sent.</cell></row><row><cell>SALICON</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0125 0.0549 0.0136 0.0498</cell><cell>658 614</cell><cell>95.22% 93.12%</cell><cell>95.34%</cell></row><row><cell>COCO</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0037 0.0182 0.0038 0.0187</cell><cell>1490 1444</cell><cell>81.81% 78.02%</cell><cell>93.80%</cell></row><row><cell>Flickr8k (Validation)</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0400 0.1094 0.0367 0.1026</cell><cell>389 411</cell><cell>98.30% 99.30%</cell><cell>97.90%</cell></row><row><cell>Flickr8k (Test)</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0419 0.1119 0.0385 0.1041</cell><cell>404 423</cell><cell>98.50% 99.60%</cell><cell>97.60%</cell></row><row><cell>Flickr30k (Validation)</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0565 0.1439 0.0577 0.1445</cell><cell>699 715</cell><cell>99.90% 99.61%</cell><cell>98.62%</cell></row><row><cell>Flickr30k (Test)</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0585 0.1549 0.0580 0.1508</cell><cell>682 711</cell><cell>99.90% 99.70%</cell><cell>98.20%</cell></row><row><cell>PASCAL-50S</cell><cell cols="3">Soft Attention Saliency+Context Attention 0.0468 0.1359 0.0475 0.1379</cell><cell>465 456</cell><cell>97.10% 96.40%</cell><cell>94.80%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 14, No. 2, Article 48</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tylin/coco-caption</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical Boundary-Aware Neural Encoder for Video Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costantino</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilke</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://saliency.mit.edu/" />
	</analytic>
	<monogr>
		<title level="j">MIT Saliency Benchmark</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Where should saliency models look next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Deep Multi-Level Network for Saliency Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09571</idno>
		<title level="m">Predicting Human Eye Fixations via an LSTM-based Saliency Attentive Model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What the eyes say about speaking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="274" to="279" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-End Saliency Mapping via Probability Distribution Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengsheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilke</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Hierarchical Approach for Generating Descriptive Image Paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency Unified: A Deep Architecture for Simultaneous Eye Fixation Prediction and Salient Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vennela</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gudisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jaley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Dholakiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepGaze I: Boosting saliency prediction with feature maps trained on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>K?mmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>K?mmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01563</idno>
		<title level="m">DeepGaze II: Reading fixations from deep features trained on object recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01708</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Role of Context for Object Detection and Semantic Segmentation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shallow and Deep Convolutional Networks for Saliency Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Gir?-I</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Top-down Visual Saliency Guided by Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasili</forename><surname>Ramanishka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collecting image annotations using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT Workshops</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Dynamic Representation of Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10476</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05203</idno>
		<title level="m">Seeing with humans: Gaze-assisted neural image captioning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hamed R Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Paying Attention to Descriptions Generated by Image Captioning Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Hamed R Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Lik</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">I2t: Image parsing to text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mun</forename><forename type="middle">Wai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1485" to="1508" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
