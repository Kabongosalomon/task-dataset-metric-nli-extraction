<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Dayton</orgName>
								<address>
									<addrLine>300 College Park</addrLine>
									<postCode>45469</postCode>
									<settlement>Dayton</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>?</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Medical imaging</term>
					<term>Semantic segmentation</term>
					<term>Convolutional Neural Networks</term>
					<term>U-Net</term>
					<term>Residual U-Net</term>
					<term>RU-Net</term>
					<term>and R2U-Net</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning (DL) based semantic segmentation methods have been providing state-of-the-art performance in the last few years. More specifically, these techniques have been successfully applied to medical image classification, segmentation, and detection tasks. One deep learning technique, U-Net, has become one of the most popular for these applications. In this paper, we propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN. There are several advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature accumulation with recurrent residual convolutional layers ensures better feature representation for segmentation tasks. Third, it allows us to design better U-Net architecture with same number of network parameters with better performance for medical image segmentation. The proposed models are tested on three benchmark datasets such as blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. The experimental results show superior performance on segmentation tasks compared to equivalent models including U-Net and residual U-Net (ResU-Net).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>are available for training CNN models <ref type="bibr" target="#b0">[1]</ref>. However, in most cases, models are explored and evaluated using classification tasks on very large-scale datasets like ImageNet <ref type="bibr" target="#b0">[1]</ref>, where the outputs of the classification tasks are single label or probability values. Alternatively, small architecturally variant models are used for semantic image segmentation tasks. For example, a fully-connected convolutional neural network (FCN) also provides state-of-the-art results for image segmentation tasks in computer vision <ref type="bibr" target="#b1">[2]</ref>. Another variant of FCN was also proposed which is called SegNet <ref type="bibr" target="#b9">[10]</ref>. Due to the great success of DCNNs in the field of computer vision, different variants of this approach are applied in different modalities of medical imaging including segmentation, classification, detection, registration, and medical information processing. The medical imaging comes from different imaging techniques such as Computer Tomography (CT), ultrasound, X-ray, and Magnetic Resonance Imaging (MRI). The goal of Computer-Aided Diagnosis (CAD) is to obtain a faster and better diagnosis to ensure better treatment of a large number of people at the same time. Additionally, efficient automatic processing without human involvement to reduce human error and also reduces overall time and cost. Due to the slow process and tedious nature of Mahmudul Hasan 2 , is with Comcast Labs, Washington, DC, USA. (e-mail: mahmud.ucr@gmail.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for</head><p>Medical Image Segmentation N manual segmentation approaches, there is a significant demand for computer algorithms that can do segmentation quickly and accurately without human interaction. However, there are some limitations of medical image segmentation including data scarcity and class imbalance. Most of the time the large number of labels (often in the thousands) for training is not available for several reasons <ref type="bibr" target="#b10">[11]</ref>. Labeling the dataset requires an expert in this field which is expensive, and it requires a lot of effort and time. Sometimes, different data transformation or augmentation techniques (data whitening, rotation, translation, and scaling) are applied for increasing the number of labeled samples available <ref type="bibr">[12, 13, and 14]</ref>. In addition, patch based approaches are used for solving class imbalance problems. In this work, we have evaluated the proposed approaches on both patch-based and entire image-based approaches. However, to switch from the patch-based approach to the pixel-based approach that works with the entire image, we must be aware of the class imbalance problem. In the case of semantic segmentation, the image backgrounds are assigned a label and the foreground regions are assigned a target class. Therefore, the class imbalance problem is resolved without any trouble. Two advanced techniques including cross-entropy loss and dice similarity are introduced for efficient training of classification and segmentation tasks in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Furthermore, in medical image processing, global localization and context modulation is very often applied for localization tasks. Each pixel is assigned a class label with a desired boundary that is related to the contour of the target lesion in identification tasks. To define these target lesion boundaries, we must emphasize the related pixels. Landmark detection in medical imaging <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> is one example of this. There were several traditional machine learning and image processing techniques available for medical image segmentation tasks before the DL revolution, including amplitude segmentation based on histogram features <ref type="bibr" target="#b16">[17]</ref>, the region based segmentation method <ref type="bibr" target="#b17">[18]</ref>, and the graph-cut approach <ref type="bibr" target="#b18">[19]</ref>. However, semantic segmentation approaches that utilize DL have become very popular in recent years in the field of medical image segmentation, lesion detection, and localization <ref type="bibr" target="#b19">[20]</ref>. In addition, DL based approaches are known as universal learning approaches, where a single model can be utilized efficiently in different modalities of medical imaging such as MRI, CT, and X-ray.</p><p>According to a recent survey, DL approaches are applied to almost all modalities of medical imagining <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Furthermore, the highest number of papers have been published on segmentation tasks in different modalities of medical imaging <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. A DCNN based brain tumor segmentation and detection method was proposed in <ref type="bibr" target="#b21">[22]</ref>.</p><p>From an architectural point of view, the CNN model for classification tasks requires an encoding unit and provides class probability as an output. In classification tasks, we have performed convolution operations with activation functions followed by sub-sampling layers which reduces the dimensionality of the feature maps. As the input samples traverse through the layers of the network, the number of feature maps increases but the dimensionality of the feature maps decreases. This is shown in the first part of the model (in green) in <ref type="figure" target="#fig_1">Fig. 2</ref>. Since, the number of feature maps increase in the deeper layers, the number of network parameters increases respectively. Eventually, the Softmax operations are applied at the end of the network to compute the probability of the target classes.</p><p>As opposed to classification tasks, the architecture of segmentation tasks requires both convolutional encoding and decoding units. The encoding unit is used to encode input images into a larger number of maps with lower dimensionality. The decoding unit is used to perform up-convolution (deconvolution) operations to produce segmentation maps with the same dimensionality as the original input image. Therefore, the architecture for segmentation tasks generally requires almost double the number of network parameters when compared to the architecture of the classification tasks. Thus, it is important to design efficient DCNN architectures for segmentation tasks which can ensure better performance with less number of network parameters.</p><p>This research demonstrates two modified and improved segmentation models, one using recurrent convolution networks, and another using recurrent residual convolutional networks. To accomplish our goals, the proposed models are evaluated on different modalities of medical imagining as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The contributions of this work can be summarized as follows:</p><p>1) Two new models RU-Net and R2U-Net are introduced for medical image segmentation.</p><p>2) The experiments are conducted on three different modalities of medical imaging including retina blood vessel segmentation, skin cancer segmentation, and lung segmentation.</p><p>3) Performance evaluation of the proposed models is conducted for the patch-based method for retina blood vessel segmentation tasks and the end-to-end image-based approach for skin lesion and lung segmentation tasks. 4) Comparison against recently proposed state-of-the-art methods that shows superior performance against equivalent models with same number of network parameters.</p><p>The paper is organized as follows: Section II discusses related work. The architectures of the proposed RU-Net and R2U-Net models are presented in Section III. Section IV, explains the datasets, experiments, and results. The conclusion and future direction are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Semantic segmentation is an active research area where DCNNs are used to classify each pixel in the image individually, which is fueled by different challenging datasets in the fields of computer vision and medical imaging <ref type="bibr">[23, 24, and 25]</ref>. Before the deep learning revolution, the traditional machine learning approach mostly relied on hand engineered features that were used for classifying pixels independently. In the last few years, a lot of models have been proposed that have proved that deeper networks are better for recognition and segmentation tasks <ref type="bibr" target="#b4">[5]</ref>. However, training very deep models is difficult due to the vanishing gradient problem, which is resolved by implementing modern activation functions such as Rectified Linear Units (ReLU) or Exponential Linear Units (ELU) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Another solution to this problem is proposed by He et al., a deep residual model that overcomes the problem utilizing an identity mapping to facilitate the training process <ref type="bibr" target="#b25">[26]</ref>.</p><p>In addition, CNNs based segmentation methods based on FCN provide superior performance for natural image segmentation <ref type="bibr" target="#b1">[2]</ref>. One of the image patch-based architectures is called Random architecture, which is very computationally intensive and contains around 134.5M network parameters. The main drawback of this approach is that a large number of pixel overlap and the same convolutions are performed many times. The performance of FCN has improved with recurrent neural networks (RNN), which are fine-tuned on very large datasets <ref type="bibr" target="#b26">[27]</ref>. Semantic image segmentation with DeepLab is one of the state-of-the-art performing methods <ref type="bibr" target="#b27">[28]</ref>. SegNet consists of two parts, one is the encoding network which is a 13-layer VGG16 network <ref type="bibr" target="#b4">[5]</ref>, and the corresponding decoding network uses pixel-wise classification layers. The main contribution of this paper is the way in which the decoder upsamples its lower resolution input feature maps <ref type="bibr" target="#b9">[10]</ref>. Later, an improved version of SegNet, which is called Bayesian SegNet was proposed in 2015 <ref type="bibr" target="#b28">[29]</ref>. Most of these architectures are explored using computer vision applications. However, there are some deep learning models that have been proposed specifically for the medical image segmentation, as they consider data insufficiency and class imbalance problems.</p><p>One of the very first and most popular approaches for semantic medical image segmentation is called "U-Net" <ref type="bibr" target="#b11">[12]</ref>. A diagram of the basic U-Net model is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. According to the structure, the network consists of two main parts: the convolutional encoding and decoding units. The basic convolution operations are performed followed by ReLU activation in both parts of the network. For down sampling in the encoding unit, 2?2 max-pooling operations are performed. In the decoding phase, the convolution transpose (representing up-convolution, or de-convolution) operations are performed to up-sample the feature maps. The very first version of U-Net was used to crop and copy feature maps from the encoding unit to the decoding unit. The U-Net model provides several advantages for segmentation tasks: first, this model allows for the use of global location and context at the same time. Second, it works with very few training samples and provides better performance for segmentation tasks <ref type="bibr" target="#b11">[12]</ref>. Third, an end-to-end pipeline process the entire image in the forward pass and directly produces segmentation maps. This ensures that U-Net preserves the full context of the input images, which is a major advantage when compared to patch-based segmentation approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. However, U-Net is not only limited to the applications in the domain of medical imaging, nowadays this model is massively applied for computer vision tasks as well <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Meanwhile, different variants of U-Net models have been proposed, including a very simple variant of U-Net for CNN-based segmentation of Medical Imaging data <ref type="bibr" target="#b31">[32]</ref>. In this model, two modifications are made to the original design of U-Net: first, a combination of multiple segmentation maps and forward feature maps are summed (element-wise) from one part of the network to the other. The feature maps are taken from different layers of encoding and decoding units and finally summation (element-wise) is performed outside of the encoding and decoding units. The authors report promising performance improvement during training with better convergence compared to U-Net, but no benefit was observed when using a summation of features during the testing phase <ref type="bibr" target="#b31">[32]</ref>. However, this concept proved that feature summation impacts the performance of a network. The importance of skipped connections for biomedical image segmentation tasks have been empirically evaluated with U-Net and residual networks <ref type="bibr" target="#b32">[33]</ref>. A deep contour-aware network called Deep Contour-Aware Networks (DCAN) was proposed in 2016, which can extract multi-level contextual features using a hierarchical architecture for accurate gland segmentation of histology images and shows very good performance for segmentation <ref type="bibr" target="#b33">[34]</ref>. Furthermore, Nabla-Net: a deep dig-like convolutional architecture was proposed for segmentation in 2017 <ref type="bibr" target="#b34">[35]</ref>.</p><p>Other deep learning approaches have been proposed based on U-Net for 3D medical image segmentation tasks as well. The 3D-Unet architecture for volumetric segmentation learns from sparsely annotated volumetric images <ref type="bibr" target="#b12">[13]</ref>. A powerful end-toend 3D medical image segmentation system based on volumetric images called V-net has been proposed, which consists of a FCN with residual connections <ref type="bibr" target="#b13">[14]</ref>. This paper also introduces a dice loss layer <ref type="bibr" target="#b13">[14]</ref>. Furthermore, a 3D deeply supervised approach for automated segmentation of volumetric medical images was presented in <ref type="bibr" target="#b35">[36]</ref>. High-Res3DNet was proposed using residual networks for 3D segmentation tasks in 2016 <ref type="bibr" target="#b36">[37]</ref>. In 2017, a CNN based brain tumor segmentation approach was proposed using a 3D-CNN model with a fully connected CRF <ref type="bibr" target="#b37">[38]</ref>. Pancreas segmentation was proposed in <ref type="bibr" target="#b38">[39]</ref>, and Voxresnet was proposed in 2016 where a deep voxel wise residual network is used for brain segmentation. This architecture utilizes residual networks and summation of feature maps from different layers <ref type="bibr" target="#b39">[40]</ref>.</p><p>Alternatively, we have proposed two models for semantic segmentation based on the architecture of U-Net in this paper. The proposed Recurrent Convolutional Neural Networks (RCNN) model based on U-Net is named RU-Net, which is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Additionally, we have proposed a residual RCNN based U-Net model which is called R2U-Net. The following section provides the architectural details of both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type="bibr" target="#b6">[7]</ref>, RCNN <ref type="bibr" target="#b40">[41]</ref>, and U-Net <ref type="bibr" target="#b11">[12]</ref>, we propose two models for segmentation tasks which are named RU-Net and R2U-Net. These two approaches utilize the strengths of all three recently developed deep learning models. RCNN and its variants have already shown superior performance on object recognition tasks using different benchmarks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. The recurrent residual convolutional operations can be demonstrated mathematically according to the improved-residual networks in <ref type="bibr" target="#b42">[43]</ref>. The operations of the Recurrent Convolutional Layers (RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type="bibr" target="#b40">[41]</ref>. Let's consider the input sample in the ? layer of the residual RCNN (RRCNN) block and a pixel located at ( , ) in an input sample on the k th feature map in the RCL. Additionally, let's assume the output of the network ( ) is at the time step t. The output can be expressed as follows as:</p><formula xml:id="formula_0">( ) = ( ) * ( , ) ( ) + ( ) * ( , ) ( ? 1) +<label>(1)</label></formula><p>Here ( , ) ( ) and ( , ) ( ? 1) are the inputs to the standard convolution layers and for the ? RCL respectively. The and values are the weights of the standard convolutional layer and the RCL of the k th feature map respectively, and is the bias. The outputs of RCL are fed to the standard ReLU activation function and are expressed:</p><formula xml:id="formula_1">?( , ) = ( ( )) = max (0, ( ))<label>(2)</label></formula><p>?( , ) represents the outputs from of l th layer of the RCNN unit. The output of ?( , ) is used for down-sampling and up-sampling layers in the convolutional encoding and decoding units of the RU-Net model respectively. In the case of R2U-Net, the final outputs of the RCNN unit are passed through the residual unit that is shown <ref type="figure" target="#fig_3">Fig. 4(d)</ref>. Let's consider that the output of the RRCNN-block is +1 and can be calculated as follows:</p><formula xml:id="formula_2">+1 = + ?( , )<label>(3)</label></formula><p>Here, represents the input samples of the RRCNN-block. The +1 sample is used the input for the immediate succeeding sub-sampling or up-sampling layers in the encoding and decoding convolutional units of R2U-Net. However, the number of feature maps and the dimensions of the feature maps for the residual units are the same as in the RRCNN-block shown in <ref type="figure" target="#fig_3">Fig. 4 (d)</ref>. The proposed deep learning models are the building blocks of the stacked convolutional units shown in <ref type="figure" target="#fig_3">Fig. 4(b) and (d)</ref>.</p><p>There are four different architectures evaluated in this work. First, U-Net with forward convolution layers and feature concatenation is applied as an alternative to the crop and copy method found in the primary version of U-Net <ref type="bibr" target="#b11">[12]</ref>. The basic convolutional unit of this model is shown in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. Second, U-Net with forward convolutional layers with residual connectivity is used, which is often called residual U-net (ResU-Net) and is shown in <ref type="figure" target="#fig_3">Fig. 4(c)</ref>  <ref type="bibr" target="#b13">[14]</ref>. The third architecture is U-Net with forward recurrent convolutional layers as shown in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, which is named RU-Net. Finally, the last architecture is U-Net with recurrent convolution layers with residual connectivity as shown in <ref type="figure" target="#fig_3">Fig. 4(d)</ref>, which is named R2U-Net. The pictorial representation of the unfolded RCL layers with respect to time-step is shown in <ref type="figure" target="#fig_4">Fig 5.</ref> Here t=2 (0 ~ 2), refers to the recurrent convolutional operation that includes one single convolution layer followed by two subsequential recurrent convolutional layers. In this implementation, we have applied concatenation to the feature maps from the encoding unit to the decoding unit for both RU-Net and R2U-Net models. The differences between the proposed models with respect to the U-Net model are three-fold. This architecture consists of convolutional encoding and decoding units same as U-Net. However, the RCLs and RCLs with residual units are used instead of regular forward convolutional layers in both the encoding and decoding units. The residual unit with RCLs helps to develop a more efficient deeper model. Second, the efficient feature accumulation method is included in the RCL units of both proposed models. The effectiveness of feature accumulation from one part of the network to the other is shown in the CNN-based segmentation approach for medical imaging. In this model, the element-wise feature summation is performed outside of the U-Net model <ref type="bibr" target="#b31">[32]</ref>. This model only shows the benefit during the training process in the form of better convergence. However, our proposed models show benefits for both training and testing phases due to the feature accumulation inside the model. The feature accumulation with respect to different time-steps ensures better and stronger feature representation. Thus, it helps extract very low-level features which are essential for segmentation tasks for different modalities of medical imaging (such as blood vessel segmentation). Third, we have removed the cropping and copying unit from the basic U-Net model and use only concatenation operations, resulting a much-sophisticated architecture that results in better performance. There are several advantages of using the proposed architectures when compared with U-Net. The first is the efficiency in terms of the number of network parameters. The proposed RU-Net, and R2U-Net architectures are designed to have the same number of network parameters when compared to U-Net and ResU-Net, and RU-Net and R2U-Net show better performance on segmentation tasks. The recurrent and residual operations do not increase the number of network parameters. However, they do have a significant impact on training and testing performance. This is shown through empirical evidence with a set of experiments in the following sections <ref type="bibr" target="#b42">[43]</ref>. This approach is also generalizable, as it easily be applied deep learning models based on SegNet <ref type="bibr" target="#b9">[10]</ref>, 3D-UNet <ref type="bibr" target="#b12">[13]</ref>, and V-Net <ref type="bibr" target="#b13">[14]</ref> with improved performance for segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP AND RESULTS</head><p>To demonstrate the performance of the RU-Net and R2U-Net models, we have tested them on three different medical imaging datasets. These include blood vessel segmentations from retina images (DRIVE, STARE, and CHASE_DB1 shown in <ref type="figure" target="#fig_5">Fig. 6</ref>), skin cancer lesion segmentation, and lung segmentation from 2D images. For this implementation, the Keras, and TensorFlow frameworks are used on a single GPU machine with 56G of RAM and an NIVIDIA GEFORCE GTX-980 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Database Summary 1) Blood Vessel Segmentation</head><p>We have experimented on three different popular datasets for retina blood vessel segmentation including DRIVE, STARE, and CHASH_DB1. The DRIVE dataset is consisted of 40 color retinal images in total, in which 20 samples are used for training and remaining 20 samples are used for testing. The size of each original image is 565?584 pixels <ref type="bibr" target="#b43">[44]</ref>. To develop a square dataset, the images are cropped to only contain the data from columns 9 through 574, which then makes each image 565?565 pixels. In this implementation, we considered 190,000 randomly selected patches from 20 of the images in the DRIVE dataset, where 171,000 patches are used for training, and the remaining 19,000 patches used for validation. The size of each patch is 48?48 for all three datasets shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. The second dataset, STARE, contains 20 color images, and each image has a size of 700?605 pixels <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Due to the smaller number of samples, two approaches are applied very often for training and testing on this dataset. First, training sometimes performed with randomly selected samples from all 20 images <ref type="bibr" target="#b50">[53]</ref>.  Another approach is the "leave-one-out" method, in which each image is tested, and training is conducted on the remaining 19 samples <ref type="bibr" target="#b46">[47]</ref>. Therefore, there is no overlap between training and testing samples. In this implementation, we used the "leaveone-out" approach for STARE dataset. The CHASH_DB1 dataset contains 28 color retina images and the size of each image is 999?960 pixels <ref type="bibr" target="#b47">[48]</ref>. The images in this dataset were collected from both left and right eyes of 14 school children. The dataset is divided into two sets where samples are selected randomly. A 20-sample set is used for training and the remaining 8 samples are used for testing.</p><p>As the dimensionality of the input data larger than the entire DRIVE dataset, we have considered 250,000 patches in total from 20 images for both STARE and CHASE_DB1. In this case 225,000 patches are used for training and the remaining 25,000 patches are used for validation. Since the binary FOV (which is shown in second row in <ref type="figure" target="#fig_5">Fig. 6)</ref> is not available for the STARE and CHASE_DB1 datasets, we generated FOV masks using a similar technique to the one described in <ref type="bibr" target="#b46">[47]</ref>. One advantage of the patch-based approach is that the patches give the network access to local information about the pixels, which has impact on overall prediction. Furthermore, it ensures that the classes of the input data are balanced. The input patches are randomly sampled over an entire image, which also includes the outside region of the FOV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Skin Cancer Segmentation</head><p>This dataset is taken from the Kaggle competition on skin lesion segmentation that occurred in 2017 <ref type="bibr">[49]</ref>. This dataset contains 2000 samples in total. It consists of 1250 training samples, 150 validation samples, and 600 testing samples. The original size of each sample was 700?900, which was rescaled to 256?256 for this implementation. The training samples include the original images, as well as corresponding target binary images containing cancer or non-cancer lesions. The target pixels are represented with a value of either 255 or 0 for the pixels outside of the target lesion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Lung Segmentation</head><p>The Lung Nodule Analysis (LUNA) competition at the Kaggle Data Science Bowl in 2017 was held to find lung lesions in 2D and 3D CT images. The provided dataset consisted of 534 2D samples with respective label images for lung segmentation <ref type="bibr">[50]</ref>. For this study, 70% of the images are used for training and the remaining 30% are used for testing. The original image size was 512?512, however, we resized the images to 256?256 pixels in this implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Analysis Approaches</head><p>For quantitative analysis of the experimental results, several performance metrics are considered, including accuracy (AC), sensitivity (SE), specificity (SP), F1-score, Dice coefficient (DC), and Jaccard similarity (JS). To do this we also use the variables True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN). The overall accuracy is calculated using Eq. (4), and sensitivity is calculated using Eq. Furthermore, specificity is calculated using the following Eq. (6).</p><formula xml:id="formula_3">= +<label>(6)</label></formula><p>The DC is expressed as in Eq. <ref type="bibr" target="#b6">(7)</ref> according to <ref type="bibr" target="#b48">[51]</ref>. Here GT refers to the ground truth and SR refers the segmentation result.</p><formula xml:id="formula_4">= 2 | ? | | |+| |<label>(7)</label></formula><p>The JS is represented using Eq. (8) as in <ref type="bibr" target="#b49">[52]</ref>.</p><formula xml:id="formula_5">= | ? | | ? | (8)</formula><p>However, the area under curve (AUC) and the receiver operating characteristics (ROC) curve are common evaluation measures for medical image segmentation tasks. In this experiment, we utilized both analytical methods to evaluate the performance of the proposed approaches considering the mentioned criterions against existing state-of-the-art techniques. <ref type="figure">Fig. 9</ref>. Training accuracy of the proposed models of RU-Net, and R2U-Net against ResU-Net and U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results 1) Retina Blood Vessel Segmentation Using the DRIVE Dataset</head><p>The precise segmentation results achieved with the proposed R2U-Net model are shown in <ref type="figure" target="#fig_0">Fig. 8. Figs. 9 and 10</ref> show the training and validation accuracy when using the DRIVE dataset. These figures show that the proposed R2U-Net and RU-Net models provide better performance during both the training and validation phase when compared to U-Net and ResU-Net. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Retina blood vessel segmentation on the STARE dataset</head><p>The experimental outputs of R2U-Net when using the STARE dataset are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. The training and validation accuracy for the STARE dataset is shown in <ref type="figure" target="#fig_0">Figs. 12  and 13</ref> respectively.</p><p>R2U-Net shows a better performance than all other models during training. In addition, the validation accuracy in <ref type="figure" target="#fig_0">Fig. 13</ref> demonstrates that the RU-Net and R2U-Net models provide better validation accuracy when compared to the equivalent U-Net and ResU-Net models. Thus, the performance demonstrates the effectiveness of the proposed approaches for segmentation tasks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) CHASE_DB1</head><p>For qualitative analysis, the example outputs of R2U-Net are shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> For quantitative analysis, the results are given in <ref type="table" target="#tab_1">Table I</ref>. From the table, it can be concluded that in all cases, the proposed RU-Net and R2U-Net models show better performance in terms of AUC and accuracy. The ROC for the highest AUCs for the R2U-Net model on each of the three retina blood vessel segmentation datasets is shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Skin Cancer Lesion Segmentation</head><p>In this implementation, this dataset is preprocessed with mean subtraction and normalized according to the standard deviation. We used the ADAM optimization technique with a learning rate of 2?10 -4 and binary cross entropy loss. In addition, we also calculated MSE error during the training and validation phase. In this case 10% of the samples are used for validation during training with a batch size of 32 and 150 epochs.</p><p>The training accuracy of the proposed models R2U-Net and RU-Net was compared with that of ResU-Net and U-Net for an end-to-end image based segmentation approach. The result is shown in <ref type="figure" target="#fig_0">Fig. 16</ref>. The validation accuracy is shown in <ref type="figure" target="#fig_0">Fig. 17</ref>. In both cases, the proposed models show better performance when compared with the equivalent U-Net and ResU-Net models. This clearly demonstrates the robustness of the proposed models in end-to-end image-based segmentation tasks.  The quantitative results of this experiment were compared against existing methods as shown in <ref type="table" target="#tab_1">Table II</ref>. Some of the example outputs from the testing phase are shown in <ref type="figure" target="#fig_0">Fig. 18</ref>. The first column shows the input images, the second column shows the ground truth, the network outputs are shown in the third column, and the fourth column demonstrates the final outputs after performing post processing with a threshold of 0.5. <ref type="figure" target="#fig_0">Figure 18</ref> shows promising segmentation results. In most cases, the target lesions are segmented accurately with almost the same shape of ground truth. However, if we observe the second and third rows in <ref type="figure" target="#fig_0">Fig. 18</ref>, it can be clearly seen that the input images contain two spots, one is a target lesion and the other bright spot which is not a target. This result is obtained even though the non-target lesion is brighter than the target lesion shown in the third row in <ref type="figure" target="#fig_0">Fig. 18</ref>. The R2U-Net model still segments the desired part accurately, which clearly shows the robustness of the proposed segmentation method.</p><p>We have compared the performance of the proposed approaches against recently published results with respect to sensitivity, specificity, accuracy, AUC, and DC. The proposed R2U-Net model provides a testing accuracy 0.9424 with a higher AUC, which is 0.9419. The average AUC for skin lesion segmentation is shown in <ref type="figure" target="#fig_0">Fig. 19</ref>. In addition, we calculated the average DC in the testing phase and achieved 0.8616, which is around 1.26% better than recently proposed alternatives <ref type="bibr" target="#b59">[62]</ref>. Furthermore, the JSC and F1 scores are calculated and the R2U-Net model obtains 0.9421 for JSC and 0.8920 for F1 score for skin lesion segmentation with t=3. These results are achieved with a R2U-Net model that only contains about 1.037 million (M) network parameters. Contrarily, the work presented in <ref type="bibr" target="#b58">[61]</ref> evaluated VGG-16 and Incpetion-V3 models for skin lesion segmentation, but those networks contained around 138M and 23M network parameters respectively. <ref type="figure" target="#fig_0">Fig. 18</ref>. This results demonstrates qualitative assessment of the proposed R2U-Net for skin cancer segmentation task with t=3. First column is the input sample, second column is ground truth, third column shows the outputs from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Lung Segmentation</head><p>Lung segmentation is very important for analyzing lung related diseases, and can be applied to lung cancer segmentation and lung pattern classification for identifying other problems. In this experiment, the ADAM optimizer is used with a learning rate of 2?10 -4 . We used binary cross entropy loss, and also calculated MSE during training and validation. In this case 10% of the samples were used for validation with a batch size of 16 and 150 epochs 150. <ref type="table" target="#tab_1">Table III</ref> shows the summary of how well the proposed models performed against equivalent U-Net and ResU-Net models. The experimental results show that the proposed models outperform the U-Net and ResU-Net models with same number of network parameters. <ref type="figure" target="#fig_0">Fig. 19</ref>. ROC-AUC for skin segmentation four models with t=2 and t=3.</p><p>Furthermore, many models struggle to define the class boundary properly during segmentation tasks <ref type="bibr" target="#b61">[64]</ref>. However, if we observe the experimental outputs shown in <ref type="figure" target="#fig_1">Fig. 20</ref>, the outputs in the third column show different hit maps on the border, which can be used to define the boundary of the lung region, while the ground truth tends to have a smooth boundary.</p><p>In addition, if we observe the input, ground truth, and output of this proposed approaches in the second row, it can be observed that the output of the proposed approaches shows better segmentation with appropriate contour. The ROC with AUCs are shown <ref type="figure" target="#fig_0">Fig. 21</ref>. The highest AUC is achieved with the proposed approach of R2U-Net with t=3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>Most of the cases, the networks are evaluated for different segmentation tasks with following architectures: 1?64?128?256?512?256 ? 128?64?1 that require 4.2M network parameters and 1?64?128?256?512?256 ? 128?64?1, which require about 8.5M network parameters respectively. However, we also experimented with U-Net, ResU-Net, RU-Net, and R2U-Net models with following structure: 1?16?32?64?128?64 ? 32?16?1. In this case we used a time-step of t=3, which refers to one forward convolution layer followed by three subsequent recurrent convolutional layers. This network was tested on skin and lung lesion segmentation. Though the number of network parameters increase little bit with respect to the time-step in the recurrent convolution layer, further improved performance can be clearly seen in the last rows of <ref type="table" target="#tab_1">Table II</ref> and III. Furthermore, we have evaluated both of the proposed models for patch-based modeling on retina blood vessel segmentation and end-to-end image-based methods for skin and lung lesion segmentation.</p><p>In both cases, the proposed models outperform existing stateof-the-art methods including ResU-Net and U-Net in terms of AUC and accuracy on all three datasets. The network architectures with different numbers of network parameters with respect to the different time-step are shown in <ref type="table" target="#tab_1">Table IV</ref>. The processing times during the testing phase for the STARE, CHASE_DB, and DRIVE datasets were 6.42, 8.66, and 2.84 seconds per sample respectively. In addition, skin cancer segmentation and lung segmentation take 0.22 and 1.145 seconds per sample respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational time</head><p>The computational time for testing per sample is shown in <ref type="table" target="#tab_5">Table V</ref> for blood vessel segmentation for retina images, skin cancer, and lung segmentation respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORKS</head><p>In this paper, we proposed an extension of the U-Net architecture using Recurrent Convolutional Neural Networks and Recurrent Residual Convolutional Neural Networks. The proposed models are called "RU-Net" and "R2U-Net" respectively. These models were evaluated using three different applications in the field of medical imaging including retina blood vessel segmentation, skin cancer lesion segmentation, and lung segmentation. The experimental results demonstrate that the proposed RU-Net, and R2U-Net models show better performance in segmentation tasks with the same number of network parameters when compared to existing methods including the U-Net and residual U-Net (or ResU-Net) models on all three datasets. In addition, results show that these proposed models not only ensure better performance during the training but also in testing phase. In future, we would like to explore the same architecture with a novel feature fusion strategy from encoding to the decoding units. <ref type="figure" target="#fig_0">Fig. 21</ref>. ROC curve for lung segmentation four models with t=2 and t=3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Medical image segmentation: retina blood vessel segmentation in the left, skin cancer lesion segmentation, and lung segmentation in the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>U-Net architecture consisted with convolutional encoding and decoding units that take image as input and produce the segmentation feature maps with respective pixel classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>RU-Net architecture with convolutional encoding and decoding units using recurrent convolutional layers (RCL) based U-Net architecture. The residual units are used with RCL for R2U-Net architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Different variant of convolutional and recurrent convolutional units (a) Forward convolutional units, (b) Recurrent convolutional block (c) Residual convolutional unit, and (d) Recurrent Residual convolutional units (RRCU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Unfolded recurrent convolutional units for t = 2 (left) and t = 3 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Example images from training dataset: left column from DRIVE dataset, middle column from STARE dataset and right column from CHASE-DB1 dataset. The first row shows the original images, second row shows fields of view (FOV), and third row shows the target outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Example patches in the left and corresponding outputs of patches are shown in the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Experimental outputs for DRIVE dataset using R2UNet: first row shows input image in gray scale, second row show ground truth, and third row shows the experimental outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Validation accuracy of the proposed models against ResU-Net and U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Experimental outputs of STARE dataset using R2UNet: first row shows input image after performing normalization, second row show ground truth, and third row shows the experimental outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Training accuracy in STARE dataset for R2U-Net, RU-Net, ResU-Net, and U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Validation accuracy in STARE dataset for R2U-Net, RU-Net, ResU-Net, and U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Qualitative analysis for CHASE_DB1 dataset. The segmentation outputs of 8 testing samples using R2U-Net. First row shows the input images, second row is ground truth, and third row shows the segmentation outputs using R2U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>AUC for retina blood vessel segmentation for the best performance achieved with R2U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Validation accuracy for skin lesion segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Md Zahangir Alom 1* , Student Member, IEEE, Mahmudul Hasan 2 , Chris Yakopcic 1 , Member, IEEE, Tarek M. Taha 1 , Member, IEEE, and Vijayan K. Asari 1 , Senior Member, IEEE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>EXPERIMENTAL RESULTS OF PROPOSED APPROACHES FOR RETINA BLOOD VESSEL SEGMENTATION AND COMPARISON AGAINST OTHER TRADITIONAL AND DEEP LEARNING-BASED APPROACHES.</figDesc><table><row><cell>Dataset</cell><cell>Methods</cell><cell>Year</cell><cell>F1-score</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>AUC</cell></row><row><cell>DRIVE</cell><cell>Chen [53]</cell><cell>2014</cell><cell>-</cell><cell>o.7252</cell><cell>0.9798</cell><cell>0.9474</cell><cell>0.9648</cell></row><row><cell></cell><cell>Azzopardi [54]</cell><cell>2015</cell><cell>-</cell><cell>0.7655</cell><cell>0.9704</cell><cell>0.9442</cell><cell>0.9614</cell></row><row><cell></cell><cell>Roychowdhury[55]</cell><cell>2016</cell><cell>-</cell><cell>0.7250</cell><cell>0.9830</cell><cell>0.9520</cell><cell>0.9620</cell></row><row><cell></cell><cell>Liskowsk [56]</cell><cell>2016</cell><cell>-</cell><cell>0.7763</cell><cell>0.9768</cell><cell>0.9495</cell><cell>0.9720</cell></row><row><cell cols="2">Qiaoliang Li [57] U-Net Fig. 16. Training accuracy for skin lesion segmentation.</cell><cell>2016 2018</cell><cell>-0.8142</cell><cell>0.7569 0.7537</cell><cell>0.9816 0.9820</cell><cell>0.9527 0.9531</cell><cell>0.9738 0.9755</cell></row><row><cell></cell><cell>Residual U-Net</cell><cell>2018</cell><cell>0.8149</cell><cell>0.7726</cell><cell>0.9820</cell><cell>0.9553</cell><cell>0.9779</cell></row><row><cell></cell><cell>Recurrent U-Net</cell><cell>2018</cell><cell>0.8155</cell><cell>0.7751</cell><cell>0.9816</cell><cell>0.9556</cell><cell>0.9782</cell></row><row><cell></cell><cell>R2U-Net</cell><cell>2018</cell><cell>0.8171</cell><cell>0.7792</cell><cell>0.9813</cell><cell>0.9556</cell><cell>0.9784</cell></row><row><cell>STARE</cell><cell>Marin et al. [58]</cell><cell>2011</cell><cell>-</cell><cell>0.6940</cell><cell>0.9770</cell><cell>0.9520</cell><cell>0.9820</cell></row><row><cell></cell><cell>Fraz [59]</cell><cell>2012</cell><cell>-</cell><cell>0.7548</cell><cell>0.9763</cell><cell>0.9534</cell><cell>0.9768</cell></row><row><cell></cell><cell>Roychowdhury[55]</cell><cell>2016</cell><cell>-</cell><cell>0.7720</cell><cell>0.9730</cell><cell>0.9510</cell><cell>0.9690</cell></row><row><cell></cell><cell>Liskowsk [56]</cell><cell>2016</cell><cell>-</cell><cell>0.7867</cell><cell>0.9754</cell><cell>0.9566</cell><cell>0.9785</cell></row><row><cell></cell><cell>Qiaoliang Li [57]</cell><cell>2016</cell><cell>-</cell><cell>0.7726</cell><cell>0.9844</cell><cell>0.9628</cell><cell>0.9879</cell></row><row><cell></cell><cell>U-Net</cell><cell>2018</cell><cell>0.8373</cell><cell>0.8270</cell><cell>0.9842</cell><cell>0.9690</cell><cell>0.9898</cell></row><row><cell></cell><cell>Residual U-Net</cell><cell>2018</cell><cell>0.8388</cell><cell>0.8203</cell><cell>0.9856</cell><cell>0.9700</cell><cell>0.9904</cell></row><row><cell></cell><cell>Recurrent U-Net</cell><cell>2018</cell><cell>0.8396</cell><cell>0.8108</cell><cell>0.9871</cell><cell>0.9706</cell><cell>0.9909</cell></row><row><cell></cell><cell>R2U-Net</cell><cell>2018</cell><cell>0.8475</cell><cell>0.8298</cell><cell>0.9862</cell><cell>0.9712</cell><cell>0.9914</cell></row><row><cell>CHASE_DB1</cell><cell>Fraz [59]</cell><cell>2012</cell><cell>-</cell><cell>0.7224</cell><cell>0.9711</cell><cell>0.9469</cell><cell>0.9712</cell></row><row><cell></cell><cell>Fraz [60]</cell><cell>2014</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.9524</cell><cell>0.9760</cell></row><row><cell></cell><cell>Azzopardi [54]</cell><cell>2015</cell><cell>-</cell><cell>0.7655</cell><cell>0.9704</cell><cell>0.9442</cell><cell>0.9614</cell></row><row><cell></cell><cell>Roychowdhury[55]</cell><cell>2016</cell><cell>-</cell><cell>0.7201</cell><cell>0.9824</cell><cell>0.9530</cell><cell>0.9532</cell></row><row><cell></cell><cell>Qiaoliang Li [57]</cell><cell>2016</cell><cell>-</cell><cell>0.7507</cell><cell>0.9793</cell><cell>0.9581</cell><cell>0.9793</cell></row><row><cell></cell><cell>U-Net</cell><cell>2018</cell><cell>0.7783</cell><cell>0.8288</cell><cell>0.9701</cell><cell>0.9578</cell><cell>0.9772</cell></row><row><cell></cell><cell>Residual U-Net</cell><cell>2018</cell><cell>0.7800</cell><cell>0.7726</cell><cell>0.9820</cell><cell>0.9553</cell><cell>0.9779</cell></row><row><cell></cell><cell>Recurrent U-Net</cell><cell>2018</cell><cell>0.7810</cell><cell>0.7459</cell><cell>0.9836</cell><cell>0.9622</cell><cell>0.9803</cell></row><row><cell></cell><cell>R2U-Net</cell><cell>2018</cell><cell>0.7928</cell><cell>0.7756</cell><cell>0.9820</cell><cell>0.9634</cell><cell>0.9815</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc>EXPERIMENTAL RESULTS OF PROPOSED APPROACHES FOR SKIN CANCER LESION SEGMENTATION AND COMPARISON AGAINST OTHER EXISTING APPROACHES. JACCARD SIMILARITY SCORE (JSC).</figDesc><table><row><cell cols="4">network, and fourth column show the final resulting after performing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>thresholding with 0.5.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Year</cell><cell>SE</cell><cell>SP</cell><cell>JSC</cell><cell>F1-score</cell><cell>AC</cell><cell>AUC</cell><cell>DC</cell></row><row><cell>Conv. classifier VGG-16 [61]</cell><cell>2017</cell><cell>0.533</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.6130</cell><cell>0.6420</cell><cell>-</cell></row><row><cell>Conv. classifier Inception-v3[61]</cell><cell>2017</cell><cell>0.760</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.6930</cell><cell>0.7390</cell><cell>-</cell></row><row><cell>Melanoma detection [62]</cell><cell>2017</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>o.9340</cell><cell>-</cell><cell>0.8490</cell></row><row><cell>Skin Lesion Analysis [63]</cell><cell>2017</cell><cell>0.8250</cell><cell>0.9750</cell><cell>-</cell><cell>-</cell><cell>0.9340</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net (t=2)</cell><cell>2018</cell><cell>0.9479</cell><cell>0.9263</cell><cell>0.9314</cell><cell>0.8682</cell><cell>0.9314</cell><cell>0.9371</cell><cell>0.8476</cell></row><row><cell>ResU-Net (t=2)</cell><cell>2018</cell><cell>0.9454</cell><cell>0.9338</cell><cell>0.9367</cell><cell>0.8799</cell><cell>0.9367</cell><cell>0.9396</cell><cell>0.8567</cell></row><row><cell>RecU-Net (t=2)</cell><cell>2018</cell><cell>0.9334</cell><cell>0.9395</cell><cell>0.9380</cell><cell>0.8841</cell><cell>0.9380</cell><cell>0.9364</cell><cell>0.8592</cell></row><row><cell>R2U-Net (t=2)</cell><cell>2018</cell><cell>0.9496</cell><cell>0.9313</cell><cell>0.9372</cell><cell>0.8823</cell><cell>0.9372</cell><cell>0.9405</cell><cell>0.8608</cell></row><row><cell>R2U-Net (t=3)</cell><cell>2018</cell><cell>0.9414</cell><cell>0.9425</cell><cell>0.9421</cell><cell>0.8920</cell><cell>0.9424</cell><cell>0.9419</cell><cell>0.8616</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>ARCHITECTURE AND NUMBER OF NETWORK PARAMETERS.</figDesc><table><row><cell>t</cell><cell>Network architectures</cell><cell>Number of parameters</cell></row><row><cell></cell><cell></cell><cell>(million)</cell></row><row><cell>2</cell><cell>1-&gt; 16-&gt;32-&gt;64&gt;128-&gt;64 -&gt; 32-</cell><cell>0.845</cell></row><row><cell></cell><cell>&gt;16-&gt;1</cell><cell></cell></row><row><cell>3</cell><cell>1-&gt; 16-&gt;32-&gt;64&gt;128-&gt;64 -&gt; 32-</cell><cell>1.037</cell></row><row><cell></cell><cell>&gt;16-&gt;1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III .</head><label>III</label><figDesc>EXPERIMENTAL OUTPUTS OF PROPOSED MODELS OF RU-NET AND R2U-NET FOR LUNG SEGMENTATION AND COMPARISON AGAINST RESU-NET AND U-NET MODELS.Fig. 20. Qualitative assessment of R2U-Net performance on Lung segmentation dataset: first column input images, second column ground truth, and third column outputs with R2U-Net.</figDesc><table><row><cell>Methods</cell><cell>Year</cell><cell>SE</cell><cell>SP</cell><cell>JSC</cell><cell>F1-Score</cell><cell>AC</cell><cell>AUC</cell></row><row><cell>U-Net (t=2)</cell><cell>2018</cell><cell>0.9696</cell><cell>0.9872</cell><cell>0.9858</cell><cell>0.9658</cell><cell>0.9828</cell><cell>0.9784</cell></row><row><cell>ResU-Net(t=2)</cell><cell>2018</cell><cell>0.9555</cell><cell>0.9945</cell><cell>0.9850</cell><cell>0.9690</cell><cell>0.9849</cell><cell>0.9750</cell></row><row><cell>RU-Net (t=2)</cell><cell>2018</cell><cell>0.9734</cell><cell>0.9866</cell><cell>0.9836</cell><cell>0.9638</cell><cell>0.9836</cell><cell>0.9800</cell></row><row><cell>R2U-Net (t=2)</cell><cell>2018</cell><cell>0.9826</cell><cell>0.9918</cell><cell>0.9897</cell><cell>0.9780</cell><cell>0.9897</cell><cell>0.9872</cell></row><row><cell>R2U-Net (t=3)</cell><cell>2018</cell><cell>0.9832</cell><cell>0.9944</cell><cell>0.9918</cell><cell>0.9823</cell><cell>0.9918</cell><cell>0.9889</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V .</head><label>V</label><figDesc>COMPUTATIONAL TIME FOR TESTING PHASE.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Time (Sec.)/ sample</cell></row><row><cell>Blood vessel</cell><cell>DRIVE</cell><cell>6.42</cell></row><row><cell>segmentation</cell><cell>STARE</cell><cell>8.66</cell></row><row><cell></cell><cell>CHASE_DB1</cell><cell>2.84</cell></row><row><cell cols="2">Skin cancer segmentation</cell><cell>0.22</cell></row><row><cell cols="2">Lung segmentation</cell><cell>1.15</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transferring rich feature hierarchies for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04587</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ciresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?zg?n</forename><surname>?i?ek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated anatomical landmark detection ondistal femur surface using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Biomedical Imaging (ISBI)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-modal vertebrae recognition using transformed deep convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunliang</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Thresholding based on histogram approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings-Vision</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computer aided segmentation of medical images based on hybridized approach of edge and region based techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit Kumar</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Mathematical Biology&apos;, Mathematical Biology Recent Trends by Anamaya Publishers</title>
		<meeting>International Conference on Mathematical Biology&apos;, Mathematical Biology Recent Trends by Anamaya Publishers</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in ND images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference on</title>
		<meeting>Eighth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayit</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Bram Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1153" to="1159" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>23] S. Song, S. P</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The virtual skeleton database: an open access repository for biomedical research and collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kistler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Conditional random fields as recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Road Extraction by Deep Residual U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10684</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">DeepUNet: A Deep Fully Convolutional Network for Pixel-level Sea-Land Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruirui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00201</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CNN-based Segmentation of Medical Imaging Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Kayalibay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grady</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03056</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">International Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
	<note>The importance of skip connections in biomedical image segmentation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dcan: Deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">International Workshop on Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Mckinley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Nabla-net: A Deep Dag-Like Convolutional Architecture for Biomedical Image Segmentation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3D deeply supervised network for automated segmentation of volumetric medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="40" to="54" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Voxresnet: Deep voxelwise residual networks for volumetric brain segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05895</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Inception Recurrent Convolutional Neural Network for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zahangir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07709</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improved Inception-Residual Convolutional Neural Network for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zahangir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joes</forename><surname>Staal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automated vessel segmentation using infinite perimeter active contour model with hybrid region information with application to retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1797" to="1807" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-D Gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation methodologies in retinal images-a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moazam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="407" to="433" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The distribution of the flora in the alpine zone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New phytologist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discriminative vessel segmentation in retinal images by fusing context-aware hybrid features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine vision and applications 25</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1779" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Trainable COSFIRE filters for vessel delineation with application to retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Azzopardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation of fundus images by major vessel extraction and subimage classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><forename type="middle">D</forename><surname>Sohini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshab</forename><forename type="middle">K</forename><surname>Koozekanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1118" to="1128" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Segmenting Retinal Blood Vessels With Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe?</forename><surname>Liskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Krawiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2369" to="2380" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A cross-modality learning approach for vessel segmentation in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A new supervised method for blood vessel segmentation in retinal images by using gray-level and moment invariants-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An ensemble classification-based approach applied to retinal blood vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moazam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="2538" to="2548" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Delineation of blood vessels in pediatric retinal images using decision trees-based ensemble classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moazam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="795" to="811" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking Skin Lesion Segmentation in a Convolutional Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05006</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Skin Lesion Analysis Towards Melanoma Detection Using Deep Learning Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00577</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Contour extraction in medical images using initial boundary pixel selection and segmental contour following</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaoming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multidimensional Systems and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="469" to="498" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zahangir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01164</idno>
		<title level="m">The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
