<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LogoDet-3K: A Large-Scale Image Dataset for Logo Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vol</forename><forename type="middle">X</forename><surname>Multimedia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>No</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Month</forename><surname>Xx</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Year</surname></persName>
						</author>
						<title level="a" type="main">LogoDet-3K: A Large-Scale Image Dataset for Logo Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Logo detection has been gaining considerable attention because of its wide range of applications in the multimedia field, such as copyright infringement detection, brand visibility monitoring, and product brand management on social media. In this paper, we introduce LogoDet-3K, the largest logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. We describe the collection and annotation process of our dataset, analyze its scale and diversity in comparison to other datasets for logo detection. We further propose a strong baseline method Logo-Yolo, which incorporates Focal loss and CIoU loss into the state-of-the-art YOLOv3 framework for large-scale logo detection. Logo-Yolo can solve the problems of multi-scale objects, logo sample imbalance and inconsistent bounding-box regression. It obtains about 4% improvement on the average performance compared with YOLOv3, and greater improvements compared with reported several deep detection models on LogoDet-3K. The evaluations on other three existing datasets further verify the effectiveness of our method, and demonstrate better generalization ability of LogoDet-3K on logo detection and retrieval tasks. The LogoDet-3K dataset is used to promote large-scale logo-related research and it can be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Logo-related research has always been extensively studied in the field of multimedia <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. As an important branch of logo research, logo detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> plays a critical role for its various applications and services, such as intelligent transportation <ref type="bibr" target="#b8">[9]</ref>, brand visibility monitoring <ref type="bibr" target="#b9">[10]</ref> and analysis <ref type="bibr" target="#b10">[11]</ref>, trademark infringement detection <ref type="bibr" target="#b0">[1]</ref> and video advertising research <ref type="bibr" target="#b11">[12]</ref>.</p><p>Currently, deep-learning approaches have been widely used in logo detection, like Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>, SSD <ref type="bibr" target="#b13">[14]</ref> and YOLOv3 <ref type="bibr" target="#b14">[15]</ref>. By supporting the learning process of deep networks with millions of parameters, large-scale logo datasets are crucial in logo detection. However, most existing logo researches focus on small-scale datasets, such as BelgaLogos <ref type="bibr" target="#b15">[16]</ref> and FlickrLogos-32 <ref type="bibr" target="#b1">[2]</ref>. Recently, although some large-scale logo datasets are proposed for recognition and detection, such as WebLogo-2M <ref type="bibr" target="#b16">[17]</ref>, PL2K <ref type="bibr" target="#b17">[18]</ref> and Logo-2K+ <ref type="bibr" target="#b18">[19]</ref>, these logo datasets are either only labeled on imagelevel <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref> or not publicly available <ref type="bibr" target="#b17">[18]</ref>. As we all known, the emergence of large-scale datasets with a diverse and general set of objects, like ImageNet DET <ref type="bibr" target="#b19">[20]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> has contributed greatly to rapid advances of object detection. As a special case of object detection, compared with ImageNet DET <ref type="bibr" target="#b19">[20]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref>, existing logo detection benchmarks lack a large number of categories and well-defined annotations.</p><p>Therefore, we introduce LogoDet-3K, a new large-scale, high-quality logo detection dataset. Compared with existing logo datasets, LogoDet-3K has three distinctive characteristics: (1) Large-scale. LogoDet-3K consists of 3,000 logo categories, 158,652 images and 194,261 bounding boxes. It has larger coverage on logo categories and larger quantity on annotated objects compared with existing logo datasets.</p><p>(2) High-quality. Each image in the construction progress is strictly conformed to the pipeline which is carefully designed, including logo image collection, logo image filtering and logo object annotation. (3) High-challenge. Logo objects typically consist of mixed text and graphic symbols. Even the same logo can appear in different scenarios such as various nonrigid, coloring and lighting transformations. For example, a rigid logo object when appearing in a real clothing image often becomes non-rigid, making it difficult to be detected. As arXiv:2008.05359v1 [cs.CV] 12 Aug 2020 shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our proposed LogoDet-3K dataset far exceeds the existing logo dataset both in the number of categories and the number of images. <ref type="figure" target="#fig_1">Fig. 2</ref> gives some image samples from various categories of LogoDet-3K. In addition, imbalanced samples and very small logo objects make this dataset more challenging.</p><p>We further propose a strong baseline method Logo-Yolo based on the network architecture YOLOv3 for logo detection. Logo-Yolo takes characteristics of LogoDet-3K, such as various logo object sizes, sample imbalance and different background scenarios into consideration, and incorporates Focal Loss <ref type="bibr" target="#b21">[22]</ref> into the state-of-the-art detection framework YOLOv3 for logo detection. CIoU loss <ref type="bibr" target="#b22">[23]</ref> is further adopted to obtain more accurate regression results. Finally, we conduct comprehensive experiments on LogoDet-3K using several state-of-the-art object detection models and our proposed method, as well as ablation study and qualitative analysis. This paper has three main contributions. (1) We introduce a new large-scale logo dataset LogoDet-3K 1 with 3,000 classes, 194,261 objects and 158,652 images, which is the largest logo classes with full annotation. <ref type="bibr" target="#b1">(2)</ref> We propose a strong baseline method Logo-Yolo, which adopts the YOLOv3 detection framework, and combines Focal loss and CIoU loss to achieve better detection performance on LogoDet-3K. <ref type="bibr" target="#b2">(3)</ref> We perform extensive experiments on LogoDet-3K by using <ref type="bibr" target="#b0">1</ref> We will release the dataset upon publication. several baseline models and our method, and further verify the effectiveness of our method and better generalization ability of LogoDet-3K on logo detection and retrieval tasks.</p><p>The rest of this paper is organized as follows. Section II reviews related work. Section III given the process of datasets construction and statistics. And Section IV elaborates the proposed large-scale logo detection method. Experimental results and analysis are reported in Section V. Finally, we conclude the paper and give future work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work is closely related to two research fields: (1) logo detection datasets and (2) logo detection researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Logo Detection Datasets</head><p>The large-scale dataset is an important factor for supporting advanced object detection algorithms, especially in the deep learning era, and it is no exception in logo detection. The first benchmark for logo detection is the BelgaLogos dataset <ref type="bibr" target="#b15">[16]</ref>, which contains only 37 logo categories totaling 1,000 images. Over the years, some larger logo datasets such as FlickrLogos-32 <ref type="bibr" target="#b1">[2]</ref> and Logos in the wild <ref type="bibr" target="#b23">[24]</ref> have been proposed. However, these datasets lack the diversity and coverage in logo categories and images. For example, FlickrLogos-32 only consists of 32 logo categories with 70 images each category. This is far less than millions of images required  <ref type="bibr" target="#b17">[18]</ref>. However, WebLogo-2M is collected from online search engines and just automatically be labeled at image level with much noise, while PL2K and LOGO-Net are not publicly available.</p><p>In order to solve the problem, we propose the LogoDet-3K, which is a large-scale, high-coverage and high-quantity dataset with 3,000 logo categories, 158,652 images and 194,261 objects. <ref type="table" target="#tab_0">Table I</ref> summarizes the statistics of existing logo datasets and LogoDet-3K. We can see that LogoDet-3K has more logo categories and logo objects, which is more helpful to explore data-driven deep learning techniques for logo detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Logo Detection</head><p>In previous years, DPM <ref type="bibr" target="#b30">[31]</ref> and HOG <ref type="bibr" target="#b24">[25]</ref>, are widely used as traditional object detection methods. Later, with the development of convolutional neural networks, more and more works start to utilize deep learning techniques, such as Faster RCNN <ref type="bibr" target="#b12">[13]</ref>, YOLO <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b31">[32]</ref> self-attention for logo detection. In general, deep learning based object detector could be divided into two types: two-stage detector and single-stage detector. The popular two-stage detectors are the series of R-CNN like Faster RCNN <ref type="bibr" target="#b12">[13]</ref>, which introduced the region proposal network and individual blocks to improve the detection performance. In contrast, the paradigm of single-stage detector aims to be faster and more efficient solution by classifying anchors directly and then refining them without proposal generation network, such as SSD <ref type="bibr" target="#b13">[14]</ref>, RetinaNet <ref type="bibr" target="#b21">[22]</ref> and YOLO series <ref type="bibr" target="#b14">[15]</ref>. Recently, the proposed anchor-free method CornerNet <ref type="bibr" target="#b32">[33]</ref> is highly acclaimed, while SNIPER <ref type="bibr" target="#b33">[34]</ref> and Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> are introduced to further improve the performance.</p><p>In general, logo detection has little advanced as a kind of generic object detection. An important reason is that the development of logo detection technology is limited by the size of logo dataset. Early logo detection methods are established on hand-crafted visual features (e.g. SIFT and HOG <ref type="bibr" target="#b24">[25]</ref>) and conventional classification models (e.g. SVM <ref type="bibr" target="#b2">[3]</ref>). Recently, some deep learning techniques have been applied in logo detection <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b37">[38]</ref>. For example, Oliveira et al. <ref type="bibr" target="#b38">[39]</ref> adopted pre-trained CNN models and used them as a part of Fast Region-Based Convolutional Networks recognition pipeline. Feh?rv?ri et al. <ref type="bibr" target="#b17">[18]</ref> combined metric learning and basic object detection networks to achieve few-shot logo detection. Compared with existing logo detectors, our proposed Logo-Yolo is more effective for large-scale logo category and sample imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LOGODET-3K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Construction</head><p>The construction of LogoDet-3K is comprised of three steps, namely logo image collection, logo image filtering and logo object annotation. Each image is manually examined and reviewed to guarantee the quality of LogoDet-3K after filtering and annotation. The dataset building process is detailed in the following subsections. Additionally, each logo name is assigned to one of nine super-classes based on the daily need of life and the main positioning of common enterprises, namely Clothing, Food, Transportation, Electronics, Necessities, Leisure, Medicine, Sport and Others. In this paper, <ref type="table" target="#tab_0">Table II</ref> gives the statistics of super classes of LogoDet-3K dataset.</p><p>Logo Image Collection. A large-scale logo detection dataset should include comprehensive categories. Before crawling logo images, we built a comprehensive logo list based on the 'Forbes Global 2,000' 2 and other famous logo lists. Finally, we collected 3,000 logo names for our logo vocabulary, which covers nine super-classes.</p><p>Subsequently, we used the logo name from the logo vocabulary as the query to crawl logo images from the Google search engine. Top-500 retrieved results were kept for the logo   relevance for each query. In order to increase diversity of the dataset, we also crawled logo images from other online search engines including Bing and Baidu. In order to crawl more relevant images, we changed the search terms by adding 'brand' or 'logo' in search keywords. For example, there were so many images of shoes without any logo in the 'Clarks' category, which is a famous British shoe company. We extended the search term such as 'Clarks brand' or 'Clarks logo' and obtained more relevant logo images as we expected.</p><p>Logo Image Filtering. To guarantee the data quality, we cleaned the collected images manually before annotating them. Considering that not all the logo images are acceptable, we check each logo category to guarantee that it contained corresponding logo images with a suitable size and aspect ratio via both automatic processing and manual cleaning. Particularly, we removed the following logo images, including: (1) images with length or height less than 300 pixels or extreme aspect ratio, (2) images with extreme aspect ratio, (3) duplicated images, (4) images without logos and (5) images with logos were not included in the logo vocabulary. In addition, a brand may have different types of logos, such as a symbolic logo and a textual logo or even more. In this case, different types of logos should be treated as different logo categories for this brand similar to <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> shows some examples, the suffix '-1', '-2' is added to the logo name as the new logo category, such as the 'Lexus-1' presents the 'Lexus' symbolic logo while 'Lexus-2' presents its textual logo for the brand 'Lexus'.</p><p>Logo Object Annotation. As the most important step in constructing logo detection datasets, the annotation process takes a lot of time. The final annotation results follow some criterions. For example, if the logo is occluded, the annotators are instructed to draw the box around its visible parts. If an image contains multiple logo instances, each logo object needs to be annotated. In order to ensure the annotation quality of LogoDet-3K, each bounding box was annotated manually as close as possible to the logo object to avoid extra backgrounds. After finishing the above works, we inspected and examined all the annotated images labeled by the annotators. If an annotated image does not meet these requirements, the image will be rejected and need to be re-annotating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Statistics</head><p>Our resulting LogoDet-3K consists of 3,000 logo classes, 158,652 images and 194,261 logo objects. To delve into the  details of our dataset, we provide the statistics at the superclass and category level. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the distribution of images for each logo in LogoDet-3K. The thicker the columnar area in histogram, the larger the proportion. From <ref type="figure" target="#fig_3">Fig. 4</ref>, we can see that imbalanced distribution across different logo categories are one characteristic of LogoDet-3K, posing a challenge for effective logo detection with few samples.</p><p>In addition, <ref type="figure" target="#fig_4">Fig. 5</ref> summarizes the distribution of images and categories in LogoDet-3K. <ref type="figure" target="#fig_4">Fig. 5 (A)</ref> shows the distribution of the number of images for each category. <ref type="figure" target="#fig_4">Fig. 5 (B)</ref> shows the distribution of the number of objects of each class. As we can see, there exists imbalanced distribution across different logo objects and images for different logo categories. <ref type="figure" target="#fig_4">Fig. 5</ref> (C) gives the number of objects in each image. We can see that most images contain one or two logo objects. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref> (D), LogoDet-3K is composed of 4.81% small instances (area &lt; 32 2 ), 29.79% medium instances (32 2 &lt;= area &lt;= 96 2 ) and 65.40% large instances (area &gt; 96 2 ). The large percentage of small and medium logo objects (? 35%) will create another challenge to logo detection on this dataset, since small logos are harder to detect.</p><p>We also provide the statistics of logo categories, images and logo objects in 9 different super classes in <ref type="figure" target="#fig_5">Fig. 6</ref>, which can direct to getting the difference on numbers. The Food, Clothes and Necessities class are larger in objects and images compared with other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>Taking characteristics of LogoDet-3K into consideration, we propose a strong baseline Logo-Yolo for logo detection, which adopted the state-of-the-art deep detector YOLOv3 as the backbone to cope with small-scale and multi-scale logos. Since the logo image contains fewer objects, there will be conducted more negative samples and hard samples, we utilized Focal Loss <ref type="bibr" target="#b21">[22]</ref> to solve the problem of logo sample imbalance.</p><p>In addition, we adopted K-means clustering statistics to recompute the pre-anchors size for LogoDet-3K to select the best anchor size, and introduced recent proposed CIoU loss <ref type="bibr" target="#b22">[23]</ref> to obtain more accurate regression results.</p><p>Improved Losses for Logo Detection. Fewer logo objects in the image produce more negative samples, leading to an imbalance between positive and negative samples. Focal Loss <ref type="bibr" target="#b21">[22]</ref> is proposed to solve the problem of sample imbalance. Therefore, we incorporates the Focal Loss into the whole loss of Logo-Yolo, the classification loss is formulated as follows:</p><formula xml:id="formula_0">Focal Loss = ??(1 ? y ) ? log y , y = 1 ?(1 ? ?)y ? log(1 ? y ) , y = 0<label>(1)</label></formula><p>where y ? {?1} is a ground-truth class and y ? [0, 1] is the model's estimated probability by activation function. Focus loss introduces two factors ? and ?, where ? is used to balance positive and negative samples, while ? focuses more on difficult samples. In addition, L n -norm loss is widely adopted for bounding box regression, while it is not tailored to the evaluation metric (Intersection over Union (IoU)) in existing methods. We further incoporate the CIoU loss <ref type="bibr" target="#b22">[23]</ref> into the whole loss of YOLOv3 to solve the problem of inconsistency between the metric and the border regression on logo detection, and the IoU-based loss can be defined as,</p><formula xml:id="formula_1">L CIoU = 1 ? IoU + R CIoU (B pd , B gt )<label>(2)</label></formula><p>where R CIoU is penalty term for predicted box B pd and target box B gt . CIoU loss considered three geometric factors in the bounding box regression, including overlap area, central point distance and aspect ratio to solve the problem of inconsistency between the metric and the border regression during logo detection. Therefore, the method to minimize the normalized distance between central points of two bounding boxes, and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Classes <ref type="table" target="#tab_0">Images Objects Trainval Test  LogoDet-3K-1000 1,000 85,344 101,345 75,785 11,236  LogoDet-3K-2000 2,000 116,393 136,815 103,356 13,037  LogoDet-3K  3,000 158,652 194,261 142,142 16,510</ref> the penalty term can be defined as,</p><formula xml:id="formula_2">R CIoU = ? 2 (b, b gt ) c 2 + ? 4 ? 2 (arc tan w gt h gt ? arc tan w h ) 2<label>(3)</label></formula><p>where b and b gt denote the central points of B pd and B gt , ?(?) is the Euclidean distance, and c is the diagonal length of the smallest enclosing box covering the two boxes. ? is a positive trade-off parameter. w, h are aspect ratio of the prediected box, respectively. Pre-anchors Design for Logo Detection. Anchor boxes are a set of initial fixed width-and-height candidate boxes. Those defined by the original network are no longer suitable for LogoDet-3K. Therefore, we use K-means clustering algorithm to perform clustering analysis on the bounding boxes for objects of LogoDet-3K and then select the average overlap degree (Avg IoU) as the metric for clustering result analysis. We can obtain the number of anchor boxes based on the relationship between the number of samples and Avg IoU.</p><p>The aggregated Avg IoU objective function f can be expressed as,</p><formula xml:id="formula_3">f = argmax k i=1 N k j=1 I IoU (B, C) N<label>(4)</label></formula><p>where B represents the ground-truth sample and C represents the center of the cluster. N represents the total number of samples, k represents the number of clusters. In general, we adopt the K-means clustering algorithm to select the number of candidate anchor boxes and aspect ratio dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT A. Experimental Setup</head><p>For parameter settings, we design pre-anchor boxes for different object detectors via calculations on LogoDet-3K dataset. In our method, the number of anchor boxes is set For the evaluation metric, we use mean Average Precision (mAP) <ref type="bibr" target="#b39">[40]</ref> and the IoU threshold is 0.5, which means that a detection will be considered as positive if the IoU between the predicted box and ground-truth box exceeds 50%.</p><p>For the experiment datasets, we define various data subsets as different benchmarks by means of random division on the overall LogoDet-3K dataset. Particularly, we divide the LogoDet-3K dataset into three subsets including 1,000, 2,000 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Classes <ref type="table" target="#tab_0">Images Objects Trainval Test  Food  932  53,350 64,276  47,321 6,029  Clothes  604  31,266 37,601  27,732 3,534  Necessities  432  24,822 30,643</ref> 22,017 2,805 ResNet-101 52.10 FPN <ref type="bibr" target="#b40">[41]</ref> ResNet-101 49.63 Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> ResNet-101 48.14 Distance-IoU <ref type="bibr" target="#b22">[23]</ref> DarkNet-53 53.06 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> DarkNet-53 55.21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo-Yolo</head><p>DarkNet-53 58.86</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LogoDet-3K-2000</head><p>Faster RCNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 41.86 SSD <ref type="bibr" target="#b13">[14]</ref> VGGNet-16 38.97 RetinaNet <ref type="bibr" target="#b21">[22]</ref> ResNet-101 49.00 FPN <ref type="bibr" target="#b40">[41]</ref> ResNet-101 47.91 Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> ResNet-101 46.32 Distance-IoU <ref type="bibr" target="#b22">[23]</ref> DarkNet-53 51.69 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> DarkNet-53 52.32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo-Yolo</head><p>DarkNet-53 56.42</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LogoDet-3K</head><p>Faster RCNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 38.30 SSD <ref type="bibr" target="#b13">[14]</ref> VGGNet-16 34.47 RetinaNet <ref type="bibr" target="#b21">[22]</ref> ResNet-101 44.32 FPN <ref type="bibr" target="#b40">[41]</ref> ResNet-101 42.84 Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> ResNet-101 41.23 Distance-IoU <ref type="bibr" target="#b22">[23]</ref> DarkNet-53 46.34 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> DarkNet-53 48.61</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo-Yolo</head><p>DarkNet-53 52.28 and 3,000 categories, respectively. Through those experiments, we verify the robustness of our method as the number of categories and images increases. The statistics of three subdatasets are shown in <ref type="table" target="#tab_0">Table III</ref>. In addition, we conduct experiments based on super categories. The categories with the largest number of the three categories are also common logo categories in real world, including Food, Clothes, and Necessities. This experiment is to explore the detection effect of our method on common categories and the characteristics of the three categories of datasets. The statistics of three subsets from these super categories are shown in <ref type="table" target="#tab_0">Table IV</ref>. Experiments are performed with state-of-the-art object detectors: Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>, SSD <ref type="bibr" target="#b13">[14]</ref>, RetinaNet <ref type="bibr" target="#b21">[22]</ref>, FPN <ref type="bibr" target="#b40">[41]</ref>, Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref>, Distance-IoU <ref type="bibr" target="#b22">[23]</ref> and YOLOv3 <ref type="bibr" target="#b14">[15]</ref>. For their backbones, we adopt the general setting: ResNet101 is selected as the backbone for Faster R-CNN, RetinaNet, FPN and Cascade R-CNN. Darknet-53 is used as the backbone of YOLOv3 and Distance-IoU, and VGGNet-16 <ref type="bibr" target="#b41">[42]</ref> for SSD. The experiments are conducted in PyTorch and DarkNet framework, GPU with the NVIDIA Tesla K80 and Tesla V100.   small logo objects and fewer objects for many images in realworld scenarios, and the one-stage method is more suitable for this case. Therefore, we use the one-stage YOLOv3 detector as the basis of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>We then compare the performance of Logo-Yolo with all baselines, and observe that Logo-Yolo achieves the best performance among these models. It's worth noting that mAP of Logo-Yolo is 58.86%, 56.42% and 52.28% on three benchmarks, and Logo-Yolo achieves the performance gain with 3.65%, 4.10% and 3.67% compared with YOLOv3 in <ref type="table" target="#tab_4">Table V</ref>. Our method Logo-Yolo detection performance achieves the best result on the 1000-2000-3000 datasets, which proves the stability of the method.</p><p>Some detection results of Logo-Yolo are given in <ref type="figure" target="#fig_7">Fig. 7</ref>, including the regression bounding box and the classification accuracy. The red box represents the prediction box and the green box is the ground-truth box. Clearly, Logo-Yolo can detect objects with occlusion, ambiguities and smaller, it obtains more accurate bounding box regression. And as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, the detector YOLOv3 makes some detection mistakes, such as treating a person or hamburger as logos, and thus the bounding boxes of detected logos are inaccurate, or missing. In contrast, our method obtains better performance both in the bounding box regression and the confidence of detected logos. In particular, our method has an advantage in small logo detection, such as the detected logos in the last two images in <ref type="figure" target="#fig_8">Fig. 8</ref>.</p><p>In addition, <ref type="table" target="#tab_0">Table VI</ref> gives the comparison of three superclasses on different methods. Compared with existing baselines, the Logo-Yolo detector also obtains better results with 56.73%, 61.32% and 61.43% on the super classes of Food, Clothes, and Necessities, respectively, which are 3.24%, 4.31% and 3.75% higher than YOLOv3. This experiment also illustrates the effectiveness of our method. As we can see from <ref type="table" target="#tab_0">Table VI</ref>, the number of Necessities categories is 172 less than the clothes categories, but relatively similar detection results have been obtained (61.32% vs 61.43%), indicating that the Necessities category dataset is more difficult to detect. Analyzing food logos with a large number of categories and images, the detection performance of the 932 food category   ResNet-101 51.46 FPN <ref type="bibr" target="#b40">[41]</ref> ResNet-101 51.10 Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> ResNet-101 52.46 Distance-IoU <ref type="bibr" target="#b22">[23]</ref> DarkNet-53 53.11 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> DarkNet-53 53.49</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo-Yolo</head><p>DarkNet-53 56.73</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clothes</head><p>Faster RCNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 51.63 SSD <ref type="bibr" target="#b13">[14]</ref> VGGNet-16 49.74 RetinaNet <ref type="bibr" target="#b21">[22]</ref> ResNet-101 55.98 FPN <ref type="bibr" target="#b40">[41]</ref> ResNet-101 55.62 Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> ResNet-101 56.90 Distance-IoU <ref type="bibr" target="#b22">[23]</ref> DarkNet-53 56.54 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> DarkNet-53 57.01</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo-Yolo</head><p>DarkNet-53 61.32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necessities</head><p>Faster RCNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 52.22 SSD <ref type="bibr" target="#b13">[14]</ref> VGGNet-16 50.03 RetinaNet <ref type="bibr" target="#b21">[22]</ref> ResNet-101 54.01 FPN <ref type="bibr" target="#b40">[41]</ref> ResNet-101 53.37 Cascade R-CNN <ref type="bibr" target="#b34">[35]</ref> ResNet-101 55.49 Distance-IoU <ref type="bibr" target="#b22">[23]</ref> DarkNet-53 57.20 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> DarkNet-53 57.68</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo-Yolo</head><p>DarkNet-53 61.43</p><p>is slightly lower than the 1000 subset (56.73% vs 58.86%). The result shows that food-related logo detection is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis</head><p>Since Logo-Yolo and YOLOv3 obtain better detection performance, we next focus on the analysis via the comparison between two methods.</p><p>Dataset Scale. According to <ref type="table" target="#tab_4">Table V</ref>, the drop of Logo-Yolo in mAP is 2.44% and 4.14% when the number of categories increases from 1,000 to 2,000 and 2,000 to 3,000. Compared with YOLOv3, our model achieves better performance than other baselines on datasets with different scales, which proves a higher robustness on LogoDet-3K. We further calculate the Precision and Recall to illustrate the accuracy and missed detection rate. We use the Precision-Recall curve to show the trade-off between Precision and Recall in <ref type="figure" target="#fig_6">Fig. 9</ref> between YOLOv3 and Logo-Yolo. The larger the enclosing area under the curve, the better the detection performance. As shown in <ref type="figure" target="#fig_6">Fig. 9</ref>, Logo-Yolo has significantly improved the recall rate, which indicates that our method alleviated the problem of missing small objects in logo detection.</p><p>Parameter Sensitivity. We evaluate the performance by varying different IoU thresholds from 0.5 to 0.8 at an interval of 0.05. As shown in <ref type="figure" target="#fig_0">Fig. 10 (Left)</ref>, Logo-Yolo (red curve) has a more stable performance improvement than YOLOv3 (blue curve) when changing the IoU threshold. We also set different iterations to compare the convergence and accuracy  Method mAP Faster RCNN <ref type="bibr" target="#b12">[13]</ref> 41.80 SSD <ref type="bibr" target="#b13">[14]</ref> 38.70 YOLO <ref type="bibr" target="#b42">[43]</ref> 44.58 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> 50.10 Logo-Yolo 52.17 Logo-Yolo (Pre-trained) 53.62 of models. <ref type="figure" target="#fig_0">Fig. 10</ref> (Right) shows higher performance with increasing iterations. It can be seen that our method converges at about 400,000 iterations and keeps higher accuracy than YOLOv3 in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We conduct a comprehensive analysis of effects of three sub-variables and two modules from Logo-Yolo. <ref type="table" target="#tab_0">Table VII</ref> shows an ablation study on the effects of different combinations of K-means, Focal Loss and CIoU loss. Firstly, three modules are added to YOLOv3, and the results improve 1.51%, 0.60% and 1.25%, which proves the effectiveness of the Pre-anchors Design, Focal Loss and CIoU loss, respectively. Then, we conduct the two modules experiments from Logo-Yolo. The result of Logo-Yolo is higher than Logo-Yolo without Pre-anchors Design, which explains the effectiveness of two losses. Similarly, compared to Logo-Yolo without Focal Loss or CIoU loss, our proposed method achieves improvement, which demonstrates the effectiveness of another two modules for Logo-Yolo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Generalization Ability on Logo Detection</head><p>To evaluate the robustness and generalization ability of Logo-Yolo architecture and its pre-trained models, we explore other two datasets Top-Logo-10 <ref type="bibr" target="#b26">[27]</ref> and FlickrLogos-32 <ref type="bibr" target="#b1">[2]</ref>. The former contains 10 unique logo classes with 70 images for each logo class, and the latter is a popular logo dataset with full annotations, comprising 8,240 images from 32 categories. Logo-Yolo (per-trained) first loades the model trained on LogoDet-3K, and is then trained on the target dataset while Logo-Yolo is directly trained on the target dataset with random parameter initialization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>mAP Bag of Words (BoW) <ref type="bibr" target="#b4">[5]</ref> 54.50 Deep Logo <ref type="bibr" target="#b36">[37]</ref> 74.40 BD-FRCN-M <ref type="bibr" target="#b38">[39]</ref> 73.50 Faster RCNN <ref type="bibr" target="#b12">[13]</ref> 70.20 YOLO <ref type="bibr" target="#b42">[43]</ref> 68.70 YOLOv3 <ref type="bibr" target="#b14">[15]</ref> 71.70 Logo-Yolo 74.62 Logo-Yolo (Pre-trained) 76.11</p><p>1.5 percent improvement after pre-training on LogoDet-3K, showing better generalization ability of LogoDet-3K. We can also see similar trends on FlickrLogo-32 in <ref type="table" target="#tab_0">Table IX</ref>. Overall, the evaluation on these two datasets verify the effectiveness of Logo-Yolo, and also shows better generalization ability of LogoDet-3K on other logo detection datasets.</p><p>In addition, we further select QMUL-OpenLogo dataset to evaluate the general object detection. This dataset is the largest publicly available logo detection dataset, and contains 352 categories and 27,038 images. To further exploit the fine-tuning capability of LogoDet-3K, we analyze the difference between LogoDet-3K pre-trained weights and QMUL-OpenLogo pretrained weights.</p><p>According to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Generalization Ability on Logo Retrieval</head><p>For the retrieval experiments, each of the ten FlickrLogos-32 train samples for each brand serves as query sample. This allows to assess the statistical significance of results similar to a 10-fold-cross-validation strategy. As shown in <ref type="table" target="#tab_0">Table XI</ref> the ResNet101+Litw <ref type="bibr" target="#b23">[24]</ref> is the better logo retrieval method. Detected logos are described by the feature extraction network outputs where three different state-of-the-art classification architectures, namely VGG16, ResNet101 and DenseNet161,    <ref type="table" target="#tab_0">Table XI</ref>. In addition, we adopt the proposed method Logo-Yolo to FlickrLogos-32 dataset retrieval experiments, including baseline network and pretrained on LogoDet-3K network. We used the latest retrieval based detection method Deepvision <ref type="bibr" target="#b17">[18]</ref>, which adopts two different state-of-the-art classification architectures, namely ResNet101 and DenseNet161, and the experimental results are 52.62% and 50.38%, respectively. The pre-train model on LogoDet-3K is used to the baseline method Deepvision <ref type="bibr" target="#b17">[18]</ref>, the results are 54.17% and 52.91% mAP, with the 1.55% and 2.31% improvement compared with Deepvision. The experimental results show that the pre-trained model generated by our dataset is also effective in the logo retrieval task, further illustrating the value of LogoDet-3K in logo-related research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Discussion</head><p>Compared with existing methods, our proposed method obtains better detection performance, especially in solving small objects and complex backgrounds of logo images compared with YOLOv3. However, it can not achieve high detection performance for some cases. <ref type="figure" target="#fig_0">Fig. 11</ref> shows some failure cases from Logo-Yolo. Logo-Yolo is difficult to detect the smaller scale logos, leading to missed detection, such as the third image in <ref type="figure" target="#fig_0">Fig. 11</ref>. In addition, the logos under the same brand are similar and often appear in the same image, so there will be some problems in the object classification, such as the four images. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, we found that our method encountered lower performance when the following cases appear, such as blocked logo objects, logo objects close to the background and very small objects. Therefore, the logo detection on LogoDet-3K still has great challenges, such as the multi-label problem and large-scale problem, and it meanwhile highlights the comparative difficulty of the LogoDet-3K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we present LogoDet-3K dataset, the largest logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 high-quality manually annotated logo objects and 158,652 images. Detailed analysis shows the LogoDet-3K was highly diverse and more challenging than previous logo datasets. Therefore, it establishes a more challenging benchmark and can benefit many existing localization sensitive logo-relate tasks. In addition, we propose a new strong baseline method Logo-Yolo, which can get better detection performance than other state-of-art baselines. And we also report results of various detection models and demonstrate the effectiveness of our method and better generalization ability on other three logo datasets and logo retrieval tasks.</p><p>In the future, we hope LogoDet-3K will become a new benchmark dataset for a broad range of logo related research. Such as logo detection, logo retrieval and logo synthesis tasks. With the rapid development of major brands, real-time logo detection will become the trend of future research. We will continue to explore the characteristics of the LogoDet-3K dataset, and use anchor-free and lightweight design methods specifically for logo detection to achieve faster and more accurate logo detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Statistics of LogoDet-3K categories and images. The abscissa represents the number of logo images, the ordinate represents the number of categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Image samples from various categories of LogoDet-3K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Multiple logo categories for some brands, where a distinction between these logo categories via adding the suffix '-1', '-2'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Sorted distribution of images for each logo in LogoDet-3K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The detailed statistics of LogoDet-3K about Image and object distribution in per category, the number of objects in per image and object size in per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Distributions of categories, images and objects from LogoDet-3K on super-classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>as 9 ,</head><label>9</label><figDesc>according to the relationship between the number of samples and Avg IoU via K-means clustering. The final results of anchor centers are (53, 35), (257, 151), (75, 104), (271, 248), (159, 118), (134, 220), (270,73), (115, 46) and (193, 58), which are width and height of the corresponding cluster centers on the LogoDet-3K dataset. For the Focal loss of Logo-Yolo, ? = 0.25, ? = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Some detection results of Logo-Yolo on LogoDet-3K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative result comparison on LogoDet-3K between YOLOv3 and Logo-Yolo. Green boxes: ground-truth boxes. Red boxes: correct detection boxes. yellow boxes: mistakes detection boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>The Precision-Recall curve of Logo-Yolo and YOLOv3. The larger the enclosing area under the curve, the better the detection effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Left: Performance evaluation for different IoU thresholds. Right: The comparison of Logo-Yolo and YOLOv3 with increasing iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Qualitative result of some failure cases on Logo-Yolo. Green boxes denotes the ground-truth. Red boxes represent correct logo detections, while yellow are mistakes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison between LogoDet-3K and existing logo datasets.</figDesc><table><row><cell>Datasets</cell><cell>Logos</cell><cell>Brands</cell><cell>Images</cell><cell>Objects</cell><cell>Supervision</cell><cell>Public</cell></row><row><cell>BelgaLogos [16]</cell><cell>37</cell><cell>37</cell><cell>10,000</cell><cell>2,695</cell><cell>Object-Level</cell><cell>Yes</cell></row><row><cell>FlickrLogos-27 [2]</cell><cell>27</cell><cell>27</cell><cell>1,080</cell><cell>4,671</cell><cell>Object-Level</cell><cell>Yes</cell></row><row><cell>FlickrLogos-32 [2]</cell><cell>32</cell><cell>32</cell><cell>8,240</cell><cell>5,644</cell><cell>Object-Level</cell><cell>Yes</cell></row><row><cell>FlickrLogos-47 [2]</cell><cell>47</cell><cell>47</cell><cell>8,240</cell><cell>-</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>Logo-18 [25]</cell><cell>18</cell><cell>10</cell><cell>8,460</cell><cell>16,043</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>Logo-160 [25]</cell><cell>160</cell><cell>100</cell><cell>73,414</cell><cell>130,608</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>Logos-32plus [26]</cell><cell>32</cell><cell>32</cell><cell>7,830</cell><cell>12,302</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>Top-Logo-10 [27]</cell><cell>10</cell><cell>10</cell><cell>700</cell><cell>-</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>SportsLogo [28]</cell><cell>20</cell><cell>20</cell><cell>2,000</cell><cell>-</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>CarLogo-51 [29]</cell><cell>51</cell><cell>51</cell><cell>11903</cell><cell>-</cell><cell>Image-Level</cell><cell>No</cell></row><row><cell>WebLogo-2M [17]</cell><cell>194</cell><cell>194</cell><cell>1,867,177</cell><cell>-</cell><cell>Image-Level</cell><cell>Yes</cell></row><row><cell>Logos-in-the-Wild [24]</cell><cell>871</cell><cell>871</cell><cell>11,054</cell><cell>32,850</cell><cell>Object-Level</cell><cell>Yes</cell></row><row><cell>QMUL-OpenLogo [30]</cell><cell>352</cell><cell>352</cell><cell>27,083</cell><cell>-</cell><cell>Object-Level</cell><cell>Yes</cell></row><row><cell>PL2K [18]</cell><cell>2,000</cell><cell>2,000</cell><cell>295,814</cell><cell>-</cell><cell>Object-Level</cell><cell>No</cell></row><row><cell>Logo-2K+ [19]</cell><cell>2,341</cell><cell>2,341</cell><cell>167,140</cell><cell>-</cell><cell>Image-Level</cell><cell>Yes</cell></row><row><cell>LogoDet-3K</cell><cell>3,000</cell><cell>2864</cell><cell>158,652</cell><cell>194,261</cell><cell>Object-Level</cell><cell>Yes</cell></row><row><cell cols="3">in deep learning. Some researchers constructed some larger</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">datasets, such as WebLogo-2M [17], LOGO-Net [25] and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PL2K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Data statistics on LogoDet-3K.</figDesc><table><row><cell cols="4">Root-Category Sub-Category Images Objects</cell></row><row><cell>Food</cell><cell>932</cell><cell cols="2">53,350 64,276</cell></row><row><cell>Clothes</cell><cell>604</cell><cell cols="2">31,266 37,601</cell></row><row><cell>Necessities</cell><cell>432</cell><cell cols="2">24,822 30,643</cell></row><row><cell>Others</cell><cell>371</cell><cell cols="2">15,513 20,016</cell></row><row><cell>Electronic</cell><cell>224</cell><cell cols="2">9,675 12,139</cell></row><row><cell>Transportation</cell><cell>213</cell><cell cols="2">10,445 12,791</cell></row><row><cell>Leisure</cell><cell>111</cell><cell>5,685</cell><cell>6,573</cell></row><row><cell>Sports</cell><cell>66</cell><cell>3,953</cell><cell>5,041</cell></row><row><cell>Medical</cell><cell>47</cell><cell>3,945</cell><cell>5,185</cell></row><row><cell>Total</cell><cell>3,000</cell><cell cols="2">158,652 194,261</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Statistics of three benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Statistics of three super-classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Comparison of baselines on different benchmarks (%).</figDesc><table><row><cell>Benchmarks</cell><cell>Methods</cell><cell>Backbones</cell><cell>mAP</cell></row><row><cell></cell><cell>Faster RCNN [13]</cell><cell>ResNet-101</cell><cell>45.16</cell></row><row><cell></cell><cell>SSD [14]</cell><cell>VGGNet-16</cell><cell>43.32</cell></row><row><cell></cell><cell>RetinaNet [22]</cell><cell></cell><cell></cell></row><row><cell>LogoDet-3K-1000</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table V summarizes the results on three subsets among different detection models. Compared with existing baselines Faster RCNN, SSD and RetinaNet etc., YOLOv3 detector obtains better results on three subsets, which are 55.21%, 52.32% and 48.60% respectively. The results of YOLOv3 are higher than Faster RCNN detector, because there are more</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of super-classes on different methods (%).</figDesc><table><row><cell>Benchmarks</cell><cell>Methods</cell><cell>Backbones</cell><cell>mAP</cell></row><row><cell></cell><cell>Faster RCNN [13]</cell><cell>ResNet-101</cell><cell>47.32</cell></row><row><cell></cell><cell>SSD [14]</cell><cell>VGGNet-16</cell><cell>46.18</cell></row><row><cell></cell><cell>RetinaNet [22]</cell><cell></cell><cell></cell></row><row><cell>Food</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Evaluation on individual modules and two modules of Logo-Yolo (%).</figDesc><table><row><cell>Model</cell><cell>mAP</cell></row><row><cell>YOLOv3</cell><cell>48.61</cell></row><row><cell>YOLOv3+Pre-anchors Design</cell><cell>50.12</cell></row><row><cell>YOLOv3+Focal Loss</cell><cell>49.21</cell></row><row><cell>YOLOv3+CIoU loss</cell><cell>49.86</cell></row><row><cell>Logo-Yolo(w/o Pre-anchors Design)</cell><cell>49.92</cell></row><row><cell>Logo-Yolo(w/o Focal Loss)</cell><cell>51.50</cell></row><row><cell>Logo-Yolo(w/o CIoU loss)</cell><cell>50.64</cell></row><row><cell>Logo-Yolo</cell><cell>52.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>The performance of Logo-Yolo on Top-Logo-10 (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table</head><label></label><figDesc></figDesc><table /><note>VIII summarizes experimental results for Top-Logo- 10. We observe that our method Logo-Yolo achieves better per- formance compared with other models. There is further about</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX :</head><label>IX</label><figDesc>The performance of Logo-Yolo on FlickrLogos-32 (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table X, our LogoDet-3K dataset shows strong generalization ability. Compared with YOLOv3 and Logo-Yolo method, our fine-tuned LogoDet-3K model for QMUL-OpenLogo detection can significantly boost the performance, with 1.73 points (53.69% vs 51.96%) for YOLOv3, and 2.16 points (55.37% vs 53.21%) for Logo-Yolo, the Logo-Yolo gains a 1.68 improvement (55.37% vs 53.69%).The results are shown that the effectiveness of pre-trained models and Logo-Yolo method. By pre-training the LogoDet-3K dataset which removes the 352 categories from QMUL-OpenLogo (LogoDet-3K w/o QMUL-OpenLogo), we can still achieve competitive results with 52.36% on the QMUL-OpenLogo benchmark, 0.4 points higher than the result in YOLOv3 method, and 1.25 points for Logo-Yolo. It shows that the LogoDet-3K dataset has the generalization ability. Compared with QMUL-OpenLogo, our LogoDet-3K benchmark has much higher performance gain. By involving QMUL-OpenLogo Pre-training before LogoDet-3K, we can slightly improve the YOLOv3 with 0.34. For the Logo-Yolo, the QMUL-OpenLogo pre-training before LogoDet-3K can further bring in 0.73 points gain. The results shows LogoDet-3K contains richer logo features than QMUL-OpenLogo dataset, which can be widely used for logo detection.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE X :</head><label>X</label><figDesc>Generalization ability of general object detection results on the QMUL-OpenLogo dataset (%).</figDesc><table><row><cell>Method</cell><cell>Pre-trained Dataset</cell><cell>mAP</cell></row><row><cell>YOLO9000 [44]</cell><cell>QMUL-OpenLogo</cell><cell>26.33</cell></row><row><cell>YOLOv2+CAL [30]</cell><cell>QMUL-OpenLogo</cell><cell>49.17</cell></row><row><cell>FR-CNN+CAL [30]</cell><cell>QMUL-OpenLogo</cell><cell>51.03</cell></row><row><cell>YOLOv3</cell><cell>QMUL-OpenLogo</cell><cell>51.96</cell></row><row><cell>YOLOv3</cell><cell cols="2">LogoDet-3K w/o QMUL-OpenLogo 52.36</cell></row><row><cell>YOLOv3</cell><cell>LogoDet-3K</cell><cell>53.69</cell></row><row><cell>YOLOv3</cell><cell cols="2">QMUL-OpenLogo -&gt; LogoDet-3K 54.03</cell></row><row><cell>Logo-Yolo</cell><cell>QMUL-OpenLogo</cell><cell>53.21</cell></row><row><cell>Logo-Yolo</cell><cell cols="2">LogoDet-3K w/o QMUL-OpenLogo 54.46</cell></row><row><cell>Logo-Yolo</cell><cell>LogoDet-3K</cell><cell>55.37</cell></row><row><cell>Logo-Yolo</cell><cell cols="2">QMUL-OpenLogo -&gt; LogoDet-3K 56.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XI :</head><label>XI</label><figDesc>Evaluation retrieval results on FlickrLogos-32 (%).</figDesc><table><row><cell>Method</cell><cell>mAP</cell></row><row><cell>baseline [27]</cell><cell>36.00</cell></row><row><cell>ResNet101</cell><cell>32.70</cell></row><row><cell>DenseNet161</cell><cell>36.80</cell></row><row><cell>ResNet101+Litw [24]</cell><cell>46.40</cell></row><row><cell>DenseNet161+Litw [24]</cell><cell>44.80</cell></row><row><cell>Deepvision(ResNet101)</cell><cell>52.62</cell></row><row><cell>Deepvision(DenseNet161)</cell><cell>50.78</cell></row><row><cell>Deepvision(ResNet101+Pre-trained)</cell><cell>54.17</cell></row><row><cell cols="2">Deepvision(DenseNet161+Pre-trained) 52.91</cell></row><row><cell>serve as base networks in</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.forbes.com/global2000/list/tab:overall</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brand data gathering from live social media streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable logo recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlation-based burstiness for logo retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="965" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable triangulation-based logo recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trevisiol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bundle min-hashing for logo recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic video logo detection and removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei-Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="page" from="379" to="391" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Region-based cnn for logo detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">F R L Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Multimedia Computing and Service</title>
		<imprint>
			<biblScope unit="page" from="319" to="322" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving small object proposals for company logo detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brehm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Filtering of brand-related microblogs using social-smooth multiview embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="2115" to="2126" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual listening in: Extracting brand image portrayed on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mizik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video ecommerce++: Toward large scale online video advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integration of local and global shape analysis for logo classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="page" from="1449" to="1457" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WebLogo-2M: scalable logo detection by deep learning from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable logo recognition using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Feh?rv?ri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Appalaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="715" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Logo-2K+: a large-scale logo dataset for scalable logo classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6194" to="6201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet: a largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance-IoU loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open set logo detection and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>T?zk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">LOGO-Net: large-scale deep logo detection and brand recognition with deep region-based convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02462</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for logo recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buzzelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep learning logo detection with data expansion by synthesising context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mutual enhancement for detection of multiple logos in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4856" to="4865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and accurate near-duplicate image search with affinity propagation on the imageweb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Z L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Image Understand</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Open logo detection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A coarse-to-fine facial landmark detection method based on self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SNIPER: efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9333" to="9343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Logo recognition using CNN features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buzzelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="438" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">DeepLogo: hitting logo recognition with the deep neural network hammer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02131</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable logo detection by self colearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107003</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic graphic logo detection via fast region-based convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fraz?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
