<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clemson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Clemson University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional self-supervised monocular depth prediction methods are based on a static environment assumption, which leads to accuracy degradation in dynamic scenes due to the mismatch and occlusion problems introduced by object motions. Existing dynamic-object-focused methods only partially solved the mismatch problem at the training loss level. In this paper, we accordingly propose a novel multi-frame monocular depth prediction method to solve these problems at both the prediction and supervision loss levels. Our method, called DynamicDepth, is a new framework trained via a self-supervised cycle consistent learning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is proposed to disentangle object motions to solve the mismatch problem. Moreover, novel occlusion-aware Cost Volume and Re-projection Loss are designed to alleviate the occlusion effects of object motions. Extensive analyses and experiments on the Cityscapes and KITTI datasets show that our method significantly outperforms the state-of-the-art monocular depth prediction methods, especially in the areas of dynamic objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D environmental information is crucial for autonomous vehicles, robots, and AR/VR applications. Self-supervised monocular depth prediction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref> provides an efficient solution to retrieve 3D information from a single camera without requiring expensive sensors or labeled data. In recent years these methods are getting more and more popular in both the research and industry communities.</p><p>Conventional self-supervised monocular depth prediction methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12</ref>] take a single image as input and predicts the dense depth map. They generally use a re-projection loss which constraints the geometric consistency between adjacent frames in the training loss level, but they are not capable of geometric reasoning through temporal frames in the network prediction level, which limits their overall performance.  <ref type="figure">Fig. 1</ref>: Conventional monocular depth prediction methods like Manydepth <ref type="bibr" target="#b46">[47]</ref> makes severe mistakes on dynamic object areas due to mismatch and occlusion problems introduced by object motions. Our method achieved significant improvement with our proposed Dynamic Object Motion Disentanglement and Occlusion Alleviation.</p><p>Temporal and spatially continuous images are available in most real-world scenarios like autonomous vehicles <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref> or smart devices <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. Recent years multi-frame monocular depth prediction methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">54]</ref> are proposed to utilize the temporal image sequences to improve the depth prediction accuracy. Cost-volume-based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> adopted the cost volume from stereo match tasks to enable the geometric reasoning through temporal image sequences in the network prediction level, and achieved overall state-of-the-art depth prediction accuracy while not requiring time-consuming recurrent networks.</p><p>However, both the re-projection loss function and the cost volume construction are based on the static environment assumption, which does not hold for most real-world scenarios. Object motion will violate this assumption and cause re-projection mismatch and occlusion problems. The cost volume and loss values in the dynamic object areas are unable to reflect the quality of depth hypothesis and prediction, which will mislead the model training. Recent work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22]</ref> attempted to optimize depth prediction of dynamic object areas and achieved noticeable improvements, but they still have several drawbacks. (1) They only solve the mismatch problem at the loss function level, still cannot reason geometric constraints through temporal frames for dynamic objects, which limits its accuracy potential. <ref type="bibr" target="#b1">(2)</ref> The occlusion problem introduced by object motions is still unsolved. (3) Redundant object motion prediction networks increased the model complexity and does not work for the motions of non-rigid objects.</p><p>Pursuing accurate and generic depth prediction, we propose DynamicDepth, a self-supervised temporal depth prediction framework that disentangles the dynamic object motions. First, we predict a depth prior from the target frame and project to the reference frames for an implicit estimation of object motion without rigidity assumption, which is later disentangled by our Dynamic Object Motion Disentanglement (DOMD) module. We then build a multi-frame occlusion-aware cost volume to encode the temporal geometric constraints for the final depth prediction. In the training level, we further propose a novel occlusionaware re-projection loss to alleviate the occlusion from the object motions, and a novel cycle consistent learning scheme to enable the final depth prediction and the depth prior prediction to mutually improve each other. To summarize, our contributions are as follows:</p><p>-We propose a novel Dynamic Object Motion Disentanglement <ref type="bibr">(DOMD)</ref> module which leverages an initial depth prior prediction to solve the object motion mismatch problem in the final depth prediction. -We devise a Dynamic Object Cycle Consistent training scheme to mutually reinforce the Prior Depth and the Final Depth prediction. -We design an Occlusion-aware Cost Volume to enable geometric reasoning across temporal frames even in object motion occluded areas, and a novel Occlusion-aware Re-projection Loss to alleviate the motion occlusion problem in training supervision. -Our method significantly outperforms existing state-of-the-art methods on the Cityscapes <ref type="bibr" target="#b2">[3]</ref> and KITTI <ref type="bibr" target="#b29">[30]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review self-supervised depth prediction approaches relevant to our proposed method in the following three categories: (1) single-frame, (2) multi-frame, (3) dynamic-objects-optimized. Self-supervised Single-frame Monocular Depth Prediction: Due to the limited availability of labeled depth data, self-supervised monocular depth prediction methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref> have become more and more popular. Mon-odepth2 <ref type="bibr" target="#b9">[10]</ref> set a benchmark for robust monocular depth, FeatDepth <ref type="bibr" target="#b37">[38]</ref> tried to improve the low-texture area depth prediction, and PackNet <ref type="bibr" target="#b11">[12]</ref> explored a more effective network backbone. These self-supervised depth models generally take a single frame as input and predict the dense depth map. In the training stage, the temporally neighboring frames are projected to the current image plane by the predicted depth map. If the prediction is accurate, the re-projected images are supposed to be identical to the actual current frame image. The training is based on enforcing the re-projection photo-metric <ref type="bibr" target="#b44">[45]</ref> consistency.</p><p>These methods provided a successful paradigm to learn the depth prediction without labeled data, but they have a major and common problem with dynamic objects: the re-projection loss function assumes the environment is static, which does not hold for real-world applications. When objects are moving, even if the prediction is perfect, the re-projected reference image will still not match the target frame image. The loss signal from the dynamic object areas will generate misleading gradients to degrade the model performance. In contrast, our proposed Dynamic Object Motion Disentanglement solves this mismatch problem and achieves superior accuracy, especially in the dynamic object areas.</p><p>Multi-frame Monocular Depth Prediction: The above mentioned reprojection loss only uses temporal constraints at the training loss function level. The model itself does not take any temporal information as input for reasoning, which limits its performance. One promising way to improve self-supervised monocular depth prediction is to leverage the temporal information in the input and prediction stage. Early works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">54]</ref> explored recurrent networks to process image sequences for monocular depth prediction. These recurrent models are computationally expensive and do not explicitly encode and reason geometric constraints in their prediction. Recently, Manydepth <ref type="bibr" target="#b46">[47]</ref> and MonoRec <ref type="bibr" target="#b47">[48]</ref> adopt the cost volumes from stereo matching tasks to enable the geometric-based reasoning during inference. They project the reference frame feature map to the current image plane with multiple pre-defined depth hypothesises, whose difference to the current frame feature maps are stacked to form the cost volume. Hypothesises which are closer to the actual depth are supposed to have a lower value in the cost volume, while the entire cost volume is supposed to encode the inverse probability distribution of the actual depth value. With this integrated cost volume, they achieve great overall performance improvement while preserving real-time efficiency.</p><p>However, the construction of the cost volume relies on the static environment assumption as well, which leads to catastrophic failure in the dynamic object area. They either circumvent this problem <ref type="bibr" target="#b47">[48]</ref> or simply use a L1 loss <ref type="bibr" target="#b46">[47]</ref> to mimic the prediction of the single-frame model, which makes less severe mistakes for dynamic objects. This L1 loss alleviates but does not actually solve the problem. Our proposed Dynamic Object Motion Disentanglement, Occlusionaware Cost Volume, and Re-projection Loss solve the mismatch and occlusion problem at both the reasoning and the training loss levels and outperform all other methods, especially in the dynamic object areas.</p><p>Dynamic Objects in Self-supervised Depth Prediction: The research community has attempted to solve the above-mentioned ill-posed re-projection geometry for dynamic objects. SGDepth <ref type="bibr" target="#b19">[20]</ref> tried to exclude the moving objects from the loss function, Li et al. <ref type="bibr" target="#b25">[26]</ref> proposed to build a dataset only containing non-moving dynamic-category objects. The latest state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> tried to predict pixel-level or object-level translation and incorporate it into the loss function re-projection geometry.</p><p>However, these methods still have several drawbacks. First, their single frame input did not enable the model to reason from the temporal domain. Second, explicitly predicting object motions requires redundant models and increased complexity. Third, they only focused on the re-projection mismatch, the occlusion problem introduced by object motions is still unsolved. Our proposed Dynamic Object Motion Disentanglement works at both the cost volume and the loss function levels, solving the re-projection mismatch problem while enabling the geometric reasoning through temporal frames in the inference stage, without additional explicit object motion prediction. Furthermore, we propose Occlusion-aware Cost Volume and Occlusion-aware Re-projection Loss to solve the occlusion problem introduced by object motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given two images I t?1 ? R W ?H?3 and I t ? R W ?H?3 of a target scene, our purpose is to estimate a dense depth map D t of I t by taking advantage of two views'  from which dynamic-object-disentangled frame I d t?1 is generated by the DOMD module for the final depth prediction D t . The occlusion-aware cost volume is constructed to facilitate geometric reasoning and the Dynamic Object Cycle Consistency Loss is devised for mutual reinforcement between D t and D pr t . Green arrows indicates knowledge flow.</p><p>observations while solving the mismatch and occlusion problems introduced by object motions.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our model contains three major innovations: We first use a Depth Prior Net ? DP N and Pose Net ? p to predict an initial depth prior D pr t and ego-motion, which is sent to the (1) Dynamic Object Motion Disentanglement (DOMD) to solve the object motion mismatch problem (see Sec. 3.2). The disentangled frame I d t?1 and the current frame I t are encoded by the Depth Encoder to construct the (2) Occlusion-aware Cost Volume for reasoning through temporal frames while diminishing the motion occlusion problem (see Sec. 3.3). The final depth prediction D t is generated by the Depth Decoder from our cost volume. During training, our (3) Dynamic Object Cycle Consistency Loss L c enables the mutual improvement of the depth prior D pr t and the final depth prediction D t , while our Occlusion-aware Re-projection Loss L or solved the object motion occlusion problem (see Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Object Motion Disentanglement (DOMD)</head><p>There is an observation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> that single-frame monocular depth prediction models suffer from dynamic objects, which cause even more severe problems in multiframe methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. This is because the static environment assumption does not hold for dynamic objects, which introduce mismatch and occlusion problems. Here, we describe our DOMD to solve the mismatch problem.</p><p>Why the Cost Volume and Self-supervision Mismatch on Dynamic Objects: Either in the cost volume or re-projection loss function, the current frame feature map F t or image I t is projected to the 3D space and re-projected back to the reference frame t ? 1 by the depth hypothesis or predictions. We illustrate the re-projection geometry in <ref type="figure">Fig. 3</ref>. The dynamic object moves from W t?1 to W t , its corresponding image patches are C t?1 and C t respectively. Conventional methods suppose the photo-metric difference between C t?1 and the is our depth prior prediction. Conventional methods tend to mismatch at W ? . We re-project C t to C d t?1 with depth prior D pr t to replace C t?1 to disentangle the object motion. This solves the mismatch problem, making our cost volume and re-projection loss correctly converge at W t . re-projected C t is lowest when the depth prediction or hypothesis is correctly close to W t . However, due to the object motions, image or feature patches tend to mismatch at W ? instead:</p><formula xml:id="formula_0">E(C t?1 , ? t?1 (W ? )) &lt; E(C t?1 , ? t?1 (W t ))</formula><p>, ? is the projection operator. This mismatch misleads the reasoning in the cost volume and the supervision in the re-projection loss.</p><p>Dynamic Object Motion Disentanglement: Our DOMD module M o takes two image frames (I t?1 , I t ) with its dynamic category (e.g.,vehicle, people, bike) segmentation masks (S t?1 , S t ) as input to generate the disentangled image</p><formula xml:id="formula_1">I d t?1 . M o : (I t , I t?1 , S t?1 , S t ) ? I d t?1 .<label>(1)</label></formula><p>We first use a single-frame depth prior network ? DP N to predict an initial depth prior D pr t . As shown in <ref type="figure">Fig. 3</ref>, the D pr t is used to re-project the dynamic object image patch C t to C d t?1 , which indicates the t ? 1 camera perspective of the dynamic object at location W t . Finally, we replace the C t?1 with C d t?1 to form the dynamic object motion disentangled image I d t?1 . Note that we do not require the rigidity of the dynamic object.</p><formula xml:id="formula_2">C a = I a ? S a , C d t?1 = ? t?1 (? ?1 t (C t , D pr t )), I d t?1 = I t?1 (C t?1 ? C d t?1 ). (2)</formula><p>Our Multi-frame model ? M F then construct the geometric constraint in the cost volume with the disentangled image frame I d t?1 and current image frame I t to predict the final depth D t .</p><p>We further propose a Dynamic Object Cycle Consistency Loss L c (Details in Sec. 3.4 and Sec. 4.4.) to enable the D t to backward supervise the D pr t training. Both the D pr t and D t could be greatly improved with our cycle consistent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5:</head><p>Occlusion-aware Cost Volume: Feature map F d t?1 of the I d t?1 is warped to the I t plane with multiple pre-defined depth hypothesizes P i to construct the cost volume. The black area in the cost volume indicates the noise from object motion occlusion, which is replaced with the nearby non-occluded area to avoid polluting the cost distribution.</p><p>learning. Our ? DP N already outperforms the existing dynamic-object-focused state-of-the-art methods such as InstaDM <ref type="bibr" target="#b21">[22]</ref> with joint and cycle consistent learning.</p><p>Why Final Depth Improves Over Depth Prior: As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, when the depth prior prediction is inaccurate, the re-projected image patch C d t?1 will occlude some background pixels which are visible at time t. Those pixels will generate a higher photometric error in the re-projection loss. To minimize it, the network will manage to decode the error of depth prior from the disentangled image I d t?1 to predict a better final depth to improve the depth prior prediction by our later introduced cycle-consistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Occlusion-aware Cost Volume</head><p>To encode the geometric constraints through the temporal frames while solving the occlusion problem introduced by dynamic objects motions, we propose an Occlusion-aware Cost Volume CV occ ? R |P |?W ?H?C , where P = {p 1 , p 2 , ..., p |P | } is the pre-defined depth hypothesis, C is the channel number.</p><p>As shown in <ref type="figure">Fig. 5</ref>, we warp the feature map F d t?1 of the dynamic object disentangled image I d t?1 to the current frame image plane with all pre-defined depth hypothesis P . The cost volume layer CV i is the L1 difference between the warped feature map F w i and the current frame feature map F t . We obtain the cost volume CV by stacking all the layers. For each pixel, the cost value is supposed to be lower when the corresponding depth hypothesis is closer to the actual depth. The cost values over different depth hypotheses are supposed to encode the inverse probability distribution of the actual depth.</p><formula xml:id="formula_3">CV i = |F t ? F w i | 1 , F w i = ? t (? ?1 t?1 (F d t?1 , p i )).<label>(3)</label></formula><p>In <ref type="figure">Fig. 5</ref>, the black area in the image I d t?1 corresponds to the backgrounds which may be visible at time t but are occluded by the dynamic object at time t ? 1. The L1 difference between the feature of backgrounds at time t and the feature of black pixels is meaningless, which pollutes the distribution of the cost volume. We propose to replace these values with non-occluded area cost values from neighboring depth hypothesis p ? . This preserves the global cost distribution and leads the training gradients flow to the nearby non-occluded areas. Our ablation study in Sec. 4 confirms the effectiveness of our design.</p><formula xml:id="formula_4">CV occ p,w,h = CV p,w,h , if F w p,w,h ? V, CV p ? ,w,h , if F w p,w,h ? O, F w p ? ,w,h ? V, p ? ? r,<label>(4)</label></formula><p>where O/V are the set of occluded/visible areas in F w , r is the neighbors of p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Functions</head><p>During the training of our framework, our proposed Occlusion-aware Re-projection Loss L or enforces the re-projection consistency between adjacent frames while alleviating the influence of the object-motion-caused occlusion problem. Our joint learning and novel Dynamic Object Cycle Consistency Loss L c further enables the depth prior prediction D pr t and final depth prediction D t to mutually reinforce each other to achieve the best performance.</p><p>Dynamic Object Cycle Consistency Loss: As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, during the self-supervised learning, our initial depth prior prediction D pr t is used in our Dynamic Object Motion Disentanglement (DOMD) module to produce the motion disentangled reference frame I d t?1 which is later encoded in our Occlusion-aware Cost Volume to guide the final depth prediction D t . To enable the multi-frame final depth D t to backward guide the learning of single-frame depth prior D pr t to achieve a mutual reinforcement scheme, we propose a novel Dynamic Object Cycle Consistency Loss L c to enforce the consistency between D t and D pr t . Since only the dynamic objects area of D pr t are employed in our DOMD module, we only apply the Dynamic Object Cycle Consistency Loss L c at these areas and only active when the inconsistency is large enough:</p><formula xml:id="formula_5">A = {i ? I t | D i t ? D pr,i t 1 min{D i t , D pr,i t } &gt; 1},<label>(5)</label></formula><formula xml:id="formula_6">L c = 1 |A ? S| i?(A?S) D i t ? D pr,i t 1 .<label>(6)</label></formula><p>Where S is the semantic segmentation mask of dynamic category objects.</p><p>Occlusion-aware Re-projection Loss: In self-supervised monocular depth prediction, the image from reference frames (I t?1 , I t+1 ) are warped to the current image plane with the predicted depth map D t . If the depth prediction is correct, the conventional re-projection loss L r supposes the warped image (I t?1?t , I t+1?t ) to be identical with the current frame image I t . They penalize the photo-metric error E between them.</p><formula xml:id="formula_7">E a = E(I t , I a?t ), L r = 1 2 (? t?1 +? t+1 ).<label>(7)</label></formula><p>As mentioned above, the dynamic object motions break the static environment assumption and lead to the mismatch problem in this re-projection geometry. Our Dynamic Object Motion Disentanglement (DOMD) module M o could solve this mismatch problem but the background pixels occluded by the dynamic object at reference time (t ? 1, t + 1) are still missing. As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, using the photo-metric error E between these occluded pixels in the warped image ((I t?1?t , I t+1?t )) and visible background pixels in I t as training loss only introduces noise and misleads the model learning.</p><p>Fortunately, object motions are normally consistent in a short time window, which means the backgrounds occluded at time t ? 1 are usually visible at time t + 1 and vise-versa. It is possible to switch the source frame between t ? 1 and t + 1 for each pixel to avoid the occlusion. The widely used per-pixel minimum re-projection loss <ref type="bibr" target="#b9">[10]</ref> L min r assumes the visible source pixels will have lower photo-metric error than the occluded ones, they thus proposed to choose the minimum error source frame for each pixel: L min r = 1 |It| i?It min(? i t?1 ,? i t+1 ). However, in practice, as shown in the right columns of <ref type="figure" target="#fig_3">Fig. 6</ref> we observe that around half of the visible source pixels do not have a lower photo-metric error than the occluded source. Since we can obtain the exact occlusion mask O and visible mask V from our DOMD module M o , we propose Occlusion-aware Reprojection Loss L or , which always choose the non-occluded source frame pixels for photo-metric error. More details are in the supplementary materials.</p><p>Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b55">55]</ref>, a combination of L1 norm and SSIM <ref type="bibr" target="#b44">[45]</ref> with coefficient ? is used as our photo-metric error E p . The SSIM takes the pixels within a local window into account for error computation. In I t?1?t and I t+1?t the occluded pixels thus influence the neighboring non-occluded pixel's SSIM error. We propose Occlusion Masking M a , which paints the corresponding pixels in target frame I t to be black when calculating the SSIM error with reference frames.  <ref type="figure">Fig. 7</ref>: Error Visualization: In the left t ? 1 image, red image patch is the original data used by the Manydepth <ref type="bibr" target="#b46">[47]</ref> while the blue patch is generated by the DOMD module for our prediction. We project the dynamic object depths into point clouds. Our prediction matches the ground truth better.</p><p>This neutralizes the influence of the occlusion areas on neighboring pixels in SSIM. The ablation study in Sec. 4.4 confirms applying our source pixel switching and occlusion masking mechanisms together makes the best improvement in the depth prediction quality.</p><formula xml:id="formula_8">E p [I a , I b ] = ? 2 (1?SSIM(I a , I b ))+(1??) |I a ? I b | 1 .<label>(8)</label></formula><formula xml:id="formula_9">EO t ? = E p [M a (I t ), I t ? ?t ] ,<label>(9)</label></formula><p>We further adopt the edge-aware metric from <ref type="bibr" target="#b40">[41]</ref> into our smoothness loss L s to make it invariant to output scale, which is formulated as:</p><formula xml:id="formula_10">L s = |? x d * t | e ?|?xIt| + |? y d * t | e ?|?yIt| ,<label>(10)</label></formula><p>where d * t = d t /d t is the mean-normalized inverse depth, ? is the image gradient. Our final loss L is the sum of our Dynamic Object Cycle Consistency Loss L c , Occlusion-aware Re-projection Loss L or , and smoothness loss L s :</p><formula xml:id="formula_11">L = L c + L or + 1e ?3 ? L s .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experiments are mainly focused on the challenging Cityscapes <ref type="bibr" target="#b2">[3]</ref> dataset, which contains many dynamic objects. To comprehensively compare with more state-of-the-art methods, we also report the performance on the widely-used KITTI <ref type="bibr" target="#b29">[30]</ref> dataset. Since our method is mainly focused on the dynamic objects, we further conduct additional evaluation on the depth errors of the dynamic objects areas, which clearly demonstrate the effectiveness of our method. The design decision and the effectiveness of our proposed framework is evaluated by an extensive ablation study.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details:</head><p>We use frames {I t?1 , I t , I t+1 } for training and {I t?1 , I t } for testing. All dynamic objects is this paper are determined by an off-the-shelf semantic segmentation model EffcientPS <ref type="bibr" target="#b30">[31]</ref>. Note that we do not need instance-level masks and interframe correspondences, all dynamic category pixels are projected together at once. All network modules including the depth prior net ? DP N are trained together from scratch or ImageNet <ref type="bibr" target="#b4">[5]</ref> pre-training. ResNet18 <ref type="bibr" target="#b15">[16]</ref> is used as the backbone. We use the Adam <ref type="bibr" target="#b18">[19]</ref> optimizer with a learning rate of 10 ?4 to train for 10 epochs, which takes about 10 hours on a single Nvidia A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cityscapes Results</head><p>Cityscapes <ref type="bibr" target="#b2">[3]</ref> is a challenging dataset with significant amount of dynamic objects. It contains 5, 000 videos each with 30 frames, totaling 150, 000 image frames. We exclude the first, last, and static-camera frames in each video for training, resulting in 58, 335 frames training data. The official testing set contains 1, 525 image frames. <ref type="table" target="#tab_4">Table 1</ref> shows the depth prediction results on the Cityscapes <ref type="bibr" target="#b2">[3]</ref> testing set. Following the convention, we rank all methods based on the absolute-relative-   <ref type="bibr" target="#b2">[3]</ref> datasets. The best results are in bold, second best are underlined. Our depth prior prediction D pr t already outperform the state-of-the-art method InstaDM <ref type="bibr" target="#b21">[22]</ref> using the same single frame input, while our final depth prediction D t sets a new benchmark.</p><p>errors. Since the Cityscapes dataset contains significant amount of dynamic objects, the object-motion-optimized method InstaDM <ref type="bibr" target="#b21">[22]</ref> achieved the best accuracy among all the existing methods. With the help of our proposed Dynamic Object Motion Disentanglement (DOMD), Dynamic Object Cycle Consistency Loss, Occlusion-aware Cost Volume and the Occlusion-aware Re-projection Loss, our method outperforms the InstaDM <ref type="bibr" target="#b21">[22]</ref> by a large margin in all of the metrics using a lower resolution and more concise architecture (we do not require the explicit per-object-motion network, instance level segmentation prior and inter-frame correspondences). Qualitative visualizations are in <ref type="figure" target="#fig_5">Fig. 8</ref>. <ref type="table" target="#tab_6">Table 2</ref> shows the depth errors in the dynamic objects area. Our Depth Prior Network ? DP N shares a similar architecture with the Monodepth2 <ref type="bibr" target="#b9">[10]</ref> while trained jointly with our multi-frame model ? M F using Dynamic Object Cycle Consistency Loss L c . It outperforms all the existing methods including Monodepth2 <ref type="bibr" target="#b9">[10]</ref> and InstaDM <ref type="bibr" target="#b21">[22]</ref>. Manydepth <ref type="bibr" target="#b46">[47]</ref> suffers catastrophic failure on the dynamic objects due to the aforementioned mismatch and occlusion problems. They employed an separate single-frame model as a teacher for dynamic objects area. However, since it does not actually solve the mismatch and occlusion problems, it still makes severe mistakes on dynamic objects. In contrast, with our proposed innovations, our multi-frame model ? M F boosts up the accuracy even higher, achieves superior advantages on all the metrics, showing its significant effectiveness. We show a qualitative visualization in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">KITTI Results</head><p>Our proposed framework is further evaluated on the widely-used KITTI <ref type="bibr" target="#b29">[30]</ref> dataset Eigen <ref type="bibr" target="#b5">[6]</ref>   pixels in the KITTI <ref type="bibr" target="#b29">[30]</ref> dataset are dynamic category objects (e.g.,Vehicle, Person, Bike), and most of the vehicles are not moving. The comparison of our method with the state-of-the-art single-frame models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref>, multi-frame models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>, and dynamic-objects-optimized models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is summarized in <ref type="table" target="#tab_4">Table 1</ref>. Unsurprisingly dynamic-objects-focused methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref> showed a minor advantage on this dataset. Our method only achieve 2% improvement over the existing state-of-the-art method Manydepth <ref type="bibr" target="#b46">[47]</ref>. However, when we only focus on dynamic objects as in <ref type="table" target="#tab_6">Table 2</ref>, our method achieve a much more significant 14.3% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To comprehensively understand the effectiveness of our proposed modules and prove our design decision, we perform an extensive ablation study on the challenging Cityscapes <ref type="bibr" target="#b2">[3]</ref> dataset. As shown in <ref type="table" target="#tab_8">Table 3</ref>, our experiments fall into three groups, evaluating Dynamic Object Motion Disentanglement, Occlusionaware Cost Volume and Loss, and Cycle Consistent Training.</p><p>Dynamic Object Motion Disentanglement: In the first group of the <ref type="table" target="#tab_8">Table 3</ref>, we evaluate our proposed Dynamic Object Motion Disentanglement (DOMD) module. When the DOMD is enabled, the cost volume and the reprojection loss is based on the disentangled I d t?1 image instead of the original I t?1 image. The Abs Rel Error reduced by 4%, confirms its effectiveness.</p><p>Occlusion-aware Cost Volume and Loss: The second group of the <ref type="table" target="#tab_8">Table 3</ref> shows the effectiveness of the proposed Occlusion-aware Cost Volume CV occ and Occlusion-aware Re-projection Loss L or . Our innovation in the Occlusionaware Re-projection Loss includes two operations: the switching and masking. Solely using either the switching or masking mechanism does not improve the accuracy. These results meet our expectation. The re-projection loss switching mechanism is designed to switch the re-projection source between two reference frames I d t?1 and I d t+1 to avoid occlusion areas, and the masking mechanism is designed to neutralize the influence on the photo-metric error <ref type="bibr" target="#b44">[45]</ref> from oc- In the Histograms, most pixels of our method has lower depth error. In the error map, our method has lighter red color which indicates lower depth errors. We project the dynamic object area depths to 3D point clouds and compare them with ground truth point clouds in the last column. Our prediction matches the ground truth significantly better. More comparisons are provided in the supplementary document. clusion areas to neighboring non-occluded areas. Only avoiding the occlusion area while ignoring its influence on the neighboring areas or vise-versa could not solve the problem. Applying both mechanisms together can significantly improve the depth accuracy. As for the Occlusion-aware Cost Volume, our occlusionfilling mechanism replaces the noisy occluded cost voxels with neighboring nonoccluded voxel values to recover the distribution of the costs and guide the training gradients. Experiments confirm the effectiveness of our design.</p><p>Cycle Consistent training: The depth prior prediction D pr t from ? DP N is used in our DOMD module to disentangle the dynamic objects motion, which is further encoded with geometric constraints in the cost volume to predict the final depth D t . The proposed Dynamic Object Cycle Consistency Loss L c enables the final depth D t to backwards supervise the training of the depth prior prediction D pr t and forms a closed-loop mutual reinforcement. In the first row of the <ref type="table" target="#tab_8">Table 3</ref> third group, we first train the Depth Prior Net ? DP N separately, then freeze it and train the later multi-frame model to cut off the backwards supervision. In this experiment, ? DP N performs similar as normal single-frame model Monodepth2 <ref type="bibr" target="#b9">[10]</ref> and the final depth prediction only shows limited performance. In the last row, when we unfreeze the ? DP N to enable the joint and consistent training, our model achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a novel self-supervised multi-frame monocular depth prediction model, namely DynamicDepth. It disentangle object motions and diminish occlusion effects caused by dynamic objects, achieved the state-of-the-art performance especially at the dynamic object areas on the Cityscapes <ref type="bibr" target="#b2">[3]</ref> and KITTI <ref type="bibr" target="#b29">[30]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials 1 Additional Implementation Details</head><p>Occlusion-aware Re-projection Loss: We obtain the exact occlusion mask O and visible mask V from our DOMD module M o , our Occlusion-aware Reprojection Loss L or always choose the non-occluded source frame pixels for photo-metric error.</p><formula xml:id="formula_12">L or = 1 |I t ? (O t?1 ? O t+1 )| i?It E i or ,<label>(12)</label></formula><formula xml:id="formula_13">E i or = ? ? ? ? ? ? ? ? ? EO i t?1 , if I i t?1 ? V t?1 , I i t+1 ? O t+1 , EO i t+1 , if I i t?1 ? O t?1 , I i t+1 ? V t+1 , min(EO i t?1 , EO i t+1 ), if I i t?1 ? V t?1 , I i t+1 ? V t+1 , 0, if I i t?1 ? O t?1 , I i t+1 ? O t+1 .<label>(13)</label></formula><p>Depth Prior Net: Our Depth Prior Net ? DP N consists of a depth encoder and a depth decoder. We use an ImageNet <ref type="bibr" target="#b4">[5]</ref> pre-trained ResNet18 <ref type="bibr" target="#b15">[16]</ref> as backbone for depth encoder, which has 4 pyramidal scales. Features in each scale are fed to the depth decoder by several UNet <ref type="bibr" target="#b36">[37]</ref> style skip connections. The depth decoder consists of multiple convolution layers for the encoder feature fusion and nearest interpolations for up-sampling.</p><p>Pose Net: Our Pose Net shares a similar architecture as our Depth Prior Net, but it outputs a 6-degree-of-freedom camera ego-motion vector P o instead of the depth map.</p><p>DOMD: Our Dynamic Object Motion Disentanglement (DOMD) module projects the object image patches C t to C d t?1 to replace C t?1 to disentangle the object motion. The projection is based on the depth prior prediction D pr t , known camera intrinsics K, and camera ego-motion prediction P o . We do not need instance-level masks and inter-frame correspondences, all dynamic objects are projected together at once. We use an off-the-shelf semantic segmentation model EffcientPS <ref type="bibr" target="#b30">[31]</ref> to provide the dynamic category segmentation masks. We define the dynamic category as follows: {person, rider, car, truck, bus, caravan, trailer, motorcycle, bicycle}.</p><p>Cost Volume: We pre-define 96 different depth hypothesis bins and reduce the channel number to 1. The cost volume is constructed at the third scale which is in 48 ? 160 resolution, resulting in an CV ? R 96?160?48?1 . Our cost volume only consumes 2.8M B memory when using Float32 data type.</p><p>Depth Encoder and Decoder: Our depth encoder and decoder in the multi-frame model ? M F shares the same architecture with the Depth Prior Net ? DP N . The Occlusion-aware Cost Volume is integrated at the third scale of the encoder.</p><p>Training: We use frames {I t?1 , I t , I t+1 } for training and {I t?1 , I t } for testing. Our model is trained using an Adam <ref type="bibr" target="#b18">[19]</ref> optimizer with a learning rate of 10 ?4 for 10 epochs, which takes about 10 hours on a single Nvidia A100 GPU.</p><p>Evaluation Metrics: Following the state-of-the-art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref>, we use Absolute Relative Error (Abs Rel), Squared Relative Error (Sq Rel), Root Mean Squared Error (RMSE), Root Mean Squared Log Error (RMSE log ), and ? 1 , ? 2 , ? 3 as the metrics to evaluate the depth prediction performance. These metrics are formulated as:</p><formula xml:id="formula_14">AbsRel = 1 n i |pi?gi| gi , SqRel = 1 n i (pi?gi) 2 gi , RMSE = 1 n i (p i ? g i ) 2 , RMSE log = 1 n i (log p i ? log g i ) 2 , ? 1 , ? 2 , ? 3 = % of thresh &lt; 1.25, 1.25 2 , 1.25 3 ,</formula><p>where g and p are the depth values of ground truth and prediction in meters, thresh = max( g p , p g ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additional Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KITTI Benchmark Scores</head><p>The original Eigen <ref type="bibr" target="#b5">[6]</ref> split of KITTI <ref type="bibr" target="#b29">[30]</ref> dataset uses the re-projected singleframe raw LIDAR points as ground truth for evaluation, which may contain outliers such as reflection on transparent objects. We only reported results with this original ground truth in the main paper since it is the most widely used. Jonas et al. <ref type="bibr" target="#b39">[40]</ref> introduced a set of high-quality ground truth depth maps for the KITTI dataset, accumulates 5 consecutive frames to form the denser ground truth depth map, and removed the outliers. This improved ground truth depth is provided for 652 (or 93%) of the 697 test frames contained in the Eigen test split <ref type="bibr" target="#b5">[6]</ref>. We evaluate our method on these 652 improved ground truth frames and compare with existing state-of-art published methods in <ref type="table" target="#tab_10">Table 4</ref>. Following the convention, we clip the predicted depths to 80 meters to match the Eigen evaluation. Methods are ranked by the Absolute Relative Error. Our method outperforms all existing state-of-the-art methods, even some stereo-based and supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Full Quantitative Results</head><p>Due to the space limitation, we only show a part of the quantitative comparison of depth prediction in the main paper. Here we show an extensive comparison to existing state-of-the-art methods on the KITTI <ref type="bibr" target="#b29">[30]</ref> and Cityscapes <ref type="bibr" target="#b2">[3]</ref> dataset in <ref type="table">Table.</ref> 5. Following the convention, methods are sorted by the Abs Rel, which is the relative error with the ground truth. Our method outperforms all other state-of-the-art methods by a large margin, especially on the challenging Cityscapes <ref type="bibr" target="#b2">[3]</ref> dataset, which contains significantly more dynamic objects. Our method even outperformed some stereo-based and supervised methods on the KITTI dataset. Note that all KITTI results in this section are based on the widely-used original <ref type="bibr" target="#b29">[30]</ref> ground truth, which generates much greater error than the improved <ref type="bibr" target="#b39">[40]</ref> ground truth.   We compare our results with other state-of-the-art methods. The I d t?1 image disentangled the dynamic object motion to solve the mismatch problem. As shown in the histograms, most pixels of our method have lower depth error. Our method has lighter red color in the error map which indicates lower depth errors. The dynamic object area depths are projected to 3D point clouds and compared with ground truth point clouds, our prediction matches the ground truth significantly better. In the histograms, most pixels of our method has lower depth error. In the error map, our method has lighter red color which indicates lower depth errors. We project the dynamic object area depths to 3D point clouds and compare them with ground truth point clouds in the last column. Our prediction matches the ground truth significantly better.  <ref type="figure">Fig. 10</ref>: Additional Qualitative visualization: The left column shows the input image frames and our disentangled image I d t?1 , later columns show the comparison with other state-of-the-art methods. In the histograms, most pixels of our method has lower depth error. In the error map, our method has lighter red color which indicates lower depth errors. We project the dynamic object area depths to 3D point clouds and compare them with ground truth point clouds in the last column. Our prediction matches the ground truth significantly better.  <ref type="table">Table 5</ref>: Depth Prediction on KITTI and Cityscapes Dataset. Following the convention, methods in each category are sorted by the Abs Rel, which is the relative error with the ground truth. Best methods are in bold. Our method out-performs all other state-of-the-art methods by a large margin especially on the challenging Cityscapes <ref type="bibr" target="#b2">[3]</ref> dataset, which contains significantly more dynamic objects. Our method even outperformed some stereo based and supervised methods on KITTI dataset. Note that all KITTI result in this table are based on the widely-used original <ref type="bibr" target="#b29">[30]</ref> ground truth, which generates much greater error than the improved <ref type="bibr" target="#b39">[40]</ref> ground truth.</p><p>Legend: Sup -Supervised by ground truth depth S -Stereo M -Monocular</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>DynamicDepth Architecture: The inputs are images I t?1 and I t ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Dynamic object motion disentangled image: Left is the I d t?1 when depth prior is accurate. The right blue image patch shows the re-projected C d t?1 with inaccurate depth prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Occlusion-aware Re-projection Loss: Using the non-occluded source pixels for the re-projection loss could avoid most occlusions. The widelyused<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b37">38]</ref> per-pixel minimum L min r fails when the occluded pixels do not have lower photo-metric error. We propose Occlusion-aware Re-projection Loss L or to solve this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative visualization: The left column shows the input image frames and our disentangled image I d t?1 , later columns show the comparison with other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 9</head><label>9</label><figDesc>shows a full version of the qualitative results and Fig 10 shows an additional set of comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Full Qualitative visualization: The left column shows the input image frames and our disentangled image I d t?1 , later columns show the comparison with other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input Data from Cityscapes Our Dynamic Object Motion Disentanglement Error Map of Manydepth Error Map of Our Method Depth Prediction of Manydepth Depth Prediction of Our Method Depth Error vs Pixels Statistics Improvement of Our Method over Manydepth</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The higher the better Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell></cell><cell>Method</cell><cell>Test frames</cell><cell>WxH</cell><cell cols="2">The lower the better</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ranjan et al.[36]</cell><cell>1</cell><cell cols="2">832 x 256 0.148 1.149 5.464</cell><cell>0.226</cell><cell>0.815</cell><cell>0.935</cell><cell>0.973</cell></row><row><cell></cell><cell>EPC++ [27]</cell><cell>1</cell><cell cols="2">832 x 256 0.141 1.029 5.350</cell><cell>0.216</cell><cell>0.816</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell></cell><cell>Struct2depth (M) [1]</cell><cell>1</cell><cell cols="2">416 x 128 0.141 1.026 5.291</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell></cell><cell>Li et al.[24]</cell><cell>1</cell><cell cols="2">416 x 128 0.130 0.950 5.138</cell><cell>0.209</cell><cell>0.843</cell><cell>0.948</cell><cell>0.978</cell></row><row><cell></cell><cell>Videos in the wild [11]</cell><cell>1</cell><cell cols="2">416 x 128 0.128 0.959 5.230</cell><cell>0.212</cell><cell>0.845</cell><cell>0.947</cell><cell>0.976</cell></row><row><cell>KITTI</cell><cell>Monodepth2 [10] Lee et al. [23] InstaDM [22] Packnet-SFM [12]</cell><cell>1 1 1 1</cell><cell cols="2">640 x 192 0.115 0.903 4.863 832 x 256 0.114 0.876 4.715 832 x 256 0.112 0.777 4.772 640 x 192 0.111 0.785 4.601</cell><cell>0.193 0.191 0.191 0.189</cell><cell>0.877 0.872 0.872 0.878</cell><cell>0.959 0.955 0.959 0.960</cell><cell>0.981 0.981 0.982 0.982</cell></row><row><cell></cell><cell>Johnston et al. [17]</cell><cell>1</cell><cell cols="2">640 x 192 0.106 0.861 4.699</cell><cell>0.185</cell><cell>0.889</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell></cell><cell>Guizilini et al.[13]</cell><cell>1</cell><cell cols="2">640 x 192 0.102 0.698 4.381</cell><cell>0.178</cell><cell>0.896</cell><cell>0.964</cell><cell>0.984</cell></row><row><cell></cell><cell>Patil et al.[32]</cell><cell>N</cell><cell cols="2">640 x 192 0.111 0.821 4.650</cell><cell>0.187</cell><cell>0.883</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell></cell><cell>Wang et al.[43]</cell><cell cols="3">2 (-1, 0) 640 x 192 0.106 0.799 4.662</cell><cell>0.187</cell><cell>0.889</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell></cell><cell>ManyDepth [47]</cell><cell cols="3">2 (-1, 0) 640 x 192 0.098 0.770 4.459</cell><cell>0.176</cell><cell>0.900</cell><cell>0.965</cell><cell>0.983</cell></row><row><cell></cell><cell>DynamicDepth</cell><cell cols="4">2 (-1, 0) 640 x 192 0.096 0.720 4.458 0.175</cell><cell>0.897</cell><cell>0.964</cell><cell>0.984</cell></row><row><cell></cell><cell>Pilzer et al.[34]</cell><cell>1</cell><cell cols="2">512 x 256 0.240 4.264 8.049</cell><cell>0.334</cell><cell>0.710</cell><cell>0.871</cell><cell>0.937</cell></row><row><cell></cell><cell>Struct2Depth 2 [2]</cell><cell>1</cell><cell cols="2">416 x 128 0.145 1.737 7.280</cell><cell>0.205</cell><cell>0.813</cell><cell>0.942</cell><cell>0.976</cell></row><row><cell></cell><cell>Monodepth2 [10]</cell><cell>1</cell><cell cols="2">416 x 128 0.129 1.569 6.876</cell><cell>0.187</cell><cell>0.849</cell><cell>0.957</cell><cell>0.983</cell></row><row><cell>Cityscapes</cell><cell>Videos in the Wild [11] Li et al.[24] Lee et al. [23] InstaDM [22]</cell><cell>1 1 1 1</cell><cell cols="2">416 x 128 0.127 1.330 6.960 416 x 128 0.119 1.290 6.980 832 x 256 0.116 1.213 6.695 832 x 256 0.111 1.158 6.437</cell><cell>0.195 0.190 0.186 0.182</cell><cell>0.830 0.846 0.852 0.868</cell><cell>0.947 0.952 0.951 0.961</cell><cell>0.981 0.982 0.982 0.983</cell></row><row><cell></cell><cell>Struct2Depth 2 [2]</cell><cell cols="3">3 (-1, 0, +1) 416 x 128 0.151 2.492 7.024</cell><cell>0.202</cell><cell>0.826</cell><cell>0.937</cell><cell>0.972</cell></row><row><cell></cell><cell>ManyDepth [47]</cell><cell cols="3">2 (-1, 0) 416 x 128 0.114 1.193 6.223</cell><cell>0.170</cell><cell>0.875</cell><cell>0.967</cell><cell>0.989</cell></row><row><cell></cell><cell>DynamicDepth</cell><cell cols="4">2 (-1, 0) 416 x 128 0.103 1.000 5.867 0.157</cell><cell>0.895</cell><cell>0.974</cell><cell>0.991</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Depth Prediction on KITTI and Cityscapes Dataset. Following the convention, methods in each category are sorted by the Abs Rel, which is the relative error with the ground truth. Best methods are in bold. Our method out-performs all other state-of-the-art methods by a large margin especially on the challenging Cityscapes [3] dataset, which contains significantly more dynamic objects. Note that all KITTI result in this table are based on the widely-used original [30] dataset, which generates much greater error than the improved [40] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>MethodWxH The lower the better The higher the better Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell></cell><cell>Monodepth2 [10]</cell><cell>640 x 192 0.169 1.878 5.711</cell><cell>0.271</cell><cell>0.805</cell><cell>0.909</cell><cell>0.944</cell></row><row><cell>KITTI</cell><cell cols="2">InstaDM [22] ManyDepth [47] Our Depth Prior 640 x 192 0.155 1.317 5.253 832 x 256 0.151 1.314 5.546 640 x 192 0.175 2.000 5.830</cell><cell>0.271 0.278 0.269</cell><cell>0.805 0.776 0.805</cell><cell>0.905 0.895 0.908</cell><cell>0.946 0.943 0.946</cell></row><row><cell></cell><cell cols="3">DynamicDepth 640 x 192 0.150 1.313 5.146 0.264</cell><cell>0.807</cell><cell>0.915</cell><cell>0.949</cell></row><row><cell>Cityscapes</cell><cell cols="3">Monodepth2 [10] InstaDM [22] ManyDepth [47] Our Depth Prior 416 x 128 0.137 1.285 4.674 416 x 128 0.159 1.937 6.363 832 x 256 0.139 1.698 5.760 416 x 128 0.169 2.175 6.634 DynamicDepth 416 x 128 0.129 1.273 4.626 0.168 0.201 0.181 0.218 0.174</cell><cell>0.816 0.859 0.789 0.852 0.862</cell><cell>0.950 0.959 0.921 0.961 0.965</cell><cell>0.981 0.982 0.969 0.985 0.986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Depth Error on Dynamic Objects.</figDesc><table><row><cell>We evaluate the depth predic-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>split, which contains 39, 810 training images, 4, 424 validation images, and 697 testing images. According to our statistic, only 0.34% of the Dynamic Object Dynamic Object Occlusion-aware Occlusion-aware Loss The Lower the Better Motion Disentanglement Cycle Consistency Cost Volume Switching Masking Abs Rel Sq Rel RMSE RMSElog</figDesc><table><row><cell></cell><cell cols="5">Evaluating Dynamic Object Motion Disentanglement</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.114 1.193 6.223</cell><cell>0.170</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.110 1.172 6.220</cell><cell>0.166</cell></row><row><cell></cell><cell></cell><cell cols="3">Evaluating Occlusion-aware CV and Loss</cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.110 1.172 6.220</cell><cell>0.166</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>0.110 1.168 6.223</cell><cell>0.166</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>0.110 1.167 6.210</cell><cell>0.167</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>0.108 1.139 5.992</cell><cell>0.163</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>0.108 1.131 5.994</cell><cell>0.162</cell></row><row><cell></cell><cell cols="5">Evaluating Dynamic Object Cycle Consistent Training</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.107 1.121 5.924</cell><cell>0.160</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">0.103 1.000 5.867 0.157</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study: Evaluating the effects for our proposed Dynamic Object Motion Disentanglement, Cycle Consistent Training, Occlusion-aware Cost Volume and Re-projection Loss on the Cityscapes [3] dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>WxH</cell><cell cols="2">The lower the better</cell><cell cols="3">The higher the better</cell></row><row><cell>Zhan FullNYU [52]</cell><cell>Sup</cell><cell>608 x 160</cell><cell>0.130 1.520 5.184</cell><cell>0.205</cell><cell>0.859</cell><cell>0.955</cell><cell>0.981</cell></row><row><cell cols="2">Kuznietsov et al. [21] Sup</cell><cell>621 x 187</cell><cell>0.089 0.478 3.610</cell><cell>0.138</cell><cell>0.906</cell><cell>0.980</cell><cell>0.995</cell></row><row><cell>DORN [7]</cell><cell>Sup</cell><cell>513 x 385</cell><cell>0.072 0.307 2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.995</cell></row><row><cell>Monodepth [9]</cell><cell>S</cell><cell>512 x 256</cell><cell>0.109 0.811 4.568</cell><cell>0.166</cell><cell>0.877</cell><cell>0.967</cell><cell>0.988</cell></row><row><cell>3net [35] (VGG)</cell><cell>S</cell><cell>512 x 256</cell><cell>0.119 0.920 4.824</cell><cell>0.182</cell><cell>0.856</cell><cell>0.957</cell><cell>0.985</cell></row><row><cell>3net [35] (ResNet 50)</cell><cell>S</cell><cell>512 x 256</cell><cell>0.102 0.675 4.293</cell><cell>0.159</cell><cell>0.881</cell><cell>0.969</cell><cell>0.991</cell></row><row><cell>SuperDepth [33]</cell><cell>S</cell><cell cols="2">1024 x 384 0.090 0.542 3.967</cell><cell>0.144</cell><cell>0.901</cell><cell>0.976</cell><cell>0.993</cell></row><row><cell>Monodepth2 [10]</cell><cell>S</cell><cell>640 x 192</cell><cell>0.085 0.537 3.868</cell><cell>0.139</cell><cell>0.912</cell><cell>0.979</cell><cell>0.993</cell></row><row><cell>EPC++ [27]</cell><cell>S</cell><cell>832 x 256</cell><cell>0.123 0.754 4.453</cell><cell>0.172</cell><cell>0.863</cell><cell>0.964</cell><cell>0.989</cell></row><row><cell>SfMLearner [57]</cell><cell>M</cell><cell>416 x 128</cell><cell>0.176 1.532 6.129</cell><cell>0.244</cell><cell>0.758</cell><cell>0.921</cell><cell>0.971</cell></row><row><cell>Vid2Depth [28]</cell><cell>M</cell><cell>416 x 128</cell><cell>0.134 0.983 5.501</cell><cell>0.203</cell><cell>0.827</cell><cell>0.944</cell><cell>0.981</cell></row><row><cell>GeoNet [51]</cell><cell>M</cell><cell>416 x 128</cell><cell>0.132 0.994 5.240</cell><cell>0.193</cell><cell>0.833</cell><cell>0.953</cell><cell>0.985</cell></row><row><cell>DDVO [42]</cell><cell>M</cell><cell>416 x 128</cell><cell>0.126 0.866 4.932</cell><cell>0.185</cell><cell>0.851</cell><cell>0.958</cell><cell>0.986</cell></row><row><cell>Ranjan [36]</cell><cell>M</cell><cell>832 x 256</cell><cell>0.123 0.881 4.834</cell><cell>0.181</cell><cell>0.860</cell><cell>0.959</cell><cell>0.985</cell></row><row><cell>EPC++ [27]</cell><cell>M</cell><cell>832 x 256</cell><cell>0.120 0.789 4.755</cell><cell>0.177</cell><cell>0.856</cell><cell>0.961</cell><cell>0.987</cell></row><row><cell>Johnston et al. [17]</cell><cell>M</cell><cell>640 x 192</cell><cell>0.081 0.484 3.716</cell><cell>0.126</cell><cell>0.927</cell><cell>0.985</cell><cell>0.996</cell></row><row><cell>Monodepth2 [10]</cell><cell>M</cell><cell>640 x 192</cell><cell>0.090 0.545 3.942</cell><cell>0.137</cell><cell>0.914</cell><cell>0.983</cell><cell>0.995</cell></row><row><cell>Packnet-SFM [12]</cell><cell>M</cell><cell>640 x 192</cell><cell>0.078 0.420 3.485</cell><cell>0.121</cell><cell>0.931</cell><cell>0.986</cell><cell>0.996</cell></row><row><cell>Patil et al.[32]</cell><cell>M</cell><cell>640 x 192</cell><cell>0.087 0.495 3.775</cell><cell>0.133</cell><cell>0.917</cell><cell>0.983</cell><cell>0.995</cell></row><row><cell>Wang et al.[43]</cell><cell>M</cell><cell>640 x 192</cell><cell>0.082 0.462 3.739</cell><cell>0.127</cell><cell>0.923</cell><cell>0.984</cell><cell>0.996</cell></row><row><cell>ManyDepth [47]</cell><cell>M</cell><cell>640 x 192</cell><cell>0.070 0.399 3.455</cell><cell>0.113</cell><cell>0.941</cell><cell>0.989</cell><cell>0.997</cell></row><row><cell>DynamicDepth</cell><cell>M</cell><cell cols="3">640 x 192 0.068 0.362 3.454 0.111</cell><cell>0.943</cell><cell>0.991</cell><cell>0.998</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>KITTI Evaluation on Improved Ground Truth<ref type="bibr" target="#b39">[40]</ref>: Following the convention, methods in each category are sorted by the Abs Rel, which is the relative error with the ground truth. Best methods are in bold. Our method out-performs all other state-of-the-art methods, even some stereo-based and supervised methods.</figDesc><table><row><cell>Legend:</cell><cell>Sup -Supervised by ground truth depth</cell><cell>S -Stereo</cell><cell>M -Monocular</cell></row><row><cell cols="2">3 Additional Qualitative Results</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>MethodTraining WxH The lower the better The higher the better Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell></cell><cell>Zhan FullNYU [52]</cell><cell>Sup</cell><cell>608 x 160</cell><cell cols="2">0.135 1.132 5.585</cell><cell>0.229</cell><cell>0.820</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell></cell><cell>Kuznietsov et al. [21]</cell><cell>Sup</cell><cell>621 x 187</cell><cell cols="2">0.113 0.741 4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell></cell><cell>Gur et al. [14]</cell><cell>Sup</cell><cell>416 x 128</cell><cell cols="2">0.110 0.666 4.186</cell><cell>0.168</cell><cell>0.880</cell><cell>0.966</cell><cell>0.988</cell></row><row><cell></cell><cell>Dorn [7]</cell><cell>Sup</cell><cell>513 x 385</cell><cell cols="2">0.099 0.593 3.714</cell><cell>0.161</cell><cell>0.897</cell><cell>0.966</cell><cell>0.986</cell></row><row><cell></cell><cell>MonoDepth [9]</cell><cell>S</cell><cell>512 x 256</cell><cell cols="2">0.133 1.142 5.533</cell><cell>0.230</cell><cell>0.830</cell><cell>0.936</cell><cell>0.970</cell></row><row><cell></cell><cell>MonoDispNet [49]</cell><cell>S</cell><cell>512 x 256</cell><cell cols="2">0.126 0.832 4.172</cell><cell>0.217</cell><cell>0.840</cell><cell>0.941</cell><cell>0.973</cell></row><row><cell></cell><cell>MonoResMatch [39]</cell><cell>S</cell><cell cols="3">1280 x 384 0.111 0.867 4.714</cell><cell>0.199</cell><cell>0.864</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell></cell><cell>MonoDepth2 [10]</cell><cell>S</cell><cell>640 x 192</cell><cell cols="2">0.107 0.849 4.764</cell><cell>0.201</cell><cell>0.874</cell><cell>0.953</cell><cell>0.977</cell></row><row><cell></cell><cell>UnDeepVO [25]</cell><cell>S</cell><cell>512 x 128</cell><cell cols="2">0.183 1.730 6.570</cell><cell>0.268</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DFR [53]</cell><cell>S</cell><cell>608 x 160</cell><cell cols="2">0.135 1.132 5.585</cell><cell>0.229</cell><cell>0.820</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell></cell><cell>EPC++ [27]</cell><cell>S</cell><cell>832 x 256</cell><cell cols="2">0.128 0.935 5.011</cell><cell>0.209</cell><cell>0.831</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell></cell><cell>DepthHint [46]</cell><cell>S</cell><cell>640 x 192</cell><cell cols="2">0.100 0.728 4.469</cell><cell>0.185</cell><cell>0.885</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell></cell><cell>FeatDepth [38]</cell><cell>S</cell><cell>640 x 192</cell><cell cols="2">0.099 0.697 4.427</cell><cell>0.184</cell><cell>0.889</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell></cell><cell>SfMLearner [57]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.208 1.768 6.958</cell><cell>0.283</cell><cell>0.678</cell><cell>0.885</cell><cell>0.957</cell></row><row><cell></cell><cell>Vid2Depth [28]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.163 1.240 6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell></cell><cell>LEGO [50]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.162 1.352 6.276</cell><cell>0.252</cell><cell>0.783</cell><cell>0.921</cell><cell>0.969</cell></row><row><cell>KITTI Original</cell><cell>GeoNet [51] DDVO [41] DF-Net [58] Ranjan et al.[36] EPC++ [27] Struct2depth (M) [1]</cell><cell>M M M M M M</cell><cell>416 x 128 416 x 128 576 x 160 832 x 256 832 x 256 416 x 128</cell><cell cols="2">0.155 1.296 5.857 0.151 1.257 5.583 0.150 1.124 5.507 0.148 1.149 5.464 0.141 1.029 5.350 0.141 1.026 5.291</cell><cell>0.233 0.228 0.223 0.226 0.216 0.215</cell><cell>0.793 0.810 0.806 0.815 0.816 0.816</cell><cell>0.931 0.936 0.933 0.935 0.941 0.945</cell><cell>0.973 0.974 0.973 0.973 0.976 0.979</cell></row><row><cell></cell><cell>SIGNet [29]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.133 0.905 5.181</cell><cell>0.208</cell><cell>0.825</cell><cell>0.947</cell><cell>0.981</cell></row><row><cell></cell><cell>Li et al.[24]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.130 0.950 5.138</cell><cell>0.209</cell><cell>0.843</cell><cell>0.948</cell><cell>0.978</cell></row><row><cell></cell><cell>Videos in the wild [11]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.128 0.959 5.230</cell><cell>0.212</cell><cell>0.845</cell><cell>0.947</cell><cell>0.976</cell></row><row><cell></cell><cell>DualNet [56]</cell><cell>M</cell><cell cols="3">1248 x 384 0.121 0.837 4.945</cell><cell>0.197</cell><cell>0.853</cell><cell>0.955</cell><cell>0.982</cell></row><row><cell></cell><cell>SuperDepth [33]</cell><cell>M</cell><cell cols="2">1024 x 384 0.116 1.055</cell><cell>-</cell><cell>0.209</cell><cell>0.853</cell><cell>0.948</cell><cell>0.977</cell></row><row><cell></cell><cell>Monodepth2 [10]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.115 0.903 4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell></cell><cell>Lee et al. [23]</cell><cell>M</cell><cell>832 x 256</cell><cell cols="2">0.114 0.876 4.715</cell><cell>0.191</cell><cell>0.872</cell><cell>0.955</cell><cell>0.981</cell></row><row><cell></cell><cell>InstaDM [22]</cell><cell>M</cell><cell>832 x 256</cell><cell cols="2">0.112 0.777 4.772</cell><cell>0.191</cell><cell>0.872</cell><cell>0.959</cell><cell>0.982</cell></row><row><cell></cell><cell>Patil et al.[32]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.111 0.821 4.650</cell><cell>0.187</cell><cell>0.883</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell></cell><cell>Packnet-SFM [12]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.111 0.785 4.601</cell><cell>0.189</cell><cell>0.878</cell><cell>0.960</cell><cell>0.982</cell></row><row><cell></cell><cell>Wang et al.[43]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.106 0.799 4.662</cell><cell>0.187</cell><cell>0.889</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell></cell><cell>Johnston et al. [17]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.106 0.861 4.699</cell><cell>0.185</cell><cell>0.889</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell></cell><cell>FeatDepth [38]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.104 0.729 4.481</cell><cell>0.179</cell><cell>0.893</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell></cell><cell>Guizilini et al.[13]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.102 0.698 4.381</cell><cell>0.178</cell><cell>0.896</cell><cell>0.964</cell><cell>0.984</cell></row><row><cell></cell><cell>ManyDepth [47]</cell><cell>M</cell><cell>640 x 192</cell><cell cols="2">0.098 0.770 4.459</cell><cell>0.176</cell><cell>0.900</cell><cell>0.965</cell><cell>0.983</cell></row><row><cell></cell><cell>DynamicDepth</cell><cell>M</cell><cell cols="4">640 x 192 0.096 0.720 4.458 0.175</cell><cell>0.897</cell><cell>0.964</cell><cell>0.984</cell></row><row><cell></cell><cell>Pilzer et al.[34]</cell><cell>M</cell><cell>512 x 256</cell><cell cols="2">0.240 4.264 8.049</cell><cell>0.334</cell><cell>0.710</cell><cell>0.871</cell><cell>0.937</cell></row><row><cell></cell><cell>Struct2Depth 2 [2]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.145 1.737 7.280</cell><cell>0.205</cell><cell>0.813</cell><cell>0.942</cell><cell>0.976</cell></row><row><cell></cell><cell>Monodepth2 [10]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.129 1.569 6.876</cell><cell>0.187</cell><cell>0.849</cell><cell>0.957</cell><cell>0.983</cell></row><row><cell>Cityscapes</cell><cell>Videos in the Wild [11] Li et al.[24] Lee et al. [23] InstaDM [22]</cell><cell>M M M M</cell><cell>416 x 128 416 x 128 832 x 256 832 x 256</cell><cell cols="2">0.127 1.330 6.960 0.119 1.290 6.980 0.116 1.213 6.695 0.111 1.158 6.437</cell><cell>0.195 0.190 0.186 0.182</cell><cell>0.830 0.846 0.852 0.868</cell><cell>0.947 0.952 0.951 0.961</cell><cell>0.981 0.982 0.982 0.983</cell></row><row><cell></cell><cell>Struct2Depth 2 [2]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.151 2.492 7.024</cell><cell>0.202</cell><cell>0.826</cell><cell>0.937</cell><cell>0.972</cell></row><row><cell></cell><cell>ManyDepth [47]</cell><cell>M</cell><cell>416 x 128</cell><cell cols="2">0.114 1.193 6.223</cell><cell>0.170</cell><cell>0.875</cell><cell>0.967</cell><cell>0.989</cell></row><row><cell></cell><cell>DynamicDepth</cell><cell>M</cell><cell cols="4">416 x 128 0.103 1.000 5.867 0.157</cell><cell>0.895</cell><cell>0.974</cell><cell>0.991</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth and ego-motion learning with structure and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depthnet: A recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cs Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attentional separation-andaggregation network for self-supervised depth-pose learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09369</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Digging into selfsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3D packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image depth estimation trained via depth from defocus cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7683" to="7692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-quality depth from uncalibrated small motion clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern Recognition</title>
		<meeting>the IEEE conference on computer vision and pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5413" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Micro-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Microsoft Research Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning monocular depth in dynamic scenes via instance-aware projection consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th AAAI Conference on Artificial Intelligence/33rd Conference on Innovative Applications of Artificial Intelligence/11th Symposium on Educational Advances in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1863" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentive and contrastive learning for joint depth and motion field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4862" to="4871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7286" to="7291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4521" to="4530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and egomotion from monocular video using 3D geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Signet: Semantic instance aided unsupervised 3d geometry perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sunarjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01004</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Meng_SIGNet_Semantic_Instance_Aided_Unsupervised_3D_Geometry_Perception_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9810" to="9820" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientps: Efficient panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1551" to="1579" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t forget the past: Recurrent depth estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6813" to="6820" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial depth estimation using cycled generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01003</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Tosi_Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="9799" to="9809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Self-supervised joint learning framework of depth estimation via implicit cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09876</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent neural network for (un-) supervised learning of monocular video visual odometry and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5555" to="5564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00225</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00225" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="2162" to="2171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The temporal opportunist: Self-supervised multi-frame monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1164" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monorec: Semisupervised dense reconstruction in dynamic environments from a single moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wimbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Stumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6112" to="6122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00579</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Wong_Bilateral_Cyclic_Constraint_and_Adaptive_Regularization_for_Unsupervised_Monocular_Depth_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5644" to="5653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">LEGO: learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2018.00031</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Yang_LEGO_Learning_Edge_CVPR_2018_paper.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00043</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Exploiting temporal consistency for real-time video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1725" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computational imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00697</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00697" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="6871" to="6880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
