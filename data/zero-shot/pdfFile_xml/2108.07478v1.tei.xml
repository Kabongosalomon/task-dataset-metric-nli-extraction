<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Liang</surname></persName>
							<email>eezhihaoliang@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DexForce Technology Co</orgName>
								<orgName type="institution">Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
							<email>zhihao.li@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
							<email>xusongcen@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instance segmentation in 3D scenes is fundamental in many applications of scene understanding. It is yet challenging due to the compound factors of data irregularity and uncertainty in the numbers of instances. State-of-theart methods largely rely on a general pipeline that first learns point-wise features discriminative at semantic and instance levels, followed by a separate step of point grouping for proposing object instances. While promising, they have the shortcomings that (1) the second step is not supervised by the main objective of instance segmentation, and (2) their point-wise feature learning and grouping are less effective to deal with data irregularities, possibly resulting in fragmented segmentations. To address these issues, we propose in this work an end-to-end solution of Semantic Superpoint Tree Network (SSTNet) for proposing object instances from scene points. Key in SSTNet is an intermediate, semantic superpoint tree (SST), which is constructed based on the learned semantic features of superpoints, and which will be traversed and split at intermediate tree nodes for proposals of object instances. We also design in SST-Net a refinement module, termed CliqueNet, to prune superpoints that may be wrongly grouped into instance proposals. Experiments on the benchmarks of ScanNet and S3DIS show the efficacy of our proposed method. At the time of submission, SSTNet ranks top on the ScanNet (V2) leaderboard, with 2% higher of mAP than the second best method. The source code in PyTorch is available at https:// github.com/Gorilla-Lab-SCUT/SSTNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of 3D instance segmentation is fundamental in many applications concerned with 3D scene understanding. Given an observed scene of point cloud reconstructed from depth cameras via multi-view fusion techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, the task is to both assign semantic labels of pre-defined object * Correspondence to Kui Jia &lt;kuijia@scut.edu.cn&gt; categories to individual scene points, and differentiate those belonging to different object instances. Learning to achieve 3D instance segmentation is challenging at least in the following aspects: <ref type="bibr" target="#b0">(1)</ref> observed scene points are usually sparse and irregular, which poses difficulties for learning pointwise classification based on shape features of local (and possibly global) contexts around individual points; (2) the unknown number of object instances in a scene introduces additional uncertainties to the problem of learning pointinstance associations that is already combinatorial; (3) even though point-wise classification and point-instance associations can be conducted, learning consistencies among spatially adjacent points are not guaranteed, which may cause fragmented segmentations, especially around object boundaries (cf. <ref type="figure" target="#fig_2">Fig. 1</ref> for an illustration).</p><p>State-of-the-art methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b8">9]</ref>, e.g., those ranking top on the ScanNet benchmark <ref type="bibr" target="#b6">[7]</ref>, tackle (some of) the above challenges with the following general pipeline. They first train networks to learn point-wise features that are discriminative at both the semantic and instance levels, followed by a separate step of point clustering that groups together those believed to be on same instances, using the learned point-wise features. While promising, they have the following shortcomings. Firstly, the second step of point clustering is independent of network training, whose results are thus not guaranteed by guiding towards the ground-truth groupings of object instances. Secondly, while superpoints <ref type="bibr" target="#b18">[19]</ref> have been commonly used for semantic segmentation of 3D points <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref>, when coming to instance segmentation, these state-of-the-art methods, except OccuSeg <ref type="bibr" target="#b14">[15]</ref>, choose to conduct both feature learning and grouping in a point-wise manner, which takes away their chance to leverage the geometric regularities established at the mid-level shape representation of superpoints.</p><p>To overcome these shortcomings, we are motivated to develop an end-to-end solution for proposing object instances from an observed scene of points. Considering that a superpoint represents a geometrically homogeneous neighborhood, we choose to work with superpoints pre-computed from the scene points, and the problem of instance segmentation boils down as learning a network that groups superpoints on same object instances. In this work, we design such a solution called Semantic Superpoint Tree Network (SSTNet), as illustrated in <ref type="figure">Fig. 2</ref>. Similar to existing methods, SSTNet starts with a backbone that learns point-wise semantic and instance-level features; differently from them, SSTNet immediately aggregates these features as superpoint-wise ones efficiently via point-wise pooling. Key in SSTNet is an intermediate, semantic superpoint tree (SST), with the superpoints as its tree leaves. SST is constructed based on the pooled semantic (and instancelevel) features of superpoints, and will be traversed and split by the subsequent SSTNet module of binary classification; starting from the root, a proposal of object instance is formed as the superpoints of a tree branch when nonsplitting decision is made at the intermediate tree node that spans the branch (cf. <ref type="figure" target="#fig_1">Fig. 3</ref> for an illustration). Our tree construction is highly efficient by choosing ways of feature inheritance from leaves to the root and pair-wise similarity metric, which support fast algorithms such as nearestneighbor chain <ref type="bibr" target="#b35">[35]</ref>. We note that erroneous assignments of superpoints to instances may occur when constructing and traversing the tree. To compensate, we design a subsequent refinement module termed CliqueNet, which converts each proposed branch as a graph clique and learns to prune some of the branch nodes. A ScoreNet <ref type="bibr" target="#b15">[16]</ref> is finally used to evaluate the generated proposals, which gives instance segmentation results of our SSTNet.</p><p>Thorough experiments on the benchmark datasets of ScanNet <ref type="bibr" target="#b6">[7]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref> show the efficacy of our proposed method. Notably, SSTNet outperforms all existing methods on the two benchmarks, and at the time of submission, it ranks top on the ScanNet (V2) leaderboard, with 2% higher of mAP than the second best method. We finally summarize our technical contributions as follows.</p><p>? We propose an end-to-end solution of Semantic Superpoint Tree Network (SSTNet) to directly propose and evaluate object instances from observed 3D scenes. By working with superpoints, our method enjoys the benefit of geometric regularity that supports consistent and sharp segmentations, especially at object boundaries.</p><p>? We choose a strategy of divisive grouping in SSTNet, which first builds the tree, followed by tree traversal for object proposal via node splitting. By constructing the tree with appropriate node merging and feature inheritance, our strategy is an order of magnitude faster than the alternative, agglomerative grouping, thus enabling efficient training and inference of SSTNet.</p><p>? Considering that erroneous assignments of superpoints to instances may occur when constructing and travers-ing the tree, we design a refinement module in SST-Net, termed CliqueNet, which converts each proposed branch as a graph clique and learns to prune some of the branch nodes. Experiments show its efficacy. <ref type="figure" target="#fig_2">Figure 1</ref>. Visualization of example instance segmentation results from an existing, point-wise grouping method (PointGroup <ref type="bibr" target="#b15">[16]</ref>, left) and our SSTNet (right). Different colors represent segmented instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we briefly review the literature of 3D segmentation, focusing on those relevant to elements of our proposed method. 3D Semantic Segmentation Establishing geometric regularities is essential to realize semantic segmentation for an irregular point cloud. Recent methods used projection <ref type="bibr" target="#b19">[20]</ref>, voxelization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref> or local aggregation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">34]</ref> to perform brief regularization, while the subsequent semantic learning task is still challenging. Instead, superpoint-based <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> methods aggregated the geometrically homogeneous points as superpoints to establish a certain degree of geometric regularities. Furthermore, superpoints become the mid-level shape representation to boild down the problem of instance segmentation as grouping the superpoints that belong to the same instance. 3D Instance Segmentation Considering bottom-up methods, which cluster results based on semantic segmentation. <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b16">17]</ref> heuristically <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref> clustered instance masks based on discriminative instance-level features <ref type="bibr" target="#b1">[2]</ref>. Intuitively, PointGroup <ref type="bibr" target="#b15">[16]</ref> utilized the adjacency of instancewise coordinates. The above clustering results relied on the boundary conditions due to the lack of explicit boundary supervision. To address this issue, SSTNet combines the bottom-up clustering strategy with top-down traversal to realize end-to-end learning proposal generation. Image Segmentation for Object Proposals To overcome the complexity caused by sliding windows <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">28]</ref>, segmentation-based <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31]</ref> methods treat 2D detection as image segmentation, where the candidates are hypothesized from hierarchical image segmentation using an agglomeration manner. Furthermore, SSTNet involves a greedy agglomeration strategy and employs a learning splitting classifier to get rid of dependence on the times of agglomeration and generate precise mask results. <ref type="bibr" target="#b0">(1)</ref> Over-Segmentation <ref type="bibr" target="#b1">(2)</ref> Sparse 3D U-Net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling in each Superpoint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree Split Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-up Construction</head><p>Top-down traversal NN Chain <ref type="bibr" target="#b2">(3)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ScoreNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Instance Predictions</head><p>CliqueNet <ref type="bibr" target="#b3">(4)</ref> Refined Proposals  <ref type="figure">Figure 2</ref>. Overview of our proposed Semantic Superpoint Tree Network (SSTNet). Please refer to the main text for details of the individual modules. N is the number of scene points, M is the number of superpoints, K is the number of categories, and n is the dimension of output features from the backbone.f ,?,? denote the point-wise features, semantic scores and offsets respectively. (1) input scene, (2) generated superpoint set {Pi} M i=1 , (3) foreground superpoints and Semantic Superpoint Tree(SST)T , (4) generated proposals R after tree traversal and splitting. Nearest-neighbor chain (NN Chain) is the algorithm we use for efficiently constructing the tree.</p><formula xml:id="formula_0">HNIR ? ? -Feature</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Assume an input point set I = {p ? R 3 } of reconstructed 3D scene from depth cameras via multi-view fusion techniques (e.g., SLAM <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>), which contains an unknown number of object instances of K categories. The task is to segment out those points in I that define each of such instances, and is challenging as analyzed in Section 1. To alleviate the difficulty, we choose to establish a certain degree of geometric regularities for points in I, by leveraging a mid-level shape representation called superpoints <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> -for an input I, superpoints define geometrically homogeneous neighborhoods of its local points, and are usually computed by over-segmenting I using graph partition. <ref type="bibr" target="#b0">1</ref> With the set of superpoints {P} pre-computed from I, the problem of instance segmentation boils down as grouping together spatially close {P} that belong to a same object instance and assigning them a semantic label. This is technically a clustering/grouping problem in the 3D space where superpoints live; given spatial compactness for superpoints on a same instance, it is natural to consider hierarchical clustering/grouping to achieve the goal. The strategy resembles those used for object proposals in 2D images via hierarchical image segmentation <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b12">13]</ref>. To implement the above idea, we propose in this work an end-to-end, hierarchical segmentation network that is trained to semantically group superpoints of a scene as object instances of pre-defined categories. The key in our network is an intermediate, semantic superpoint tree (SST); it is constructed based on the learned semantic features in the preceding network module, and will be traversed and split in the subsequent network module; we thus term our proposed method as Semantic Superpoint Tree Network (SSTNet). <ref type="figure">Fig. 2</ref> gives the illustration.</p><p>More specifically, SSTNet starts with a backbone that learns point-wise featuref ? R n for each p ? I, which is then fed into a subsequent module of semantic scoring to output semantic score? ? [0, 1] K and offset? ? R 3 , where? is a K-dimensional probability vector representing soft label prediction of the point p, and? is a predicted coordinate offset relative to center of the instance to which p belongs. In parallel with this module we apply over-segmentation to I to have the superpoints {P}; note that this is applied only once during network training. The point-wise {f }, {?}, and {?} are aggregated, via average pooling, inside each superpoint to form the superpoint feature f ? R n , score a ? [0, 1] K , and offset o ? R 3 . Assume that a collection of superpoints are obtained from I, we use the thus obtained {f }, {a}, and {o} for use in subsequent modules of the network. To achieve efficient training of SSTNet, we choose divisive grouping (i.e., in a topdown manner) after construction of semantic superpoint tree T , instead of agglomerative grouping commonly used in hierarchical image segmentation <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b32">32]</ref>, which means that the whole tree T is first constructed whose leaf nodes represent individual superpoints. We then design a module of tree traversal and splitting that learns to hierarchically split the tree nodes; starting from the root, a proposal of object instance is formed as a tree branch when nonsplitting decision is made at an intermediate tree node. We note that erroneous assignments between superpoints and object instances may occur during both stages of tree con-struction and tree traversal and splitting. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> demonstrated the refinement can achieve higher accuracy for mesh reconstruction. Inspired by them, we design a subsequent refinement module termed CliqueNet to compensate for some of these errors. This module converts each proposal branch as a graph clique and learns to prune some of the branch nodes. We finally use a ScoreNet <ref type="bibr" target="#b15">[16]</ref> to evaluate the generated proposals, which gives instance segmentation results of our SSTNet. The whole network is trained in an end-to-end manner, which, to the best of our knowledge, is the first one for the task of 3D instance segmentation on point set. The intermediate SST construction is highly efficient, whose computational complexity and running time are given in Section 4.2. Section 4 also presents individual modules of the network and compares with alternative designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Individual Modules of the Proposed Network 4.1. Backbone and Semantic Scoring</head><p>Assume that the input I contains N points. Given</p><formula xml:id="formula_1">{p i ? I} N i=1 , we use a 3D convolutional backbone of U-Net style [30] to learn the point-wise features {f i ? R n } N i=1</formula><p>, whose layers are implemented as submanifold sparse convolution (SSC) or sparse convolution (SC) <ref type="bibr" target="#b11">[12]</ref>. We give layer specifics in the supplementary material.</p><p>We obtain the semantic scoring</p><formula xml:id="formula_2">{? i ? [0, 1] K } N i=1 and offset prediction {? i ? R 3 } N i=1 from {f i } N i=1</formula><p>, by employing two multi-layer perceptrons (MLPs) respectively. Let {? * i } N i=1 denote the ground-truth semantic labels of the N points in the form of K-dimensional, one-hot vector. We use the following loss to train the MLP for semantic scoring</p><formula xml:id="formula_3">L semantic = ? 1 N N i=1 CE(? i ,? * i ) + 1 ? 2 N i=1? i? * i N i=1? i? i + N i=1? * i? * i ,<label>(1)</label></formula><p>where CE(?, ?) denotes the cross-entropy loss, and the remaining terms in (1) define a dice loss that alleviates the imbalance among the K categories <ref type="bibr" target="#b21">[22]</ref>. Let c * p denote the geometric center of the object instance to which any p ? I belongs. We use the following loss to train the MLP for offset prediction</p><formula xml:id="formula_4">L offset = 1 N N i=1 ? i ? (c * pi ? p i ) 2 ? I(p i ) ? 1 N N i=1? i ? i 2 ? c * pi ? p i c * pi ? p i 2 ? I(p i ),<label>(2)</label></formula><p>where I(p) ? {0, 1} is an indicator function telling whether the point p belongs to any object instance, and N = N i=1 I(p i ) counts the number of such points. We give specifics of the two MLPs in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Construction of Semantic Superpoint Tree</head><p>As stated in the preceding section, our construction of SST T is based on superpoints {P} pre-computed from the input I; without loss of generality, we assume M ones are computed from I.</p><formula xml:id="formula_5">Features {f i ? R n } M i=1 , semantic scores {a i ? [0, 1] K } M i=1 , and offsets {o i ? R 3 } M i=1</formula><p>at the superpoint level are obtained simply via average pooling over those point-wise ones inside each of superpoints</p><formula xml:id="formula_6">{P i } M i=1 . Given the predicted {f i , a i , o i } M i=1 for {P i } M i=1</formula><p>, a tree can grow greedily <ref type="bibr" target="#b22">[23]</ref>, starting from merging the leaf nodes of superpoints (cf. <ref type="figure" target="#fig_1">Fig. 3</ref> for an illustration). To define the linkage criteria, there exist many choices of similarity metric between any pair of P i and P j . In this work, we choose semantic score and offset prediction over the triple {f , a, o} to define the metric. Specifically, for a superpoint P, we first compute the predicted geometric center of a (possible) object instance to which it may belong as c P = o + 1 |P| |P| i=1 p i , and then concatenate a ? = [a; c P ] ? R K+3 2 . We use the augmented a ? i and a ? j to represent P i and P j , and compute the Euclidean distance a ? i ? a ? j as the linkage criterion that determines the ordering of pair-wise superpoint merging. Merging two superpoints P i and P j results in an intermediate tree node, denoted as t ? T . We compute semantic score of t, via weighted averaging, as</p><formula xml:id="formula_7">a t = w i a i + w j a j ,<label>(3)</label></formula><p>where the weights w i and w j are proportional to the respective sizes of P i and P j , i.e., w i = |P i |/(|P i | + |P j |) and w j = |P j |/(|P i | + |P j |). Offset prediction of t is computed similarly as o t = w i o i + w j o j . We then compute the augmented a ? t from the obtained a t and o t . Note that we also compute the feature f t = w i f i +w j f j for the node t, which will be used in the subsequent module of proposal generation via tree traversal and splitting. Given the augmented a ? t for any t ? T and the pair-wise similarity metric based on Euclidean distance, the tree can be constructed hierarchically, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, whose depth ranges between log M 2 and M ? 1. For clarity, we write the M leaf nodes as {t Pi ? T } M i=1 and any root or intermediate one as t ? T . Our use of the augmented semantic score a ? = [a; c P ] to represent each t P (and t) is based on the argument that for any pair of P i and P j on a same instance, both their se-2 Considering the domain difference of a ? [0, 1] K and c P ? R 3 , we ever try weighted concatenation such as a ? = [?a; ?c P ], where ? and ? are hyper-parameters. We end with the empirical setting of ? = ? = 1, which gives good results in practice.  mantic scores and instance centers are expected to be consistent. Empirical results in Section 5.1 show that it gives better performance than alternative choices do, which verifies the hypothesis. We thus term the constructed T as semantic superpoint tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree Split Classifier</head><p>Remarks Given the M superpoints, the hierarchical tree construction described above has a complexity of O(M 3 ).</p><p>Due to the linear feature inheritance (3) and the use of Euclidean distance as the similarity metric, construction of T can be made highly efficient by employing the fast algorithm of nearest-neighbor chain <ref type="bibr" target="#b22">[23]</ref>, which results in a same T at a complexity of O(M 2 ). On a machine running at 13 Hz, it takes ? 75 milliseconds per construction (e.g., scenes of ScanNet <ref type="bibr" target="#b6">[7]</ref>), thus supporting online SST construction per iteration of network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Proposal Generation via Tree Traversal and Splitting</head><p>Given the constructed SST T , our proposed SSTNet generates proposals of object instance by learning a binary classifier that traverses and splits nodes of T . For any root or intermediate node t, denote its two child nodes as s 1 ? T and s 2 ? T . Each t in fact defines a tree branch, denoted as B t , that contains leaf nodes of superpoints. As stated in Section 4.2, feature f t and augmented score a ? t associated with each t have been hierarchically inherited from its contained superpoints. We use the concatenated</p><formula xml:id="formula_8">f ? t = [f t ; a ? t ] ? R n+K+3</formula><p>as feature of node t. Denote the binary classifier to be learned as ? : R n+K+3 ? R n+K+3 ? (0, 1). Starting from the root node, we maintain a queue of tree traversal in a breadth-first manner. Let Q and R be two empty sets, and push the root into the queue Q. A node t is to be split once ?(f ? s1 , f ? s2 ) &lt; 0.5 , i.e., the two child nodes of t are believed to belong to different object instances; we then push s 1 and s 2 into the queue Q. Conversely, when ?(f ? s1 , f ? s2 ) ? 0.5, we consider all superpoints contained in the tree branch B t as a proposal of object instance, and push t into R; we stop traversing the intermediate nodes contained in B t . Note that we have es-tablished an index table of the hierarchical node-inheriting relations when constructing T , which supports efficient retrieval of both intermediate and leaf nodes/superpoints contained in any branch B t . All proposals of object instance would be obtained in R when the queue Q becomes empty. Algorithm 1 gives pseudo code of the above procedure.</p><p>In this work, we implement the classifier ? as an MLP, whose details are given in the supplementary material. To train ?, we define the instance-level, ground-truth labels for nodes of the tree as follows. Assume that a training scene I contains J object instances, which may belong to some of the K categories. For any superpoint P (i.e, a leaf node t P ), we assign its instance-level, soft label q * P ? [0, 1] J according to what proportions its contained points belong to (some of) the J instances. The soft label q * t ? [0, 1] J for any intermediate or root t is again hierarchically inherited, via weighted averaging, from those of superpoints, similar to the inheritance of features. Given that s 1 and s 2 are the two child nodes of t in T , we use the following loss symmetric to them to train ?</p><formula xml:id="formula_9">L splitting = E t?T /{t P i } M i=1 1 2 [BCE(?(f ? s1 , f ? s2 ), q * s1 q * s2 ) + BCE(?(f ? s2 , f ? s1 ), q * s1 q * s2 )],<label>(4)</label></formula><p>where BCE(?, ?) denotes a binary cross-entropy loss, and q * s1 q * s2 ? [0, 1] indicates, in a soft manner, whether the two child nodes belong to a same instance. Remarks In the proposed SSTNet, we choose to first build the tree, as described in Section 4.2, and then learn to traverse and split tree nodes to generate instance proposals; in other words, we choose a strategy of divisive grouping, instead of an agglomerative one commonly used in hierarchical image segmentation <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b12">13]</ref>. Our motivation for such a design is mostly computational: by using nearestneighbor chain <ref type="bibr" target="#b22">[23]</ref>, our tree construction has a complexity of O(M 2 ), and the tree traversal to propose all the branches of object instances has a complexity of O(M ), giving rise to an overall complexity of O(M 2 + M ); in contrast, learning to generate proposals in an agglomerative manner has an order-of-magnitude higher complexity of O(M 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CliqueNet for Refinement of Proposals</head><p>We note that in the forward pass of SSTNet, once a superpoint P truly on an object instance is constructed into a wrong branch B t of SST T , e.g., B t corresponding to the background or a different instance, the mistake cannot be corrected. Nevertheless, when any branch B t is proposed as an object instance, we have the chance to improve its score evaluation (cf. Section 4.5) by pruning its contained superpoints that may belong to other instances or the background.</p><p>Consider a proposed branch B t consisting of M t leaf nodes of superpoints. A straightforward way to implement the pruning is to concatenate feature representation ift.isleaf() then 6:  f ? t at node t with each f ? P of {f ? Pi } Mt i=1 , and to learn a binary classifier that decides whether the superpoint P should be removed. This, however, involves only the pairwise relation between f ? t and each f ? P , and we empirically find that it is less effective to prune erroneously assigned superpoints. In this work, we propose a more effective scheme, termed CliqueNet, that determines which superpoints to remove by learning the feature interactions among {f ? t , f ? P1 , . . . , f ? P M t }. Specifically, given the proposed branch B t as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, we first connect the node t directly with individual leaf nodes/superpoints, which forms a clique C when thinking of the whole SST T as a graph -we note that the cliques formed for different proposed branches are independent with each other, i.e., they are not on a same graph. An adjacency matrix A C ? {0, 1} (Mt+1)?(Mt+1) can be computed that specifies node connections of the clique. Let? C = A C + I, where I is an identity matrix, and write features of clique nodes compactly as</p><formula xml:id="formula_10">{s 1 , s 2 } = t.getchild() 7: f ? s1 = s 1 .getfeature() 8: f ? s2 = s 2 .getfeature() 9: if ?(f ? s1 , f ? s2 ) ? 0.</formula><formula xml:id="formula_11">F ? C = [f ? t , f ? P1 , . . . , f ? P M t ] ? R (n+K+3)?(Mt+1)</formula><p>. Denote the CliqueNet as a function ?, the first layer of ? computes</p><formula xml:id="formula_12">ReLU(D ?1/2 C? CD ?1/2 C F ? C W 1 ? ),<label>(5)</label></formula><p>whereD C is the diagonal degree matrix of? C , and W 1 ? denotes weight matrix of the first layer of ?. In this work, we use a three-layer CliqueNet whose specifics are given in the supplementary material. CliqueNet outputs scores ?(F ? C , A C ) ? (0, 1) Mt+1 defined respectively for the M t + 1 nodes in C. To train ?, we impose supervision on each node pair of t and P i , i ? {1, . . . , M t }, giving rise to</p><formula xml:id="formula_13">L refining = 1 M t Mt i=1 BCE(?(F ? C , A C ), q * t q * Pi ),<label>(6)</label></formula><p>where the instance-level, soft labels q * t ? [0, 1] J and q * P ? [0, 1] J are defined in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Proposal Evaluation</head><p>Denote a proposed branch of object instance, after pruning some superpoints by CliqueNet, as B ? t , and assume that it contains N ? t raw points. Recall that their point-wise features have been computed by the backbone of SSTNet. We write these features compactly as</p><formula xml:id="formula_14">F B ? t = [f 1 , . . . ,f N ? t ] ? R n?N ? t .</formula><p>We follow <ref type="bibr" target="#b15">[16]</ref> and use a ScoreNet, denoted as ?, to evaluate the proposal. The ScoreNet is simply a miniature of U-Net; one may refer to <ref type="bibr" target="#b15">[16]</ref> for the network details. Depending on the intersection-over-union (IoU) value with the ground-truth instances in the scene I, we define label of the proposal as v * t ? [0, 1] (cf. the supplementary material for details of setting the v * t value), and train the ScoreNet with the following loss</p><formula xml:id="formula_15">L evaluation = 1 |R| t?R BCE(?( F B ? t ), v * t ),<label>(7)</label></formula><p>where |R| is the number of proposals generated by our SST-Net (cf. Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Training and Inference</head><p>We write our overall objective for training SSTNet as</p><formula xml:id="formula_16">L SSTNet = L semantic + L offset + L splitting + L refining + L evaluation . (8)</formula><p>Note that SSTNet is trained in a greedy, module-wise manner, which means that the individual loss terms applied to their respective modules are sequentially invoked into the overall loss <ref type="bibr" target="#b7">(8)</ref>. Although the tree T needs to be constructed in every forward pass of SSTNet, it is highly efficient as indicated by the complexity and practical running time given in preceding sections. The complexity of tree traversal for instance proposals is linear w.r.t. the number of superpoints; furthermore, once a proposal is formed at an intermediate tree node, it is not necessary to traverse all the descendant nodes. The inference is simply a same procedure as a forward pass of SSTNet training. Given the nonoverlapping nature of our proposed object instances, postprocessing steps such as non-maximum suppression are not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Semantic GT Semantic Pred Instance Pred Instance GT <ref type="figure">Figure 5</ref>. Visualization of the semantic and instance segmentation results on the validation set of ScanNet v2 (top) and S3DIS (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets We conduct experiments using the benchmark datasets of ScanNet (V2) <ref type="bibr" target="#b6">[7]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>. ScanNet has 1201 training, 312 validation, and 100 test scenes that contain object instances of 18 categories. Surface normals are also provided for each scene. We do analysis and ablation studies on its validation set, and submit our results to the hidden test set. S3DIS contains 6 large-scale indoor scenes with 13 object classes, we evaluate our model in the following aspects: (1) Area-5 is treated as the testing, while residuals are used for training, and (2) 6-fold cross validation that each area is treated as the testing once. Implementation Details For each input scene, we concatenate the RGB values and point coordinates as the point-wise inputs of SSTNet. The network is trained using AdamW optimizer <ref type="bibr" target="#b20">[21]</ref>, with an initial learning rate of 1e-3 and weight decay of 1e-4; learning rates follow a polynomial learning rate policy. We set the batch size as 4. We pre-process scenes of S3DIS dataset by sub-sampling its points at a rate of 1/4. We employ a graph-based segmentation method <ref type="bibr" target="#b9">[10]</ref> to generate superpoints for ScanNet scenes. For S3DIS, each scene is represented by colored point cloud and we employ SPP + SPG <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> to generate its superpoints. Module and layer specifics of SSTNet are given in the supplementary material. Evaluation Metrics Following the official ScanNet (V2) evaluation protocol, we report mean Average Precisions (mAPs) at different thresholds of IoU as the evaluation metric to compare different methods. The mAP@25 and mAP@50 denote the average precision scores with IoU thresholds respectively set to 25% and 50%, and the mAP averages the scores with IoU thresholds set from 50% to 95%, with a step size of 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Studies and Analyses</head><p>We first conduct ablation studies to evaluate the efficacy of individual components proposed in SSTNet. These studies are conducted on the ScanNet (V2) dataset <ref type="bibr" target="#b6">[7]</ref>. Analysis on Features for SST Construction The quality of SST depends on what features are used when contructing the tree. In this work, for a superpoint P, we choose semantic score a and predicted instance center c P over the triple {f , a, c P }, where c P is computed from the offset prediction o (cf. Section 4.2), and form the augmented a ? = [a; c P ] for SST construction. Results in <ref type="table" target="#tab_2">Table 3</ref> verify our argument that for any pair of superpoints on a same instance, their semantic scores and instance centers are expected to be consistent, while their superpoint-wise features are not necessarily to be similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficacy of Proposal Generation via Tree Traversal and Split Learning</head><p>To verify the efficacy of our main proposal generation scheme via SST, we compare with two alternatives. The first alternative conducts the same traversal of SST but replaces the node-splitting classifier ? with a simple thresholding scheme, which we term as SST-Thresholding; to determine whether an intermediate node t is to be split into its child nodes s 1 and s 2 , it thresholds the Euclidean distance a ? s1 ? a ? s2 2 where we optimally tune the thresholds for its best performance <ref type="bibr" target="#b2">3</ref>   the second alternative, instead of relying on SST construction, given the M superpoints with its augmented semantic scores {a ? i } M i=1 , we first build a K-nearest-neighbor graph based on pair-wise Euclidean distances, and then train a classifier to decide whether some graph edges should be disconnected; the resulting, disconnected graph cliques are proposed as object instances; we term this alternative as Superpoint Graph, which can be interpreted as a flattened version of learning to propose object proposals. <ref type="table">Table 4</ref> shows that SST-thresholding performs the best at the low-precision metric of mAP@25, suggesting our construction of SST is indeed useful for generation of object proposals. On the averaged metric of mAP, SSTNet greatly outperforms the two alternatives. Efficacy of the CliqueNet Refinement Ablation study on the efficacy of CliqueNet is presented in <ref type="table" target="#tab_5">Table 5</ref>, which shows that pruning superpoints from proposed tree branches is effective at high-precision regimes of mAP metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on the ScanNet Benchmark</head><p>We train SSTNet on the training set of ScanNet (V2) and submit our model onto the testing sever. <ref type="table">Table 1</ref>   <ref type="table">Table 4</ref>. Analyses on the efficacy of our proposal generation via traversal and node-splitting learning of semantic superpoint tree. Experiments are conducted on the validation set of ScanNet (V2) <ref type="bibr" target="#b6">[7]</ref>. Reer to the main text for how the two alternatives are designed.   <ref type="table">Table 6</ref>. Results of instance segmentation on the S3DIS validation set. Methods without the ? marks are evaluated on Area-5; methods marked with ? are evaluated on 6-fold cross validation.</p><p>AP@25 and AP@50 are reported in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on S3DIS</head><p>Following the protocols used in previous methods, we employ the Area-5 and 6-fold cross validation, and use the mAP/AP@50/mean precision (mPrec)/mean recall(mRec) with IoU threshold 0.5 to evaluate SSTNet on the S3DIS dataset. One may refer to <ref type="bibr" target="#b33">[33]</ref> for precise definitions of these metrics. <ref type="table">Table 6</ref> shows that SSTNet outperforms all exist methods, confirming the generalizable advantage of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration on the construction and traversal of semantic superpoint tree (SST). ? represents the bottom-up construction process; ? represents the top-down traversal process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Pseudo code of proposal generation via tree traversal and splitting Input: tree T , node features {f ? i } |T | i=1 , classifier ?; 1: initialize R = ? to store proposals, and queue Q = ?; 2: push the root of T into Q; 3: while -Q.isempty() do 4: t = Q.dequeue() 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 then 10 :</head><label>10</label><figDesc>push t into R, and B t = t.getbranch()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration on conversion of a tree branch as a (graph) clique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. For Method AP bath bed bkshf cab chair cntr curt desk door ofurn pic fridg showr sink sofa table toilet wind 3D-MPA[9] 35.5 45.7 48.4 29.9 27.7 59.1 4.7 33.2 21.2 21.7 27.8 19.3 41.3 41.0 19.5 57.4 35.2 84.9 21.3 SSEN[38] 38.4 85.2 49.4 19.2 22.6 64.8 2.2 39.8 29.9 27.7 31.7 23.1 19.4 51.4 19.6 58.6 44.4 84.3 18.4 PE[37] 39.6 66.7 46.7 44.6 24.3 62.4 2.2 57.7 10.6 21.9 34.0 23.9 48.7 47.5 22.5 54.1 35.0 81.8 27.3 PointGroup[16] 40.7 63.9 49.6 41.5 24.3 64.5 2.1 57.0 11.4 21.1 35.9 21.7 42.8 66.0 25.6 56.2 34.1 86.0 29.1 OccuSeg[15] 48.6 80.2 53.6 42.8 36.9 70.2 20.5 33.1 30.1 37.9 47.4 32.7 43.7 86.2 48.5 60.1 39.4 84.6 27.3 Our SSTNet 50.6 73.8 54.9 49.7 31.6 69.3 17.8 37.7 19.8 33.0 46.3 57.6 51.5 85.7 49.4 63.7 45.7 94.3 29.0 Table 1. 3D instance segmentation on ScanNet (V2) benchmark (hidden testing set). Results of SSTNet are obtained by submitting onto the testing server the model trained on the ScanNet training set on January 4th, 2021. Table 2. 3D instance segmentation on ScanNet (V2) benchmark (hidden testing set). Results of SSTNet are obtained by submitting onto the testing server the model trained on the ScanNet training set on January 4th, 2021.</figDesc><table><row><cell cols="2">Method</cell><cell cols="3">mAP AP@50 AP@25</cell></row><row><cell cols="2">3D-MPA[9]</cell><cell>35.5</cell><cell>61.1</cell><cell>73.7</cell></row><row><cell cols="2">SSEN[38]</cell><cell>38.4</cell><cell>57.5</cell><cell>72.4</cell></row><row><cell cols="2">PE[37]</cell><cell>39.6</cell><cell>64.5</cell><cell>77.6</cell></row><row><cell cols="3">PointGroup[16] 40.7</cell><cell>63.6</cell><cell>77.8</cell></row><row><cell cols="2">OccuSeg[15]</cell><cell>48.6</cell><cell>67.2</cell><cell>74.2</cell></row><row><cell cols="2">Our SSTNet</cell><cell>50.6</cell><cell>69.8</cell><cell>78.9</cell></row><row><cell cols="2">Superpoint Semantic</cell><cell>Instance</cell><cell>mAP</cell><cell>AP@50 AP@25</cell></row><row><cell>feature</cell><cell>score</cell><cell>center</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>40.1</cell><cell>55.3</cell><cell>66.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>43.5</cell><cell>59.8</cell><cell>72.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>47.3</cell><cell>61.6</cell><cell>71.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>48.9</cell><cell>63.6</cell><cell>72.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>49.4</cell><cell>64.3</cell><cell>74.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Analysis on the features used for SST construction. Experiments are conducted on the validation set of ScanNet (V2)<ref type="bibr" target="#b6">[7]</ref>. Refer to Section 4.2 for how the three types of features are computed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>shows that on the leaderboard of ScanNet (V2) test set, SSTNet outperforms all existing methods. Results at the metrics of</figDesc><table><row><cell>Method</cell><cell cols="3">mAP AP@50 AP@25</cell></row><row><cell cols="2">SST-Thresholding 46.3</cell><cell>62.6</cell><cell>74.7</cell></row><row><cell cols="2">Superpoint Graph 44.4</cell><cell>60.6</cell><cell>69.5</cell></row><row><cell>Our SSTNet</cell><cell>49.4</cell><cell>64.3</cell><cell>74.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the efficacy of CliqueNet for proposal refinement. Experiments are conducted on the validation set of ScanNet (V2)<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">mAP AP@50 mPrec mRec</cell></row><row><cell>ASIS[33]</cell><cell>-</cell><cell>-</cell><cell>55.3</cell><cell>42.4</cell></row><row><cell>PointGroup[16]</cell><cell>-</cell><cell>57.8</cell><cell>61.9</cell><cell>62.1</cell></row><row><cell>Our SSTNet</cell><cell>42.7</cell><cell>59.3</cell><cell>65.5</cell><cell>64.2</cell></row><row><cell>ASIS  ? [33]</cell><cell>-</cell><cell>-</cell><cell>63.6</cell><cell>47.5</cell></row><row><cell>3D-BoNet  ? [36]</cell><cell>-</cell><cell>-</cell><cell>65.6</cell><cell>47.6</cell></row><row><cell>OccuSeg  ? [15]</cell><cell>-</cell><cell>-</cell><cell>72.8</cell><cell>60.3</cell></row><row><cell>PointGroup  ? [16]</cell><cell>-</cell><cell>64.0</cell><cell>69.6</cell><cell>69.2</cell></row><row><cell>Our SSTNet  ?</cell><cell>54.1</cell><cell>67.8</cell><cell>73.5</cell><cell>73.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">By using superpoints, we rely on the assumption that individual superpoints would not be across object boundaries; while this is not guaranteed, it is the cost that we would like to trade for the benefit of geometric regularities which the superpoints bring to the original, irregular point set I.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also try thresholding of the Euclidean distance f ?s 1 ? f ? s 2 2 , where f ? s 1 = [fs 1 ; a ? s 1 ]and f ? s 2 is computed similarly. It empirically gives even worse performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Density-based clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davoud</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joerg</forename><surname>Sander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-04" />
			<biblScope unit="volume">7819</biblScope>
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cascaded non-local neural network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">An</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15488</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface re-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bastian Leibe, and Matthias Nie?ner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9031" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hierarchical image segmentation algorithm based on an observation scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Jamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guimar?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Cousty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukiko</forename><surname>Kenmochi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flashfusion: Real-time globally consistent dense 3d reconstruction using cpu computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2940" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointgroup: Dual-set point grouping for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Marc Pollefeys, and Martin R Oswald. 3d instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9256" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7440" to="7449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Felix J?remo Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>M?llner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.2378</idno>
		<title level="m">Modern hierarchical, agglomerative clustering algorithms</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinyu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep mesh reconstruction from single rgb images via topology modification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9964" to="9973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating object segmentation proposals using global and local search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Rantalankila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems -Volume 1, NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems -Volume 1, NIPS&apos;15<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image segmentation by cascaded region agglomeration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object proposal by multibranch hierarchical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical image segmentation ensemble for objectness in rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multidimensional clustering algorithmsmurtagh, f</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6737" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Point cloud instance segmentation using probabilistic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00145</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Spatial semantic embedding network: Fast 3d instance segmentation with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junha</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Min</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03169</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
