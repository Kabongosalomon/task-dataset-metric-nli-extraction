<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
							<email>emanuel.benbaruch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Karklinsky</surname></persName>
							<email>matan.karklinsky@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Biton</surname></persName>
							<email>yossi.biton@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ben-Cohen</surname></persName>
							<email>avi.bencohen@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
							<email>hussam.lawen@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
							<email>nadav.zamir@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>arXiv:2201.06945v2 [cs.CV] 5 Apr 2022 It&apos;s All in the Head: Representation Knowledge Distillation through Classifier Sharing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation knowledge distillation aims at transferring rich information from one model to another. Common approaches for representation distillation mainly focus on the direct minimization of distance metrics between the models' embedding vectors. Such direct methods may be limited in transferring high-order dependencies embedded in the representation vectors, or in handling the capacity gap between the teacher and student models. Moreover, in standard knowledge distillation, the teacher is trained without awareness of the student's characteristics and capacity. In this paper, we explore two mechanisms for enhancing representation distillation using classifier sharing between the teacher and student. We first investigate a simple scheme where the teacher's classifier is connected to the student backbone, acting as an additional classification head. Then, we propose a student-aware mechanism that asks to tailor the teacher model to a student with limited capacity by training the teacher with a temporary student's head. We analyze and compare these two mechanisms and show their effectiveness on various datasets and tasks, including image classification, fine-grained classification, and face verification. In particular, we achieve state-of-the-art results for face verification on the IJB-C dataset for a MobileFaceNet model: TAR@(FAR=1e-5)=93.7%. Code is available at https://github.com/Alibaba-MIIL/HeadSharingKD. ? Equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge distillation (KD) is a commonly used technique for improving the accuracy of a compact model using the guidance of a larger teacher model. In the original KD approach <ref type="bibr" target="#b11">[12]</ref>, the teacher's knowledge is transferred to the student by minimizing an objective function that operates only on the final output predictions of the models. Thus, the transferred knowledge from the teacher may be partial and limited.</p><p>Representation distillation is often favored for transferring richer information and semantic knowledge from the teacher to the student <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>. Enabling robust (a) Teacher-Head KD (TH-KD) (b) Student-Head KD (SH-KD) <ref type="figure">Fig. 1</ref>: Illustration of the two proposed schemes based on classifier sharing between teacher and student. (a) The first approach uses the teacher's classifier as an auxiliary head, with frozen weights, to aid the student learning. (b) In the second approach, we copy the classifier's weights of a pre-trained student to the teacher's classifier (and freeze them) to regularize the representation learning of the teacher. Then, a new student is trained using knowledge distillation.</p><p>representation transfer may be particularly beneficial for tasks that highly rely on the structure of the representation space and its discrimination quality, such as fine-grained classification <ref type="bibr" target="#b34">[35]</ref>, face recognition <ref type="bibr" target="#b19">[20]</ref>, and image retrieval <ref type="bibr" target="#b3">[4]</ref>. Yet, most existing approaches for representation distillation are based on minimizing a loss function as a distance between the teacher and student representation vectors. For example, the L2 norm can be used as a reconstruction loss between the embedding vectors extracted by the teacher and student models <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr" target="#b29">[30]</ref> proposed transferring the knowledge via intermediate representations learned by the teacher's hidden layers, and <ref type="bibr" target="#b41">[42]</ref> introduced an approach that encourages the student to mimic the attention maps of the teacher. In <ref type="bibr" target="#b33">[34]</ref>, they proposed a contrastive loss for representation distillation based on maximizing a lower-bound to the mutual information between the teacher and student embedding vectors.</p><p>Such direct approaches, which aim at minimizing a distance metric between the embedding vectors, may be limited in transferring the representational knowledge from the teacher to the student <ref type="bibr" target="#b32">[33]</ref>, as the discriminative power may reside in singular dimensions or be hidden in complex correlations between the embedding dimensions. In addition, as the teacher's complexity can be significantly higher than the student's complexity, the student may not have the capacity to mimic the representation space of the teacher. This is known as the capacity gap problem <ref type="bibr" target="#b24">[25]</ref>. Therefore, we ask for learning strategies that will support the representation distillation process by bridging the gap between the teacher and the student.</p><p>In particular, we investigate the ability to use the models' classifiers to aid the training process.</p><p>A model's classifier captures essential information regarding the representation space structure and the discrimination capabilities of the model. For example, in <ref type="bibr" target="#b13">[14]</ref>, they tackle the imbalance in long-tail recognition by adjusting only the classifier weights. Previous works used the classifier weights of a pre-trained model when training a new model for backward representation compatibility <ref type="bibr" target="#b32">[33]</ref>, or for unsupervised domain adaptation <ref type="bibr" target="#b20">[21]</ref>. Inspired by these approaches, in this work we propose to enhance representational knowledge distillation by sharing the classifiers between the teacher and the student models.</p><p>Specifically, we explore two methods that deploy classifier weights sharing between the teacher and the student. In the first method, the teacher's classifier is used to constrain the student representation learning by connecting the teacher's classifier to the student backbone as an additional head (with frozen parameters). We name this approach Teacher-Head KD, denoted by TH-KD ( <ref type="figure">Fig. 1a</ref>). Sharing the classification boundaries of the teacher in the student optimization process may help shape its representation space to be similar to the one of the teacher. Closest to this scheme is the work presented in <ref type="bibr" target="#b39">[40]</ref>, where the student is trained such that the teacher's and the student's embeddings produce the same output when passed through the teacher's classifier. Herein, we propose a more generalized scheme. First, we propose to use a combination of both the teacher's and the student's classifiers in the distillation loss. Second, during inference, we suggest using the predictions from both classifiers, aggregated by a weighted sum. This way, the TH-KD scheme enables to deploy different training and inference configurations. In particular, it can be configured such that the teacher head completely replaces the student head.</p><p>Next, we propose a student-aware mechanism for representation distillation based on sharing a student's classifier with the teacher learning process. In this approach, we first train a temporary student model. Then, the parameters of the student's classifier are used to initialize the teacher's head and are fixed during the training of the teacher's backbone. A final student model is trained using the representation distillation loss. We name this approach Student-Head KD, denoted by SH-KD ( <ref type="figure">Fig. 1b</ref>). While a conventional training of a teacher can produce a high-quality feature space in terms of class separation and test accuracy results, in practice, it may be hard for the student to follow the complexity of the teacher's representation due to the limited capacity of the student backbone. Training the teacher with the student's head enforces the teacher's backbone to learn features that better suit the capacity of the student. This way, the student can mimic the teacher's representation more easily.</p><p>Methods for training a student-aware teacher for knowledge distillation were proposed in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b25">[26]</ref>. A meta-learning framework was introduced in <ref type="bibr" target="#b43">[44]</ref>, that enables to train a teacher with the feedback from the distilled student performance. In <ref type="bibr" target="#b25">[26]</ref>, they suggested to train a teacher along with the student branches jointly to obtain student-friendly representations. In this paper, we propose a simple approach that does not require sophisticated mechanisms such as bi-level optimization or joint training of teacher and student but focuses on sharing the student's classifier with the teacher.</p><p>We analyze the capabilities of the two explored schemes and compare them to other KD methods. In particular, by measuring the angle between the features of the teacher and the student's, we show that the student-aware mechanism of SH-KD enables the student to learn features that are closer to the ones of the teacher compared to other baseline approaches. Both TH-KD and SH-KD methods are tested for various tasks on several datasets: CIFAR-100 <ref type="bibr" target="#b18">[19]</ref>, Stanford-cars <ref type="bibr" target="#b17">[18]</ref> FoodX-251 <ref type="bibr" target="#b14">[15]</ref> and for face verification on the IJB-C dataset <ref type="bibr" target="#b23">[24]</ref>. Specifically, using the SH-KD scheme, we achieve state-of-the-art results on the IJB-C dataset when using MobileFaceNet model: TAR(1e-5)=93.7%.</p><p>The contribution of the paper can be summarized as follows:</p><p>-We explore and analyze two mechanisms for representation distillation based on classifier sharing between the teacher and the student models: TH-KD and SH-KD. These techniques are easy-to-implement and complementary to other knowledge distillation approaches. -We introduce SH-KD: a novel student-aware mechanism for representation distillation that enables tailoring the teacher model to a specific student, and to mitigate the capacity gap between the teacher and the student, by training the teacher with a temporary student's head. -Our methods achieve consistent accuracy improvement for various settings, across datasets and on different architectures, including obtaining state-ofthe-art results for face verification on the IJB-C dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Representational Knowledge Distillation through Classifier Sharing</head><p>In this section, we introduce two approaches based on classifier sharing between the teacher and student models to facilitate the representation distillation process. In the first scheme the teacher's classifier is used to constrain the student representation learning. In the second scheme, a student's classifier is used to regularize the representation learning of the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Given a teacher model f t , we aim at training a smaller student model f s , guided by the teacher. For a given input sample x, we denote by z t = f t (x; ? t ) and z s = f s (x; ? s ) the representation (embedding) vectors of the teacher and student models, respectively, where ? t and ? s are the teacher and student models' parameters, respectively. The teacher's classifier is defined by g t (z) = W t z + b t , and the student's classifier is defined by g s (z) = W s z+b s . The final prediction is given by applying the softmax activation h(?): p t = h(g t (z t )), and p s = h(g s (z s )) for the teacher and the student, respectively. For simplicity, we denote the classifier weights and bias terms of the teacher and the student by ? t = {W t , b t } and ? s = {W s , b s }, respectively. For a given training sample x and a corresponding ground-truth label vector y, a general form of the loss function used to train the student model can be written as:</p><formula xml:id="formula_0">L = L CE (p s , y) + ?H(p s , p t ) + ?D(z s , z t ),<label>(1)</label></formula><p>where L CE (?) is the cross-entropy loss, and H(?) is the knowledge distillation distance function between the probabilistic outputs of the teacher and student models, e.g. the KL-divergence <ref type="bibr" target="#b11">[12]</ref>. The term D(?) refers to a distance metric applied on the representation vectors of the teacher and student models, as the L2 loss, cosine distance or a contrastive loss <ref type="bibr" target="#b33">[34]</ref>, where ? and ? are constant hyper-parameters that control the contribution of each loss term.</p><p>In particular, the L2 loss for representation distillation was found to be useful for face recognition <ref type="bibr" target="#b19">[20]</ref> and other general fine-grained classification tasks. The L2 loss is computed by the euclidean distance of the normalized embedding vectors. We term this loss as L2E. Note that in case that the embedding dimensions of the teacher and student differ, we add a linear transformation to the architecture's head to match their dimensions.</p><p>While the KD loss H(?) enables the transfer of valuable knowledge encapsulated in the soft predictions of the teacher, minimizing the representation loss D(?), enforces the embedding space of the student to be aligned with the teacher's embedding space. Thus, these loss terms are complementary and together they enable a robust knowledge transfer from the teacher to the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Teacher-Head Sharing (TH-KD)</head><p>We aim to utilize the discrimination information represented by the classification decision boundaries of the teacher to guide the student model in the optimization process. In this scheme, we propose to use the teacher's classifier as an auxiliary head for training the student model. Let p TH s = h(g t (z s )) be the prediction vector output from the teacher's classifier for a given student's embedding input, we combine the KD losses computed for the two classifiers as follows,</p><formula xml:id="formula_1">H ? = (1 ? ? TH )H(p s , p t ) + ? TH H(p TH s , p t ),<label>(2)</label></formula><p>where ? TH is a constant hyper-parameter that balances between the losses of the two classification heads. Similarly, the classification loss is given by,</p><formula xml:id="formula_2">L ? CE = (1 ? ? TH )L CE (p s , y) + ? TH L CE (p TH s , y).<label>(3)</label></formula><p>In inference time, the final prediction can be obtained by combining the head outputs:</p><formula xml:id="formula_3">p ? s = (1 ? ? TH )p s + ? TH p TH s .<label>(4)</label></formula><p>This method, named TH-KD, is illustrated in <ref type="figure">Fig. 1a</ref>. Note that for ? TH = 1, the student's head is simply the teacher's head whose weights are fixed during the training. Setting ? TH = 0 leads to the conventional scheme for knowledge distillation.</p><p>Incorporating the teacher-head loss encourages the student to mimic the representation space of the teacher while resolving its high dimensional dependencies. In section 3.2 ( <ref type="figure" target="#fig_2">Fig. 4</ref>), we show that the TH-KD scheme leads to an enhanced representation quality, which is expressed in terms of a higher interclass separability and a lower intra-class variation of the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Student-Head Sharing (SH-KD)</head><p>The second approach aims at tackling the limited capacity of the student in the distillation process. In a conventional knowledge distillation, the teacher model is trained independently and in isolation from the student training process. Typically, the capacity of the teacher model is higher than the capacity of the student model, and thus the features learned by the teacher may not be applicable for the student training.</p><p>To this end, we propose to train a teacher while considering the limited capacity of the student, by initializing the teacher's classifier with the weights of a temporary student's head and fixing them during the training. This process can be viewed as a regularization mechanism that enforces the teacher to learn useful features suited for the student's limitations. The method can be depicted as a three-step training procedure:</p><p>step-I 0 : A student model is trained, with KD or without, providing a backbone and a classifier head with parameters, {? 0 s , ? s }. step-I 1 : A teacher model is trained by initializing and fixing its classifier with ? s to obtain the teacher model parameters, {? t , ? s }.</p><p>step-I 2 : A student model is trained using the loss in equation <ref type="formula" target="#formula_0">(1)</ref>, with the teacher model obtained in step I 1 , to produce the final student parameters:</p><formula xml:id="formula_4">{? 1 s , ? s }.</formula><p>The approach, named SH-KD, is illustrated in <ref type="figure">Fig. 1b</ref>. Note that for cases where the embedding dimensions of the teacher and the student are not the same, we add a linear transformation to the model's backbone. SH-KD method offers an easy-to-implement yet effective scheme that enables to tailor the teacher model to comply with the student's capacity during the teacher's training at the cost of an additional training iteration.</p><p>Indeed, the accuracy of the teacher may be decreased when using the SH-KD scheme compared to the accuracy obtained by a teacher that was trained conventionally. However, in knowledge distillation, we do not aim at optimizing the teacher, but rather to improve the accuracy of the student. Specifically, a lower accuracy of the teacher does not necessarily lead to a lower accuracy of the student. The same observation was made in the teacher assistant (TA) technique <ref type="bibr" target="#b24">[25]</ref>. Instead of using a teacher model with the largest capacity which produces the highest accuracy, it was shown that under some conditions, a teacher model with intermediate-size (teacher assistant) can provide superior performance for the student. While both TA and SH-KD offer a way to mitigate the capacity gap between the teacher and the student, SH-KD enables to tailor the teacher to the specific student at hand.</p><p>In section 3.2, we show that training with SH-KD leads to a higher similarity between the student and teacher representations, accompanied by an improvement in student accuracy.</p><p>Theoretical Formulation for SH-KD. We follow the works in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b24">[25]</ref> to shed some light on why the SH-KD scheme can be effective for knowledge distillation. For simplicity, we assume a pure KD loss, i.e. L = H(p s , p t ). Also, in the following formulation, the model functions include both the backbone and the classifier: the student function is denoted byf s = h(f s ? g s ), and the teacher function is denoted byf t = h(f t ? g t ). In the case of a baseline knowledge distillation, the expected error of the student model R(f s ) according to the VC theory <ref type="bibr" target="#b35">[36]</ref> can be expressed as,</p><formula xml:id="formula_5">R(f s ) ? R(f t ) ? O |F s | C n rst + ? st ,<label>(5)</label></formula><p>where | ? | C is a function class capacity measure, and F s is the student function class. Here, O(?) and ? st are the estimation and the approximation errors of the student, respectively, and n is the number of the training samples. Also, 1 2 ? r st ? 1 is the rate of learning which relates to the training difficulty. A difficult task is characterized by a smaller r st while for an easy task, r st is close to 1. Similarly, the expected error for a student model, learned by the SH-KD scheme is given by,</p><formula xml:id="formula_6">R(f s ) ? R(f * t ) ? O |F s | C n r * st + ? * st ,<label>(6)</label></formula><p>wheref * t is the function of the teacher model trained with the student's head:</p><formula xml:id="formula_7">f * t = h(f t ? g s ).</formula><p>Here, r * st and ? * st are the learning rate, and the approximation error of the student learned using the SH-KD teacher. Under the worst-case assumption, the classification error of the SH-KD teacher is higher than the conventional teacher's, i.e. R(f * t ) = R(f t ) + ?, for ? ? 0. As aforementioned in the previous section, our objective is to minimize R(f s ). A lower teacher error R(f t ) does not necessarily lead to a lower R(f s ). Consequently, we can write the upper bound for R(f s ) ? R(f t ), as follows: (c) IJB-C dataset <ref type="figure">Fig. 3</ref>: Average angle between the teacher and student embedding vectors, and student model accuracy. Both TH-KD, and to a greater extent SH-KD, reduce the angle between the teacher and student embedding vectors and improve test accuracy.</p><formula xml:id="formula_8">R(f s ) ? R(f t ) = R(f s ) ? R(f * t ) + ? (7) ? O |F s | C n r * st + ? * st + ?.<label>(8)</label></formula><p>Therefore, in order for SH-KD to outperform the baseline KD, the following equation should be satisfied:</p><formula xml:id="formula_9">O |F s | C n r * st + ? * st + ? ? O |F s | C n rst + ? st .<label>(9)</label></formula><p>The task of learning from a teacher that was trained with a student classifier is assumably simpler compared to learning from a conventional teacher because the teacher trained in the SH-KD scheme is tailored to the student's capacity. Thus, typically r * st ? r st . This is supported experimentally in section 3.2 by comparing the convergence rate of each training scheme as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Equation <ref type="formula" target="#formula_9">(9)</ref> is also reasonable under the assumption that ? * st ? ? st ? ?; while the SH-KD teacher may have lower accuracy than the baseline teacher, as expressed in a positive expected error gap ?, the fact that in SH-KD the teacher and the student share the same classifier encourages a smaller approximation error. This is supported by <ref type="figure">Fig. 3</ref>; student and teacher features are closer in SH-KD than in baseline KD. In other words, to obtain better performance with SH-KD, the lower approximation error ? st ? ? * st should compensate for the teacher's accuracy drop ?. A conceptual comparison between the SH-KD training scheme and the baseline KD approach is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. In case the drop of the teacher accuracy is too high, equation (9) may not hold. Another failure case is when the classifier of the temporary student consists of a deficient decision boundary which can limit the learning ability of the teacher, i.e. r * st may be small and close to 1/2 . Note that as in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b24">[25]</ref>, the inequality (9) holds in the asymptotic regime, and is based on loose upper bounds. Yet, it offers motivation for using SH-KD, and highlights its potential advantages and failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we report our main results on three domains: image classification, fine-grained classification and face-verification. Also, we study the impact of the proposed schemes, TH-KD and SH-KD, on the representation distillation quality. The training details are provided in the Appendix.  <ref type="table">Table 1</ref>: Test accuracy (%) on CIFAR100 dataset. We follow the same protocol as in the CRD work <ref type="bibr" target="#b33">[34]</ref>. SH-KD achieves superior results. When combined with the CRD loss, further improvement is obtained in most experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmark Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>The CIFAR-100 <ref type="bibr" target="#b18">[19]</ref> consists of 50K training images (500 samples per class) and 10K test images. In <ref type="table">Table 1</ref> we show the results obtained by our two explored schemes, TH-KD and SH-KD, and compare them to previous approaches for KD. As the proposed schemes are complementary to other methods with different objectives, we tested the TH-KD and SH-KD with the original KD loss and with CRD <ref type="bibr" target="#b33">[34]</ref>. When combined with CRD, both TH-KD and, to a greater extent SH-KD, outperform all the baseline approaches.</p><p>Fine-grained Classification We evaluated our methods on two fine-grained classification datasets, Stanford-cars <ref type="bibr" target="#b17">[18]</ref> and FoodX-251 <ref type="bibr" target="#b14">[15]</ref>. Stanford-cars contains 196 classes and consists of 8,041 training images and 8,144 test images. FoodX-251 <ref type="bibr" target="#b14">[15]</ref> contains 251 classes and consists of 118K training images and 28K test images. We tested six training configurations with different architectures for the teacher and student models. As teachers, we used TResNet-M and TResNet-L <ref type="bibr" target="#b28">[29]</ref>, and ResNet101 <ref type="bibr" target="#b9">[10]</ref>. As students, we used once-for-all (OFA) models <ref type="bibr" target="#b1">[2]</ref>; OFA-595 and OFA-62, and small ResNet variants; ResNet18 and ResNet26. The OFA architectures were designed for cost-effective mobile deployment.    <ref type="table">Table 4</ref>: Results on the IJB-C dataset. The reported results of the first three rows, denoted by *, were taken from the papers <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b19">[20]</ref>.</p><p>We compared five training regimes: a vanilla training without any KD loss, regular KD, training with L2E for representation distillation, and the proposed approaches TH-KD and SH-KD.</p><p>We summarize the results obtained on Stanford-cars and FoodX-251 in <ref type="table" target="#tab_2">Table 2 and Table 3</ref>, respectively. On the Stanford-cars dataset, the SH-KD method was consistently superior in all training configurations. Interestingly, for Stanfordcars, regular KD degrades the accuracy compared to a vanilla training. On the FoodX-251 dataset, the highest results were achieved by TH-KD or SH-KD. The TH-KD method was superior in four out of the six tested configurations. Face Verification We evaluated our methods on the face verification task of the IJB-C dataset <ref type="bibr" target="#b23">[24]</ref>. For training, we used a refined version of the popular MS-Celeb-1M dataset <ref type="bibr" target="#b8">[9]</ref> named MS1MV3 <ref type="bibr" target="#b6">[7]</ref> which contains about 93K identities and 5.2M images. We used a ResNet-like network <ref type="bibr" target="#b9">[10]</ref>, R100 as a teacher, and MobileFaceNet as a student <ref type="bibr" target="#b2">[3]</ref>. We used the L2 loss between the embedding features of the teacher and the student (L2E) as the representation distillation loss, and the large-margin cosine loss, CosFace <ref type="bibr" target="#b36">[37]</ref> as the base loss.</p><p>In <ref type="table">Table 4</ref> we show the results obtained by our approaches, TH-KD and SH-KD, and compare them to other baselines and previous state-of-the-art methods <ref type="bibr" target="#b19">[20]</ref>. We report three common metrics for evaluating the performance of the IJB-C dataset: TAR(@FAR=1e-6), TAR(@FAR=1e-5) and TAR(@FAR=1e-4). In the vanilla training, we used the CosFace loss only. The TH-KD scheme was performed with ? TH = 1 in equation <ref type="bibr" target="#b2">(3)</ref>. We also report the results obtained by the teacher models: the regular teacher and the teacher training with the student's classifier following the SH-KD method. As can be seen, both teachers provide similar metric results.</p><p>The student model trained using the TH-KD scheme outperforms the baseline approaches considerably for the TAR(@FAR=1e-6) metric, improving the L2E method from 88.47% to 89.82%. Using the SH-KD scheme, we obtain a significant improvement compared to the other baselines and previous state-ofthe-art approaches in all the tested metrics. For example, the TAR(@FAR=1e-6) metric is improved to 90.24%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>In this section, we study notable properties of the proposed approaches, TH-KD and SH-KD, including their ability to accelerate the training process and how they impact the representation learning of the student.</p><p>Representation Similarity Between Teacher and Student. To demonstrate the ability to reliably transfer the representation knowledge from the  teacher to the student, we measure the angle between corresponding embedding vectors extracted by the teacher and the student. For each image the angle is given by,</p><formula xml:id="formula_10">d(z t , z s ) = arccos z t ? z s z t 2 z s 2 .<label>(10)</label></formula><p>In <ref type="figure">Fig. 3</ref>, we show the averaged angle between the teacher and student embeddings for Standford-cars, FoodX-251 and the face IJB-C datasets. The reported angle is obtained by averaging the instance angles d(z t , z t ) computed for all the test samples. Examination of <ref type="figure">Fig. 3</ref> yields several observations. First, the angle obtained by the regular KD is similar to the angle obtained in the vanilla model (between two unrelated models). This may arise from the fact that regular KD operates solely on the final network predictions. Therefore, KD does not offer any ability to transfer representational knowledge from the teacher to the student. Second, L2E significantly improves both the similarity between the embedding vectors of the teacher and the student and the accuracy of the student. Third, the TH-KD and SH-KD further improve the similarity between the embedding vectors of the student and the teacher, as well as the classification accuracy. Fourth, using SH-KD considerably reduces the angle between the embedding vectors of the teacher and student models, across all datasets.</p><p>TH-KD Improves Representation Quality. We use the mean silhouette coefficient (MSC) score <ref type="bibr" target="#b30">[31]</ref> to measure the clustering quality of the embeddings generated by the models' backbones, in terms of intra-class variation and inter-class separability. Let Z be the set of extracted embedding vectors: Z = {z 1 , ..., z N }, where N is the number of samples. The MSC is defined as,</p><formula xml:id="formula_11">MSC = 1 N z?Z ?(z) ? ?(z) max ?(z), ?(z) ,<label>(11)</label></formula><p>where ?(z) is the averaged distance between z to the other embedding vectors residing in the same category as z, and ?(z) is the minimum distance between z and the centers of the other categories. Higher ?(z) implies larger inter-class separability, and lower ?(z) implies smaller intra-class variation. Typically, stronger models with higher capacity produce an embedding space with a higher MSC. In <ref type="figure" target="#fig_2">Fig. 4</ref>, we plot the MSC score computed throughout the training epochs for several training modes: vanilla model trained without KD, KD, L2E, and TH-KD. In both datasets (Stanford-cars and FoodX-251), training with TH-KD increases the MSC score compared to training with L2E. This supports the claim that training the student with the teacher head enables better representational knowledge transfer from the teacher to the student. Note that in both datasets, the regular KD produces a poor MSC score. In the case of Stanford-cars the MSC is even degraded compared to the vanilla model.</p><p>KD Training Convergence. Often, training with knowledge distillation requires many more epochs than regular training <ref type="bibr" target="#b0">[1]</ref>. This is particularly true for representation distillation where the optimization involves the minimization of a distance function between the teacher and student embeddings. A slow training process increases the computational cost and limits resource utilization.</p><p>In addition to improving a model's accuracy, we observe that networks trained using TH-KD and SH-KD converge faster compared to a baseline training with representation distillation. Classifier sharing reduces the number of trainable parameters and eases the training. TH-KD circumvents the learning of the student's classifier weights by initializing them with the teacher's head weights and freezing them. Moreover, SH-KD further accelerates the training because the features extracted by the teacher are more applicable for the student. In <ref type="figure" target="#fig_4">Fig. 5</ref>, we show the test accuracy on FoodX-251 and Stanford-cars, over the training epochs for TH-KD and SH-KD methods and compare them to a baseline training using the L2E loss. We used the TResNet-L as a teacher and OFA-62 as a student. As can be seen, both TH-KD and SH-KD result in faster training convergence compared to a baseline training with the L2E, and SH-KD further accelerates the training process. To some extent, this compensates for the fact that SH-KD scheme requires an additional phase of training the initial student.</p><p>Effect of Initial Student Capacity on SH-KD. In the SH-KD scheme, the teacher classifier head is replaced by the classifier head of the initial student. Typically, a student model has lower capacity than the teacher. How does this capacity gap affect the trained teacher and the training of its final student?</p><p>We investigated how the capacity of the initial student affects the SH-KD training process, by examining a variety of initial student backbones. We used heads of students with different architectures to train different teacher models with similar backbones (TResNet-L). Then, we used each teacher to train a final student model (OFA-62). In <ref type="figure">Fig. 6</ref>, we show the effects of the initial model selection on the outcome and process of SH-KD training.</p><p>In <ref type="figure">Fig. 6a</ref> we report the accuracy of each teacher and its final student. We observe that decreasing initial student capacity reduces the teacher's accuracy, but may positively affect the final student's accuracy. Notably, using an initial student model from the OFA family yields a better final student than either  <ref type="figure">Fig. 6</ref>: Effect of initial student capacity on SH-KD. We trained a set of teachers (TResNet-L), each with a head from an initial student of different capacity. Then, we trained a final student (OFA-62) based on each teacher. For each teacher-student pair, we report their accuracies, confidence gaps, and embedding angles on FoodX-251.</p><p>using TH-KD or other architectures, even if the initial student architecture does not exactly match the final OFA-62 student architecture. For the same settings, we present in <ref type="figure">Fig. 6b</ref> the confidence gap between the teacher and the student. Following <ref type="bibr" target="#b38">[39]</ref>, we measure the model confidence by the mean difference between first and second prediction values. A high confidence gap between the teacher and student predictions may imply a high capacity gap between the models <ref type="bibr" target="#b7">[8]</ref>. As can be seen, the confidence gap between the teacher and the final student (whose architectures are fixed) is substantially reduced when the initial student model belongs to the OFA family. This is an indication that SH-KD can mitigate the capacity gap between student and teacher by adapting the properties of the teacher to match the capacity of the student.</p><p>To further understand the SH-KD process and its outcomes, we present the difference between teacher's and student's embedding vectors, as measured by their angle (see equation <ref type="formula" target="#formula_0">(10)</ref>), in <ref type="figure">Fig. 6c</ref>. On one hand, the difference between the embedding vectors of each initial student model and its matching teacher increases accordingly when their capacity gap increases. On the other hand, for the final student, its embedding vectors match the teacher's embedding vectors more closely if its teacher was trained with a smaller initial student model. Thus, using an initial student of a capacity matching the low capacity of the final student constraints the trained teacher, and therefore allows the final student to match it better, and eventually yield better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we explored two techniques for representation distillation that are based on classifier sharing between the teacher and the student. The TH-KD approach shares the teacher's classifier with the student to constrain the representation distillation process. The SH-KD enables sharing the student's classifier within the teacher's training at the cost of another training iteration. Extensive experiments and analyses demonstrate the effectiveness of the proposed schemes on various domains and datasets. We show that both TH-KD and SH-KD accelerate the representation distillation process. Moreover, the TH-KD technique is useful in improving the discrimination power of the embeddings extracted by the student's backbone. Finally, training with SH-KD and TH-KD increases the similarity between the teacher and student embeddings and leads to the desired improvement in the student accuracy. For future work, we would like to investigate ways to apply the proposed approaches to an ensemble of teachers. In addition, a theoretical formulation based on tighter upper bounds may yield a better understanding of the possible benefits and limitations of the proposed methods.</p><p>1 Training Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">CIFAR-100 Training Details</head><p>For a fair comparison, we used the public code provided for the CRD work <ref type="bibr" target="#b33">[34]</ref> and followed the same experimental protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Fine-grained Classification Training Details</head><p>We tested our approaches on two fine-grained classification datasets; FoodX-251 <ref type="bibr" target="#b14">[15]</ref> and Stanford Cars <ref type="bibr" target="#b17">[18]</ref>. We used both modern network architectures of TResNet <ref type="bibr" target="#b27">[28]</ref> and OFA <ref type="bibr" target="#b1">[2]</ref> and classical ResNet architectures <ref type="bibr" target="#b9">[10]</ref>. Specifically, we tested six training configurations with teachers: TResNet-L, TResNet-M, and ResNet101, and students: OFA-62, OFA-389, OFA-595, ResNet18 and ResNet26.</p><p>We trained all models with a combination of a base cross-entropy loss, a triplet-loss <ref type="bibr" target="#b31">[32]</ref>, and the specified KD losses. The Stanford-cars dataset was trained for 100 epochs with a learning-rate of 5e-4 and a weight decay of 2e-4. The FoodX-251 dataset was trained for 40 epochs with a learning-rate of 3e-4 and a weight decay of 1e-4. In all experiments, we used the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with a cosine decay learning-rate schedule. All models were pre-trained on the ImageNet-21k dataset <ref type="bibr" target="#b27">[28]</ref>. The input image size was 224 x 224. The embedding dimension was 2,048. For regularization, we used standard augmentation techniques <ref type="bibr" target="#b4">[5]</ref>. We used a single V100 machine for seach run. For TH-KD and SH-KD, we used ? TH = 1 in equation (3) of the paper. We used L2E loss with ? = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Training Details for Face Verification</head><p>Our implementation is based on the PyTorch framework and combines different blocks from the repositories Insightface <ref type="bibr" target="#b5">[6]</ref> and timm <ref type="bibr" target="#b37">[38]</ref>. We used the same hyper-parameters for each training process.</p><p>The models were trained for 30 epochs, with initial learning-rate of 1e-2 and cosine decay schedule. We set the weight decay to 1e-4, except for the classification layer which did not have weight decay at all. We used the RMSprop optimizer with momentum of 0.9. The input faces were normalized into a patch of size 112 x 112, using the alignment method from <ref type="bibr" target="#b5">[6]</ref>. We used 3 types of data augmentations techniques : random horizontal flip (probability of 0.5), color jitter (brightness, contrast and saturation jitter of ?0.4), random erasing (probability of 0.1) <ref type="bibr" target="#b42">[43]</ref>. For the CosFace loss we used the constants s = 64 and m = 0.4. We used the L2E loss with ? = 0 and ? = 5. The TH-KD scheme was performed with ? TH = 1 in equation (3) of the paper. The embedding dimension used in all methods was 512.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>A conceptual comparison between baseline KD and the proposed SH-KD scheme. Training with the SH-KD scheme reduces the teacher's capacity and narrows the space of the teacher function class. In SH-KD, the teacher is aware of the student's capacity and may produce smaller approximation and estimation errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>MSC score over training epochs for different KD methods. TH-KD consistently increases the MSC score over the L2E baseline. Regular KD produces poor MSC score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Training convergence. Training with TH-KD and SH-KD converges faster compared to regular training with L2E loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) on Stanford-cars. The SH-KD method outperforms other approaches consistently in all training configurations.</figDesc><table><row><cell>Teacher</cell><cell>TResNetM</cell><cell>TResNetM</cell><cell>TResNetL</cell><cell>TResNetL</cell></row><row><cell>Student</cell><cell>OFA-62</cell><cell>OFA-595</cell><cell>OFA-62</cell><cell>OFA-595</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%) on FoodX-251. The highest results are achieved by the TH-KD or SH-KD approaches. TH-KD is superior in most of the training configurations.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>TAR@ FAR=1e-6</cell><cell>TAR@ FAR=1e-5</cell><cell>TAR@ FAR=1e-4</cell></row><row><cell>Martinez*</cell><cell>MobileFaceNet</cell><cell>-</cell><cell>92.20</cell><cell>94.70</cell></row><row><cell cols="2">L2E+ES-sampling* MobileFaceNet</cell><cell>-</cell><cell>93.20</cell><cell>95.39</cell></row><row><cell>L2E+IS-sampling*</cell><cell>MobileFaceNet</cell><cell>-</cell><cell>93.25</cell><cell>95.49</cell></row><row><cell>Teacher, Vanilla</cell><cell>R100</cell><cell>91.49</cell><cell>95.52</cell><cell>97.00</cell></row><row><cell>Teacher, SH-KD</cell><cell>R100</cell><cell>90.88</cell><cell>95.58</cell><cell>97.03</cell></row><row><cell>Vanilla</cell><cell>MobileFaceNet</cell><cell>88.72</cell><cell>92.75</cell><cell>95.42</cell></row><row><cell>L2E</cell><cell>MobileFaceNet</cell><cell>88.47</cell><cell>93.49</cell><cell>95.48</cell></row><row><cell>TH-KD (Ours)</cell><cell>MobileFaceNet</cell><cell>89.82</cell><cell>93.50</cell><cell>95.48</cell></row><row><cell>SH-KD (Ours)</cell><cell>MobileFaceNet</cell><cell>90.24</cell><cell>93.73</cell><cell>95.64</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="428" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<title level="m">Deep image retrieval: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lightweight face recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reducing the teacher-student gap via spherical knowledge disitllation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Foodx-251: A dataset for fine-grained food classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Toronto, Ontario</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 0</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifying the data bias in knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1477" to="1486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cycle self-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Unifying distillation and privileged information</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Benchmarking lightweight face architectures on specific face recognition scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Martinez-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolas-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mendez-Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Luevano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gonzalez-Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark -c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICB2018.2018.00033</idno>
		<ptr target="https://doi.org/10.1109/ICB2018.2018.00033" />
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<title level="m">Improved knowledge distillation via teacher assistant</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning student-friendly teacher networks for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<title level="m">Learning deep representations with probabilistic knowledge transfer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Tresnet: High performance gpudedicated architecture</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/0377042787901257" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298682</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298682" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<title level="m">Towards backward-compatible representation learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<title level="m">Grafit: Learning fine-grained image representations with coarse labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Computation-efficient knowledge distillation via uncertainty-aware mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Knowledge distillation via softmax regression representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.754</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.754" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7130" to="7138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/7000" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<title level="m">Meta learning for knowledge distillation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
