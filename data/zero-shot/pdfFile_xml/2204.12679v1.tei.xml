<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-Level Relation Extraction with Sentences Importance Estimation and Focusing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
							<email>xuwang@hit-mtlab.net</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. Computing Science</orgName>
								<orgName type="institution">Alberta Machine Intelligence Institute (Amii) University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document-Level Relation Extraction with Sentences Importance Estimation and Focusing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level relation extraction (DocRE)</head><p>aims to determine the relation between two entities from a document of multiple sentences. Recent studies typically represent the entire document by sequence-or graphbased models to predict the relations of all entity pairs. However, we find that such a model is not robust and exhibits bizarre behaviors: it predicts correctly when an entire test document is fed as input, but errs when non-evidence sentences are removed. To this end, we propose a Sentence Importance Estimation and Focusing (SIEF) framework for DocRE, where we design a sentence importance score and a sentence focusing loss, encouraging DocRE models to focus on evidence sentences. Experimental results on two domains show that our SIEF not only improves overall performance, but also makes DocRE models more robust. Moreover, SIEF is a general framework, shown to be effective when combined with a variety of base DocRE models. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document-level relation extraction (DocRE) aims to predict entity relations across multiple sentences. It plays a crucial role in a variety of knowledgebased applications, such as question answering <ref type="bibr">(Sorokin and Gurevych, 2017)</ref> and large-scale knowledge graph construction <ref type="bibr" target="#b0">(Baldini Soares et al., 2019)</ref>. Different from sentence-level relation extraction <ref type="bibr" target="#b21">(Zeng et al., 2014;</ref><ref type="bibr" target="#b13">Xiao and Liu, 2016;</ref><ref type="bibr">Song et al., 2019)</ref>, the supporting evidence in the DocRE setting may involve multiple sentences scattering in the document. Thus, DocRE is more a realistic setting, attracting increasing attention in the field of information extraction.</p><p>Most recent DocRE studies use the entire document as a clue to predict the relations of <ref type="bibr">1</ref> The code is publicly available at https://github. com/xwjim/SIEF all entity pairs without concerning where the evidence is located <ref type="bibr" target="#b9">(Nan et al., 2020;</ref><ref type="bibr" target="#b22">Zeng et al., 2020;</ref><ref type="bibr">Xu et al., 2021a,b)</ref>. However, one can identify the relation of a specific entity pair from a few sentences.  show that irrelevant sentences in the document would hinder the performance of the model. Moreover, we observe that a DocRE model, trained on the entire document, may err when non-evidence sentences are removed. In <ref type="figure">Figure 1</ref>, for example, we need to identify the relation "MemberOf" between the entities Brad Wilk and Rage Against the Machine. The evidence sentences are {1,2}, and humans can easily identify such a relation when reading sentences {1,2} only. However, the recent DocRE model GAIN <ref type="bibr" target="#b22">(Zeng et al., 2020)</ref> identifies the relation "MemberOf" correctly from the entire document {1,2,3}, but predicts "not MemberOf" from sentences {1,2}. Intuitively, removing sentence {3} should not change the result, as this sentence does not provide information regarding whether "MemberOf" holds or not for the two entities. Such model behaviors are undesired, because it shows that the model is not robust and lacks interpretability.</p><p>To this end, we propose a novel Sentence Importance Estimation and Focusing (SIEF) framework to encourage the model to focus on evidence sentences for predicting the relation of an entity pair. Specifically, we first evaluate the importance of each sentence by the difference between the output probabilities of the document with and without this sentence. If the predicted probability of a relation does not change, or even increases, when a sentence is removed, it typically indicates that the sentence is non-evidence. Then, we propose an auxiliary loss to encourage the model to produce the same output distribution, when the entire document is fed as input and when a non-evidence sentence is removed. In this way, the model pays more attention to the evidence sentences for the classification. Our SIEF method is a general framework that can be combined with different underlying DocRE models.</p><p>We evaluated the generality and effectiveness of our approach on the large-scale DocRED dataset <ref type="bibr" target="#b18">(Yao et al., 2019)</ref>. Experimental results show that the proposed approach combines well with various recent DocRE models and significantly improves the performance. We further evaluated our approach on a dialogue relation extraction dataset, DialogRE <ref type="bibr" target="#b20">(Yu et al., 2020)</ref>; our SIEF yields consistent improvement, showing the generality of our approach in different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Relation extraction (RE) can be categorized by its granularity, such as sentence-level <ref type="bibr" target="#b3">(Doddington et al., 2004;</ref><ref type="bibr" target="#b16">Xu et al., 2016;</ref> and document-level <ref type="bibr" target="#b4">(Gupta et al., 2019;</ref><ref type="bibr" target="#b25">Zhu et al., 2019)</ref>.</p><p>Early work mainly focuses on sentence-level relation extraction. <ref type="bibr">Pantel and Pennacchiotti (2006)</ref> propose a rule-based approach, and <ref type="bibr" target="#b8">Mintz et al. (2009)</ref> manually design features for classifying relations. In the past several years, neural networks have become a prevailing approach for relation extraction <ref type="bibr" target="#b17">(Xu et al., 2015;</ref><ref type="bibr">Song et al., 2019)</ref>. Document-level relation extraction (DocRE) is attracting increasing attention in the community, as it considers the interactions of entity mentions expressed in different sentences <ref type="bibr" target="#b18">Yao et al., 2019)</ref>. Compared with the sentence level, DocRE requires the model collecting and integrating inter-sentence information effectively. Recent efforts design sequence-based and graphbased models to address such a problem.</p><p>Sequence-based DocRE models encode a document by the sequence of words and/or sentences, for example, using the Transformer architecture <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>. <ref type="bibr" target="#b24">Zhou et al. (2021)</ref> argue that the Transformer attentions are able to extract useful contextual features across sentences for DocRE, and they adopt an adaptive threshold for each entity pair. <ref type="bibr" target="#b23">Zhang et al. (2021)</ref> model DocRE as a semantic segmentation task and predict an entity-level relation matrix to capture local and global information.</p><p>Graph-based DocRE models abstract a document by graphical structures. For example, a node can be a sentence, a mention, and/or an entity; their co-occurrence is modeled by an edge. Then graph neural networks are applied to aggregate inter-sentence information <ref type="bibr">(Quirk and Poon, 2017;</ref><ref type="bibr" target="#b1">Christopoulou et al., 2019;</ref><ref type="bibr" target="#b22">Zeng et al., 2020)</ref>. <ref type="bibr" target="#b22">Zeng et al. (2020)</ref> construct double graphs, applying graph neural networks to mention-document graphs and performing path reasoning over entity graphs. <ref type="bibr" target="#b14">Xu et al. (2021a)</ref> explicitly incorporate logical reasoning, commonsense reasoning, and coreference reasoning into DocRE, based on both sequence and graph features.</p><p>Different from previous work, our paper proposes SIEF as a general framework that can be combined with various sequence-based and graph-based DocRE models. In our approach, we propose a sentence importance score and a sentence focusing loss to encourage the model to focus on evidence sentences, improving the robustness and the overall performance of DocRE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>In this section, we present the formulation of document relation extraction (DocRE). Consider an unstructured document comprising N sentences, D = {s 1 , s 2 , ? ? ? , s N }, where each sentence s n is a sequence words. In a DocRE dataset, the document D is typically annotated with entity mentions, each mention (e.g., U.S. and USA) labeled by its conceptual entity e and its entity type (e.g., location).</p><p>A DocRE model F is usually formulated as multi-label classification <ref type="bibr" target="#b18">(Yao et al., 2019)</ref>. F j predicts whether the jth relation holds for the ith marked entity pair in a document, given by</p><formula xml:id="formula_0">P ij = F j (D, e i h , e it ) = Pr[r ij = 1|D, e i h , e it ]</formula><p>(1) where e i h is the head entity and e it is the tail entity; r ij ? {0, 1} is the groundtruth label regarding entity pair i and relation j.</p><p>[1] The Sacramento Bee is a daily newspaper published in Sacramento, California, in the United States.</p><p>[2] Since its founding in 1857, The Bee has become the largest newspaper in Sacramento, the fifth largest newspaper in California, and the 27th largest paper in the U.S.</p><p>[3] The Bee is the flagship of the American <ref type="figure">McClatchy</ref>   <ref type="figure">Figure 2</ref>: We estimate the sentence importance (for a specific entity pair and relation) by the difference of the classification probabilities with and without the sentence. Then, we encourage the DocRE model to predict the same probability when the entire document is fed as input and when a non-evidence sentence is removed.</p><p>To train the model, the binary cross-entropy loss is used as the objective for parameter estimation:</p><formula xml:id="formula_1">L rel = ? D?C i h =it j?R {r ij log P ij +(1 ? r ij ) log(1 ? P ij )} (2)</formula><p>where C denotes the entire corpus and R denotes the set of relation types.</p><p>During inference, we obtain the relation(s) of a given entity pair by thresholding the predicted probabilities, following most previous work <ref type="bibr" target="#b18">(Yao et al., 2019;</ref><ref type="bibr" target="#b24">Zhou et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we will describe our approach in detail. The overview of our framework is shown in <ref type="figure">Figure 2</ref>. First, we describe the estimation of sentence importance in Section 4.1. Sentences with low importance scores are treated as non-evidence. Then, Sections 4.2 and 4.3 present our approach that encourages the model to produce the same output distribution, when the entire document is fed as input and when non-evidence sentences are removed. Section 4.4 further presents the architectures of DocRE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence Importance Estimation</head><p>We estimate the importance of each sentence for a specific entity pair. Low-scored sentences will be treated as non-evidence, and in principle, can be removed without changing DocRE predictions.</p><p>We propose a sentence importance score based on the DocRE predictions with and without the sentence in question. Our observation is that the relation extraction task is usually monotonic to evidence, i.e., (non-strictly) more relations will be predicted with more sentences. If we remove a sentence and the predicted probability of a relation decreases, then the sentence is likely to be the evidence. If the predicted probability does not change, then the sentence is likely to be nonevidence. Moreover, the predicted probability may sometimes increase when a sentence is removed, in which case the DocRE model is not robust, as this violates monotonicity.</p><p>Formally, we consider removing one sentence at a time, and the document with the nth sentence removed is denoted byD (?n) = {s 1 , ? ? ? , s n?1 , s n+1 , ? ? ? , s N }. For a DocRE model F , we obtain the classification probabilities P ij = F j (D, e i h , e it ) based on the original document, andP</p><formula xml:id="formula_2">(?n) ij = F j (D (?n) , e i h , e it ) with sentence n removed.</formula><p>We propose the importance score as</p><formula xml:id="formula_3">g (?n) ij = P ij log P i? P (?n) ij<label>(3)</label></formula><p>The formula appears similar to Kullback-Leibler (KL) divergence. However, we only take one term in the KL summation, because the KL divergence, albeit asymmetric in its two arguments, cannot model the increase or decrease ofP</p><formula xml:id="formula_4">(?n) ij , whereas our g (?n) ij is monotonically decreasing withP (?n) ij .</formula><p>Compared with a naive difference or ratio between P ij andP (?n) ij , we find that our KL-like score is more robust in the scale of P ij when determining non-evidence sentences.</p><p>We treat a sentence n as non-evidence if g (?n) ij &lt; ? for a thresholding hyperparameter ?. The resulting set of non-evidence sentences is denoted by K ij for the an entity pair (e i h , e it ) and relation j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence Focusing Loss</head><p>We propose a sentence focusing loss to encourage the model to produce the same output distribution when the entire document is fed as input and when non-evidence sentences are removed.</p><p>Ideally, the predicted probability should remain the same if we remove any combination of the sentences in K ij . Therefore, we penalize the extent to which the predicted probability is changed.</p><p>We propose the sentence focusing loss as:</p><formula xml:id="formula_5">L sf = ? D?C i h =it j?R J ij ?K ij {P ij log(P (?J ij ) ij ) +(1 ? P ij ) log(1 ?P (?J i ) ij )} (4) where J ij is a subset of K ij andP (?J ij ) ij = F j (D\J ij , e i h , e it )</formula><p>is the predicted probability with J ij removed from D, and the total loss is L = (L rel + L sf )/2.</p><p>Essentially, our sentence focusing loss ensures P ij is close toP</p><formula xml:id="formula_6">(?J ij ) ij</formula><p>, which intuitively makes sense because non-evidence sentences should not affect the prediction. Our approach can also be thought of as a way of data augmentation. However, compared with one-hot groundtruth labels, our sentence focusing loss works with soft labels P ij andP</p><formula xml:id="formula_7">(?J ij ) ij</formula><p>, which are believed to contain more information <ref type="bibr" target="#b5">(Hinton et al., 2015)</ref>, and our gradient propagates to both P ij andP</p><formula xml:id="formula_8">(?J ij ) ij</formula><p>for training. The calculation of Eqn. (4) is time-and resourceconsuming, because the number of the subsets J ij grows combinatorially with the number of non-evidence sentences. Moreover it should be calculated repeatedly once the parameter of the model is updated. To this end, we propose a simplified training strategy to approximate Eqn. (4) in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Strategy</head><p>We propose a strategy to simplify the calculation and the training procedure. Concretely, we only remove one non-evidence sentence in K ij at a time instead of a subset of J ij ? K ij , and we aggregate the effect of different non-evidence sentences by:</p><formula xml:id="formula_9">L sf = ? D?C N n=1 i h =it j?R I(g (?n) ij &lt; ?) {P ij log(P (?n) ij ) + (1 ? P ij ) log(1 ?P (?n) ij )}<label>(5)</label></formula><p>where I is the indicator function. Essentially, we linearly approximate the combination of multiple non-evidence sentences in (4) by an outer summation. In this way, the number of terms does not grow combinatorially, but linearly w.r.t. N .</p><p>In implementation, we further simply the summation over n by Monte Carlo sampling of a randomly selected sentence n in each gradient update. The loss is reformulated as follows:</p><formula xml:id="formula_10">L sf = ? D?C i h =it j?R I(g (?n) ij &lt; ?) {P ij log(P (?n) ij ) + (1 ? P ij ) log(1 ?P (?n) ij )}</formula><p>(6) As seen, we need to forward the base models twice in each update, with and without the sentence n.  propose a similar idea but train different entity pairs in a document based on different sets of sentences; all sentence are processed repeatedly among entity pairs in a document. Their approach is much slower than ours.</p><p>To sum up, the proposed SIEF framework identifies non-evidence sentences and penalizes the difference of predicted probabilities when a nonevidence sentence is removed. Our approach is a generic framework and can be adapted to various DocRE model easily, without introducing extra parameters into the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">DocRE Model Architectures</head><p>Our SIEF can be applied to various base DocRE models. To evaluate its generality, we consider the following recent models.</p><p>BiLSTM <ref type="bibr" target="#b18">(Yao et al., 2019)</ref> 2 . A bi-directional long short term memory (BiLSTM) encodes the document, and an entity is representated by BiLSTM's hidden states, averaged over entity mentions. The head and tail entity representations are fed to a multi-layer perceptron (MLP) for relation extraction.</p><p>BERT base <ref type="bibr">(Devlin et al., 2019) 3</ref> . A pre-trained language model is used for document encoding.</p><p>HeterGSAN <ref type="bibr">(Xu et al., 2021b) 4</ref> . HeterGSAN is a recent graph-based DocRED model, which constructs a heterogeneous graph of sentence, mention, and entity nodes; it uses graph neural networks for relation extraction.  GAIN <ref type="bibr" target="#b22">(Zeng et al., 2020)</ref> 3 . GAIN constructs two graphs: mention-document graphs and entity graphs, and performs graph and path reasoning over the two graphs separately. When combining our SIEF with GAIN, we achieve the best performance among all the base models with SIEF on DocRED. Thus, we will explain this model in more detail.</p><p>Essentially, a node in the mention-document graph is either a mention or a document. The mentions are connected to its document, and two mentions are connected if they co-occur in one sentence. In the entity graph, two entities are connected if they are mentioned in one sentence. To classify the relation, GNN is applied to the mentiondocument graph, enhanced with path information in the entity graph, shown in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>When combining SIEF with GAIN, we randomly remove one sentence from the document. The corresponding nodes and edges are removed in the GAIN's graphs. Then we obtain the output probabilities with and without the sentence, P ij andP (?n) ij , separately. If the sentence important score g (?n) ij in Eqn. (3) is below a threshold ?, the sentence is treated as non-evidence for the entity pair (e i h , e it ) and relation j. We apply the sentence focusing loss Eqn. (4) to improve the robustness.</p><p>For prediction, we apply the trained DocRE model to the entire document, because with our approach the model is already robust when nonevidence sentences are presented. Empirical results will show that our SIEF consistently improves the performance of base DocRE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Datasets. DocRED is a large-scale humanannotated dataset for document-level relation extraction <ref type="bibr" target="#b18">(Yao et al., 2019)</ref>.</p><p>The dataset is constructed from Wikipedia and Wikidata, containing 3053 documents for training, 1000 for development, and 1000 for test. In total, it has 132,375 entities and 56,354 relational facts in 96 relation types. More than 40% of the relational facts require reasoning over multiple sentences. The standard evaluation metrics are F1 and Ign F1 <ref type="bibr" target="#b18">(Yao et al., 2019;</ref><ref type="bibr" target="#b22">Zeng et al., 2020)</ref>, where Ign F1 refers to the F1 score excluding the relational facts in the training set.</p><p>We also evaluated our approach on DialogRE (V2, <ref type="bibr" target="#b20">Yu et al., 2020)</ref>, which contains 36 relation types, 17 of which are interpersonal. We followed the standard split with 1073 training dialogues, 358 validation, and 357 test. Following <ref type="bibr" target="#b20">Yu et al. (2020)</ref>, we report macro F1 scores in both the standard and conversational settings; the latter is denoted by F1 c .</p><p>Competing Methods. We experimented our SIEF on a number of base models, namely, BiLSTM, BERT base , HeterGSAN, and GAIN (Section 4.4). These base models are all considered for comparison.</p><p>For DocRED, we consider additional competing methods: Two Phase <ref type="bibr" target="#b11">(Wang et al., 2019)</ref>, which first predicts whether the entity pair has a relation and then predicts the relation type; LSR <ref type="bibr" target="#b9">(Nan et al., 2020)</ref>, which constructs the graph by inducing a latent document-level graph; Reconstructor <ref type="bibr" target="#b15">(Xu et al., 2021b)</ref>, which encourages the model to reconstruct a reasoning path during training; DRN <ref type="bibr" target="#b14">(Xu et al., 2021a)</ref>, which considers different reasoning skills explicitly and uses graph representation and context representation to model the reasoning skills; ATLOP <ref type="bibr" target="#b24">(Zhou et al., 2021)</ref>, which aggregates contextual information by the Transformer attentions and adopts an adaptive threshold for different entity pairs; and DocuNet <ref type="bibr" target="#b23">(Zhang et al., 2021)</ref>, which models DocRE as a semantic segmentation task.</p><p>For DialogRE, we followed <ref type="bibr" target="#b20">Yu et al. (2020)</ref> and considered BERT and BERT s for comparison, 5 where BERT s prevents a model from overfitting by replacing of the interpersonal augment with a special token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev</head><p>Test Ign F1 F1 Ign F1 F1 DocRE Systems with GloVe LSR <ref type="bibr" target="#b9">(Nan et al., 2020)</ref> 48.82 55.17 52.15 54.18 Reconstructor <ref type="bibr" target="#b15">(Xu et al., 2021b</ref><ref type="bibr">) 54.25 55.70 53.25 55.13 DRN (Xu et al., 2021a</ref> 54.61 56.49 54.35 56.33 BiLSTM <ref type="bibr" target="#b18">(Yao et al., 2019)</ref> 48  <ref type="bibr" target="#b9">(Nan et al., 2020)</ref> 52.43 59.00 56.97 59.05 Reconstructor <ref type="bibr" target="#b15">(Xu et al., 2021b</ref><ref type="bibr">) 58.13 60.18 57.12 59.45 DRN (Xu et al., 2021a</ref> 59.33 61.39 59.15 61.37 ATLOP <ref type="bibr" target="#b24">(Zhou et al., 2021)</ref> 59.22 61.09 59.31 61.30 DocuNet <ref type="bibr" target="#b23">(Zhang et al., 2021)</ref> 59.86 61.83 59.93 61.86 BERT base <ref type="bibr" target="#b19">(Ye et al., 2020)</ref> 54   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details.</head><p>We use the repositories 2,3,4,5 of base models to implement our approach. We mostly followed the standard hyperparameters used in the base models. Our SIEF has one hyperparameter ? in Eqn. (5). It was set to 0.8, and Section 5.2 presents the effect of tuning ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analyses</head><p>Main results. <ref type="table" target="#tab_5">Table 1</ref> presents the detailed results on the development and test sets of the DocRED dataset. We first compare DocRE systems with GloVe embeddings <ref type="bibr" target="#b18">(Yao et al., 2019)</ref>. We see that the proposed SIEF method significantly improves the performance of all base models, including the sequence model (i.e., BiLSTM) and graph models (i.e., HeterGSAN and GAIN); the average improvement is 2.05 points in terms of test F1. This shows that SIEF is compatible with both sequence and graph models, indicating the generality and effectiveness of the proposed method. For the DocRE system with BERT base , SIEF also   <ref type="bibr" target="#b22">(Zeng et al., 2020)</ref> with BERT base encoding yields state-of-the-art performance in terms of F1. We further conducted experiments on the DialogRE dataset, and compare our approach with the BERT baselines in <ref type="bibr" target="#b20">Yu et al. (2020)</ref>. As seen, the results are consistent with the improvement on DocRED, as our SIEF largely improves F1 and F1 c for both base models. This further confirms the generality of our approach in different domains.</p><p>In the rest of this section, we present in-depth analyses to better understand our model with DocRED as the testbed. All base models use GloVe embeddings as opposed to BERT due to efficiency concerns.</p><p>Intra-and Inter-Sentence Performance. We breakdown the relation classification performance into intra-sentence reasoning and inter-sentence reasoning. Ideally, if only one sentence is needed to determine the relation of an entity pair, then it belongs to the intra-sentence category; if two or more sentences are needed, then it belongs to the inter-sentence category. We follow <ref type="bibr" target="#b9">Nan et al. (2020)</ref> and approximate it by checking whether two entities are mentioned in one sentence.</p><p>The results are shown in <ref type="table" target="#tab_8">Table 3</ref>. SIEF again consistently improves base models in terms of both Intra-F1 and Inter-F1. However, the improvement on Intra-F1 is larger than that on Inter-F1. This is because our SIEF encourages the model to focus on evidence by removing one sentence at a time, but does not explicitly model sentence relations. Based on this analysis, we plan to extend the SIEF framework with multi-sentence DocRE reasoning in our future work.</p><p>Performance of predicting evidence sentences. In our paper, we propose a sentence importance score to measure how much a sentence contributes to the classification without  using additional annotation. We evaluate such performance in <ref type="table" target="#tab_10">Table 4</ref> by Precision, Recall, and F1 scores against manually annotated evidence sentences that are provided in the dataset. In this analysis, we do not perform relation prediction, but concern about entity pairs knowingly having certain relations. Specifically, for entity pair (e i h , e it ) with relation j, we calculate the importance score g (?n) ij for each sentence and cut off evidence/non-evidence sentences with a threshold based on the development F1 score.</p><p>As seen, all base models achieve above 60% F1, suggesting that the proposed importance score is indeed indicative for predicting evidence and nonevidence sentences.</p><p>With the proposed SIEF framework, the performance improves for all metrics, with an average improvement of 2.95 F1 points across three base models. This further verifies that our SIEF framework not only improves relation extraction performance, but also is able to better detect evidence and non-evidence sentences, which is important for the interpretability of machine learning models.</p><p>Robustness of DocRE models. We further investigate the robustness of DocRE models by showing the difference between the predicted distributions with and without non-evidence sentences. We show in <ref type="figure" target="#fig_1">Figure 5</ref> the scatter plots of the probability P based on the entire document and the probabilityP (?n) ij with a random non-evidence sentence removed.</p><p>As shown in the figure, the points of the base models (left magenta plots) scatters over a wider range, whereas our SIEF training (right cyan plots) makes them more concentrated on the diagonal, indicating that the prediction P ij on the entire document is mostly the same asP (?n) ij with a nonevidence removed. This shows the robustness of SIEF-trained models, as they are less sensitive to non-evidences sentences for DocRE.</p><p>Analysis on hyperparameter ?. Our SIEF framework has one hyperaparameter ? that controls how strict we treat a sentence as evidence or nonevidence (Section 4.3). We analyze the effect of ? in <ref type="figure">Figure 4</ref>.   As seen, our SIEF approach consistently benefits the base models with a large range of ? values. Intuitively, if ? is too small, very few sentences will be treated as non-evidence and our sentence focusing loss is less effective; if ? is too large, it has a high false positive rate of non-evidence sentences. Empirically, a moderate ? around (0.6-0.8) yields the highest performance. From the plots, we also see that our hyperparameter ? is insensitive to the base models, justifying our design of Eqn. <ref type="formula" target="#formula_3">(3)</ref>.</p><p>Sentence importance score VS other heuristics. To investigate the effectiveness of our sentence importance score in Eqn. (3), we compare it with several alternative heuristics: 1) We randomly select half of the sentences as the nonevidence set, denoted by Rand; and 2) We consider the non-evidence set as the sentences without entity mentions, denoted by NoMention.</p><p>The results of the performance in terms of F1 and Ign F1 on the development set are shown in <ref type="table" target="#tab_12">Table 5</ref>. As seen, the simple heuristic Rand outperforms the base model, as Rand can be thought of as noisy data augmentation. The NoMention heuristic outperforms Rand, as sentences without entity mentions are more likely to be non-evidence. Moroever, SIEF is superior to both Rand and NoMention, showing that our sentence importance scores is a more effective indicator of evidence and non-evidence sentences.</p><p>Our sentence focusing loss VS learning from groundtruth. We encourage the DocRE models to generate consistent output probabilities with and without non-evidence (Section 4.2) by a crossentropy loss between two soft distributions P ij andP  alternative choice: we learnP (?n) ij directly from the groundtruth label r ij . <ref type="table" target="#tab_13">Table 6</ref> shows the results on the development set in terms of F1 and Ign F1. As seen, both methods can improve the performance of the base models. This confirms that removing non-evidence sentences can serve as a way of data augmentation, boosting the performance of DocRE models. Moreover, we observe that our sentence focusing loss is better than learning from the groundtruth labels, showing that the soft predictions provide more information than one-hot labels, consistent with knowledge distillation literature <ref type="bibr" target="#b5">(Hinton et al., 2015)</ref>.</p><formula xml:id="formula_11">(?n) ij .</formula><p>Case Study. <ref type="figure" target="#fig_2">Figure 6</ref> shows a case study of GAIN and GAIN+SIEF models. For the entity pair (Brad Wilk, Rage Against the Machine), both GAIN and GAIN+SIEF predicts the relation "MemberOf", which is consistent with the reference. We see that Sentence 3 is non-evidence, and in principle, it should not affect DocRE prediction in this case. However, the base GAIN model makes a wrong prediction "not MemberOf", as the predicted probability is below the threshold, which is determined by validation based on predicted binary probabilities of all relations. By contrast, our SIEF model is able to make correct predictions when different non-evidence sentences are removed, demonstrating its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel Sentence Information Estimation and Focusing (SIEF) approach to document relation extraction (DocRE).</p><p>We design a sentence importance score and a sentence focusing loss to encourage the model to focus on evidence sentences. The proposed SIEF is a general framework, and can be combined with various base DocRE models. Experimental results show that SIEF consistently improves the performance of base models in different domains, and that it improves the robustness of DocRE models. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The model architecture of GAIN with SIEF. A sentence is randomly removed from the document. The corresponding nodes and edges are removed from the mention-document graph and the entity graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Robustness of DocRE models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Case Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Rage Against the Machine is an American rap metal band from Los Angeles, California. [2] Formed in 1991, the group consists of vocalist Zack de la Rocha, guitarist Tom Morello, bassist Tim Commerford and drummer Brad Wilk. [3] After a self-issued demo, the band signed with Epic Records and released its debut album Rage Against the Machine in 1992. ?</figDesc><table><row><cell>[1] Relation: MemberOf</cell><cell></cell><cell>Supporting Evidence: {1,2}</cell></row><row><cell>Model Input</cell><cell>{1,2,3}</cell><cell>{1,2}</cell></row><row><cell>Ground Truth</cell><cell>MemberOf</cell><cell>MemberOf</cell></row><row><cell>GAIN Prediction</cell><cell>MemberOf</cell><cell>not MemberOf (undesired)</cell></row><row><cell cols="3">Figure 1: A DocRE model predicts correctly for</cell></row><row><cell cols="3">an entire document, but errs when a non-evidence</cell></row><row><cell cols="2">sentence is removed.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Company.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Input Document</cell><cell></cell><cell cols="3">Importance Scores Ranking</cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell cols="3">Evidence Sentence</cell></row><row><cell></cell><cell>2 3</cell><cell>2</cell><cell>3</cell><cell>2 3</cell><cell>3</cell><cell>1</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 2 3</cell><cell>1 2</cell></row><row><cell></cell><cell>DocRE model</cell><cell>DocRE model</cell><cell>DocRE model</cell><cell>DocRE model</cell><cell>DocRE</cell><cell cols="2">DocRE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>model</cell><cell cols="2">model</cell></row><row><cell>Original Document</cell><cell cols="4">Sentence Importance Estimation Scores 3 1 Importance 2</cell><cell cols="3">Sentence Focusing Loss</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The Sacramento Bee is a daily newspaper published in Sacramento, California in the United States. [2] Since its founding in 1857, The Bee has become the 27th largest paper in the U.S. [3] The Bee is the flagship of the nationwide McClatchy Company.</figDesc><table><row><cell cols="9">[1] ?</cell></row><row><cell>Sentence Remove</cell><cell>Encoder</cell><cell>2 3 1 3</cell><cell>2 1</cell><cell>1</cell><cell>GCNs</cell><cell>Path Info</cell><cell>? rel 11 &lt; (?3)</cell><cell>? rel 21 ? (?3)</cell></row><row><cell>Randomly</cell><cell>Encoder</cell><cell cols="4">2 3 1 Mention-document graph 2 1 1 GCNs</cell><cell>Entity graph</cell><cell cols="2">? sf Entity Pair 1 Entity Pair 2</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell cols="2">GAIN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1 2 1 3 Mention</cell><cell></cell><cell cols="2">Document</cell><cell></cell><cell>Entity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Results on the development and test sets of the DocRE dataset. Bold indicates the best performance.</figDesc><table><row><cell>Model</cell><cell>F1</cell><cell>Dev F1 c</cell><cell>F1</cell><cell>Test F1 c</cell></row><row><cell cols="5">BERT (Yu et al., 2020) 60.6 55.4 58.5 53.2</cell></row><row><cell>+SIEF</cell><cell cols="4">61.4 57.6 59.9 56.1</cell></row><row><cell cols="5">BERT s (Yu et al., 2020) 63.0 57.3 61.2 55.4</cell></row><row><cell>+SIEF</cell><cell cols="4">64.3 60.6 61.8 58.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Results on DialogRE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Results of Intra-F1 results and Infer-F1 on development set of DocRED. The difference is compared between SIEF and the respective base model.</figDesc><table><row><cell>consistently improves the base models, showing</cell></row><row><cell>that SIEF is complementary to the modern BERT</cell></row><row><cell>architecture. Especially, combining SIEF and</cell></row><row><cell>GAIN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Results of the evidence prediction on the development set of DocRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>50.94 52.17 54.40 53.05 55.29 +SIEF 52.08 54.20 54.49 56.30 55.07 56.96 +Rand 50.63 52.63 52.75 54.70 53.41 55.63 +NoMention 51.56 53.79 54.07 55.95 54.66 56.52</figDesc><table><row><cell>Method</cell><cell>BiLSTM Ign F1 F1</cell><cell>HeterGSAN Ign F1 F1</cell><cell>GAIN Ign F1</cell><cell>F1</cell></row><row><cell>Base</cell><cell>48.87</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Results of our approach and other heuristics.</figDesc><table><row><cell>Method</cell><cell>BiLSTM Ign F1 F1</cell><cell>HeterGSAN Ign F1 F1</cell><cell>GAIN Ign F1</cell><cell>F1</cell></row><row><cell>Base</cell><cell cols="4">48.87 50.94 52.17 54.40 53.05 55.29</cell></row><row><cell>+SIEF</cell><cell cols="4">52.08 54.20 54.49 56.30 55.07 56.96</cell></row><row><cell cols="5">+GTruth 50.36 52.56 52.65 54.69 53.75 55.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Comparing our sentence focusing loss with learning from groundtruth labels (denoted by GTruth).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>To investigate the effect of such a sentence focusing loss, we compare it with an Rage Against the Machine is an American rap metal band from Los Angeles, California. [2] Formed in 1991, the group consists of vocalist Zack de la Rocha, guitarist Tom Morello, bassist Tim Commerford and drummer Brad Wilk. [3] After a self-issued demo, the band signed with Epic Records and released its debut album Rage Against the Machine in 1992. ?</figDesc><table><row><cell>[1]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Entity Pair: {Brad Wilk, Rage Against the Machine}</cell></row><row><cell cols="2">Reference: MemberOf</cell><cell cols="3">Evidence: {1,2}</cell></row><row><cell>Predicted Probability</cell><cell cols="3">GAIN 0.713 0.283 0.106 0.319</cell></row><row><cell>0.574</cell><cell></cell><cell></cell><cell></cell><cell>Threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell></cell><cell>{1,2,3} {1,2}</cell><cell>{1,3}</cell><cell>{2,3}</cell><cell>Sentences</cell></row><row><cell>Predicted Probability</cell><cell cols="3">GAIN+SIEF 0.796 0.744 0.280 0.381</cell></row><row><cell>0.506</cell><cell></cell><cell></cell><cell></cell><cell>Threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell></cell><cell>{1,2,3} {1,2}</cell><cell>{1,3}</cell><cell>{2,3}</cell><cell>Sentences</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics, pages 113-120. Chris Quirk and Hoifung Poon. 2017. Distant supervision for relation extraction beyond the sentence boundary. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pages 1171-1182. Linfeng Song, Yue Zhang, Daniel Gildea, Mo Yu, Zhiguo Wang, and Jinsong Su. 2019. Leveraging dependency forest for neural medical relation extraction. Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing. Daniil Sorokin and Iryna Gurevych. 2017. Contextaware representations for knowledge base relation extraction. In Proceedings of the Conference on</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/thunlp/DocRED 3 https://github.com/DreamInvoker/GAIN 4 https://github.com/xwjim/DocRE-Rec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/nlpdata/dialogre</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers and meta reviewers for their insightful comments and suggestions. This work is funded by the National <ref type="figure">Key</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Connecting the dots: Documentlevel neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="837" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural relation extraction within and across sentence boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subburam</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016513</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advance of Artificial Intelligence</title>
		<meeting>the Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6513" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems Deep Learning and Representation Learning Workshop</title>
		<meeting>the Annual Conference on Neural Information Processing Systems Deep Learning and Representation Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three sentences are all you need: Local path enhanced document relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.126</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="998" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: A resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baw068</idno>
	</analytic>
	<monogr>
		<title level="j">Database: The Journal of Biological Databases and Curation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for DocRED with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic relation classification via hierarchical recurrent neural network with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1254" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.144</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1653" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advance of Artificial Intelligence</title>
		<meeting>the Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="14167" to="14175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved relation classification by deep recurrent neural networks with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1461" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coreferential reasoning learning for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.582</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7170" to="7186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dialogue-based relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/551</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3999" to="4006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advance of Artificial Intelligence</title>
		<meeting>the Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="14612" to="14620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph neural networks with generated parameters for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1331" to="1339" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Maosong Sun</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
