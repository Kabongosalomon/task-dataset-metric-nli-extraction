<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
							<email>chenguangwang@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Haoyun</roleName><forename type="first">Zui</forename><surname>Chen</surname></persName>
							<email>chenzui19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
							<email>dawnsong@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on taskspecific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the taskspecific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pretraining task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information extraction refers to the task of automatically extracting structured information from unstructured resources, benefiting a wide range of applications such as information retrieval and knowledge base population. Information extraction covers a great variety of tasks in natural language processing <ref type="bibr">(NLP)</ref>, such as open information extraction and relation classification. For example, given a sentence "Born in Glasgow, Fisher is a graduate of the London Opera Centre", open information extraction seeks to extract (Fisher; Born in; Glasgow), and "city_of_birth" is predicted as the relation between a given pair of entities "Fisher" and "Glasgow" for relation classification.</p><p>Most current approaches design task-specific pipelines for different information extraction tasks. Yet, this presents two limitations for information extraction. First, since most of the approaches employ a task-specific model, it is difficult to leverage a single pipeline to solve many tasks or adapt a model trained on one task to another without changing any task-specific modules. Second, those supervised state-of-the-arts are trained on task-specific corpora to predict from a fixed set of task-specific categories, which restricts their usability since additional labeled data is needed to specify any other classes. Such task-specific labeled data is scarce in information extraction. For example, the largest training set for open information extraction contains only 3,200 sentences <ref type="bibr" target="#b37">(Stanovsky et al., 2018)</ref>. Motivated by this, we aim to solve information extraction tasks within the same framework in a task-agnostic setting.</p><p>In this paper, we propose a unified framework for information extraction. The basic idea is to treat every information extraction problem as a "text-totriple" problem, i.e., translating input text to output triples. We successfully apply our framework to three information extraction tasks, greatly improving zero-shot performance on many datasets and sometimes even reaching competitiveness with the current state-of-the-art fully supervised approaches. <ref type="figure">Figure 1</ref> shows how different information extraction tasks are handled within our framework. The framework encodes task priors in the input text and decodes the output triples to finally produce task predictions. We achieve this by leveraging the same translation process on all tasks, the only difference among tasks being the input encoding. This is in contrast with previous approaches using task-specific models and datasets. The design of the common translation module for all tasks is important: by leveraging the task priors encoded in <ref type="figure">Figure 1</ref>: Our DEEPEX translates between input text and output triples, and the output is then decoded into task predictions.</p><p>the input text, we enable the zero-shot transfer of the general knowledge that a pre-trained LM has about the task. We demonstrate that a simple pretraining task of predicting which relational triple goes with which text on a task-agnostic corpus further enhances the zero-shot capabilities on all tasks. To the best of our knowledge, this is the first framework to handle a variety of information extraction tasks in a zero-shot setting.</p><p>Our contributions are summarized below.</p><p>1. We introduce DEEPEX, a unified framework that solves information extraction tasks in a zero-shot setting. We cast information extractions as text-to-triple problems by incorporating the task priors in the input text and translating the input text to triples as output.</p><p>2. We apply our framework to (i) open information extraction, (ii) relation classification, and (iii) factual probe. In all tasks, we achieve competitive zero-shot performance to the current state-of-the-art including the fully supervised methods, and we achieve new state-ofthe-art performance on open information extraction (OIE2016, WEB, NYT, and PENN) and factual probe (T-REx). For instance, our zero-shot approach significantly outperforms the supervised open information extraction by averaging 37.5 points in F1.</p><p>3. We also show that our framework delivers more interpretable results while achieving comparable performance on all tasks, thanks to the transparency of the text-to-triple translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We cast a suite of information extraction tasks into a text-to-triple translation framework. As shown in <ref type="figure">Figure 1</ref>, input and output are designed in a format that is appropriate for a given task. The translation process takes the input text and produces triples as output. The decoding step generates task predictions from the output. In this section, we describe the input and output format, the translation, and the decoding process. We use open information extraction (OIE) as a running example in this section. For OIE, we are given a sentence and asked to extract triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input and Output Format</head><p>The input is a NP-chunked sentence, and the output is a set of triples. The NPs are encoded as task priors in the input. Below is an example.</p><p>Input Born in Glasgow NP , Fisher NP is a graduate of the London Opera Centre NP . Output (Fisher; Born in; Glasgow), (Fisher; is a graduate of; London Opera Centre).</p><p>NP denotes the noun phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Zero-Shot Translation</head><p>We aim to translate the above input text to output triples. Information extraction tasks lack highquality training data, therefore training an end-toend supervised approach <ref type="bibr">(Paolini et al., 2021)</ref> is not feasible. Pre-trained language models (LM) (e.g., BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> and <ref type="bibr">GPT (Brown et al., 2020)</ref>) have demonstrated their zero-shot capabilities in a wide range of NLP tasks, thanks to the general information that they know about the tasks. We therefore propose a zero-shot translation process consisting of two steps: generating and ranking, as shown in <ref type="figure">Figure 2</ref>. The generating stage produces general information about the task via pre-trained LMs, and the ranking stage looks for specific information about the task via a ranking model pre-trained on a task-agnostic corpus.</p><p>Generating The generating stage produces a set of candidate triples that contain general information about the task from pre-trained LMs. OIE is  <ref type="figure">Figure 2</ref>: Summary of our approach. The framework encodes task-relevant information in the input text and decodes the output triples to produce task predictions. The zero-shot translation first generates general information that a pre-trained language model has about the input, then ranks to find the output triples of interest to the task via a ranking model pre-trained on a task-agnostic relational corpus. formulated as extracting a set of sequences in the input that are generally relevant to an argument pair (i.e., NP pair). We particularly use the attention scores in pre-trained LMs to measure the relevance between the sequence and the argument pair.</p><p>We frame the process as a search problem. Given an argument pair (e.g., "Fisher" and "London Opera Centre"), we aim to search for the sequences (e.g., "is a graduate of") with the largest attention scores connecting the pair. To compute a score for every possible sequence is computationally expensive, especially when the sequence length is large. Therefore the exhaustive search is intractable. We use beam search, which is an approximate strategy to explore the search space efficiently. Beam search maintains the k-best candidates. This means the time cost of beam search does not depend on the sequence length but the size of the beam and the average length of the candidates. The beam search starts with a task-specific start token <ref type="bibr">[S]</ref>. At each step, beam search simply selects top-k next tokens with the largest attention scores, and just keeps k partial candidates with the highest scores, where k is the beam size. When a candidate produces a taskspecific end token . This helps to improve the recall of the candidates. For example, a candidate triple (Fisher; Born in; Glasgow) is generated by looking at "Born in" on the left in the above example. We also need to enable bidirectionality by running the search in both directions (left to right and right to left) following <ref type="bibr" target="#b38">Wang et al. (2020)</ref>. For OIE, we implement this by allowing every argument as both [S] and [E] regardless of its position in the input. For example, "Fisher" is [S] in (Fisher; Born in; Glasgow) given "Glasgow" appears before "Fisher" in the input.</p><p>Ranking The ranking stage finds triples that are of interest to the task via a ranking model pretrained on a task-agnostic relational corpus. For OIE, the generating stage produces k candidate triples for every argument pair. However, the sequences in the candidates are relevant to the argument pairs, not just in the relational aspect. The ranking stage aims to find the triples that specifically express the relational information between the argument pair, which is important for OIE.</p><p>We propose a contrastive model to rank the triples as illustrated in <ref type="figure">Figure 2</ref>. Given a batch of N (sentence, triple) pairs, the model is trained to predict which of the N 2 possible (sentence, triple) pairs across a batch actually appeared. The model learns a joint embedding space by training a base encoder BERT. The input sequence of the BERT encoder is in the format:</p><formula xml:id="formula_0">[CLS] sentence [SEP] triple [SEP]</formula><p>, which follows the standard input format of BERT. The goal is to maximize the cosine similarity of the sentence and triple embeddings of the N true pairs in the batch while minimizing the cosine similarity of the embeddings of the remaining N 2 ? N incorrect pairs. We optimize a cross-entropy loss over these similarity scores. The loss function for a positive pair is defined by l in Eq. 1.</p><formula xml:id="formula_1">lsentence = ? log exp(sim(ui, vi)) N k=1 exp(sim(ui, v k )) l triple = ? log exp(sim(ui, vi)) N k=1 exp(sim(u k , vi)) l = lsentence + l triple 2 (1) where sim(u, v) = u v u v .</formula><p>For the i-th positive (sentence, triple) pair, u i and v i denote the sentence and triple embedding respectively.</p><p>We take advantage of the pre-trained BERT BASE as the base encoder. We further simplify the standard contrastive learning framework by removing the projection layer between the representation and the contrastive embedding space. Neither the linear <ref type="bibr" target="#b27">(Radford et al., 2021)</ref> nor non-linear <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref> projection is used. This is because sentences and triples are unified in the same embedding space of BERT. We train the model on T-REx <ref type="bibr" target="#b9">(Elsahar et al., 2019)</ref>, which is a dataset of large-scale alignments between Wikipedia abstracts and Wikidata triples. T-REx contains a large number of sentence-triple pairs (11 million triples are paired with 6.2 million sentences). T-REx also reports an accuracy of 97.8% of the pairs. The ranking model is task-agnostic. The ranking model takes the input in the same format for all tasks. At test time, the input text and each candidate triple from the generating stage is paired as the input to the ranking model. The candidate triples are ranked by the contrastive loss. We adopt the topn candidate triples returned by the ranking model as the output. n varies across different tasks 2 . For the above OIE example, the output is the top-2 triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoding</head><p>Once the output triples are produced, we decode the output triples to obtain task predictions. For OIE, the output triples serve as task predictions directly. No specific decoding strategy is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Information Extraction Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Open Information Extraction</head><p>The details are provided in Sec. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Classification</head><p>For this task, we are given an input sentence with gold head and tail entities aiming to classify the relation type in a pre-defined category.</p><p>2 Please refer to Appendix A for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>The input is a sentence encoded with gold head and tail entities, and linked relation phrases. The output is a triple. An example is below.</p><p>Input Born in place_of_birth Glasgow GOLD , Fisher GOLD is a graduate of the London Opera Centre. Output (Fisher; place_of_birth; Glasgow).</p><p>GOLD denotes the gold entity. The linked relation phrases annotated with Wikidata predicates, e.g., Born in place_of_birth , are constructed as follows. We use an offline dictionary that maps the pre-defined relations to the Wikidata predicates. Such dictionaries are often provided either by Wikidata or third-parties. In all tested datasets, we can refer to either gold Wikidata or other high-quality resources for the dictionaries. We consider a sequence of tokens linked to a certain relation if the tokens are matched with the label or alias of the particular predicate in Wikidata. In the above example, "Born in" matches an alias of the Wikidata predicate "place_of_birth". In practice, some Wikidata predicates do not provide as many aliases as others. Inspired by <ref type="bibr" target="#b0">Angeli et al. (2015)</ref>, we follow the below procedure to add new aliases to resolve the imbalance issue: We first create a large candidate set of Wikipedia relations aligned to Wikidata predicates via distant supervision, then ask human annotators to filter out the wrong alignments. The remaining aligned relation phrases are added as new aliases of the Wikidata predicates.</p><p>Relation-Constrained Translation For the beam search in the generating stage of Sec. 2.2, [S] and [E] are the gold head and tail entities respectively. As the task requires the relations to be from a pre-defined category, using the beam search directly is not efficient. Allowing generating any token at each step might lead to sequences that do not match any pre-defined relations. Similar to De Cao et al. (2021), we use constrained beam search, which only decodes tokens belonging to a linked relation phrase. We take the top-n triples from the ranking model as the output.</p><p>Decoding Relation We take the Wikidata predicates of the output triples, and map the predicates back to the relations in the pre-defined category, which serve as the task predictions. In the above input/output example, "place_of_birth" is the Wikidata predicate in the output triple. It is mapped to "city_of_birth" in the pre-defined relation category of one of the widely used relation classification datasets, TACRED. "city_of_birth" hence serves as the task prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Factual Probe</head><p>Given an input sentence with gold head entity name and relation name, the task aims to fill in the tail entity.</p><p>Input and Output Format The input is encoded as a NP-chunked sentence with gold head entity candidates and linked relation phrases. The output is a triple. An example is below.</p><p>Input Born in place_of_birth Glasgow NP , Fisher GOLD/NP is a graduate of the London Opera Centre NP . Output (Fisher; place_of_birth; Glasgow).</p><p>GOLD/NP denotes the noun phrase that matches the gold head entity. Born in place_of_birth represents a linked relation phrase annotated with a Wikidata predicate which is constructed in the same way as in Sec. 3.2.</p><p>Entity-Constrained Translation For the beam search, [S] and [E] are the gold head entity candidate and linked relation phrase respectively. Similar to the relation classification, we also constrain the search to generate possible tail entity sequences. We assume that NPs other than the gold head entity provide the set of candidate tail entities. To enable this, the search only decodes tokens belonging to the candidate NPs. In practice, we take the top-1 triple from the ranking model as the output.</p><p>Decoding Tail Entity We take the tail entities of the output triples as task predictions. For example, in the above output triple, "Glasgow" is decoded as the task prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we show that DEEPEX framework solves the different information extraction tasks considered and outperforms the task-specific stateof-the-art results on multiple datasets.</p><p>To keep the framework as simple as possible, most settings and hyperparameters are shared across all experiments. For example, we use BERT LARGE <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> for the beam search of the generating stage (Sec. 2.2) for all tasks. The details of the experimental setup, datasets, and comparison methods are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>The results are shown in <ref type="table" target="#tab_2">Table 1</ref>. We achieve state-of-the-art results on the following datasets in a zero-shot setting even outperforming fully supervised methods: (i) Open information extraction (OIE): OIE2016, WEB, NYT, PENN; and (ii) Factual probe: T-REx. The improvements are significant for OIE. In particular, the zero-shot DEEPEX outperforms RnnOIE by on average 37.5 in F1 and 44.6 in AUC, which is a supervised OIE system introduced in <ref type="bibr" target="#b37">(Stanovsky et al., 2018)</ref>. Given no specific OIE training data is used by DEEPEX, the results are encouraging, suggesting that the zero-shot transfer of the latent knowledge that a pre-trained LM has about the tasks is successful. State-of-theart OIE performance is obtained without referring to task-specific training data, and such zero-shot ability is generalizable across multiple datasets. The PR curves for all OIE test sets are depicted in <ref type="figure" target="#fig_0">Figure 3</ref>. Similar to the findings in <ref type="table" target="#tab_2">Table 1</ref>, overall, DEEPEX outperforms the comparison systems across all datasets. For each dataset, it provides a superior precision-recall curve. DEEPEX slightly outperforms the comparison methods on T-REx (factual probe). The main reason is that the taskspecific LAMA <ref type="bibr" target="#b25">(Petroni et al., 2020)</ref> can use the wrong memory of LMs to answer without needing the mention of the tail entity. An example expressing the triple (Nicholas Liverpool; place_of_death; Miami) is shown in <ref type="table" target="#tab_12">Table 6</ref> in Appendix. Thanks to the explainability and transparency of our framework, we can avoid such errors. The results demonstrate that the zero-shot DEEPEX generalizes well to different information extraction tasks.</p><p>For other datasets, we obtain comparable performance with the best comparison methods. We highlight that our approach uses a unified framework that tackles all the tasks in a zero-shot way. Our framework is task-agnostic without task-specific training or module modification, which is in contrast with task-specific models trained on specific corpora as shown in    for few-shot relation classification, our top-10 zeroshot performance sometimes is the best. While TACRED is not specifically a few-shot dataset, there are many label types that rarely appear in the training set <ref type="bibr">(Paolini et al., 2021)</ref>. This shows the importance of zero-shot information extraction in low-resource regimes. The ranking model is based on BERT BASE . It is interesting to check whether larger pre-trained LMs (e.g., BERT LARGE ) are more capable of ranking. We plan to investigate this in the future. On the other factual probing dataset, Google-RE, we perform slightly worse compared to LAMAs. This is mainly due to the missing mentions of relations in the sentences as shown in <ref type="table" target="#tab_4">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Error Analysis</head><p>To better understand the limitations of DEEPEX, we perform a detailed analysis of errors in its recall as DEEPEX lacks more in recall compared to precision across all the datasets. We use open information extraction as an example. We only show F1 and AUC in <ref type="table" target="#tab_2">Table 1</ref>, and <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the precision-recall curves showing recall errors are the main limitation. We therefore randomly sample 50 recall errors made by DEEPEX on the WEB corpus and summarized the types of common errors as below. We find 46% of the errors are due to the spaCy noun chunker identifying the wrong arguments. 12% of the recall errors are cases where the predicate is a noun or nominalized. 10% of the examined errors are involved in long sentences. Details are described in <ref type="table" target="#tab_11">Table 5</ref> in Appendix.</p><p>While most of the error types are shared across the datasets, we find a type of error due to the explainability and transparency of DEEPEX, which we cannot avoid. The error is mainly due to the missing mention of relations in the sentences. This type of error mainly appears in factual probing and relation classification datasets. The reason is that the tasks do not require the existence of the actual relation span in the input. The tasks often provide the relation as an input or the relation is expressed in a vague way that can not be linked to a predicate. We take factual probe as an example in <ref type="table" target="#tab_4">Table 2</ref>. A sentence is given to express the "place_of_death" relation can only contain mentions of "residence" relation such as "occupied by". While the gold data might consider this as a correct prediction, DEEPEX uses triples extracted from the sentences. We sacrifice performance for better explainability  and transparency. We believe it is ideal to allow a trade-off between performance and explainability. We leave this as future work. Also, "birth date" can be expressed as "(c.". Again in such cases, we sacrifice performance for explainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We perform ablation experiments to understand the relative importance of different facets of DEEPEX. We demonstrate the importance of the generating stage in particular beam search and the size of the pre-trained LM, and the ranking stage in particular the ranking model on open information extraction. We evaluate the below settings on the OIE2016 dev set. We first examine the effect brought by the beam search. As shown in <ref type="table" target="#tab_6">Table 3</ref>, we find removing beam search of the generating stage greatly hurts the performance. DEEPEX outperforms the best supervised OIE system by 6.3 in F1 and 7.5 in AUC. The result confirms our intuition that pretrained LMs enable the zero-shot transfer of the latent knowledge that they have about the task. The original RnnOIE performs similarly; this is due to the training set of RnnOIE which provides good coverage of the triples on the dev set. We secondly study the importance of the triple-oriented beam search. We find limiting the search significantly hurts the performance. It is often that the triples are expressed in inverted sentences, such as (Fisher; Born in; Glasgow) from "Born in Glasgow, Fisher is a graduate of the London Opera Centre". In fact, a considerable amount of gold triples containing valid relation sequences appear outside the argument pair. For example, 16.9% of the relation sequences are not between the argument pairs on the OIE2016 test set. More results are shown in Appendix B. We then test the impact of the size of the pre-trained LM. We find that BERT BASE performs worse than BERT LARGE . This indicates that larger pre-trained LMs (e.g., BERT LARGE ) provide more general knowledge about the task that improves the results. Next, we study the impact of the ranking model. We find that removing the ranking model significantly hurts the performance. The results suggest that the ranking model can distinguish the relational triples from the rest among the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Relation classification aims to identify the correct relation type from a pre-defined set of relations between two given entities. Language models (LM) <ref type="bibr" target="#b12">(Han et al., 2021)</ref> pre-trained with self-supervised <ref type="bibr">(Liu et al., 2021a)</ref> objectives, e.g., BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, GPT <ref type="bibr" target="#b28">(Radford et al., 2018</ref><ref type="bibr" target="#b29">(Radford et al., , 2019</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>, RoBERTa , transfer well to relation classification datasets in fine-tuning <ref type="bibr" target="#b15">(Joshi et al., 2020;</ref><ref type="bibr" target="#b11">Gao et al., 2019)</ref> or few-shot regime <ref type="bibr" target="#b34">(Soares et al., 2019)</ref> with architecture modifications. Sequenceto-sequence models, such as T5 <ref type="bibr" target="#b30">(Raffel et al., 2020)</ref>, BART  and GLM <ref type="bibr" target="#b8">(Du et al., 2021)</ref>, are adapted to the task based on data augmentation and fine-tuning <ref type="bibr">(Paolini et al., 2021)</ref>. Besides relation classification, <ref type="bibr">Paolini et al. (2021)</ref> generalize T5 to some more structured prediction tasks as well, e.g., semantic role labeling and event extraction. However, DEEPEX enables zero-shot relation classification that does not require any taskspecific training.</p><p>Many open information extraction (OIE) systems, e.g., Stanford OpenIE <ref type="bibr" target="#b0">(Angeli et al., 2015)</ref>, OLLIE <ref type="bibr" target="#b32">(Schmitz et al., 2012)</ref>, Reverb <ref type="bibr" target="#b10">(Fader et al., 2011)</ref>, and their descendant Open IE4 leverage carefully-designed linguistic patterns (e.g., based on dependencies and POS tags) to extract triples from textual corpora without using additional training sets. Recently, supervised OIE systems <ref type="bibr" target="#b37">(Stanovsky et al., 2018;</ref><ref type="bibr" target="#b31">Ro et al., 2020;</ref><ref type="bibr" target="#b17">Kolluru et al., 2020)</ref> formulate the OIE as a sequence generation problem using neural networks trained on additional training sets. Similar to our work, <ref type="bibr" target="#b38">Wang et al. (2020)</ref> use the parameters of LMs to extract triples, with the main difference that DEEPEX not only improves the recall of the beam search, but also uses a pre-trained ranking model to enhance the zero-shot capability.</p><p>LMs are used in factual probing tasks, by using the outputs alone <ref type="bibr" target="#b26">(Petroni et al., 2019)</ref> to answer the relation-specific queries in cloze statements. <ref type="bibr" target="#b25">Petroni et al. (2020)</ref> additionally feed sentences expressing the facts to the LMs and shows improved results. Other than template-based queries, learning trigger-based <ref type="bibr" target="#b33">(Shin et al., 2020)</ref> and continuous prompts <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b19">Li and Liang, 2021)</ref> are helpful in recalling the facts. The main difference is that DEEPEX explores the internal parameters of the LMs rather than the outputs, and the results are more interpretable.</p><p>Overall, in contrast to the existing approaches, DEEPEX unifies the open information extraction, relation classification, and factual probe under the same framework in zero-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated that our unified approach can handle multiple information extraction tasks within a simple framework and shows improvements in zero-shot settings. Unlike previous approaches designing complicated task-specific pipelines, DEEPEX enables conducting all considered information extraction tasks with only input and output design. Therefore, DEEPEX is flexible and can be adapted to a variety of tasks. Different from previous approaches that target pre-defined categories (e.g., fixed relation types for relation classification), DEEPEX generalizes better to unseen classes as the generating stage leverages the transfer of latent knowledge that a pre-trained language model has about the tasks. Besides, the ranking stage pre-trains on a large-scale task-agnostic dataset. DEEPEX exhibits strong zero-shot capabilities in low-resource tasks without the need of any task-specific training set. DEEPEX also exploits the in-depth information of the language models, i.e., parameters, rather than the outputs alone, which en-hances the explainability through enhanced model transparency. Based on our findings, we believe that the unified approach advances the research in understanding natural language semantics (e.g., structure prediction tasks) using deep learning models. We hope our results will foster further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>In all experiments, for the generating stage in Sec. 2.2, we use a pre-trained BERT LARGE model <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> for the beam search. In particular, we use the mean operation over the multi-head attention weights from the last layer of BERT LARGE according to the parameter study in <ref type="bibr" target="#b38">(Wang et al., 2020)</ref>. For the ranking model in Sec. 2.2, we use the pre-trained BERT BASE model. We use the implementations of the pretrained language models (LM) in the Transformers package <ref type="bibr" target="#b39">(Wolf et al., 2020)</ref>.</p><p>To keep our framework simple, we use the same hyperparameters across the majority of our experiments. For generating, we use: 8 GeForce RTX 2080 Ti GPUs with a batch size of 16 per GPU; maximum sequence length as 256 tokens. For datasets that provide examples beyond sentence level, we adopt spaCy sentencizer 3 to segment the texts into sentences, and each sample in the batch is a sentence. For ranking, we experiment with: 1 NVIDIA Tesla V100 GPU with a batch size of 8 per GPU; the AdamW optimizer (Kingma and Ba, 2015); linear learning rate decay starting from 10 ?6 ; maximum sequence length as 512 tokens. The top-1 triple from ranking model is returned except for OIE2016 (open information extraction) since it provides a dev set. We additionally report results of top-10 triples for relation classification (FewRel and TACRED).</p><p>In the rest of this section, we describe datasets, comparison methods, additional implementation details for each information extraction task, as well as more experimental insights. Results of all experiments are in <ref type="table" target="#tab_2">Table 1</ref>. We use the default evaluation metrics for each task as below. We show more input and output formats on all datasets in <ref type="table" target="#tab_13">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Open Information Extraction</head><p>Datasets We evaluate the performance of the open information extraction (OIE) systems on OIE benchmark datasets consisting of OIE2016 (Stanovsky and Dagan, 2016), a dataset from Newswire and Wikipedia automatically converted from QA-SRL <ref type="bibr" target="#b14">(He et al., 2015)</ref>; three news datasets NYT, WEB <ref type="bibr" target="#b23">(Mesquita et al., 2013)</ref>, PENN <ref type="bibr" target="#b40">(Xu et al., 2013)</ref>. The statistics of the benchmark is shown in <ref type="table" target="#tab_9">Table 4</ref>.  Evaluation Methodology We follow typical OIE metrics to evaluate the systems. First, we report precision, recall, and F1 score using a confidence threshold optimized on the development set. Second, we compute a precision-recall curve by evaluating the performance of the systems at different confidence thresholds. Third, we also measure the area under the PR curve (AUC) to evaluate the overall performance of a system. To compute the above metrics, we need to match the system extractions with the gold extractions. Regarding the matching functions, we adopt the function of OIE2016 to evaluate OIE2016, NYT, WEB, and PENN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method DEEPEX to the following prominent OIE systems recently evaluated in <ref type="bibr" target="#b37">(Stanovsky et al., 2018)</ref>: ClausIE (Del Corro and Gemulla, 2013), Open IE4 4 , PropS , Rn-nOIE <ref type="bibr" target="#b37">(Stanovsky et al., 2018)</ref>. We also compare to MAMA with BERT LARGE recently introduced in <ref type="bibr" target="#b38">(Wang et al., 2020)</ref> that also leverages pre-trained LMs to extract open triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For input text encoding, we use spaCy noun chunks 5 to identify the NPs. We use: beam size equals to 6; the top-1 triple from the ranking model for evaluation except on OIE2016. Instead, we use top-3 triples on OIE2016 based on the parameter study on its dev set in <ref type="figure" target="#fig_1">Figure 4</ref>. Experimenting on more OIE benchmark datasets such as CaRB <ref type="bibr" target="#b1">(Bhardwaj et al., 2019)</ref> is an interesting future direction to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Relation Classification</head><p>Datasets and Metrics We evaluate on FewRel <ref type="bibr" target="#b13">(Han et al., 2018)</ref> and TACRED <ref type="bibr" target="#b41">(Zhang et al., 2017)</ref>.</p><p>? FewRel contains 100 relations with 7 instances for each relation. The standard evaluation for this benchmark uses few-shot N-way K-shot settings. The entire dataset is split into train (64 relations), validation (16 relations) and test set (20 relations). We report the same results on the dev set for all the settings because of our zero-shot setting.</p><p>? TACRED is a large-scale relation classification benchmark that consists of 106,344 examples and 41 relation types including 68,164 for training, 22,671 for validation, and 15,509 for testing. We do not use train and validation sets, and report the result on the test set.</p><p>We use F1 to evaluate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method with the following supervised methods. (i) BERT-PAIR <ref type="bibr" target="#b11">(Gao et al., 2019)</ref> is a sequence classification model based on BERT, optimizing the score of two instances expressing the same relation. (ii) BERT EM + Matching the Blanks (MTB) <ref type="bibr" target="#b34">(Soares et al., 2019)</ref>, which uses entity markers (BERT EM ) and additional pre-training of relations on a largescale corpus (i.e., MTB). (iii) TANL <ref type="bibr">(Paolini et al., 2021)</ref> is a sequence to sequence model based on T5 <ref type="bibr" target="#b30">(Raffel et al., 2020)</ref> aiming to generate structured objects from an encoded natural language format.</p><p>Implementation Details For input text encoding, we attach the given gold head and tail entities to the input. As described in Sec. 3.2, linked relation phrases are also attached to the input. For TACRED, we use the relation map provided in <ref type="bibr" target="#b0">(Angeli et al., 2015)</ref> to link relation phrases to TACRED relations, and manually build a map between TACRED relations and Wikidata predicates.</p><p>For FewRel, we directly link relation phrases to gold Wikidata predicates as FewRel uses Wikidata predicates as the target category. At test time, DEEPEX makes prediction as below: given a returned triple, we first map the relation phrase of the triple back to a Wikidata predicate, then use the Wikidata predicate to find a relation in the predefined category. We report the results using two setups: top-1 and top-10 triples from the ranking model. In practice, some of the Wikidata predicates do not have aliases. We therefore follow <ref type="bibr" target="#b0">(Angeli et al., 2015;</ref><ref type="bibr" target="#b38">Wang et al., 2020)</ref> to manually add aliases for such predicates based on the alignment between Wikipedia and Wikidata. We set the beam size as 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Factual Probe</head><p>Datasets and Metrics We consider the Google-RE consisting of 3 relations and 5,527 facts, and T-REx with 41 relations and 34,039 facts of the LAMA benchmark <ref type="bibr" target="#b26">(Petroni et al., 2019)</ref>. We evaluate the results using mean precision at one (P@1), where higher values are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare to pretrained LM based methods that leverage the output probabilities of the LM to make predictions given the sentence known to express the fact. Two methods are considered: (i) LAMA <ref type="bibr" target="#b26">(Petroni et al., 2019)</ref> leverages the input sentence without the tail entity to query the LMs, and (ii) LAMA-Oracle <ref type="bibr" target="#b25">(Petroni et al., 2020)</ref>       returned triple for P@1 evaluation. The beam size equals 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Analysis of Results</head><p>Error Analysis of OIE We show detailed error analysis of DEEPEX on the OIE task in <ref type="table" target="#tab_11">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Position Distribution</head><p>We show the statistics of relation positions in the gold triples in all OIE datasets. Overall, there is a considerable amount of triples containing relation not between the entity pair across all OIE datasets as shown in <ref type="table" target="#tab_14">Table 8</ref>, demonstrating the importance of tripleoriented beam search. In particular, 14.8% of the triples contains relations outside the entity pairs on OIE datasets (OIE2016, WEB, NYT, PENN). <ref type="table" target="#tab_15">Table 9</ref> compares the statistics of the outputs of different OIE systems and gold data on OIE2016. We find that DEEPEX produces 25 more triples than the gold data, and the arguments tend to be shorter. <ref type="table" target="#tab_12">Table 6</ref> shows major error cases in LAMA-Oracle when compared with DEEPEX, where LAMA-Oracle often generates out-of-context answers due to the wrong memory of LMs. This shows that explainable extraction like DEEPEX is necessary for information extraction using LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-of-Context Cases for LAMA-Oracle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis of Ranking Model</head><p>Implementation Details We remove triples with empty predicates from the original T-REx (Elsahar  <ref type="bibr" target="#b26">(Petroni et al., 2019)</ref>. This results in approximately 4 million positive sentence-triple pairs. We set the number of fine-tuning epochs to 1. Each batch takes approximately 7 minutes, resulting in a total 12 hours for fine-tuning the ranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Results</head><p>We evaluate the performance of the ranking model. To do so, we construct negative sentence-triple samples for all the tested datasets as follows: for each sentence in a dataset, we randomly sample a triple from other sentences to construct a negative sample. We then directly evaluate the ranking performance of the model on all the datasets, and report the top-1 accuracy in <ref type="figure" target="#fig_2">Figure 5</ref>. We find that the ranking model works extremely well, obtaining nearly perfect top-1 accuracy on all the datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Precision-recall curves of the different open information extraction (OIE) systems on OIE datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Effect of the ranking model on the OIE2016 dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Top-1 accuracy of the ranking on all datasets. et al., 2019), and further remove sentence-triple pairs if the triples are in the LAMA version of T-REx</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>s1 T t1 ... ... ...beam size = 2) Task Predictions: ( Fisher; Born in; Glasgow ) ( Fisher; is a graduate of; London Opera Centre ) triple ii triple iii triple iv sentence Attention Beam Search</head><label></label><figDesc></figDesc><table><row><cell cols="2">(1) Contrastive Pre-Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(2) Zero-Shot Translation</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Input:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[CLS]</cell><cell></cell><cell></cell><cell>s1</cell><cell>s2</cell><cell>s3</cell><cell>...</cell><cell>sN</cell><cell></cell><cell cols="7">Born in GlasgowNP, FisherNP is a graduate of the London Opera CentreNP.</cell><cell>Ranking</cell><cell>s tiv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tii</cell></row><row><cell>sentence1</cell><cell>s1</cell><cell>t1</cell><cell></cell><cell cols="2">s1 T t2 s1 T t3</cell><cell></cell><cell>s1 T tN</cell><cell></cell><cell cols="2">BERT</cell><cell></cell><cell></cell><cell>(</cell><cell></cell><cell>tiii ti</cell></row><row><cell>BERT</cell><cell></cell><cell>t2</cell><cell cols="3">s2 T t1 s2 T t2 s2 T t3</cell><cell></cell><cell>s2 T tN</cell><cell></cell><cell>2 1 head</cell><cell>1 2 3</cell><cell>4</cell><cell>head</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>tail</cell><cell>Ranking Model</cell><cell>Output:</cell></row><row><cell>[SEP]</cell><cell></cell><cell>t3</cell><cell cols="3">s3 T t1 s3 T t2 s3 T t3</cell><cell></cell><cell>s3 T tN</cell><cell>ltriple</cell><cell>3 tail 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( Fisher; Born in; Glasgow )</cell></row><row><cell>triple1</cell><cell>t1</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generating</cell><cell>( Fisher; is a graduate of; London Opera Centre )</cell></row><row><cell>[SEP]</cell><cell></cell><cell>tN</cell><cell cols="3">sN T t1 sN T t2 sN T t3</cell><cell>...</cell><cell>sN T tN</cell><cell>triple i</cell><cell></cell><cell cols="6">( Fisher; Born in; Glasgow) ( Fisher; Born in; London Opera Centre)</cell><cell>(3) Decoding</cell></row><row><cell>update</cell><cell cols="2">loss = ( lsentence + ltriple ) / 2</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell cols="6">( Fisher; is a graduate of; Glasgow)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lsentence</cell><cell></cell><cell></cell><cell></cell><cell cols="7">( Fisher; is a graduate of; London Opera Centre)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>For relation classification, all the comparison methods are fully supervised and trained on the corresponding largescale corpora. Our top-1 zero-shot result serves as a lower bound, while top-10 results indicate an ideal situation when an improved ranking model is available. Interestingly, on FewRel, a benchmark Open Information Extraction (F1 and AUC) Del Corro and Gemulla, 2013) 58.8 37.6 44.9 40.1 29.6 22.9 34.6 28.4 Open IE4 4 59.6 41.7 55.7 40.5 38.3 24.0 42.6 28.1 PropS (Stanovsky et al., 2016) 55.6 33.8 58.9 48.0 37.2 22.1 39.1 27.7 RnnOIE Table</figDesc><table><row><cell></cell><cell cols="2">OIE2016</cell><cell cols="2">WEB</cell><cell cols="2">NYT</cell><cell>PENN</cell></row><row><cell>ClausIE ((Stanovsky et al., 2018)</cell><cell cols="6">67.0 44.5 58.1 43.5 28.3 10.4 34.5 18.3</cell></row><row><cell>MAMA (Wang et al., 2020)</cell><cell cols="5">36.6 12.8 54.3 30.3 32.9</cell><cell>9.4</cell><cell>33.0</cell><cell>9.0</cell></row><row><cell>DEEPEX (Zero-Shot) (ours)</cell><cell cols="6">72.6 58.6 91.2 82.4 85.5 72.5 88.5 81.5</cell></row><row><cell cols="4">Relation Classification (F1)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FewRel 1.0 (dev)</cell></row><row><cell></cell><cell>TACRED</cell><cell>5-way</cell><cell></cell><cell>5-way</cell><cell></cell><cell>10-way</cell><cell>10-way</cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell>5-shot</cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>BERT-EM  *  (Soares et al., 2019)</cell><cell>70.1</cell><cell>88.9</cell><cell></cell><cell>-</cell><cell></cell><cell>82.8</cell><cell>-</cell></row><row><cell>BERT EM +MTB  *  (Soares et al., 2019)</cell><cell>71.5</cell><cell>90.1</cell><cell></cell><cell>-</cell><cell></cell><cell>83.4</cell><cell>-</cell></row><row><cell>DG-SpanBERT  *  (Chen et al., 2020a)</cell><cell>71.5</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>BERT-PAIR  *  (Gao et al., 2019)</cell><cell>-</cell><cell>85.7</cell><cell></cell><cell>89.5</cell><cell></cell><cell>76.8</cell><cell>81.8</cell></row><row><cell>TANL  *  (Paolini et al., 2021)</cell><cell>71.9</cell><cell>94.0</cell><cell></cell><cell>96.4</cell><cell></cell><cell>82.6</cell><cell>88.2</cell></row><row><cell>DEEPEX (Zero-Shot Top-1) (ours)</cell><cell>49.2</cell><cell>48.8</cell><cell></cell><cell>48.8</cell><cell></cell><cell>48.8</cell><cell>48.8</cell></row><row><cell>DEEPEX (Zero-Shot Top-10) (ours)</cell><cell>76.4</cell><cell>92.9</cell><cell></cell><cell>92.9</cell><cell></cell><cell>92.9</cell><cell>92.9</cell></row><row><cell></cell><cell cols="2">Factual Probe (P@1)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Google-RE</cell><cell></cell><cell>T-REx</cell></row><row><cell></cell><cell>birth-</cell><cell cols="2">birth-date</cell><cell>death-</cell><cell></cell><cell>Total</cell><cell>Total</cell></row><row><cell></cell><cell>place</cell><cell></cell><cell></cell><cell>place</cell><cell></cell></row><row><cell>LAMA-Original (Petroni et al., 2019)</cell><cell>16.1</cell><cell>1.4</cell><cell></cell><cell>14.0</cell><cell></cell><cell>10.5</cell><cell>32.3</cell></row><row><cell>LAMA-Oracle (Petroni et al., 2020)</cell><cell>70.6</cell><cell>98.1</cell><cell></cell><cell>65.1</cell><cell></cell><cell>78.0</cell><cell>62.6</cell></row><row><cell>DEEPEX (Zero-Shot) (ours)</cell><cell>67.8</cell><cell>91.0</cell><cell></cell><cell>64.1</cell><cell></cell><cell>74.3</cell><cell>66.0</cell></row></table><note>*1: Results on all tasks. All evaluation scores are higher the better. An asterisk (*) indicates a supervised method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Error analysis of DEEPEX on factual probing datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation over different facets of DEEPEX on OIE2016 dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Statistics of OIE benchmark datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>enriches the query with (at most) five gold sentences as additional context. Implementation Details For input encoding, we use spaCy noun chunks 5 to identify the NPs. We additionally use Stanford NER 6 to label the missing entities from noun chunks such as dates. A NP is considered as a gold head entity if it overlaps with the mention of the gold head entity. Therefore there might be multiple NPs representing the gold head entity in the input. Similar to relation classification, linked relation phrases are attached to the input. We use the gold mapping to convert Freebase predicates to Wikidata ones for Google-RE, and use the gold Wikidata predicates for T-REx as T-REx builds based on Wikidata. At test time, a prediction is made: given a returned triple, we conduct an exact match between the tail entity of the triple and the gold tail entity. We use the top-1 Calif. -based Adobe announced in April the plans to acquire Macromedia , which makes the Flash animation software used to display graphics on Web sites .</figDesc><table><row><cell>Error type</cell><cell cols="2">Percentage Example (sentence and gold triple)</cell></row><row><cell>Wrong NP</cell><cell>46%</cell><cell>San Jose , (Adobe; acquire; Macromedia)</cell></row><row><cell cols="2">Unformatted Sentence 14%</cell><cell>Adobe acquired Macromedia! -sephiroth.it -flash &amp;amp;amp; php (Adobe; acquired; Macromedia!)</cell></row><row><cell>Nominalization</cell><cell>12%</cell><cell>Google confirms YouTube acquisition -BBC News (Google; acquisition; YouTube)</cell></row><row><cell>Long Sentence</cell><cell>10%</cell><cell>Amid all the hubbub over Google 's swallowing of YouTube , we ve heard both considered commentary and over -the -top pontification about whether it's a good deal or a bad deal , for the newly -engorged</cell></row><row><cell></cell><cell></cell><cell>company and for all those users in TV land .</cell></row><row><cell></cell><cell></cell><cell>(Google; swallowing; YouTube)</cell></row><row><cell>Noun</cell><cell>4%</cell><cell>Dean Kamen ( left ) , inventor of the Segway Human Transporter Human Transporter wearing the Plantronics Voyager 510 Bluetooth Headset</cell></row><row><cell></cell><cell></cell><cell>(Dean Kamen; inventor; the Segway Human Transporter)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Analysis of frequently-occurring recall errors of DEEPEX on a random sample of 50 sentences on the OIE task. For each type we list the percentage of sentences in which it occurs, and an example taken from the WEB corpus.Nicholas Liverpool died on 1 June 2015 in Miami, where he was receiving medical treatment. Jean-Michel Pilc (born 1960 in Paris, France) is a self-taught jazz pianist currently residing in New York. Nick Lucas's version, released on Brunswick, was a No.</figDesc><table><row><cell cols="2">Dataset Example (sentence and gold triple)</cell><cell cols="3">LAMA-Oracle DEEPEX (ours) Error Type</cell></row><row><cell>T-REx</cell><cell>Naomi Shihab Nye (born March 12, 1952) is a poet, songwriter, and novelist. (Naomi Shihab Nye; occupation; poet)</cell><cell>teacher</cell><cell>poet</cell><cell>Wrong memory</cell></row><row><cell></cell><cell>(Nicholas Liverpool; place_of_death; Miami)</cell><cell>London</cell><cell>Miami</cell><cell>Wrong memory</cell></row><row><cell></cell><cell>(Jean-Michel Pilc; genre; jazz)</cell><cell>classical</cell><cell>jazz</cell><cell>Wrong memory</cell></row><row><cell></cell><cell>(Nick Lucas; record_label; Brunswick)</cell><cell>EMI</cell><cell>Brunswick</cell><cell>Wrong memory</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Out-of-context predictions of LAMA-Oracle<ref type="bibr" target="#b25">(Petroni et al., 2020)</ref> for the factual probing task. Nobel Prize NP for discovering the structure NP of DNA NP . NP said Sulka NP operates a total NP of seven stores in the U.S. NP and overseas .(Sulka; operates a total of; the U.S), (A spokeswoman; said; the U.S), (a total; of seven stores; the U.S) TACRED Denise Maloney Pictou GOLD , one of Aquash GOLD 's daughters child , says she hopes Graham 's trial will help bring justice to her family .(Denise Maloney Pictou; child; Aquash) FewRel 1.0 (dev) Theodore II Palaiologos GOLD was a son of the Eastern Roman Emperor Manuel II Palaiologos and his wife spouse Helena Draga GOLD . Democratic member NP of the Rhode Island House of Representatives GOLD/NP . March 1915 was an Italian position_held Roman Catholic Cardinal NP , archbishop NP , and papal diplomat NP .</figDesc><table><row><cell>Dataset</cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell>OIE2016</cell><cell>He NP</cell><cell>hasn't</cell><cell>been</cell><cell>able</cell><cell>to</cell><cell>replace</cell><cell>(He; hasn't been able to replace; the M'Bow cabal)</cell></row><row><cell></cell><cell cols="3">the M'Bow cabal NP .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WEB</cell><cell cols="7">Crick NP received a (Crick; received a Nobel Prize for; the structure),</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(DNA; for discovering the structure; a Nobel Prize)</cell></row><row><cell>NYT</cell><cell cols="6">PRODUCER -Squier Knapp Dunn Communications NP</cell><cell>(David Garth; in consultation with; Squier Knapp</cell></row><row><cell></cell><cell cols="4">in consultation with David Garth NP</cell><cell></cell><cell></cell><cell>Dunn Communications)</cell></row><row><cell>PENN</cell><cell cols="7">A spokeswoman (Theodore II Palaiologos; spouse; Helena Draga)</cell></row><row><cell>Google-RE</cell><cell cols="7">Peter F Martin GOLD/NP 1941 NP ) is an who is a (Peter F Martin; date_of_birth; 1941) (born birth_date American politician NP</cell></row><row><cell>T-REx</cell><cell cols="6">Antonio Agliardi GOLD/NP in 4 September 1832</cell><cell></cell></row><row><cell></cell><cell>-19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(Antonio Agliardi; position_held; diplomat)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Input/output examples for all datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Triples Left Relation Right Relation Middle Relation Total</cell></row><row><cell cols="2">OIE2016 128</cell><cell>165</cell><cell>1,437</cell><cell>1,730</cell></row><row><cell>WEB</cell><cell>17</cell><cell>29</cell><cell>415</cell><cell>461</cell></row><row><cell>NYT</cell><cell>2</cell><cell>29</cell><cell>119</cell><cell>150</cell></row><row><cell>PENN</cell><cell>2</cell><cell>4</cell><cell>46</cell><cell>52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Statistics of relation positions of the gold triples in all OIE datasets.</figDesc><table><row><cell>System</cell><cell cols="2">#Triples Words/Arg</cell></row><row><cell>Gold</cell><cell>1,730</cell><cell>5.38</cell></row><row><cell>ClausIE</cell><cell>2,768</cell><cell>5.78</cell></row><row><cell>Open IE4</cell><cell>1,793</cell><cell>4.55</cell></row><row><cell>PropS</cell><cell>1,551</cell><cell>5.80</cell></row><row><cell>RnnOIE</cell><cell>1,993</cell><cell>4.68</cell></row><row><cell>MAMA</cell><cell>1751</cell><cell>2.56</cell></row><row><cell>DEEPEX (ours)</cell><cell>1755</cell><cell>2.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Output statistics of the different OIE systems on OIE2016.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code and datasets are available at https:// github.com/cgraywang/deepex.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://spacy.io/api/sentencizer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/dair-iitd/OpenIE-standalone 5 https://spacy.io/usage/linguistic-features/#noun-chunks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://stanfordnlp.github.io/CoreNLP/ner.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their suggestions and comments. This material is in part based upon work supported by Berkeley DeepDrive and Berkeley Artificial Intelligence Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin Jose Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Carb: A crowdsourced benchmark for open ie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangnie</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename><surname>Mausam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6263" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mohamed Elhoseiny, and Xiangliang Zhang. 2020a. Efficient long-distance relation extraction with dg-spanbert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03636</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clausie: clause-based open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">T-rex: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fewrel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6250" to="6255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pre-trained models: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AI Open</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Openie6: Iterative grid labeling and coordination analysis for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Adlakha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jing Zhang, and Jie Tang. 2021a. Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Zhilin Yang, and Jie Tang. 2021b. Gpt understands, too</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effectiveness and efficiency of open relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Schmidek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="447" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi?2OIE: Multilingual open information extraction based on multi-head attention with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbin</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilsung</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1107" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eliciting knowledge from language models using automatically generated prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Creating a large benchmark for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2300" to="2305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Getting more out of syntax with props</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01648</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supervised open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="885" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Language models are open knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11967</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Open information extraction with tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="868" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
