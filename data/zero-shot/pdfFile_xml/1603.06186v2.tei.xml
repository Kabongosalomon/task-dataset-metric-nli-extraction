<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Multiscale Laplacian Graph Kernel</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-30">30 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
							<email>hopan@uchicago.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Multiscale Laplacian Graph Kernel</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-30">30 May 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character. In contrast, by building a hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG kernels) that we define in this paper can account for structure at a range of different scales. At the heart of the MLG construction is another new graph kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels to subgraphs recursively. To make the MLG kernel computationally feasible, we also introduce a randomized projection procedure, similar to the Nystr?m method, but for RKHS operators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There is a wide range of problems in applied machine learning from web data mining <ref type="bibr" target="#b9">(Inokuchi et al., 2003)</ref> to protein function prediction  where the input space is a space of graphs. A particularly important application domain is chemoinformatics, where the graphs capture the structure of molecules. In the pharamceutical industry, for example, machine learning algorithms are regularly used to screen candidate drug compounds for safety and efficacy against specific diseases <ref type="bibr" target="#b13">(Kubinyi, 2003)</ref>.</p><p>Because kernel methods neatly separate the issue of data representation from the statistical learning component, it is natural to formulate graph learning problems in the kernel paradigm. Starting with <ref type="bibr" target="#b7">(G?rtner, 2002)</ref>, a number of different graph kernels have been appeared in the literature <ref type="bibr">Copyright 2016</ref> by the author(s).</p><p>(for an overview, see <ref type="bibr" target="#b19">(Vishwanathan et al., 2010)</ref>). In general, a graph kernel k(G 1 , G 2 ) must satisfy the following three requirements:</p><p>(a) The kernel should capture the right notion of similarity between G 1 and G 2 . For example, if G 1 and G 2 are social networks, then k might capture to what extent clustering structure, degree distribution, etc. match up between them. If, on the other hand, G 1 and G 2 are molecules, then we are probably more interested in what functional groups are present in both, and how they are arranged relative to each other. (b) The kernel is usually computed from the adjacency matrices A 1 and A 2 of the two graphs, but (unless the vertices are explicitly labeled), it must be invariant to their ordering. In other words, writing the kernel explicitly in terms of A 1 and A 2 , we must have k(A 1 , A 2 ) = k(A 1 , P A 2 P ? ) for any permutation matrix P . (c) The kernel should be efficiently computable. The time complexity of many graph kernels is O(n 3 ), where n is the number of vertices of the larger of the two graphs. However, when dealing with large graphs, we might only be able to afford O(n 2 ) or even O(n) complexity. On the other hand, in chemoinformatics applications, n might only be on the order of a 100, permitting the use of more expensive kernels.</p><p>Of these three requirements, the second one (permutation invariance) has proved to be the central constraint around which much of the graph kernels literature is organized.</p><p>In combinatorics, any function ?(A) that is invariant to reordering the vertices (i.e., ?(P AP ? ) = ?(A) for any permutation matrix P ) is called a graph invariant <ref type="bibr" target="#b15">(Mikkonen, 2007)</ref>. The permutation invariance requirement effectively stipulates that graph kernels must be built out of graph invariants. In general, efficiently computable graph invariants offered by the mathematics literature tend to fall in one of two categories:</p><p>(a) Local invariants, which can often be reduced to simply counting some local properties, such as the number of triangles, squares, etc. that appear in G as subgraphs. (b) Spectral invariants, which can be expressed as func-tions of the eigenvalues of the adjacency matrix or the graph Laplacian.</p><p>Correspondingly, while different graph kernels are motivated in very different ways from random walks <ref type="bibr" target="#b7">(G?rtner, 2002)</ref> through shortest paths <ref type="bibr" target="#b6">Feragen et al., 2013)</ref> to Fourier transforms on the symmetric group <ref type="bibr" target="#b11">(Kondor &amp; Borgwardt, 2008)</ref>, ultimately most graph kernels also reduce to computing a function of the two graphs that is either purely local or purely spectral. For example all kernels based on the "subgraph counting" idea (e.g., <ref type="bibr" target="#b17">(Shervashidze et al., 2009)</ref>) are local. On the other hand, most of the random walk based kernels are reducable to a spectral form involving the eigenvalues of either the two graphs individually, or their Kronecker product <ref type="bibr" target="#b19">(Vishwanathan et al., 2010)</ref> and therefore are really only sensitive to the large scale structure of graphs.</p><p>In practice, it would be desirable to have a kernel that is inbetween these two extremes, in the sense that it can take structure into account at multiple different scales. A kernel between molecules, for example, must be sensitive to the overall large-scale shape of the graphs (whether they are more like a chain, a ring, a chain that branches, etc.), but also to what smaller structures (e.g., functional groups) are present in the graphs, and how they are related to the global structure (e.g., whether a particular functional group is towards the middle or one of the ends of the chain).</p><p>For the most part, such a multiscale graph kernel has been missing from the literature. One notable exception is the Weisfeiler-Lehman kernel <ref type="bibr" target="#b18">(Shervashidze et al., 2011)</ref>, which uses a combination of message passing and hashing to build summaries of the local neighborhood vertices at different scales. However, in practice, the message passing step is usually only iterated a relatively small number of times, so the Weisfeiler-Lehman kernel is still mostly local. Moreover, the hashing step is somewhat ad-hoc and does not give rise to well behaved, local summaries: perturbing the edges by a small amount leads to completely different hash features.</p><p>In this paper we present a new graph kernel, the Multiscale Laplacian Graph Kernel (MLG kernel), which, we believe, is the first kernel in the literature that can truly compare structure in graphs simultaneously at multiple different scales. We begin by defining a simpler graph kernel, called the Feature Space Laplacian Graph Kernel (FLG kernel) that only operates at a single scale (Section 2). The FLG kernel combines two sources of information: a partial labeling of the nodes in terms of vertex features, and topological information about the graph supplied by its Laplacian. An important property of the the FLG kernel is that it can work with vertex labels provided implicitly, in terms of a "base kernel" on the vertices. Crucially, this makes it possible to apply the FLG kernel recursively.</p><p>The Multiscale Laplacian Graph Kernel (MLG kernel), which is the central object of the paper and is defined in Section 3, uses exactly this recursive property of the FLG kernel to build a hierarchy of subgraph kernels that are not only sensitive to the topological relationships between individual vertices, but also between subgraphs of increasing sizes. Each kernel is defined in terms of the preceding kernel in the hierarchy.</p><p>Efficient computability is a major concern in our paper, and recursively defined kernels, especially on combinatorial data structures, can be very expensive. Therefore, in Section 4 we describe a strategy based on a combination of linearizing each level of the kernel (relative to a given dataset) and a randomized low rank projection, that reduces every stage of the kernel computation to simple operations involving small matrices, leading to a very fast algorithm. Finally, section 5 presents experimental comparisons of our kernel with competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Laplacian Graph Kernels</head><p>Let G be a weighted undirected graph with vertex set V = {v 1 , . . . , v n } and edge set E. Recall that the graph Laplacian of G is an n ? n matrix L G , with</p><formula xml:id="formula_0">L G i,j = ? ? ? ? ? ?w i,j if {v i , v j } ? E j : {vi,vj }?E w i,j if i = j 0 otherwise,</formula><p>where w i,j is the weight of edge {v i , v j }. The graph Laplacian is positive semi-definite, and in terms of the adjacency matrix A and the weighted degree matrix D, it can be expressed as L = D ? A.</p><p>Spectral graph theory tells us that the low eigenvalue eigenvectors of L G (the "low frequency modes") are informative about the overall shape of G. One way of seeing this is to note that for any vector z ? R n</p><formula xml:id="formula_1">z ? L G z = {i,j}?E w i,j (z i ? z j ) 2 ,</formula><p>so the low eigenvalue eigenvectors are the smoothest functions on G, in the sense that they vary the least between adjacent vertices. An alternative interpretation emerges if we use G to construct a Gaussian graphical model (Markov Random Field or MRF) over n variables x 1 , . . . , x n with clique potentials ?(x i , x j ) = e ?wi,j (xi?xj) 2 /2 for each edge and ?(x i ) = e ??x 2 i /2 for each vertex. The joint distribution of x = (x 1 , . . . , x n ) ? is then</p><formula xml:id="formula_2">p(x) ? {vi,vj }?E e ?wi,j (xi?xj ) 2 /2 vi?V e ??x 2 i /2 = e ?x ? (L+?I) x/2 ,<label>(1)</label></formula><p>showing that the covariance matrix of x is (L G + ?I) ?1 . Note that the ? factors were only added to ensure that the distribution is normalizable, and ? is typically just a small constant "regularizer": L G actually has a zero eigenvalue eigenvector (namely the constant vector n ?1/2 (1, 1, . . . , 1) ? ), so without adding ?I we would not be able to invert it. In the following we will call L G + ?I the regularized Laplacian, and denote it simply by L.</p><p>Both the above views suggest that if we want define a kernel between graphs that is sensitive to their overall shape, comparing the low eigenvalue eigenvectors of their Laplacians is a good place to start. Following the MRF route, given two graphs G 1 and G 2 of n vertices, we can define the kernel between them to be a kernel between the corresponding distributions p 1 = N (0, L ?1 1 ) and p 2 = N (0, L ?1 2 ). Specifically, we use the Bhattacharyya kernel</p><formula xml:id="formula_3">k(p 1 , p 2 ) = p 1 (x) p 2 (x) dx,<label>(2)</label></formula><p>because for Gaussian distributions it can be computed in closed form , giving</p><formula xml:id="formula_4">k(p 1 , p 2 ) = 1 2 L 1 + 1 2 L 2 ?1 1/2 L ?1 1 1/4 L ?1 2 1/4 .</formula><p>If some of the eigenvalues of L ?1 1 or L ?1 2 are zero or very close to zero, along certain directions in space the two distributions in (2) become very flat, leading to vanishingly small kernel values (unless the "flat" directions of the two Gaussians are perfectly aligned). To remedy this problem, similarly to , we "soften" (or regularize) the kernel by adding some small constant ? times the identity to L ?1 1 and L ?1 2 . This leads to what we call the Laplacian Graph Kernel. Definition 1. Let G 1 and G 2 be two graphs of n vertices with (regularized) Laplacians L 1 and L 2 , respectively. We define the Laplacian graph kernel (LG kernel) with parameter ? between G 1 and G 2 as</p><formula xml:id="formula_5">k LG (G 1 , G 2 ) = 1 2 S ?1 1 + 1 2 S ?1 2 ?1 1/2 |S 1 | 1/4 |S 2 | 1/4 ,<label>(3)</label></formula><p>where S 1 = L ?1 1 + ?I and S 2 = L ?1 2 + ?I.</p><p>By virtue of (2), the LG kernel is guaranteed to be positive semi-definite, and because the value of the overlap integral (2) is largely determined by the extent to which the subspaces spanned by the largest eigenvalue eigenvectors of L ?1 1 and L ?1 2 are aligned, it effectively captures similarity between the overall shapes of G 1 and G 2 . However, the LG kernel does suffer from three major limitations:</p><p>1. It assumes that both graphs have exactly the same number of vertices.</p><p>2. It is only sensitive to the overall structure of the two graphs, and not to how the two graphs compare at more local scales. 3. It is not invariant to permuting the vertices.</p><p>Our goal for the rest of this paper is to overcome each of these limitations, while retaining the LG kernel's attractive spectral interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature space LG kernel</head><p>In the probabilistic view of the LG kernel, every graph generates random vectors x = (x 1 , . . . , x n ) ? according</p><p>to <ref type="formula" target="#formula_2">(1)</ref>, and the kernel between two graphs is determined by comparing the corresponding distributions. The invariance problem arises because the ordering of the variables x 1 , . . . , x n is arbitrary: even if G 1 and G 2 are topologically the same, k LG (G 1 , G 2 ) might be low if their vertices happen to be numbered differently.</p><p>One of the central ideas of this paper is to address this issue by transforming from the "vertex space variables" x 1 , . . . , x n to "feature space variables" y 1 , . . . , y m , where y i = j t i,j (x j ), and each t i,j only depends on j through local and reordering invariant properties of vertex v j . If we then compute an analogous kernel to the LG kernel, but now between the distributions of the y's rather than the x's, the resulting kernel will be permutation invariant.</p><p>In the simplest case, each</p><formula xml:id="formula_6">t i,j is linear, i.e., t i,j (x j ) = ? i (v j ) ? x j , where (? 1 , . . . , ? m ) is a collection of m lo- cal (and permutation invariant) vertex features. For exam- ple, ? i (v j ) may be the degree of vertex v j , or the value of h ? (v j , v j ),</formula><p>where h is the diffusion kernel on G with length scale parameter ? (c.f., <ref type="bibr" target="#b0">(Alexa et al., 2009)</ref>). In the chemoinformatics setting, the ? i 's might be some way of encoding what type of atom is located at vertex v j .</p><p>The linear transform of a multivariate normal random variable is multivariate normal. In particular, in our case, letting U = (? i (v j )) i,j , we have E(y) = 0 and Cov(y, y) = U Cov(x, x)U ? = U L ?1 U ? , leading to the following kernel, which is the workhorse of the present paper. Definition 2. Let G 1 and G 2 be two graphs with regularized Laplacians L 1 and L 2 , respectively, ? ? 0 a parameter, and (? 1 , . . . , ? m ) a collection of m local vertex features.</p><p>Define the corresponding feature mapping matrices</p><formula xml:id="formula_7">[U 1 ] i,j = ? i (v j ) [U 2 ] i,j = ? i (v ? j ) (where v j is the j'th vertex of G 1 and v ? j is the j'th vertex of G 2 ). The corresponding Feature space Laplacian graph kernel (FLG kernel) is k FLG (G 1 , G 2 ) = 1 2 S ?1 1 + 1 2 S ?1 2 ?1 1/2 |S 1 | 1/4 |S 2 | 1/4 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">S 1 = U 1 L ?1 1 U ? 1 + ?I and S 2 = U 2 L ?1 2 U ? 2 + ?I.</formula><p>Since the ? 1 , . . . , ? m vertex features, by definition, are local and invariant to vertex renumbering, the FLG kernel is permutation invariant. Moreover, because the distributions p 1 and p 2 now live in the space of features rather than the space defined by the vertices, there is no problem with applying the kernel to two graphs with different numbers of vertices.</p><p>Similarly to the LG kernel, the FLG kernel also captures information about the global shape of graphs. However, whereas, intuitively, the former encodes information such as "G is an elongated graph with vertex number i towards one and and vertex number j at the other", the FLG kernel can capture information more like "G is elongated with low degree vertices at one end and high degree vertices at the other". The major remaining shortcoming of the FLG kernel is that it cannot take into account structure at multiple different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The "kernelized" LG kernel</head><p>The key to boosting k FLG to a multiscale kernel is that it itself can be "kernelized", i.e., it can be computed from just the inner products between the feature vectors of the vertices (which we call the base kernel) without having to know the actual ? i (v j ) features values.</p><p>Definition 3. Given a collection ? = (? 1 , . . . , ? m ) ? of local vertex features, we define the corresponding base kernel ? between two vertices v and v ? as the dot product of their feature vectors:</p><formula xml:id="formula_9">?(v, v ? ) = ?(v) ? ?(v ? ).</formula><p>Note that in this definition v and v ? may be two vertices of the same graph, or of two different graphs. We first show that, similarly to the Representer Theorem for other kernel methods <ref type="bibr" target="#b16">(Sch?lkopf &amp; Smola, 2002)</ref>, to compute k FLG (G 1 , G 2 ) one only needs to consider the subspace of R m spanned by the feature vectors of their vertices.</p><p>Proposition 1. Let G 1 and G 2 be two graphs with vertex</p><formula xml:id="formula_10">sets V 1 = {v 1 . . . v n1 } and V 2 = {v ? 1 . . . v ?<label>n2</label></formula><p>}, and let {? 1 , . . . , ? p } be an orthonormal basis for the subspace</p><formula xml:id="formula_11">W = span ?(v 1 ), . . . , ?(v n1 ), ?(v ? 1 ), . . . , ?(v ? n2 )</formula><p>. Then, (4) can be rewritten as</p><formula xml:id="formula_12">k FLG (G 1 , G 2 ) = 1 2 S ?1 1 + 1 2 S ?1 2 ?1 1/2 |S 1 | 1/4 |S 2 | 1/4 ,<label>(5)</label></formula><formula xml:id="formula_13">where [S 1 ] i,j = ? ? i S 1 ? j and [S 2 ] i,j = ? ? i S 2 ? j .</formula><p>In other words, S 1 and S 2 are the projections of S 1 and S 2 to W .</p><p>Proof. The proposition hinges on the fact that (4) is invariant to rotation. In particular, if we extend {? 1 , . . . , ? p } to an orthonormal basis {? 1 , . . . , ? m } for the whole of R m , let O = [? 1 , . . . , ? m ] (the change of basis matrix) and set <ref type="formula" target="#formula_7">(4)</ref> can equivalently be written as</p><formula xml:id="formula_14">S 1 = O ? S 1 O, andS 2 = O ? S 2 O, then</formula><formula xml:id="formula_15">k FLG (G 1 , G 2 ) = 1 2S ?1 1 + 1 2S ?1 2 ?1 1/2 |S 1 | 1/4 |S 2 | 1/4 .<label>(6)</label></formula><p>However, in the {? 1 , . . . , ? m } basisS 1 andS 2 take on a special form. Writing S 1 in the outer product form</p><formula xml:id="formula_16">S 1 = n1 a,b=1 ?(v a )[L ?1 1 ] a,b ?(v b ) ? + ?I</formula><p>and considering that for i &gt; p, ?(v a ), ? i = 0 shows that S 1 splits into a direct sumS 1 = S 1 ? S 1 of two matrices: a p ? p matrix S 1 whose (i, j) entry is</p><formula xml:id="formula_17">? ? i S 1 ? j = n1 a,b=1 ? i , ?(v 1,a ) [L ?1 1 ] a,b ?(v 1,b ), ? j + ?? i,j ,<label>(7)</label></formula><p>where ? i,j is the Kronecker delta; and an (n ? p) ? (n ? p) dimensional matrix S 1 = ?I n?p (where I n?p denotes the n ? p dimensional identity matrix). Naturally,S 2 decomposes into S 2 ? S 2 in an analogous way.</p><p>Recall that for any pair of square matrices M 1 and M 2 ,</p><formula xml:id="formula_18">| M 1 ? M 2 | = |M 1 | ? |M 2 | and (M 1 ? M 2 ) ?1 = M ?1 1 ? M ?1 2 .</formula><p>Applying this to (6) then gives</p><formula xml:id="formula_19">k FLG (G 1 , G 2 ) = 1 2 S ?1 1 + 1 2 S ?1 2 ? ? ?1 I n?p ?1 1/2 S 1 ? ?I n?k 1/4 S 2 ? ?I n?k 1/4 = 1 2 S ?1 1 + 1 2 S ?1 2 ?1 ? ?I n?p 1/2 S 1 ? ?I n?k 1/4 S 2 ? ?I n?k 1/4 = ? (n?p)/2 ? (n?p)/4 ? (n?p)/4 1 2 S ?1 1 + 1 2 S ?1 2 ?1 1/2 S 1 1/4 S 2 1/4 .</formula><p>Similarly to kernel PCA <ref type="bibr" target="#b14">(Mika et al., 1999)</ref> or the Bhattacharyya kernel, the easiest way to get a basis for W as required by <ref type="formula" target="#formula_12">(5)</ref> is to compute the eigendecomposition of the joint Gram matrix of the vertices of the two graphs.</p><p>Proposition 2. Let G 1 and G be as in Proposition 1, V = {v 1 , . . . , v n1+n2 } be the union of their vertex sets (where it is assumed that the first n 1 vertices are {v 1 , . . . , v n1 } and the second n 2 vertices are v ? 1 , . . . , v ? n2 ), and define the joint Gram matrix K ? R (n1+n2)?(n1+n2) as</p><formula xml:id="formula_20">K i,j = ?(v i , v j ) = ?(v i ) ? ?(v j ).</formula><p>Let u 1 , . . . , u p be (a maximal orthonormal set of) the non-zero eigenvalue eigenvectors of K with corresponding eigenvalues ? 1 , . . . , ? p . Then the vectors and Q 2 = Q n1+1:n2, : (the first n 1 and remaining n 2 rows of Q, respectively), the matrices S 1 and S 2 appearing in (5) can be computed as</p><formula xml:id="formula_21">? i = 1 ? ? i n1+n2 ?=1 [u i ] ? ?(v ? )<label>(8</label></formula><formula xml:id="formula_22">S 1 = Q ? 1 L ?1 1 Q 1 + ?I, S 2 = Q ? 2 L ?1 2 Q 2 + ?I. (9) Proof. For i = j, ? ? i ? j = 1 ? i ? j n1+n2 k=1 n1+n2 ?=1 [u i ] k ?(v k ) ? ?(v ? ) ?(v k ,v ? ) [u j ] ? = (? i ? j ) ?1/2 u ? i Ku j = (? j /? i ) 1/2 u ? i u j = 0, while for i = j, ? ? i ? j = ? ?1 i u ? i Ku i = u ? i u i = 1,</formula><p>showing that {? 1 , . . . , ? p } is an orthonormal set. At the same time, p = rank(K) = dim(W ) and ? 1 , . . . , ? p ? W , proving that {? 1 , . . . , ? p } is an orthonormal basis for W .</p><p>To derive the form of S 1 , simply plug (8) into <ref type="formula" target="#formula_17">(7)</ref>:</p><formula xml:id="formula_23">? ? i S 1 ? j = 1 ? i ? j n1 k=1 n1 ?=1 n a,b=1 [u i ] k ?(v k ) ? ?(v a ) ?(v k ,va) ? ? [L ?1 1 ] a,b ?(v b ) ? ?(v ? ) ?(v b ,v ? ) [u j ] ? + ?? i,j = (? i ? j ) ?1/2 u ? i KL ?1 Ku j + ?? i,j = (? i ? j ) 1/2 u ? i L ?1 u j + ?? i,j ,</formula><p>and similarly for S 2 .</p><p>As in other kernel methods, the significance of Propositions 1 and 2 is not just that they show show how k FLG (G 1 , G 2 ) can be efficiently computed when ? is very high dimensional, but that they also make it clear that the FLG kernel can really be induced from any base kernel, regardless of whether it corresponds to actual finite dimensional feature vectors or not. For completeness, we close this section with this generalized definition of the FLG kernel. Definition 4. Let G 1 and G 2 be two graphs. Assume that each of their vertices comes from an abstract vertex space V and that ? : V ? V ? R is a symmetric positive semidefinite kernel on V. The generalized FLG kernel induced from ? is then defined as</p><formula xml:id="formula_24">k ? FLG (G 1 , G 2 ) = 1 2 S ?1 1 + 1 2 S ?1 2 ?1 1/2 |S 1 | 1/4 |S 2 | 1/4 ,<label>(10)</label></formula><p>where S 1 and S 2 are as defined in Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiscale Laplacian Graph Kernels</head><p>By a multiscale graph kernel we mean a kernel that is able to capture similarity between graphs not just based on the topological relationships between their individual vertices, but also the topological relationships between subgraphs.</p><p>The key property of the FLG kernel that allows us to build such a kernel is that it can be applied recursively. In broad terms, the construction goes as follows:</p><p>1. Given a graph G, divide it into a large number of small (typically overlapping) subgraphs and compute the FLG kernel between any two subgraphs. 2. Each subgraph is attached to some vertex of G (for example, its center), so we can reinterpret the FLG kernel as a new base kernel between the vertices. 3. We now divide G into larger subgraphs, compute the new FLG kernel between them induced from the new base kernel, and recurse L times.</p><p>Finally, to compute the actual kernel between two graphs G and G ? , we follow the same process for G ? and then compute k FLG (G, G ? ) induced from their top level base kernels. The following definitions formalize this construction. Definition 5. Let G be a graph with vertex set V , and ? a positive semi-definite kernel on V . Assume that for each</p><formula xml:id="formula_25">v ? V we have a nested sequence of L neighborhoods v ? N 1 (v) ? N 2 (v) ? . . . ? N L (v) ? V,<label>(11)</label></formula><p>and for each N ? (v), let G ? (v) be the corresponding induced subgraph of G. We define the Multiscale Laplacian Subgraph Kernels (MLS kernels), K 1 , . . . , K L : V ? V ? R as follows:</p><p>1. K 1 is just the FLG kernel k ? FLG induced from the base kernel ? between the lowest level subgraphs:</p><formula xml:id="formula_26">K 1 (v, v ? ) = k ? FLG (G 1 (v), G 1 (v ? )). 2. For ? = 2, 3, . . . , L, the MLS kernel K ? is the FLG ker- nel induced from K ??1 between G ? (v) and G ? (v ? ): K ? (v, v ? ) = k K ??1 FLG (G ? (v), G ? (v ? )).</formula><p>Definition 5 defines the MLS kernel as a kernel between different subgraphs of the same graph G. However, if two graphs G 1 and G 2 share the same base kernel, the MLS kernel can also be used to compare any subgraph of G 1 with any subgraph of G 2 . This is what allows us to define an L + 1'th FLG kernel, which compares the two full graphs.</p><p>Definition 6. Let G be a collection of graphs such that all their vertices are members of an abstract vertex space V endowed with a symmetric positive semi-definite kernel ? : V ? V ? R. Assume that the MLS kernels K 1 , . . . , K L are defined as in Definition 5, both for pairs of subgraphs within the same graph and across pairs of different graphs. We define the Multiscale Laplacian Graph Kernel (MLG kernel) between any two graphs G 1 , G 2 ? G as</p><formula xml:id="formula_27">K(G 1 , G 2 ) = k KL FLG (G 1 , G 2 ).</formula><p>Definition 6 leaves open the question of how the neighborhoods N 1 (v), . . . , N L (v) are to be defined. In the simplest case, we set N ? (v) to be the ball B r (v) (i.e., the set of vertices at a distance at most r from v), where r = r 0 ? ??1 for some ? &gt; 1. The ? = 2 case is particularly easy, because we can then construct the neighborhoods as follows:</p><formula xml:id="formula_28">1. For ? = 1, find each N 1 (v) = B r0 (v) separately. 2. For ? = 2, 3, . . . , L, for each v ? G set N ? (v) = w?N ??1 (v) N ??1 (w).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Computational complexity and caching</head><p>Definitions 5 and 6 suggest a recurisve approach to computing the MLG kernel: computing K(G 1 , G 2 ) first requires computing K L (v, v ? ) between all n1+n2 2 pairs of top level subgraphs across G 1 and G 2 ; each of these kernel evaluations requires computing K L?1 (v, v ? ) between up to O(n 2 ) level L ? 1 subgraphs, and so on. Following this recursion blindly would require up to O(n 2L+2 ) kernel evaluations, which is clearly infeasible.</p><p>The recursive strategy is wasteful because it involves evaluating the same kernel entries over and over again in different parts of the recursion tree. An alternative solution that requires only O(Ln 2 ) kernel evaluations would be to</p><formula xml:id="formula_29">first compute K 1 (v, v ? ) for all (v, v ? ) pairs, then compute K 2 (v, v ? ) for all (v, v ? )</formula><p>pairs, and so on. But this solution is also wasteful, because for low values of ?, if v and v ? are relatively distant, then they will never appear together in any level ?+1 subgraph, so K ? (v, v ? ) is not needed at all. The natural compromise between these two approaches is to use a recursive "on demand" kernel computation strategy, but once some K ? (v, v ? ) has been computed, store it in a hash table indexed by (v, v ? ), so that K ? (v, v ? ) does not need to be recomputed from scratch.</p><p>A further source of redundancy is that in many real world graph datasets certain subgraphs (e.g., functional groups) recur many times over. This leads to potentially large col-</p><formula xml:id="formula_30">lections of kernel evaluations {K ? (v, v ? 1 ), . . . , K ? (v, v ? z )} where v ? 1 . . . v ? z are distinct, but the corresponding G ? (v ? 1 ), . . . , G ? (v ? z )</formula><p>subgraphs are isomorphic (including the feature vectors), so the kernel values will all be the same. Once again, the solution is to maintain a hash table of all unique subgraphs seen so far, so that when a new subgraph is processed, our code can quickly determine whether it is identical to some other subgraph for which kernel evaluations have already been computed. Doing this process perfectly would require isomorphism testing, which is, of course, infeasible. In practice, a weak test that only detects a subset of isomorphic subgraph pairs already makes a large difference to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Linearized Kernels and Low Rank Approximation</head><p>Even with caching, MLS and MLG kernels can be expensive to compute. The main reason for this is that they involve expressions like (3), where S 1 and S 2 are initially given in different bases. To find a common basis for the two matrices via the method of Proposition 2 requires a potentially large number of lower level kernel evaluations, which require even lower level kernel evaluations, and so on. Unfortunately, this process has to be repeated anew</p><formula xml:id="formula_31">for each {G 1 , G 2 } pair, because in all K ? (v, v ? ) evaluations</formula><p>where v is from one graph and v ? is from the other, the common basis will involve both graphs. Consequently, the cost of the basis computations cannot be amortized into a per-graph precomputation stage.</p><p>In the previous section we saw that computing the MLG kernel between two graphs may involve O(Ln 2 ) kernel evaluations. At the top levels of the hierarchy each G ? (v) might have ?(n) vertices, so the cost of a single FLG kernel evaluation can be as high as O(n 3 ). Somewhat pessimistically, this means that the overall cost of computing</p><formula xml:id="formula_32">k FLG (G 1 , G 2 ) is O(Ln 5 )</formula><p>. Given a dataset of M graphs, computing their Gram matrix requires repeating this for all {G 1 , G 2 } pairs, giving O(LM 2 n 5 ), which is even more problematic.</p><p>The solution that we propose is to compute for each level ? = 1, 2, . . . , L + 1 a single joint basis for all subgraphs at the given level across all graphs G 1 , . . . , G M . For concreteness, we go back to the definition of the FLG kernel.</p><p>Definition 7. Let G = {G 1 , . . . , G M } be a collection of graphs, V 1 , . . . , V M their vertex sets, and assume that V 1 , . . . , V M ? V for some general vertex space V. Further, assume that ? : V ? V ? R is a positive semi-definite kernel on V, H ? is its Reproducing Kernel Hilbert Space, and ? : V ? H ? is the corresponding feature map satisfy-</p><formula xml:id="formula_33">ing ?(v, v ? ) = ?(v), ?(v ? ) for any v, v ? ? V. The joint vertex feature space of {G 1 , . . . , G M } is then W G = span M i=1 v?Vi {?(v)} .</formula><p>W G is just the generalization of the W space defined in Proposition 1 from two graphs to M . In particular, for any {G, G ? } pair (with G, G ? ? G) the corresponding W space will be a subspace of W G . The following generalization of Propositions 1 and 2 is then immediate.</p><formula xml:id="formula_34">Proposition 3. Let N = M i=1 | V i |, V = (v 1 , . .</formula><p>. , v N ) be the concatination of the vertex sets V 1 , . . . , V M , and K the corresponding Gram matrix</p><formula xml:id="formula_35">K i,j = ?(v i , v j ) = ?(v i ), ?(v j ) .<label>(12)</label></formula><p>Let u 1 , . . . , u P be a maximal orthonormal set of non-zero eigenvalue eigenvectors of K with corresponding eigenvalues ? 1 , . . . , ? P . Then the vectors</p><formula xml:id="formula_36">? i = 1 ? ? i N ?=1 [u i ] ? ?(v ? ) i = 1, . . . , P</formula><p>form an orthonormal basis for W G . Moreover, defining Q = [? 1/2 1 u 1 , . . . , ? 1/2 p u p ] ? R P ?P , and setting Q 1 to be the submatrix of Q composed of its first |V 1 | rows; Q 2 be the submatrix composed of the next |V 2 | rows, and so on, for any G i , G j ? G, the generalized FLG kernel induced from ? (Definition 4) can be expressed as</p><formula xml:id="formula_37">k FLG (G i , G j ) = 1 2 S ?1 i + 1 2 S ?1 j ?1 1/2 |S i | 1/4 |S j | 1/4 ,<label>(13)</label></formula><formula xml:id="formula_38">where S i = Q ? i L ?1 i Q i + ?I and S j = Q ? j L ?1 j Q j + ?I.</formula><p>The significance of Proposition 3 is that S 1 , . . . , S M are now fixed matrices that do not need to be recomputed for each kernel evaluation. Once we have constructed the joint basis {? 1 , . . . , ? P }, the S i matrix of each graph G i can be computed independently, as a precomputation step, and individual kernel evaluations reduce to just plugging them into <ref type="formula" target="#formula_2">(13)</ref>. At a conceptual level, what Proposition 3 does it to linearize the kernel ? by projecting everything down to W G . In particular, it replaces the {?(v i )} RKHS vectors with explicit finite dimensional feature vectors given by the corresponding rows of Q, just like we had in the "unkernelized" FLG kernel of Definition 2.</p><p>For our multiscale kernels this is particularly important, because linearizing not just k ? FLG , but also k K1 FLG , k K2 FLG , . . ., allows us to compute the MLG kernel level by level, without recursion. After linearizing the base kernel ?, we can attach explicit, finite dimensional vectors to each vertex of each graph. Then we compute compute k K1 FLG between all pairs of lowest level subgraphs, and linearizing this kernel as well, each vertex effectively just gets an updated feature vector. Then we repeat the process for k K2 FLG . . . k KL FLG , and finally we compute the MLG kernel K(G 1 , G 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Randomized low rank approximation</head><p>The difficulty in the above approach of course is that at each level (12) is a Gram matrix between all vertices of all graphs, so storing it is already very costly, let along computing its eigendecomposition. Morever, P = dim(W G ) is also very large, so managing the S 1 , . . . , S M matrices (each of which is of size P ? P ) becomes infeasible. The natural alternative is to replace W G by a smaller, approximate joint features space, defined as follows.</p><p>Definition 8. Let G, ?, H ? and ? be defined as in Definition 7. Let? = (? 1 , . . . ,?? ) be? ? N vertices sampled from the joint vertex set V = (v 1 , . . . , v N ). Then the corresponding subsampled vertex feature space is</p><formula xml:id="formula_39">W G = span{ ?(?) |? ?? }.</formula><p>Similarly to before, we construct an orthonormal basis {? 1 , . . . , ? P } forW by forming the (now much smaller) Gram matrixK i,j = ?(? i ,? j ), computing its eigenvalues and eigenvectors, and setting</p><formula xml:id="formula_40">? i = 1 ? ?i ? ?=1 [u i ] ? ?(? ? ). The resulting approximate FLG kernel is k FLG (G i , G j ) = 1 2S ?1 i + 1 2S ?1 j ?1 1/2 |S i | 1/4 |S j | 1/4 ,<label>(14)</label></formula><p>whereS i =Q ? i L ?1 iQ i + ?I andS j =Q ? j L ?1 jQ j + ?I are the projections of S i and S j toW G . We introduce a further layer of approximation by restrictingW G to be the space spanned by the firstP &lt; P basis vectors (ordered by descending eigenvalue), effectively doing kernel PCA on {?(?)}? ?? , equivalently, a low rank approximation ofK.</p><p>Assuming that v g j is the j'th vertex of G g , in constrast to Proposition 2, now the j'th row ofQ s consists of the coordinates of the projection of ?(v g j ) ontoW G , i.e.,</p><formula xml:id="formula_41">[Q g ] j,i = 1 ? ? i? ?=1 [u i ] ? ?(v g j ), ?(? N ) = 1 ? ? i? ?=1 [u i ] ? ?(v g j ,? N ).</formula><p>The above procedure is similar to the popular Nystr?m approximation for kernel matrices <ref type="bibr">(Williams &amp; Seeger, 2001;</ref><ref type="bibr" target="#b5">Drineas &amp; Mahoney, 2005)</ref>, except that in our case the ultimate goal is not to approximate the Gram matrix (12) itself, but the S 1 , . . . , S M matrices used to form the FLG kernel. In practice, we found that the eigenvalues of K usually drop off very rapidly, suggesting that W can be safely approximated by a surprisingly small dimensional subspace (P ? 10), and correspondingly the sample siz? N can be kept quite small as well (on the order of 100). The combination of these two factors makes computing the entire stack of kernels very fast, reducing the complexity of computing the Gram matrix for a dataset of M graphs of ?(n) vertices each to O(M L? 2P 3 + M L? 3 + M 2P 3 ).</p><p>As an example, for the ENZYMES dataset, comprised of 600 graphs, the FLG kernel between all pairs of graphs can be computed in about 2 minutes on a 16 core machine.</p><p>Note that Definition 8 is noncommittal to the sampling distribution used to select (? 1 , . . . ,?? ): in our experiments we used uniform sampling without replacement. Also note that regardless of the approximations, S 1 , . . . , S M matrices are always positive definite, and this fact alone, by the definition of the Bhattacharyya kernel, guarantees that the resulting FLG, MLS and MLG kernels are positive semidefinite kernels. For a high level pseudocode of the resulting algorithm, see the Supplementary Materials.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compared the efficacy of the MLG kernel with some of the top performing graph kernels from the literature: the Weisfeiler-Lehman Kernel, the Weisfeiler-Lehman Edge Kernel <ref type="bibr" target="#b17">(Shervashidze et al., 2009)</ref>, the Shortest Path Kernel , the Graphlet Kernel <ref type="bibr" target="#b17">(Shervashidze et al., 2009)</ref>, and the p-random Walk Kernel <ref type="bibr" target="#b19">(Vishwanathan et al., 2010)</ref>, on standard benchmark datasets( <ref type="table" target="#tab_2">Table 2)</ref>.</p><p>We perform classification using a binary C-SVM solver <ref type="bibr" target="#b3">(Chang &amp; Lin, 2011)</ref> to test our kernel method. We tuned the SVM slack parameter through 10-fold cross-validation using 9 folds for training and 1 for testing, repeated 10 times. All experiments were done on a 16 core Intel E5-2670 @ 2.6GHz processor with 32 GB of memory. Our prediction accuracy and standard deviations are shown in <ref type="table" target="#tab_1">Table 1</ref> and runtimes in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The parameters for each kernel were chosen as follows: for the Weisfeiler-Lehman kernels, the height parameter h is chosen from {1, 2, ..., 5}, the random walk size p for the p-random walk kernel was chosen from {1, 2, ..., 5}, for the Graphlets kernel the graphlet size n was chosen from {3, 4, 5} as outlined in <ref type="bibr" target="#b17">(Shervashidze et al., 2009)</ref>. For the parameters of the MLG kernel: we chose ? and ? from {0.01, 0.1, 1}, radius size n from {1, 2, 3, 4}, number of levels l from {1, 2, 3, 4}. We used the given discrete node labels to create a one-hot binary feature vector for each node and used the dot product between nodes' binary feature vector labels as the base kernel for the MLG kernel.</p><p>We achieve the highest prediction accuracy for all datasets except NCI1 and NCI109, where it performs better than all non-Weisfeiler Lehman kernels. Across all datasets, we found the optimal number of levels to be 2 or 3 and likewise for the radius size. As can be seen from the average number of nodes and average diameter values in <ref type="table" target="#tab_2">Table 2</ref>, the graphs in each dataset are small enough that a 2 or 3 level deep MLG kernel is sufficient to effectively characterize the similarity between graphs. The optimal ? and ? values were either 0.01 or 0.1 in all cases. In general, these two parameters can be set through cross validation over a small set of values. For two graphs G and?, that are reasonably similar with only slight differences(ex:? is similar to G in degree distribution, connectivity, etc), increasing the ? and/or ? value will have the effect of artificially increasing the value of k F LG (G,?), smoothing out their differences. This sort of smoothing is not desirable for all pairs of graphs, so typically the optimal ? and ? values will be small, often between 0.01 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper we have proposed two new graph kernels: (1) The FLG kernel, which is a very simple single level kernel that combines information attached to the vertices with the graph Laplacian;</p><p>(2) The MLG kernel, which is a multilevel, recursively defined kernel that captures topological relationships between not just individual vertices, but also subgraphs. Clearly, designing kernels that can optimally take into account the multiscale structure of actual chemical compounds is a challenging task that will require further work and domain knowledge. However, it is encouraging that even just "straight out of the box", tuning only one or two parameters, such as the number of levels, the MLG kernel performed on par with, or even slightly better than the other well known kernels in the literature. Beyond just graphs, the general idea of multiscale kernels is of interest for other types of data as well (such as images) that have multiresolution structure, and the way that the MLG kernel chains together local spectral analysis at multiple scales is potentially applicable to these domains as well, which will be the subject of further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification Results (Accuracy ? Standard Deviation) Method MUTAG(Debnat et al., 1991) PTC(H.Toivonen et al., 2003) ENZYMES(Borgwardt et al., 2005) PROTEINS(Borgwardt et al., 2005) NCI1(Wale et al., 2008) NCI109(Wale et al.</figDesc><table><row><cell>, 2008)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Summary of the datasets used in our experiments</figDesc><table><row><cell>Dataset</cell><cell cols="6">Size Labels Nodes Edges Diameter Classes</cell></row><row><cell>MUTAG</cell><cell>188</cell><cell>7</cell><cell>17.9</cell><cell>39.6</cell><cell>8.2</cell><cell>2 (125 vs 63)</cell></row><row><cell>PTC</cell><cell>344</cell><cell>19</cell><cell>25.6</cell><cell>51.9</cell><cell>8.9</cell><cell>2 (192 vs 152)</cell></row><row><cell cols="2">ENZYMES 600</cell><cell>3</cell><cell>32.6</cell><cell cols="2">124.3 10.9</cell><cell>6 (100 each)</cell></row><row><cell cols="3">PROTEINS 1113 3</cell><cell>39.1</cell><cell cols="2">145.6 11.6</cell><cell>2 (663 vs 450)</cell></row><row><cell>NCI1</cell><cell cols="2">4110 37</cell><cell>29.9</cell><cell>64.6</cell><cell>13.3</cell><cell>2 (2057 vs 2053)</cell></row><row><cell>NCI109</cell><cell cols="2">4127 38</cell><cell>29.7</cell><cell>64.3</cell><cell>13.1</cell><cell>2 (2079 vs 2048)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Runtime of MLG on different Datasets</figDesc><table><row><cell>Dataset</cell><cell>Wall clock time</cell><cell>CPU time</cell></row><row><cell>MUTAG</cell><cell>0min 0.86s</cell><cell>0min 5.7s</cell></row><row><cell>PTC</cell><cell>1min 11.18s</cell><cell>9min 9.5s</cell></row><row><cell>ENZYMES</cell><cell>0min 36.65s</cell><cell>4min 41.2s</cell></row><row><cell>PROTEINS</cell><cell>3min 19.8s</cell><cell>48min 23.0s</cell></row><row><cell>NCI1</cell><cell>5min 36.3s</cell><cell>84min 4.8s</cell></row><row><cell>NCI109</cell><cell>5min 42.6s</cell><cell>84min 35.9s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was completed in part with computing resources provided by the University of Chicago Research Computing Center.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Williams, Christopher K. I. and Seeger, Mattias. Using the Nystr?m method to speed up kernel machines. In Advances in Neural Information Processing Systems (NIPS), 2001.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Concise and Provably Informative Multi-Scale Signature Based on Heat Diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing of Eurographics Symposium on Geometry Processing</title>
		<meeting>essing of Eurographics Symposium on Geometry essing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intelligent Systems in Molecular Biology (ISMB)</title>
		<meeting>Intelligent Systems in Molecular Biology (ISMB)<address><addrLine>Detroit, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortestpath kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining(ICDM) 2005)</title>
		<meeting>the 5th IEEE International Conference on Data Mining(ICDM) 2005)<address><addrLine>Houston, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11-30" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung And</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Med Chem</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="786" to="97" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Nystr?m method for approximating a Gram matrix for improved kernel-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aasa</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niklas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marleen</forename><surname>Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exponential and geometric kernels for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*02 workshop on unreal data, volume Principles of modeling nonvectorial data</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complete mining of frequent patterns from graphs: Mining graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Motoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="321" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bhattacharyya and expected likelihood kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Computational Learning Theory and Kernels Workshop (COLT/KW), number 2777 in Lecture Notes in Computer Science</title>
		<editor>Sch?lkopf, B. and Warmuth, M.</editor>
		<meeting>the Annual Conference on Computational Learning Theory and Kernels Workshop (COLT/KW), number 2777 in Lecture Notes in Computer Science<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The skew spectrum of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel between sets of vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Best student paper award. Google Scholar citations: 137</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Drug research: myths, hype and reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kubinyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews: Drug Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="665" to="668" />
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kernel PCA and de-noising in feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Kearns, M. S., Solla, S. A., and Cohn, D. A.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="536" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The ring of graph invariants -graphic values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikkonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelfth International Conference on Artificial Intelligence and Statistics<address><addrLine>Clearwater Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04-16" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weisfeiler-lehman graph kernels. jmlr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="347" to="375" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
