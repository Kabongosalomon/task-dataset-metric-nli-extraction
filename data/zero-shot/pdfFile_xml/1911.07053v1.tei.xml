<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maintaining Discrimination and Fairness in Class Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Gan</surname></persName>
							<email>guojun.gan@uconn.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Connecticut</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhang</surname></persName>
							<email>bin.zhang@pcl.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Maintaining Discrimination and Fairness in Class Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common realworld problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and significantly outperform state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few years, Deep Neural Networks (DNNs) have shown remarkable performance in various applications, even surpassing human performance on some tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. The standard DNNs are typically trained on a prepared dataset, where the number of categories is fixed in advance. However, in many real-world applications, it is often required to learn new classes gradually from streaming  data, which is called class incremental learning.</p><p>In order to achieve this goal, a common method is to fine tune the old model on new data by setting the number of output nodes to be that of current classes (including old and new classes) as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, this naive method suffers from a serious problem known as catastrophic forgetting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. As can be seen from <ref type="figure" target="#fig_0">Figure 1</ref>, the old data's output probabilities corresponding to the old classes (which are highlighted in red boxes) are relatively low. Thus, the new model trained by the vanilla method generally predicts objects as new classes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>To alleviate catastrophic forgetting, many studies have been done. EWC <ref type="bibr" target="#b17">[18]</ref>, SI <ref type="bibr" target="#b33">[34]</ref>, and MAS <ref type="bibr" target="#b0">[1]</ref> attempt to solve this problem with a parameter control strategy. Knowledge distillation (KD) <ref type="bibr" target="#b11">[12]</ref> is another strategy, which has also been widely used in this field <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39]</ref>. Besides, some other studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> follow a rehearsal strategy by using a small amount of real or generated old data in the training process. In class incremental learning tasks, the new model is trained without access to the old data, even with the rehearsal strategy, the training set in an incremental step is seriously imbalanced between old classes and new classes. Thus, there are also some studies that deal with catastrophic forgetting from this perspective <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>In this paper, we first demonstrate that knowledge distillation, the commonly used technique in this field, can in-  <ref type="figure">Figure 2</ref>: The effect of our solution. KD helps model to maintain discrimination within old classes. WA helps model to maintain fairness between old and new classes. deed help the model to output more discriminative results within old classes. However, the prediction bias towards new classes cannot be alleviated. The trained model still treats old classes unfairly, causing the positive effect of KD to be hidden and limited. Then we show that the weights in the trained model's FC layer are heavily biased, which can cause the model to tend to classify samples into new classes. Based on the above, we present a simple and effective solution to mitigate catastrophic forgetting. The effect of our solution is presented in <ref type="figure">Figure 2</ref>. Firstly, we utilize KD to maintain the discrimination within old classes. This helps the model to output more discriminative results within old classes. Then, to further maintain the fairness between old and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after the normal training process. This helps the model to treat old classes and new classes fairly, and output correct predictions. In this paper, our main contributions are the following:</p><p>(i) We investigated the actual role of KD in class incremental learning by experiments, including positive and negative impacts; (ii) We presented a simple and effective solution to address catastrophic forgetting in class incremental learning that maintains both the discrimination via KD and the fairness via WA; (iii) Inspired by a prior observation of a non-incremental model, the proposed method WA attempts to align the norms of the weight vectors for new classes to those for old classes. WA makes full use of the information contained in the trained model and correct the biased weights in the FC layer, it does not need to reserve a validation set in advance or require any additional parameters to be tuned, but can handle class incremental learning tasks well; (iv) Extensive experiments were conducted, the results show that our method achieves better performance than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, many methods have been proposed to alleviate the well-known problem of catastrophic forgetting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref> suffered by ordinary DNNs . In this section, we briefly discuss these methods.</p><p>Parameter Control. The approaches of this strategy such as EWC <ref type="bibr" target="#b17">[18]</ref>, SI <ref type="bibr" target="#b33">[34]</ref>, and MAS [1] manage to constrain the important weights of old model when facing new data. These methods expect small changes in the important parameters. They differ in how to estimate the important parameters. EWC estimates the weight importance through the Fisher information matrix; SI uses the path integral over the optimization trajectory; MAS utilizes the gradients of the network output <ref type="bibr" target="#b36">[37]</ref>. However, the importance of parameters is difficult to measure accurately in a series of tasks <ref type="bibr" target="#b12">[13]</ref>. These methods tend to perform poorly in class incremental learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Knowledge Distillation. Knowledge distillation <ref type="bibr" target="#b11">[12]</ref> is a widely used method, which transfers key knowledge from a teacher model to a student model. LwF <ref type="bibr" target="#b19">[20]</ref> utilizes knowledge distillation to learn multiple tasks. A modified cross-entropy loss is used to preserve the capabilities of old model. Then, it was applied to multi-class classification, called LwF.MC <ref type="bibr" target="#b25">[26]</ref>. M 2 KD <ref type="bibr" target="#b38">[39]</ref> introduces a multi-model and multi-level knowledge distillation strategy, which utilizes all previous model snapshots instead of distilling knowledge only from the last model.</p><p>Rehearsal. The rehearsal strategy alleviates catastrophic forgetting by using some old data to make up training data. The simplest approach is to store few old data and replay them in a new incremental step. This straightforward approach has been demonstrated to be effective in many scenarios <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. Other methods construct a generative model, e.g., GANs <ref type="bibr" target="#b7">[8]</ref>, to generate samples for rehearsal instead of storing old data directly <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. However, in these methods, an additional generative model needs to be trained simultaneously. Therefore, they rely heavily on the quality of the generated model.</p><p>Class Imbalance. For class incremental learning, data of old classes is generally not available when new classes appear. Even with the rehearsal strategy, the class imbalance problem is still very serious, which is an important factor in catastrophic forgetting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. Though class imbalance is an old topic and has attracted a lot of attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, multi-class imbalance learning is still an open problem <ref type="bibr" target="#b34">[35]</ref>. In order to address it in class incremental learning, BiC <ref type="bibr" target="#b31">[32]</ref> adds a bias correction layer to correct the model's outputs. This method needs to keep a validation set to train the additional bias correction layer. In <ref type="bibr" target="#b12">[13]</ref>, cosine normalization, less-forget constraint, and inter-class separation are incorporated to mitigate the impact of class imbalance. This method combines three specific loss terms and other skills (e.g., class balance fine tune) to improve performance. IL2M <ref type="bibr" target="#b2">[3]</ref> rectifies scores of old classes by leveraging contents from a dual memory.</p><p>These strategies can be applied in combination. For example, both the distillation strategy and the rehearsal strategy are used in iCaRL <ref type="bibr" target="#b25">[26]</ref>, which also utilizes a nearest-exemplars-mean (NEM) classifier. EEIL <ref type="bibr" target="#b4">[5]</ref> also exploits these two strategies and utilizes a balanced fine tuning to alleviate class imbalance. In this paper, the proposed method is also based on these perspectives. A detailed analysis of distillation strategy is presented, including its positive and negative effects. More importantly, we deal with class imbalance in a simple and effective manner. Without any additional model parameters, hyperparameters or a reserved validation set, our method achieves better performance than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline</head><p>In this subsection, we summarize a baseline method in class incremental learning, which utilizes both the rehearsal strategy and the distillation strategy.</p><p>Let us first formulate class incremental learning. Assume there are B batches of train data</p><formula xml:id="formula_0">{D 1 , ? ? ? , D B }, with D b = {(x b 1 , y b 1 ), ? ? ? , (x b n b , y b n b )} for the b th incremental step, where x b</formula><p>i and y b i represent the input data and the target respectively, n b is the number of samples in the set D b . In the b th step of class incremental learning, the goal is to learn knowledge from new data D b , while retain the previous experiences learned from old data {D 1 , ? ? ? , D b?1 }. For each step, the trained model is evaluated on all seen classes.</p><p>For the b th incremental step, the baseline method initializes the model with the parameters learned in the previous step and adds new output nodes (weights in the FC layer are initialized randomly). Then, it attempts to learn new classes and meanwhile preserve the original capabilities with the new data D b and a few rehearsal data D b old . It is assumed that the new data D b comes from C b new classes, and the rehearsal data</p><formula xml:id="formula_1">D b old comes from C b old old classes, where C b old = b?1 k=1 C k .</formula><p>The baseline method combines the cross-entropy loss L CE with the knowledge distillation loss L KD . The combined loss containing two terms is given as:</p><formula xml:id="formula_2">L(x, y) = (1 ? ?)L CE (x, y) + ?L KD (x),<label>(1)</label></formula><p>where ? is a hyper-parameter governing the balance between the two losses. We set the hyper-parameter ? to</p><formula xml:id="formula_3">C b old C b +C b old</formula><p>, according to the recommendation in <ref type="bibr" target="#b31">[32]</ref>. The cross-entropy loss is given by:</p><formula xml:id="formula_4">L CE (x, y) = C b +C b old c=1 ?? c=y log p c (x) ,<label>(2)</label></formula><p>where ? c=y is the indicator function and p c (x) is the output probability for the c th class. And the distillation loss is given by: </p><formula xml:id="formula_5">L KD (x) = C b old c=1 ?q c (x) log q c (x) ,<label>(3)</label></formula><formula xml:id="formula_6">whereq c (x) = e? c (x)/T C b old j=1 e? j (x)/T , q c (x) = e oc (x)/T C b old j=1 e o j (x)/T ; T is the temperature scalar;? c (x) is an element of?(x), o(x) = ? 1 (x), ? ? ? ,? C b old (x)</formula><p>T , which represents the output logits of the old model obtained in the previous</p><formula xml:id="formula_7">incremental step; o c (x) is an element of o(x), o(x) = o 1 (x), ? ? ? , o C b old (x), o C b old +1 (x), ? ? ? , o C b old +C b (x) T ,</formula><p>which stands for the output logits of the current model. Note the sample (x, y) is from both the new data and the rehearsal data. Then, parameters of both the feature extraction layers and the FC layer are updated with the combined loss defined in Eq.(1) during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Effect of Knowledge Distillation</head><p>The baseline method is widely used in class incremental learning. However, there is a lack of explicit analysis of the role of knowledge distillation. To do this, we carry out experiments on the CIFAR-100 <ref type="bibr" target="#b18">[19]</ref> with 5 incremental steps (B = 5) and 20 classes per step (C b = 20, b = 1, ? ? ? , 5).</p><p>We perform class incremental learning with two methods: (a) using the cross-entropy loss; (b) using both the cross-entropy loss and the distillation loss. After 5 incremental steps, we evaluate the two models trained by method (a) and (b). The test set is comprised of two parts, one containing 80 old classes and another 20 new classes. Error analysis on two parts of the test set is reported in <ref type="table" target="#tab_2">Table 1</ref>. There are 2,000 test samples in the new part, and 8,000 samples in the old part. As can be seen, both methods have very poor performance in term of old classes, which shows that they have lost the ability to recognize old data.</p><p>We further analyze the type of misclassification of old data. As shown in <ref type="table" target="#tab_2">Table 1</ref>  <ref type="figure">Figure 3</ref>: Overview of our solution for class incremental learning. In the first phase, we train the model with the crossentropy loss (L CE ) and the distillation loss (L KD ). In the second phase, we correct the biased weights in the trained model via Weight Aligning (WA). o and? represent the output logits of the current model and the old model respectively, y stands for the true label, o corrected represents the corrected output logits by using WA.</p><p>classes is smaller than that to other old classes. If old samples are misclassified to new classes, the distillation loss still can be low, as {q c (x), c = 1, ? ? ? , C b old } are only calculated between the outputs corresponding to old classes. While, if they are misclassified to other old classes, the distillation loss will be high, as the output probability distribution is definitely not coincide with the target distribution. As a result, the model is more inclined to misclassify old samples into new classes.</p><p>Based on the above analysis, we argue that the positive effect of distillation loss is maintaining the discrimination within old classes, so that it is successful in making fewer misclassifications within old classes. However, the model still has a prediction bias towards new classes. The positive effect of knowledge distillation here is limited. Besides, if there are more than two incremental steps, i.e., B &gt; 2, the 'ill' model will become a teacher model in the next incremental step, then the deviation will accumulate, so that the positive effect will be further limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>Our method consists of two phases, as shown in <ref type="figure">Figure  3</ref>. The first phase is Maintaining Discrimination. In this phase, we train a new model on the new data and the rehearsal data with the combined loss. We expect to transfer knowledge from the old model to the new model and maintain discrimination within old classes with the help of knowledge distillation.</p><p>As knowledge distillation loss still cannot help the model to treat old classes and new classes fairly as shown in subsection 3.2, we design the second phase, called Maintaining Fairness. In this phase, we propose a method named Weight Aligning (WA) to correct the model trained in the first phase. The corrected model treats old classes and new classes fairly, and can significantly improve the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Biased Weights in the FC Layer</head><p>As shown in subsection 3.2, the model trained via the baseline method still tends to predict test samples as new classes. To study this problem conveniently, we express the FC layer of model in the b th incremental step in the following form:</p><formula xml:id="formula_8">o(x) = W T ?(x),<label>(4)</label></formula><p>where the (C b old + C b )-dimensional vector o(x) represents output logits of the current model; ?(?) is a feature extraction function (can be a CNN-based model usually), which outputs d-dimensional feature vectors;</p><formula xml:id="formula_9">W ? R d?(C b old +C b )</formula><p>stands for the weights, which can be expressed as W</p><formula xml:id="formula_10">= {w c , 1 ? c ? C b old + C b },</formula><p>where w c is a d-dimensional weight vector for the c th class. Note, for the convenience of analysis, we always set the bias term in the FC layer to zero without special instructions, which will be discussed in the ablation study.</p><p>We carry out experiments on CIFAR-100 with 5 incremental steps and 20 classes per step. After each step, we calculate the norms of the weight vectors {w c } and plot them in <ref type="figure" target="#fig_1">Figure 4</ref>. As shown in <ref type="figure" target="#fig_1">Figure 4</ref> (b), (c), (d) and (e), the norms of the weight vectors for new classes are much larger than those for old classes. This phenomenon is mainly caused by class imbalance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. Due to the output logits for the c th class is calculated as</p><formula xml:id="formula_11">o c (x) = w T c ?(x),<label>(5)</label></formula><p>if the norms of weight vectors for new classes are larger, the output logits for new classes may tend to be larger in general. As a result, the trained model may tend to predict an input image as belonging to a new class. However, as shown in <ref type="figure" target="#fig_1">Figure 4 (a)</ref>, in the first phase, the norms of the weight vectors are roughly equal, as this phase does not related to class incremental learning actually. We treat this as a priori knowledge. The phenomenon in class incremental learning does not match this prior knowledge, which inspires us to correct the biased weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weight Aligning</head><p>Based on the above, we present a simple and effective approach, called Weight Aligning (WA), to correct the biased weights in the FC layer. In WA, the norms of the weight vectors of new classes are aligned to those of old classes.</p><p>Firstly, we rewrite the weights in the FC layer in the following form</p><formula xml:id="formula_12">W = (W old , W new ), where W old = (w 1 , w 2 , ? ? ? , w C b old ) ? R d?C b old , W new = (w C b old +1 , ? ? ? , w C b old +C b ) ? R d?C b .</formula><p>Then, we denote, respectively, the norms of the weight vectors of old classes and new classes as follows Norm old = (||w 1 ||, ? ? ? , ||w C b old ||), Norm new = (||w C b old +1 ||, ? ? ? , ||w C b old +C b ||). Based on the above norms, we normalize the weights for new classes by</p><formula xml:id="formula_13">W new = ? ? W new ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">? = M ean(Norm old ) M ean(Norm new ) ,<label>(7)</label></formula><p>M ean(?) returns the mean value of elements in the vector. In this way, the average norm of the weight vectors for new classes becomes the same as that for old classes. Note that we only make the average norms become equal, in other words, within new classes (or old classes), the relative magnitude of the norms of the weight vectors does not change. Such a design is mainly used to ensure the data within new classes (or old classes) can be separated well.</p><p>The original output logits of the model trained in the first phase of our method can be expressed as</p><formula xml:id="formula_15">o(x) = o old (x), o new (x) T = W T old ?(x), W T new ?(x) T .<label>(8)</label></formula><p>After applying WA to the weights, the corrected output logits are given by:</p><formula xml:id="formula_16">o corrected (x) = W T old ?(x), W T new ?(x) T = W T old ?(x), ? ? W T new ?(x) T = o old (x), ? ? o new (x) T .<label>(9)</label></formula><p>As shown in Eq.(9) and Eq. <ref type="formula" target="#formula_14">(7)</ref>, the final effect of aligning the weights is to rescale the output logits of new classes by a coefficient. The latter experiments demonstrate that our method can effectively alleviate the prediction bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Restriction to the Weights</head><p>In fact, the magnitude relationship between the norms of weight vectors for new classes and those for old classes may not always reflect the magnitude relationship between the output logits for old classes and those for new classes. Suppose that the feature extraction function provides the feature vectors, whose elements are all non-negative. This assumption is reasonable, because in usual model architectures, the learned features are activated by the 'ReLU' function ReLU(x) = max(0, x) , which returns non-negative values. As the weight vectors {w c } usually contain both positive and negative elements, the negative elements with large absolute values contribute to a large norm of weight vectors. However, they are not in favor of large output logits. Thus, in order to make the norm of the weight vector w c more consistent with its corresponding output logits, we restrict the elements of the weight vector w c to be positive. To achieve this, weight clipping <ref type="bibr" target="#b1">[2]</ref> can be performed after each optimization step in training. The impact of restricting the weights in the FC layer to be positive will be analyzed in the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>We evaluate the methods on ImageNet ILSVRC 2012 <ref type="bibr" target="#b26">[27]</ref> and CIFAR-100 <ref type="bibr" target="#b18">[19]</ref>, which are widely used in the study of class incremental learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. ImageNet  <ref type="bibr" target="#b23">[24]</ref>. The code will be made publicly available. For ImageNet, we adopt a 18-layer ResNet <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. We use SGD to train our model and set the batch size to 256. The learning rate starts from 0.1 and reduces to 1/10 of the previous learning rate after 30, 60, 80 and 90 epochs (100 epochs in total). For CIFAR-100, we use a 32-layer ResNet. We also train the model with SGD and set the batch size to 32. The learning rate starts from 0.1 and reduces to 1/10 of the previous learning rate after 100, 150 and 200 epochs (250 epochs in total). We set the temperature scalar T to 2. For data augmentation, random cropping, horizontal flip and normalization are employed to augment training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Weight Aligning</head><p>To analyze the effect of weight aligning, we perform experiments on CIFAR-100 with 5 incremental steps and 20 classes per step. We first compare our method with three variations in the following: Variation1, training with the cross-entropy loss; Variation2, training with the crossentropy loss, and correcting the model via WA; Variation3, training with the combined loss; Ours, training with the combined loss and correcting the model via WA. <ref type="table" target="#tab_4">Table 2</ref> summarizes the results of these experiments. Variation1 is the worst one, as it only uses the cross-entropy loss. Variation3 adds the distillation loss on the basis of Variation1 to mitigate catastrophic forgetting. However, Variation3 is only a little better than Variation1. Variation2 uses WA to correct the model based on Variation1, and significantly improves performance (the gain in term of the overall performance at the end of class incremental learning is 7.5%). From the results of 'Ours', WA also gets significant improvements (more than 16% at the end of class incre-mental learning over Variation3). These results demonstrate that WA is quite effective for class incremental learning.</p><p>It is worth noting that the gain brought by the combination of KD and WA is greater than the sum of the gains from each component used separately, e.g., for the average results, the gain of the combination (Ours) is 11.3%, and the gains of WA (Variation2) and KD (Variation3) used separately are 6.2% and 1.1% respectively. As shown in subsection 3.2, the positive effect of KD is limited when used alone. KD helps the model to output more discriminative results within old classes, however, these outputs are overwhelmed by the superior outputs of new classes. For example, as shown in <ref type="figure">Figure 2</ref>, with the help of KD, the output probability for 'cat' becomes higher than that for 'fish', but still lower than that for new class 'lion' or 'dog'. In such a scenario, the positive effect of KD is hidden. As our method maintains not only the discrimination within old classes but also the fairness between old classes and new classes, it strengthens the positive effect of KD. On the other hand, the corrected outputs via WA are more accurate with the help of KD. Therefore, our method creates the "one plus one greater than two" effect and achieves significant improvements.</p><p>The confusion matrices of different methods are presented in <ref type="figure" target="#fig_2">Figure 5</ref>. From <ref type="figure" target="#fig_2">Figure 5 (a)</ref> and (c), we see that KD leads to fewer misclassifications between old classes, however, both Variation1 and Variation3 tend to predict objects as new classes. With the help of WA, Variation2 and our method make the model treat new classes and old classes fairly as shown in <ref type="figure" target="#fig_2">Figure 5</ref> (b) and (d). And our method achieves better performance with the help of KD. These results intuitively show that the proposed method can effectively maintain discrimination and fairness in the model predictions.</p><p>The proposed method weight aligning is a postprocessing technique. It is interesting to see the effect of adding a normalization layer on the weights (in the FC layer) directly, like the operation in Modified Softmax Loss <ref type="bibr" target="#b20">[21]</ref> and NormFace <ref type="bibr" target="#b29">[30]</ref>, so that the weights of all classes can have a unit norm. We implement this method as Varia-tion4: training with the combined loss and a weight normalization layer (WNL). The results are also provided in <ref type="table" target="#tab_4">Table  2</ref>. Compared with Variation1 and Variation2, this method does not bring about a significant improvement. Actually, the FC layer plays an important role in the visual representation transfer <ref type="bibr" target="#b35">[36]</ref>. If the weights in the FC layer are strictly limited during the training process, in order to adapt to new data, the bias in the feature extraction layers will become more serious. However, the bias in the feature extraction layers is harder to correct than that in the weights of FC layer, as the parameters of feature extraction layers are shared by all classes and the weights of FC layer are not shared between classes. Therefore, it is better to take a post-processing approach, such as WA. In addition, we have tested the method that normalizing the weights of all classes to have a unit norm after the usual training process. While this approach is inferior to WA. As mentioned in subsection 4.2, within the new classes (or the old classes), the relative magnitude of the norms of the weight vectors does not change in WA, such a design can maintain the differences and ensure that the classes can be separated well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to Other Methods</head><p>We compare our method with several competitive or representative methods, including LwF.MC <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>, iCaRL <ref type="bibr" target="#b25">[26]</ref>, EEIL <ref type="bibr" target="#b4">[5]</ref>, BiC <ref type="bibr" target="#b31">[32]</ref>, IL2M <ref type="bibr" target="#b2">[3]</ref>, RPS <ref type="bibr" target="#b24">[25]</ref>. Experiments are performed on ImageNet100, ImageNet1000 and CIFAR100. Evaluation on ImageNet. We conduct two experiments on this dataset. In the first one, 100 classes (ImageNet- 100) are selected randomly and split into 10 incremental batches with 10 classes per batch; In the second one, we split the 1000 classes (ImageNet-1000) into 10 incremental batches with 100 classes per batch. For the sake of fairness, we use the same set of classes in ImageNet-100 and ImageNet-1000 as the previous work <ref type="bibr" target="#b31">[32]</ref>. We store 2,000 images for old classes in ImageNet-100 experiments. And in ImageNet-1000 experiments, we store 20,000 images for old classes as the same as the previous work. We select rehearsal exemplars based on herding selection <ref type="bibr" target="#b30">[31]</ref> which is also the same as the previous work. More classes have been seen, fewer images can be retained per class. As a result, the problem of class imbalance becomes more serious. The class incremental learning results (top-5 accuracy %) on ImageNet-100 and Imagenet-1000 are shown in Table 3. We report the performance at the last incremental step and the average results over all the incremental steps except the first step here (as the first step does not related to class incremental learning actually). We also provide the detailed results of all incremental steps and the top-1 results in the supplementary material. As can be seen from these tables, the proposed method outperforms the compared methods by a large margin, especially on the large scale dataset ImageNet-1000. The overall performance at the end of class incremental learning is improved by more than 28% compared to EEIL on ImageNet-1000. In contrast to the stateof-the-art method BiC, the proposed method also achieves better results (surpasses it by 7.9% at the end of class incremental learning on ImageNet-1000). Though Eq.(9) is similar in form to the linear model in BiC, the proposed method does not need to reserve a validation set which is used in BiC to learn additional parameters. All of the rehearsal data can be utilized to learn a better feature extractor, so that the Overall, these results indicate that the proposed method is effective to handle catastrophic forgetting in class incremental learning. Our approach not only achieves better performance than state-of-the-art methods but also has a simpler structure. Evaluation on CIFAR-100. CIFAR-100 has 100 classes, which are divided into 2, 5, 10 and 20 incremental batches respectively in our experiments. The same set of classes in CIFAR-100 are used for all of the compared methods. In CIFAR-100 experiments, we store 2,000 samples in total as the same as previous work.</p><p>The average results over all the incremental steps except the first step are shown in <ref type="table" target="#tab_6">Table 4</ref>. Detailed results of all incremental steps are reported in the supplementary material. On CIFAR-100, these methods achieve similar results, which is mainly because this dataset is simple <ref type="bibr" target="#b31">[32]</ref>. Consistent with the results on ImageNet, the proposed method achieves better results compared to state-of-the-art approaches on CIFAR-100 under different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>In this subsection, we analyze the impact of the components of our method. Impact of Restriction to the Weights. We studied the impact of restricting the weights in the FC layer to be positive on ImageNet-100 with 10 incremental steps. As shown in <ref type="figure" target="#fig_3">Figure 6</ref> (a), our method obtained better performance with restriction to the weights. As discussed in subsection 4.3, this is mainly due to the norms of the weight vectors become more consistent with their corresponding output logits when restricting the weights to be positive, so that the scale factor ? obtained by Eq. <ref type="formula" target="#formula_14">(7)</ref> is more accurate to suppress the output logits of new classes. Impact of Norm Selection. We investigated the impact of different norm used in the proposed method. We compare two norms: 1-norm and 2-norm. <ref type="figure" target="#fig_3">Figure 6 (b)</ref> shows the results. 1-norm and 2-norm achieve similar results, which indicates our method is not sensitive to norm selection. Impact of the Bias Term in the FC Layer. We studied the impact of the bias term. With the bias term, the proposed method still calculates the scale factor ? by Eq. <ref type="formula" target="#formula_14">(7)</ref> based on the weight information and applies it to the output logits for new classes. In other words, the scalar factor ? obtained from weight information is used in both the weight term and the bias term in the FC layer. We compare our method with or without using the bias term in the FC layer. <ref type="figure" target="#fig_3">Figure 6</ref> (c) shows the results. We see that the bias term in the FC layer can only influence the performance slightly.</p><p>Impact of Exemplar Selection Strategies. We investigated the impact of exemplar selection strategies. Random selection and herding selection are considered. <ref type="figure" target="#fig_3">Figure 6 (d)</ref> shows the results. We see that the exemplar selection strategies can only influence the performance slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The goal of class incremental learning is to obtain desirable results on new data, at the same time, retain the previous learned experiences. In this paper, we investigated catastrophic forgetting in class incremental learning. We demonstrated the actual role of knowledge distillation in this problem and the heavily biased weights in the FC layer. We proposed a simple and effective solution to address catastrophic forgetting that maintains the discrimination via knowledge distillation and maintains the fairness via a method called weight aligning. The experimental results on ImageNet-1000, ImageNet-100, and CIFAR-100 show that the proposed method achieves better performance than the previous methods. This work may suggest that there are many useful information hidden in the trained model that is worth exploring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Vanilla method for class incremental learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Norms of the weight vectors {w c }. (a) Results of the 1 st step (20 base classes), which does not correspond to class incremental learning; (b), (c), (d) and (e) are the results of the 2 nd , 3 rd , 4 th , 5 th incremental step respectively, which show the norms of the weight vectors of new classes are much larger than those of old classes. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Ours (CE + KD + WA) Confusion matrices (with entries transformed by log(1 + x) for better visibility) of different approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Class incremental learning performance (top-5 accuracy %) on ImageNet-100 for ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Error analysis on two parts of the test set. e(o), e(n) represent the number of old samples and new samples that are wrongly predicted, respectively. Specifically, error analysis for old samples is given in detail: e(o, n), e(o, n) stand for the number of old samples that are misclassified as new classes or other old classes, respectively.</figDesc><table><row><cell></cell><cell cols="2">e(n) e(o) e(o, n) e(o, o)</cell></row><row><cell>CE</cell><cell>314 5,360 4,027</cell><cell>1,333</cell></row><row><cell cols="2">CE + KD 383 5,326 4,314</cell><cell>1,012</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, the combined loss reduces the number of old samples that are misclassified to other old classes: 1,012 (CE + KD) vs 1,333 (CE). This is consistent with the original intention of knowledge distillation, that is, to keep the knowledge of old model. However, the prediction bias towards new classes is not alleviated: there are more old samples that are misclassified to new classes: 4,314 (CE + KD) vs 4,027 (CE). Why dose the model trained with the distillation loss become more serious towards new classes? After revisiting the distillation loss, we find the cost of misclassifying old samples to new</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Legend:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>for old classes</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>for new classes</cell></row><row><cell></cell><cell>CNN</cell><cell>W</cell><cell>WA</cell></row><row><cell>new classes</cell><cell>old classes</cell><cell>?</cell></row><row><cell cols="2">Maintaining Discrimination</cell><cell></cell><cell>Maintaining Fairness</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Class incremental learning performance (top-1 accuracy %) on CIFAR-100 with 5 incremental steps and 20 classes per step. The gains on the basis of Variation1 are also reported in parentheses. The upper bound performance is obtained with all training data for all classes. The average results over all the incremental steps except the first step are also reported here (as the first step does not related to class incremental learning actually). The best results are in bold.</figDesc><table><row><cell>#classes</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>Average</cell></row><row><cell>Variation1 (CE)</cell><cell>83.5 70.7</cell><cell>58.2</cell><cell>49.2</cell><cell>43.3</cell><cell></cell><cell>55.3</cell></row><row><cell>Variation2 (CE + WA)</cell><cell cols="6">83.5 74.3 (+3.6) 64.0 (+5.8) 56.9 (+7.7) 50.8 (+7.5) 61.5 (+6.2)</cell></row><row><cell>Variation3 (CE + KD)</cell><cell cols="6">83.5 72.8 (+2.1) 60.1 (+1.9) 49.9 (+0.7) 42.9 (-0.4) 56.4 (+1.1)</cell></row><row><cell cols="7">Variation4 (CE + KD + WNL) 83.1 72.3 (+1.6) 61.6 (+3.4) 53.1 (+3.9) 46.0 (+2.7) 58.2 (+2.9)</cell></row><row><cell>Ours (CE + KD + WA)</cell><cell cols="6">83.5 75.5 (+4.8) 68.7 (+10.5) 63.1 (+13.9) 59.2 (+15.9) 66.6 (+11.3)</cell></row><row><cell>Upper Bound</cell><cell></cell><cell></cell><cell cols="2">70.1</cell><cell></cell><cell></cell></row><row><cell cols="3">ILSVRC 2012 is a large-scale dataset with 1,000 classes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">that includes about 1.2 million images for training and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">50,000 images for validation. CIFAR-100 consists 32 ? 32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">pixel color images with 100 classes. It contains 50,000 im-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ages for training with 500 images per class, and 10,000 im-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ages for evaluating with 100 images per class.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Our method are implemented with Pytorch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Class incremental learning performance (top-5 accuracy %) on ImageNet (1,000 classes and 100 classes) with 10 incremental steps. The performance at the last incremental step and the average results over all the incremental steps except the first step are reported here. The results of the compared methods are reported in the original papers. The best results are in bold.</figDesc><table><row><cell>#classes</cell><cell></cell><cell>1000</cell><cell></cell><cell>100</cell></row><row><cell></cell><cell cols="4">Last Average Last Average</cell></row><row><cell cols="2">LwF.MC [20, 26] 24.3</cell><cell>42.5</cell><cell>36.6</cell><cell>60.7</cell></row><row><cell>iCaRL [26]</cell><cell>44.0</cell><cell>60.8</cell><cell>63.8</cell><cell>81.8</cell></row><row><cell>EEIL [5]</cell><cell>52.3</cell><cell>69.4</cell><cell>80.2</cell><cell>89.2</cell></row><row><cell>BiC [32]</cell><cell>73.2</cell><cell>82.9</cell><cell>84.4</cell><cell>89.8</cell></row><row><cell>IL2M [3]</cell><cell>-</cell><cell>78.3</cell><cell>-</cell><cell>-</cell></row><row><cell>RPS [25]</cell><cell>-</cell><cell>-</cell><cell>74.0</cell><cell>86.6</cell></row><row><cell>Ours</cell><cell>81.1</cell><cell>85.7</cell><cell>84.1</cell><cell>90.2</cell></row><row><cell>Upper Bound</cell><cell></cell><cell>89.1</cell><cell></cell><cell>95.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Class incremental learning performance (top-1 accuracy %) on CIFAR100 with 2, 5, 10 and 20 incremental steps. The average results over all the incremental steps except the first step are reported. The best results are in bold.</figDesc><table><row><cell>#incremental steps</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>LwF.MC [20, 26]</cell><cell cols="4">52.6 47.1 39.7 29.7</cell></row><row><cell>iCaRL [26]</cell><cell cols="4">62.0 63.3 61.6 59.7</cell></row><row><cell>EEIL [5]</cell><cell cols="4">60.8 63.7 63.6 63.4</cell></row><row><cell>BiC [32]</cell><cell cols="4">64.9 65.1 63.5 62.1</cell></row><row><cell>Ours</cell><cell cols="4">65.1 66.6 64.5 62.6</cell></row><row><cell>Upper Bound</cell><cell></cell><cell>70.1</cell><cell></cell><cell></cell></row><row><cell cols="3">proposed method can outperform BiC.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arxiv</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Il2m: Class incremental learning with dual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eden</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks : the official journal of the International Neural Network Society</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A unifying bayesian view of continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06494</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">One-shot face recognition by promoting underrepresented classes. ArXiv, abs/1707.05574</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reevaluating continual learning scenarios: A categorization and case for strong baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Costsensitive learning of deep feature representations from imbalanced data</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<editor>Salman Hameed Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous Ahmed Sohel, and Roberto Togneri</editor>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3573" to="3587" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation -Advances in Research and Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
		<idno>abs/1710.10628</idno>
	</analytic>
	<monogr>
		<title level="j">Variational continual learning. ArXiv</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Random path selection for incremental learning. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1704.06369</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incremental classifier learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/1802.00853</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiimbalance: An open-source software for multi-class imbalance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjun</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enislay</forename><surname>Ramentol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baojun</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamido</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="143" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">In defense of fully connected layers in visual representation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Class-incremental learning via deep model consolidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serafettin</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
		<idno>abs/1903.07864</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Label mapping neural networks with response consolidation for class incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baile</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lekun</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Furao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingwei</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1905.07835</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">M2kd: Multi-model and multilevel knowledge distillation for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01769</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
