<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Deep GNNs: Rethinking Residual Connection in GNNs from a Path Decomposition Perspective for Preventing the Over-smoothing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>2junbin.gao@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Discipline of Business Analytics</orgName>
								<orgName type="department" key="dep2">Business School</orgName>
								<orgName type="institution" key="instit1">The University of Sydney</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pu</surname></persName>
							<email>jianpu@fudan.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Deep GNNs: Rethinking Residual Connection in GNNs from a Path Decomposition Perspective for Preventing the Over-smoothing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of GNNs degrades as they become deeper due to the oversmoothing. Among all the attempts to prevent over-smoothing, residual connection is one of the promising methods due to its simplicity. However, recent studies have shown that GNNs with residual connections only slightly slow down the degeneration. The reason why residual connections fail in GNNs is still unknown. In this paper, we investigate the forward and backward behavior of GNNs with residual connections from a novel path decomposition perspective. We find that the recursive aggregation of the median length paths from the binomial distribution of residual connection paths dominates output representation, resulting in over-smoothing as GNNs go deeper. Entangled propagation and weight matrices cause gradient smoothing and prevent GNNs with residual connections from optimizing to the identity mapping. Based on these findings, we present a Universal Deep GNNs (UDGNN) framework with cold-start adaptive residual connections (DRIVE) and feedforward modules. Extensive experiments demonstrate the effectiveness of our method, which achieves state-of-the-art results over non-smooth heterophily datasets by simply stacking standard GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Graph Convolutional Networks (GCNs) have been successfully applied to a wide range of graph applications, including social networks <ref type="bibr" target="#b40">[41]</ref>, traffic prediction <ref type="bibr" target="#b8">[9]</ref>, knowledge graphs <ref type="bibr" target="#b31">[32]</ref>, drug reaction <ref type="bibr" target="#b9">[10]</ref> and recommendation system <ref type="bibr" target="#b15">[16]</ref>. The behavior of most GNNs is similar to a Laplacian smoother <ref type="bibr" target="#b24">[25]</ref> or low-pass filter <ref type="bibr" target="#b29">[30]</ref>, which learns node representation by recursively aggregating neighbor information. Despite the remarkable success, such a recursive smoothing process of GNNs usually results in the over-smoothing problem, i.e., all node representations will converge to indistinguishable <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>The following two issues are crucial among the limitation of GNNs caused by over-smoothing. First, the performance of GNNs usually degenerates when more graph convolution operations are applied <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47]</ref>. As a result, recent GNNs often use shallow architectures (e.g., 2 or 3 stacked convolution operations) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39]</ref>, which ignore distant connections and can only exploit local structural information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>. Second, in some heterophily graphs that contain non-smooth signals, i.e., many connected nodes belonging to different classes, the performance of both shallow and deep GNNs is usually inferior to that of simple MLPs without neighbor aggregation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>One straightforward way to alleviate over-smoothing is to use the residual connection, as in ResNet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Unfortunately, researchers found that simple residual connections only partially relieve the over-smoothing problem, and the performance of the model still degrades as more layers are stacked <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The convergence of GNNs with residual connections has been analyzed using lazy random walk <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref> or the subspace perspective <ref type="bibr" target="#b30">[31]</ref>, and variants of JK connections <ref type="bibr" target="#b41">[42]</ref> and initial connections <ref type="bibr" target="#b4">[5]</ref> were proposed for training DeepGNNs. However, as those convergence analysis of residual connections in GNNs ignores the optimization process of the weight matrix, their convergence conditions are usually not guaranteed in real applications <ref type="bibr" target="#b7">[8]</ref>, and thus it cannot be well explained why residual connections fail for GNNs.</p><p>In this paper, we first present a novel forward and backward analysis of the stacking graph convolution layer with various kinds of skip connections, as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, from a path decomposition perspective. This perspective views GNNs with skip connections as a collection of many propagation paths instead of a single deep network. To explain why residual connections fail in DeepGNNs, we systematically analyze the behavior from both the forward path decomposition and backward gradient optimization. Then, we find that: 1) Due to the recursive aggregation process, the binomial distribution of residual connection paths causes the medium-length path to dominate the output representation, which results in over-smoothing when the GNNs become deeper. 2) The weight matrix in each graph convolution layer may provide a path selection ability. Compared with the SGC with residual connections, the ResGNNs can deactivate longer paths in the ideal scenario by adequately setting the weight matrix to learn identity mapping. 3) However, unlike ResNet, the entanglement of the propagation matrix and weight matrix would cause the phenomenon of gradient smoothing, which would prevent the GNNs with residual connections from optimizing to identity mapping. Therefore, we speculate that the reason why the residual connections fail to learn identity mapping in DeepGNNs lies in the optimization process of the weight matrix.</p><p>Building on the aforementioned analysis from the path decomposition view, we propose a simple yet effective colD staRt adaptIVe rEsidual (DRIVE) connection for GNNs to prevent over-smoothing and effectively scale up the depth. It multiplies a trainable coefficient ? to the graph convolution unit at each residual connection to control the smoothness. Moreover, we impose the direct pass identity mapping for the entire network at the beginning by initializing ? with 0. This would lead to favorable forward and backward propagation properties, i.e., prevent over-smoothing at initialization, which can solve the gradient smoothing problem and accelerate model training. Furthermore, we propose a novel Universal Deep GNNs (UDGNN) framework that applies the DRIVE connection on both the GraphConvolution and the following FeedForward module. The UDGNN framework is GraphConv agnostic and can give the freedom of any standard GNNs to learn identity mapping or DeepMLP mapping and become deeper and more powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Oversmooth Analysis. Recently, several works have attempted to understand and alleviate oversmoothing in GNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. The Markov process of simplified GNNs without weight matrices was studied in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>, which theorized that the stacking propagation matrix would converge to the eigenvectors associated with degree information. According to <ref type="bibr" target="#b4">[5]</ref>, the residual connection without weight matrices replicates a lazy random walk, converges to a stationary state, and leads to over-smoothing. Furthermore, the subspace theorem for the graph convolution with weight matrix was proposed and showed that the node representation would exponentially converge to the subspace, and the residual connection only plays the role of slowing the convergence rate <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref>. Other works propose Dirichlet energy-based analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref> and obtain a similar result to the subspace theorem. However, those analyses mainly focus on the forward behavior of GNNs and do not well explain the reason why the residual connection fails for DeepGNNs.</p><p>Decouple GNNs. Most literature reports that the speed of degeneration of GCN is faster than SGC <ref type="bibr" target="#b39">[40]</ref> without weight matrix and hypothesizes that the entanglement of transformation and propagation significantly compromises the performance of DeepGNNs <ref type="bibr" target="#b26">[27]</ref>. Hence, the decouple GNNs are proposed to separate the propagation and transformation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>, allowing for broader propagation. However, they are not real deep models but only enhance the nodes' perceptual field by multiple hops. In addition, those designs are not easily compatible with standard graph convolutions.</p><p>Skip connection in GNNs. Several works apply skip connections to relieve the over-smoothing issue, including: 1) residual connection with dilated-conv or message normalization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, 2) dense or JK connection to combine all previous layers' representations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50]</ref>, and 3) initial connection with the initial node embedding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref>. However, directly introducing plain residual connections without regularization does not work for DeepGNNs, and the dense-like connection requires large memory usage. Instead, the initial connection is widely used in the design of GNNs to alleviate over-smoothing. Therefore, by investigating the difference in behavior between the initial and residual in a fundamental way, we propose a novel DRIVE connection for DeepGNNs without modifying the GraphConv kernel or applying normalization and regularization techniques designed for graphs.</p><p>Regularization in GNNs. Several works propose training regularization techniques for graphs to alleviate over-smoothing. On the one hand, analogous to BatchNorm <ref type="bibr" target="#b18">[19]</ref>, the PairNorm <ref type="bibr" target="#b45">[46]</ref>, Node-Norm <ref type="bibr" target="#b48">[49]</ref>, MessageNorm <ref type="bibr" target="#b22">[23]</ref>, and GroupNorm <ref type="bibr" target="#b46">[47]</ref> adjust the statistics over graphs to slow down the over-smoothing. On the other hand, borrowing the idea of dropout <ref type="bibr" target="#b35">[36]</ref>, DropEdge <ref type="bibr" target="#b34">[35]</ref>, DropNode <ref type="bibr" target="#b11">[12]</ref>, and SkipNode <ref type="bibr" target="#b27">[28]</ref> introduce the randomly dropped techniques into graphs to alleviate over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation and Problem Setting</head><p>Consider an undirected graph G = (V, E), with N nodes and m edges. We use {1, . . . , N } to denote the node index of G, wheres d j denotes the degree of node j in G. Let A ? R N ?N be the adjacency matrix and D ? R N ?N be the diagonal degree matrix. Each node is given a d-dimensional feature representation x i and a c-dimensional one-hot class label y i . The feature inputs are then formed by X = [x 1 , ? ? ? , x N ], and the labels are Y = [y 1 , ? ? ? , y N ]. Given the labels Y L of the nodes L ? V, the task of node classification is to predict the labels Y U of the unlabeled nodes U = V \ L by exploiting the graph adjacency matrix A and the features X corresponding to all the nodes. In addition, given a set of node labels Y over graphs, the notion of homophily and heterophily indicates the smoothness of the signal of the label. If connected nodes tend to have the same class, the graphs correspond to high homophily and low heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Neural Networks</head><p>The general GNN is composed of information aggregation and feature transformation <ref type="bibr">[CN,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. For the l-th layer of a GNN, we use H l to represent the embedding of nodes and H 0 to represent the initial feature X or a projection of X for dimension reduction. Then, the general l-th layer Graph Convolution can be formulated as</p><formula xml:id="formula_0">H l = GraphConv(A, H l?1 ) = ?(P H l?1 W l ),<label>(1)</label></formula><p>where the propagation matrix P achieves information aggregation from neighbors, and the weight matrix W completes feature transforms. The propagation matrix P is usually calculated based on the adjacent matrix A and degree matrix D as in GCN <ref type="bibr" target="#b19">[20]</ref>, SGC <ref type="bibr" target="#b39">[40]</ref> and GraphSAGE <ref type="bibr" target="#b12">[13]</ref> or the  attention mechanism in GAT <ref type="bibr" target="#b38">[39]</ref>. Moreover, various GNNs remove the nonlinear ReLU activation ? and weight matrix W also achieve comparable performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>. Please refer to the review for more details <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of DeepGNNs from the Path Decomposition View</head><p>To better understand the role of skip connections in GNNs and explain why residual connections fail, we develop a first path decomposition perspective to study the forward and backward behavior of GNNs with different skip connections. In contrast to previous studies, we show that the GNNs with residual connections have more expressive capacity than initial connections in the forward analysis, and the weight matrix plays an important role. In the backward analysis, we find that the gradient smoothing problem causes the optimization difficulty and prevents the residual connection from learning the identity mapping to avoid over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Unified Decomposition View of GNNs with Skip Connections</head><p>Inspired by the ensemble view of ResNet <ref type="bibr" target="#b37">[38]</ref>, we decompose the path for GNNs from an ensemble view. We start by GNNs with residual and initial connections with three building blocks from inputs H 0 to H 3 and illustrate the decomposition tree of the path in <ref type="figure" target="#fig_1">Figure 2</ref>. For simplicity, we ignore the non-linear activation function ReLU since it does not influence the performance of GNNs as reported in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>. The L layers GNNs with skip connections can be seen as ensembles paths of different lengths (from 0 to L), and the length of each individual path is equivalent to the propagation times of graph convolutions. Therefore, we can formalize the general path decomposition of GNNs with skip connections as follows. Theorem 1. (Path decomposition of GNNs with skip connection) The output of a depth L GNNs with skip connection and input H 0 is given by</p><formula xml:id="formula_1">H L = path?P ath P path H 0 W path ,<label>(2)</label></formula><p>where P path = P L path ...P 1 path is the product of the propagation matrix P in GNNs, and W path = W 1 path ...W L path is the weight matrix. We have P l path = P and W l path = W l if unit l selects the graph convolution operator in the current path; otherwise, they would degenerate to an identity matrix for the shortcut connection.</p><p>Note that in the ensemble view of ResNet, most paths are assumed to be positive for the final prediction since they represent the multi-scale resolution feature for one sample (images). However, due to the stacking of the propagation matrix P, the longer path in GNNs may contain an exponential number of neighbors' information and cause over-smoothing, which may hurt the discrimination ability of samples (nodes on the graph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Forward Inference Path Decomposition</head><p>Forward Path of standard GNNs. We consider the path of SGC without the weight matrix W and the GCN with W . Obviously, the SGC and GCN only have a single path, and due to the stacking of propagation matrix P, the node representation tends to be over-smoothed when the length of the path increases.</p><formula xml:id="formula_2">(SGC): H L = P H L?1 = P L H 0 , (GCN): H L = P H L?1 W l = P L H 0 L l=0 W l .<label>(3)</label></formula><p>According to previous studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. For the SGC, the stacking exponential of propagation matrix P is similar to the Markov process and may inevitably result in the node embedding converging to the degree eigenvector, which causes the node to be indistinguishable. The GCN, which stacks the transformation matrix W on the right side, also exponentially converges to over-smoothing according to the subspace theorem.</p><p>Forward path of GNNs with residual connections. When considering the forward path decomposition for the residual connections, each unit can choose to propagate neighbor information or pass through the short cut. Therefore, the length of the path follows the binomial distribution, and the medium-length paths contribute noticeably, i.e., these paths would become over-smoothed when increasing graph convolution layers and dominate the last layer representation.</p><formula xml:id="formula_3">(SGC w/residual): H L = P H L?1 + H L?1 = (P + I) L H 0 = L l=0 L l P l H 0 ,<label>(4)</label></formula><formula xml:id="formula_4">(GCN w/residual): H L = P H L?1 W l + H L?1 = path?P ath P path H 0 j?path W j . (5)</formula><p>Recently, as reported in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>, the behavior of (P + I) L in SGC is equivalent to a lazy random walk, which eventually converges to the stationary state and thus leads to over-smoothing. However, they omit the influence of the weight matrix W . On the other hand, the convergence proof of the GCN with residual connection and W was provided in <ref type="bibr" target="#b30">[31]</ref>. However, since the upper bound of the convergence rate is larger than 1, there is no guarantee that expressive power will be lost <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recall the motivation behind ResNet is the ability of helping the network in optimizing W for identity mapping. For the GCN with weight matrix W , consider the following scenario: if all the weight matrix W can learn to be zero, the whole routes degenerate to a single pass-through from inputs to output, which can achieve the identity mapping and thus prevent over-smoothing induced by the graph convolution. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), compared with the SGC, the residual connection significantly improves the performance of GCN on Cora-Random, which means that the optimization of weight matrix W can indeed achieve identity mapping in shallow layers. However, performance degradation of GNNs with residuals occurs even with the normalization technique <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>, as the number of layers rises. Therefore, we postulate that the character of the DeepGNNs, namely the successive entanglement of P and W , causes the optimization difficulty for identity mapping. We will further discuss this point in Section 4.3.</p><p>Forward path of GNNs with initial connection. The initial connection explicitly combines the initial node embedding with each layer to preserve the distinguishability of nodes. Moreover, unlike residual connections, the length of paths follows a uniform distribution. Therefore, the smooth representations of the longer paths cannot easily dominate the final output when increasing the number of layers.</p><formula xml:id="formula_5">(SGC w/initial): H L = P H L?1 + H 0 = L l=0 P l H 0 (6) (GCN w/initial): H L = P H L?1 W L + H 0 = L l=0 P l H 0 L i=L?l W i<label>(7)</label></formula><p>However, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the paths of initial connections are the subset of residual connections, which implies that the residual connection may has more expressive capacity. Moreover, in the domain of other deep architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37]</ref>, the residual connection is also more widely used than the initial connection. In the following, to unlock the potential of residual connections, which is limited by the forward over-smoothing, we will investigate the backward gradient for the optimization of residuals in GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Gradient Smoothness Analysis from Backward Path Decomposition</head><p>Here, we give another analysis from backward path decomposition to investigate the reason why the residual connections fail in GNNs. Since the optimization of parameter W in ResNet is the key to learning the identity mapping, we take the initial step of analyzing the gradients in terms of parameters W and H. Theorem 2. (Backward gradient path decomposition of GNNs with skip connection). Given the training cross-entropy loss L, the gradient in graph convolution with respect to parameter W l is: </p><formula xml:id="formula_6">?L ?W l = P H l?1 ? l??L path?P ath P path ? ?L ?H L ? W path ,<label>(8)</label></formula><p>where ?L ?H L indicates the initial gradient of the last layer. The details and proof of each GNN with skip connections can be found in the Appendix.</p><p>Note that the backward gradients are backpropagated through the neighborhood aggregation (P ) and feature transformation (W ), which are similar to the forward inference process. Specifically, the initial gradient signal would be smoothed due to the smoothness representation H L . Then, the product of P s on the paths further smooths the gradient and causes the structural information loss of the gradient. Therefore, it was difficult for GNNs with residual connections to be optimized appropriately. Next, we show the gradient path of the GCN, ResGCN and InitGCN.</p><p>Gradient Path of GCN (with/without) initial connection. The backward gradient of GCN layers with or without an initial connection is the same, which implies that the W in the shallow layer of the initial connection is also difficult to optimize.</p><formula xml:id="formula_7">?L ?W l = H l?1 P L?l+1 ?L ?H L ? ( L j=l+1 W j ) .<label>(9)</label></formula><p>Gradient Path of GCN with residual connection. The backward gradient of the GCN with residual connections is analogous to the forward inference. The gradient of shallow layers is a collection of longer paths, which would also be smoothed.</p><formula xml:id="formula_8">?L ?W l = P H l?1 ? l??L path?P ath P path ?L ?H L ( j?path W j ) .<label>(10)</label></formula><p>To show the evolution of the gradient smoothness during training, we utilize the von Neumann entropy <ref type="bibr" target="#b1">[2]</ref> to calculate the spectral information of the gradient matrix and visualize it in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>We can see that the evolution of GCN and GCN+Init is similar, i.e., the structural information is lost at shallow layers, which verifies the backward gradient in Eq <ref type="bibr" target="#b8">(9)</ref>. However, the superhighway from the initial node embedding to deeper layers maintains the performance of the initial connection. For the residual connection, we first notice that, compared with FFN+Res, the gradient of GCN+Res also loses structural information. This causes difficulty in the optimization of GCN+Res. Therefore, the gradient smoothness problem and optimization difficulty for W are the keys to the performance degeneration for GNNs with skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Universal Deep GNNs Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DRIVE Connections for Anti-Oversmoothing</head><p>According to our analysis in Section 4.2, the optimization difficulty is the key challenge for GNNs with residual connections to learn identity mapping and prevent over-smoothing. On the one hand, from the forward perspective, at the initial stage of training, the output H of GNNs with the residual connection would be dominated by the over-smoothed longer path and cause the final representation to be indistinguishable, which also causes the initial gradient smoothing. On the other hand, from the backward perspective, the gradient becomes smoother due to the collection of stacking propagation matrix paths.</p><formula xml:id="formula_9">H l+1 = H l + ? l * GraphConv(A, H l )<label>(11)</label></formula><p>To overcome the over-smoothing of the output and gradient, we propose colD staRt adaptIVe rEsidual (DRIVE) connections that initialize all graph convolution layers as the identity mappings, using additional learned parameters ? l for each layer with zero initialization. The cold start initialization deactivates all neighbor information propagation paths at the beginning of GNNs to prevent output and gradient smoothing. Moreover, it adaptively controls the smoothness weight to fit the downstream task during training, e.g., to deal with the homophily or heterophily graph. This behavior is similar to freezing the GraphConv at the initial stage and then warming it up to control smoothness during the following training, thus, termed cold-start adaptive residual. In the experimental part, we also visualize how the magnitude of the parameter ?s evolve in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">UDGNN with FeedForward Module</head><p>Generally, in traditional deep learning, stacking multiple feature transformations can better learn the representative features and fit the data distribution <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>. However, current GNNs always entangle the propagation with transformation, which may only achieve the goal of the token mixer to some extent. Inspired by the network architectures design of transformers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>, we further introduce the channel FeedForward Module (FFN) with the proposed DRIVE connection after each graph convolution layer to increase the fitting power of GNNs. We combine these two into a Universal Deep GNNs framework (UDGNN) and use the UDGNN* to indicate the one without the FFN as Eq <ref type="bibr" target="#b10">(11)</ref>. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, where the encoder and decoder can be implemented with a linear layer, and the UDGNN is a conv-agnostic framework and can easily enhance the performance of GNNs in various datasets by stacking.</p><formula xml:id="formula_10">H 0 = Encoder(X), Y = Decoder(H L )<label>(12)</label></formula><formula xml:id="formula_11">M l = H l + ? l * GraphConv(A, H l )<label>(13)</label></formula><formula xml:id="formula_12">H l+1 = M l + ? l * FFN(M l )<label>(14)</label></formula><p>6 Experiments</p><p>In this section, we aim to answer the following questions. RQ1: Can UDGNNs improve the performance of the standard GNNs on different kind of datasets? RQ2: Compared with other techniques, does the DRIVE connection help train DeepGNNs? RQ3: How does the behavior of the residual weight ? and ?. More experimental results can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and experimental settings</head><p>Datasets. We evaluate the performance on eleven well-known real-world datasets, which are summarized in the Appendix, including four homophily datasets and seven heterophily datasets. For all benchmarks, we use the feature vectors, class labels, and standard data splits from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Baselines. We compare our method with the following baselines: (1) Classical GNNs: GCN <ref type="bibr" target="#b19">[20]</ref>, GAT <ref type="bibr" target="#b38">[39]</ref> and GraphSAGE <ref type="bibr" target="#b12">[13]</ref>; The basic MLP and the first heterophily GNNs GEOM-GCN <ref type="bibr" target="#b32">[33]</ref>;</p><p>(2) recent state-of-the-art decoupling GNNs for tackling heterophily and oversmooth: MixHop <ref type="bibr" target="#b0">[1]</ref>, GPRGNN <ref type="bibr" target="#b6">[7]</ref>, APPNP <ref type="bibr" target="#b6">[7]</ref>, DAGNN <ref type="bibr" target="#b6">[7]</ref>, (3) recent state-of-the-art DeepGNNs with skip connections: FAGCN <ref type="bibr" target="#b2">[3]</ref>, GCNII <ref type="bibr" target="#b4">[5]</ref>; DeeperGNNs <ref type="bibr" target="#b21">[22]</ref>, JKNet <ref type="bibr" target="#b41">[42]</ref>, H2GCN <ref type="bibr" target="#b49">[50]</ref>, For ease of comparison, we copy the reported results in literature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref>. Moreover, for the missing results under these splits, we rerun their released code with the best hyper-parameter over 10 times.</p><p>Parameters Setting. We adopt the same settings of the default hyper-parameters for UDGNNs and corresponding baselines (GCN, GAT and GraphSAGE), i.e., 128 hidden dimensions for OGB datasets and 64 hidden dimensions for others. We report the best best performance of UDGNNs between 2-64 layers for each dataset. For other baselines, we use their best default parameters and layers reported in the original papers. The detailed information is reported in the Appendix.  <ref type="table" target="#tab_2">Table 2</ref> provide the accuracy of different GNNs on the node classification task over both homophily and heterophily datasets. We report the best performance of each model across different layers and summarize the following observation: 1) The performance of most GNNs on the homophily dataset is similar. They all outperform MLP models since the smooth process of graph convolution is helpful for these datasets with smooth signals. Moreover, our UDGNN can maintain the performance of the standard GNNs. 2) Due to the over-smoothing, most GNNs are worse than MLP when dealing with non-smooth heterophily datasets. Although decoupling designs can improve the performance of GNNs under heterophily to some extent, they remain shallow models without deep feature transformation and cannot achieve optimal results. 3) Compared with other SOTAs with skip connections and specific architectural modifications, our UDGNN* with just a simple drive connection can strongly improve the performance of the standard GNNs and achieve new state-of-the-art results. Notably, UDGNN* GCN achieves strong results on squirrel and chameleon datasets and outperforms previous state-of-the-art methods by a large margin, e.g., greater than 70% accuracy. Moreover, the UDGNN GCN achieves the best average accuracy of 76.47% on nine datasets and outperforms others on the OGB datasets, which demonstrate the effectiveness of our framework.</p><p>The strong performance of the UDGNN reveals a surprising conclusion: we do not need specific architectural modifications for difficult heterophily datasets. Instead, just introducing a learnable parameter to the residual connection and allowing the GNNs to go deeper can well solve the oversmoothing and heterophily problem. Therefore, our proposed UDGNN could serve as a starting baseline for future GNN architecture designs.  <ref type="figure">Figure 5</ref>, we make three observations: 1) Both residual and initial connections can improve the performance on heterophily graphs at the shallow layer. Moreover, the GCN equipped with these connections outperforms the SGC counterparts since the weight matrices empower the GCN to learn identity mapping. 2) Compared with the residual connection, the ini-  tial connection significantly relieves over-smoothing. However, the results in the heterophily graphs are suboptimal, especially for the SGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies (RQ2)</head><p>3) The drive connection can successfully drive the deep graph residual to help the GCN and SGC effectively scale up the depth. Moreover, it achieves the best performance over heterophily graphs, and the FFN component slightly enhances performance on some datasets. Notice that we do not apply any normalization or regularization techniques designed for graphs. This result suggests that the drive connection can solve the problem of oversmoothing and successfully train DeepGNNs to fit various smoothnesses of graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visualization (RQ3)</head><p>To further understand how the weights ? l and ? l are learned in the training process. We visualize their dynamics of the 64 layers UDGCN using the Cora and Cora-Random (fully connected graph) datasets in <ref type="figure">Figure 6</ref>. Notice that we use the cold-start to initialize ? l and ? l to help the model learn the non-smooth node representation by preventing information aggregation from neighbors. Then, in the training process, it automatic learns whether or not to aggregate information from each node's neighbors. As shown in <ref type="figure">Figure 6</ref>, |? l | remains a small value to maintain the identity mapping and discard the over-smoothed neighbor information for Cora-Random. In contrast, |? l | of Cora dataset grows quickly for most layers. For the other parameter |? l |, since it controls the feature transformation of nodes individually, it is frozen initially and then grows quickly regardless of the neighbor information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we have examined the forward and backward behavior of GNNs with residual connections from a novel path decomposition perspective. We discovered that the gradient smoothing problem in the backward gradient optimization of GNNs with residual connections prevents the model from optimizing to learn the identity mapping. In addition, we presented a framework for Universal Deep GNNs that provides standard GNNs with DRIVE connections for GraphConv and FeedForward module to become deeper and more powerful. Extensive experiments validate the ef-fectiveness of our framework on homophily and heterophily graph datasets. Our proposed UDGNN may serve as a starting baseline for the architecture design of future GNNs.</p><p>Organization. In Section A, we provide the details of the data statistics. In Section B, we provide the implementation details and hyper-parameters of UDGNNs. In Section C, we provide the proofs of Theorem 1 and Theorem 2, respectively. In Section D, we provide more empirical results for the path decomposition view to illustrate the optimization difficulty and gradient smoothing. In Section E, we provide additional experiments to validate the effectiveness of the proposed DRIVE connection and the UDGNN framework. In Section F, we provide a discussion of limitations and potential negative impacts. The code will be available at:</p><p>https://github.com/JC-202/UDGNNs.  <ref type="bibr" target="#b32">[33]</ref>: Chameleon and Squirrel are web pages extracted from different topics in Wikipedia. Similar to WebKB, nodes and edges denote the web pages and hyperlinks among them, respectively, and informative nouns in the web pages are employed to construct the node features in the bag-of-word form. Webpages are labeled in terms of the average monthly traffic level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Statistics</head><p>-Actor <ref type="bibr" target="#b32">[33]</ref>: The actor network contains the co-occurrences of actors in films, which are extracted from the heterogeneous information networks. It describes the complex relationships among films, directors, actors and writers. In this network, nodes and edges represent actors and their co-occurrences in films, respectively. The actor's Wikipedia page is used to extract features and node labels. -Arxiv-year <ref type="bibr" target="#b25">[26]</ref>: Modifying node labels of the Arxiv dataset to the year of paper, and the goal is to predict the year of paper publication that allows for evaluation of GNNs in large scale non-homophilous settings. -Cora-Random: We construct the extremely heterophily scenario by connecting all nodes on the Cora dataset. We use the semi-supervised limited labeled data split in <ref type="bibr" target="#b19">[20]</ref> to enhance the supervision difficulty. In the Cora-Random dataset, since the neighbor of each node provides no helpful information for classification, the standard graph convolution may cause over-smoothing rapidly due to the fully-connected property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Running environment: All the GNN models implemented in PyTorch, and tested on a machine with 24 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20 GHz processors, GeForce GTX-2080 Ti 11 GB GPU, NVIDIA A100 40 GB GPU, and 128 GB memory size.</p><p>Hyper-parameters: For UDGNNs, we do not add the self-loop of graphs and employ the Adam optimizer and select the learning rate ? {0.001, 0.01, 0.05}, weight decay ? {0.00005, 0.0005} and dropout rate ? {0, 0.5} based on the validation sets. For other models, we utilize their best default parameters in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem</head><p>Proof of Theorem 1 Theorem. (Path decomposition of GNNs with skip connection) The output of a depth L GNN with skip connection and input H 0 is given by:</p><formula xml:id="formula_13">H L = path?P ath P path H 0 W path ,<label>(15)</label></formula><p>where P path = P L path ...P 1 path is the product of the propagation matrix P in GNNs, and W path = W 1 path ...W L path is the weight matrix. We have P l path = P and W l path = W l if unit l selects the graph convolution operator in the current path; otherwise, they would degenerate to an identity matrix for the shortcut connection.</p><p>Proof. For simplicity, we assume the input vector H 0 to be non-negative and remove the ReLU activation ? operation as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40]</ref>, then consider a three-layer GraphConv with a single path following from the Multiplication law that</p><formula xml:id="formula_14">H 3 = P ?(P ?(P H 0 W 0 )W 1 )W 2 = (P P P )H 0 (W 0 W 1 W 2 ) = P path H 0 W path .<label>(16)</label></formula><p>Then, the proof follows from the fact that the output of standard GNNs with skip connections is formed by the summation of all the individual paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2</head><p>Theorem. (Backward gradient path decomposition of GNNs with skip connection). Given the training cross-entropy loss L, the gradient in graph convolution with respect to parameter W l is:</p><formula xml:id="formula_15">?L ?W l = P H l?1 ? l??L path?P ath P path ? ?L ?H L ? W path ,<label>(17)</label></formula><p>where ?L ?H L indicates the initial gradient of the last layer.</p><p>Proof. Consider that the derivative L with respect to W l in GNNs with skip connections is:</p><formula xml:id="formula_16">?L ?W l = ?H l ?W l ?L ?H l = (P H l?1 ) ? ?L ?H l .<label>(18)</label></formula><p>According to the forward path decomposition, the output of a depth L GNN with skip connection and input H l is given by:</p><formula xml:id="formula_17">H L = l??L path?P ath P path ? H l ? W path<label>(19)</label></formula><p>Therefore, following the fact that the derivative of ?L ?H l is the summation of all the individual paths. we can calculate the ?L ?H l by the chain rule and stacking the propagation matrix P and the weight matrix W in the P path and W path as the following equation:</p><formula xml:id="formula_18">?L ?H l = l??L path?P ath P path ? ?L ?H L ? W path<label>(20)</label></formula><p>Combining with Equation <ref type="bibr" target="#b17">18</ref> and Equation 20 complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Empirical Study of Path Decomposition</head><p>In this section, we report additional empirical results to illustrate the correctness of our analysis based on path decomposition.</p><p>D.1 Forward Inference: Does residual connection help GraphConv learn the identity mapping? To investigate the ability to learn identity mapping, which is essential for the deep model to maintain performance as the shallow counterpart, we define the ConvRatio as the measurement of the difference between input and output for each layer. Compared with other vector similarity metrics, e.g., cosine similarity, the ConvRatio can capture the norm difference contributed by each graph convolution unit, which is important for identity mapping. The smaller of ConvRatio is, the better the identity mapping it learns.</p><formula xml:id="formula_19">ConvRatio l = N i=0 h l+1 i ? h l i 2 N ? h l+1 i<label>(21)</label></formula><p>Then, to show the ability to learn identity mapping of GraphConv with residual connection, we conduct experiments and visualize the evolution of ConvRatio over the Cora-Random dataset as in <ref type="figure" target="#fig_5">Figure 7</ref>. We can see that the residual connection can indeed help the shallow GNNs model (4,8, and 16 layers) learn identity mapping, e.g., the ConvRatio tends to be 0 and test accuracy tends to 60 (the optimal result of the MLP), but it failed in the deeper GNNs (64 layers). This indicates that the residual connection can help the GNNs learn the identity mapping, preventing over-smoothing in the fully-connected Cora-Random dataset. However, it is difficult for the residual connection to learn identity mapping when GNNs become deeper. This motivates us to hypothesize that the optimization difficulty for deeper GCN(Res) prevents it from avoiding over-smoothing. In contrast, our proposed DRIVE connection works well in the deeper layers by solving the optimization difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Backward Gradient: Gradient Smoothness Phenomenon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 von Neumann entropy for measuring the gradient matrix spectral information</head><p>Compared with over-smoothing for the node features, backward propagation would result in losing structural information of gradient matrix when the gradient is over-smoothed. Hence, to measure the gradient smoothness, we consider the information of the gradient matrix from the spectral domain. Inspired by the von Neumann entropy in quantum statistical mechanics <ref type="bibr" target="#b1">[2]</ref>, which extends the idea of entropy for positive definite symmetric matrices to measure the matrices' information. Specifically, suppose ? l 1 , ? l 2 , ..., ? l d denote singular values of weight matrix W l ? R d?d , we then normalize them so that d i=1 ? l i = 1, where i = 1, ..., d for index of singular values. The normalized von Neumann entropy of W l that represents gradient spectral information is computed by</p><formula xml:id="formula_20">von(W l ) = ? d i=1 ? l i log(? l i ) log(d)</formula><p>.</p><p>The above metric ranges from [0, 1] and can be used to quantify the spectral information and the smoothness of the gradient, i.e., a smaller of this metric indicates a smoother gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 Evolution of gradient spectral information for different skip connections</head><p>Then, to monitor the gradient smoothness during training, we visualize the evolution of the gradient spectral information in 64 layers of 5 variants under our framework, i.e., including GCN, GCN+Res, FFN+Res, GCN+Init and GCN+DRIVE.</p><formula xml:id="formula_22">H l+1 = ?(P H l W l ), 1.GCN<label>(23)</label></formula><formula xml:id="formula_23">H l+1 = H l + ?(P H l W l ), 2.GCN+Res<label>(24)</label></formula><formula xml:id="formula_24">H l+1 = H l + ?(H l W l ), 3.FFN+Res<label>(25)</label></formula><formula xml:id="formula_25">H l+1 = H 0 + ?(P H l W l ), 4.GCN+Init<label>(26)</label></formula><formula xml:id="formula_26">H l+1 = H l + ? l * ?(P H l W l ), 5.GCN+DRIVE<label>(27)</label></formula><p>From <ref type="figure">Figure 8</ref>, we have the following observations: (1) Due to stacking the propagation matrix P, the gradient of shallow layers for the GCN and GCN(Init) are over-smoothed over all datasets, e.g., the von Neumann entropy tends to 0. However, the performance of GCN(Init) is strongly better than that of GCN under 64 layers, which indicates that the GCN(Init) does not suffer the over-smoothed of features. The reason is that the initial node features directly pass through to the deeper layer, and the gradient of the deeper layer is informative, which would adequately classify the nodes according to the initial features. Therefore, the initial connection can make the behavior of the DeepGCN analogous to the shallow model and maintain performance.</p><p>(2) For the GCN(Res), compared to the grad spectral information with the FFN(Res), the gradient of GCN(Res) is also over-smoothed due to the stacking of propagation matrix P . Moreover, compared to the performance with the GCN(Init), the binomial distribution of the longer path dominates the final representation. The oversmoothed features and over-smoothed gradient cause the identity mapping to be hard to optimize and performance degeneration when increasing layers. <ref type="formula" target="#formula_2">(3)</ref> The gradient evolution of the DRIVE connection is as informative as the FFN(Res), indicating that the DRIVE connection can effectively solve the gradient smoothness problem. Therefore, GCN(DRIVE) achieves the best performance and converges quickly over all datasets.</p><p>All these results verify the backward path decomposition gradient analysis. Unlike the previous over-smoothing study, we show that the gradient smoothness problem is the key factor why the residual connection can not help DeepGNNs optimize to learn identity mapping properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Experiments of UDGNNs</head><p>In this section, we report more empirical results to show the effectiveness of our Universal Deep GNN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Comparison of SOTA with different skip connections in terms of layer</head><p>We compare the performance between state-of-the-art DeepGNNs <ref type="figure" target="#fig_1">(GCNII, H2GCN, GPRGNN)</ref> and UDGNNs when increasing layers.</p><p>? GCNII <ref type="bibr" target="#b4">[5]</ref> combines the initial connection and identity mapping by explicitly modifying the graph convolution kernel to overcome over-smoothing and achieve state-of-the-art per- formance on homophily datasets. However, the constant weight parameter for initial connection and identity mapping would restricts the performance on heterophily datasets.</p><p>? H2GCN <ref type="bibr" target="#b49">[50]</ref> proposes three designs with separate ego and neighbors, high-order neighbors, and a combination of intermediate representations by JK connection.</p><p>? GPRGNN <ref type="bibr" target="#b6">[7]</ref> decouples the propagation and transformation in graph convolution and learns an arbitrary polynomial graph filter to incorporate multi-scale information by the adaptively generalized PageRank.</p><p>From <ref type="figure" target="#fig_8">Figure 9</ref>, we have the following observations: (1) The performance of GCN and GraphSAGE drops rapidly as the number of layers grows. (2) GCNII, H2GCN, and GPRGNN are robust to the over-smoothing, especially on the homophily Cora dataset. However, their performance is not optimal in the heterophily scenario. The reason may be that the non-smooth heterophily may tend to over-smoothing even at shallow layers, which requires the model to adaptively control the smoothness.</p><p>(3) Compared with the other DeepGNNs, our UDGNN can help classical GNNs (GCN and GraphSAGE) become robust to the over-smoothing on both homophily and heterophily datasets. Moreover, due to the smoothness control ability, the UDGNN framework can well solve the heterophily datasets, e.g., increase the accuracy when stacking more layers on the Squirrel dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Effect of variants of normalization in residual connection</head><p>To show the effectiveness and elegance of our DRIVE connection for preventing the over-smoothing, we replace the DRIVE connection in UDGNN* with the residual connection and different normal-  From <ref type="figure" target="#fig_0">Figure 10</ref>, we can find that most normalization can slow down the performance degeneration of residual connections. However, their performance under the heterophily datasets is not optimal when we increase the number of layers. In contrast, without any normalization technique, our DRIVE connection can achieve the best performance over all datasets, especially for the SGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Effect of the initialization of ? in the DRIVE connection</head><p>Although the parameter ? is a learnable weight parameter in the DRIVE connection, to show the importance of the initialization of 0 to make the entire network behavior as a direct pass identity mapping, we compare the performance of different initializations of ? in UDGNN*. From the results in <ref type="figure" target="#fig_0">Figure 11</ref>, we can observe that whether GraphConv is a GCN or SGC (with/without weight matrix W ), the initialization of ? is vital to the performance in terms of layers. For the initialization of 1, the beginning behavior of the model is similar to the residual connection. As shown in the right of <ref type="figure" target="#fig_0">Figure 12</ref>, the GCN(DRIVE)-1 (? = 1, and layers = 64) would also suffer from the over-smoothed features and gradient, and cause optimization difficulty, although ? is learnable. Therefore, the alpha failed to optimize and control smoothness properly, and the model's performance also degenerates when increasing the layers. On the one hand, compared with 1, the initialization of 0.1 can solve the over-smoothing to some extent, but the performance is still not optimal. On the other hand, the 0 initialization can achieve the best performance across all datasets. Hence, both the initialization of 0 and the learnable ability of ? in DRIVE connection are vital for preventing over-smoothing. <ref type="figure" target="#fig_0">Figure 12</ref>: The test accuracy and grad spectral property of different initializations for ? in the 64-layer GCN(DRIVE) during training over the Cora dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Limitation and Potential Negative Impact</head><p>In this work, we proposed to rethink the over-smoothing problem of GNNs from a path decomposition view. One interesting finding is that we found the gradient smoothing phenomenon that prevents the residual connection in graph convolutions from optimizing to learn the identity mapping and to be anti the over-smoothing. Moreover, we proposed a DRIVE connection and a Universal Deep GNNs framework to make existing GNNs deeper and more powerful. Considering real-world graphs are complex and may be non-smooth, the Universal Deep GNN framework can easily unlock the potential of existing shallow GNNs and serve as a starting baseline for the architecture design of future GNNs.</p><p>Our analysis removes the non-linear ReLU activation for analysis. Although the ReLU may only slightly affect the performance according to the previous study, combining it into the total path decomposition analysis is interesting. Besides, we do not provide the convergence rate analysis of the over-smoothing. According to the <ref type="bibr" target="#b7">[8]</ref>, the convergence condition in terms of weight matrix W and residual connection of the feature over-smoothing subspace theorem may also not be satisfied during training. We advocate peer researchers to investigate the over-smoothing problem of GNNs by considering the gradient optimization process. It would be interesting to analyze the convergence rate with respect to gradient smoothing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of GNNs with different skip connections and the performance comparison on the fullyconnected Cora-Random (noisy neighbor information) dataset in terms of GraphConv layers. Notice that although the skip connections are helpful for shallow networks, their performance degrades as more layers are stacked, except for the proposed DRIVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The path decomposition of GNNs with residual and initial connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The grad spectral property (von Neumann entropy) of different skip connection variants of 64 graph convolutional layers during training over the Chameleon dataset. More details can be found in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FFNGraphConvFigure 4 :</head><label>4</label><figDesc>Building block of UDGNN with DRIVE connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Ablation study on variants of skip connection on SGC and GCN under our framework in both homophily and heterophily datasets.(a) Cora-Random (noisy neighbors) (b) Cora (helpful neighbors) The visualized evolution of |? l | and |? l | in the training process for 64 layers UDGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The test accuracy and ConvRatio for each layer during training over the Cora-Random dataset. We show the results of GCN(Res)-8, GCN(Res)-16, GCN(Res)-32, GCN(Res)-64, and GCN(DRIVE)-64. All variants of GraphConv with skip connections are under our encoder-decoder framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )Figure 8 :</head><label>a8</label><figDesc>Cora(Homo) dataset (b) Squirrel(Hete) dataset (c) Actor(Hete) dataset The test accuracy and grad spectral property of different skip connection variants of 64 layers during training over both homophily and heterophily datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>The performance comparison of the state-of-the-art DeepGNNs and our UDGNNs for different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>The effect of the recent normalization techniques for helping residual connection to prevent oversmoothing. Our DRIVE connection can prevent over-smoothing without any normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Ablation study on variants initialization of ? for DRIVE connection in UDGNN* over both homophily and heterophily datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean test accuracy ? stdev on 6 heterophily and 3 homophily real-world datasets over 10 fixed splits (48%/32%/20% of nodes per class for train/val/test). The best performance is highlighted. ? denotes the results obtained from<ref type="bibr" target="#b49">[50]</ref>.89?4.78 85.29?3.61 35.76?0.98 29.68?1.81 46.36?2.52 81.08?6.37 72.41?2.18 86.65?0.35 74.75?2.22 65.99 MixHop ? 77.84?7.73 75.88?4.90 32.22?2.34 43.80?1.48 60.50?2.53 73.51?6.34 76.26?1.33 85.31?0.61 87.61?0.85 68.21 GPRGNN 82.12?7.72 81.16?3.17 33.29?1.39 43.29?1.66 61.82?2.39 81.08?6.59 75.56?1.62 86.85?0.46 86.98?1.33 70.15 APPNP 78.37?6.01 81.42?4.34 34.64?1.51 33.51?2.02 47.50?1.76 77.02?7.01 77.06?1.73 87.94?0.56 87.71?1.34 67.24 DAGNN 70.27?4.93 71.76?5.25 35.51?1.10 30.29?2.23 45.92?2.30 73.51?7.18 76.44?1.97 89.37?0.52 86.82?1.67 64.43 H2GCN-1 ? 84.86?6.77 86.67?4.69 35.86?1.03 36.42?1.89 57.11?1.58 82.16?4.80 77.07?1.64 89.40?0.34 86.92?1.37 70.72 H2GCN-2 ? 82.16?5.28 85.88?4.22 35.62?1.30 37.90?2.02 59.39?1.98 82.16?6.00 76.88?1.77 89.59?0.33 87.81?1.35 70.87 JKNet-GCN ? 66.49?6.64 74.31?6.43 34.18?0.85 40.45?1.61 63.42?2.00 64.59?8.68 74.51?1.75 88.41?0.45 86.79?0.92 65.79 DeeperGCN 70.27?7.09 72.75?4.84 35.57?1.08 31.23?1.35 48.75?2.57 68.38?5.85 75.58?1.18 88.80?0.40 85.61?1.94 64.10 FAGCN 78.11?5.01 81.56?4.64 35.41?1.18 42.43?2.11 56.31?3.22 76.12?7.65 74.86?2.42 85.74?0.36 83.21?2.04 68.18 GCNII 69.72?8.90 75.29?4.64 35.58?1.25 47.21?1.73 60.79?2.35 79.19?6.12 76.82?1.67 89.26?0.48 87.89?1.88 69.07 GraphSAGE ? 82.43?6.14 81.18?5.56 34.23?0.99 41.61?0.74 58.73?1.68 75.95?5.01 76.04?1.30 88.45?0.50 86.90?1.04 69.50 UDGNN*SAGE 82.97?3.87 85.55?4.89 36.38?1.52 62.09?2.16 71.05?1.83 82.16?6.88 76.35?1.69 89.60?0.36 86.71?1.18 74.76 UDGNNSAGE 84.05?4.11 86.86?4.42 36.64?1.18 62.02?2.03 69.51?2.22 83.24?7.83 75.85?1.69 89.88?0.41 86.65?1.18 74.97 GAT ? 58.38?4.45 55.29?8.71 26.28?1.73 30.62?2.11 54.69?1.95 58.92?3.32 75.46?1.72 84.68?0.44 82.68?1.80 58.56 UDGNN*GAT 80.27?4.23 83.72?4.43 35.72?1.57 66.21?1.79 71.36?1.68 81.89?6.11 74.92?1.47 89.62?0.47 85.55?1.41 74.36 UDGNNGAT 83.43?4.33 85.47?3.97 36.13?1.02 63.41?1.84 68.15?1.54 82.71?4.06 75.09?1.75 89.78?0.56 85.37?1.25 74.39 GCN ? 59.46?5.25 59.80?6.99 30.26?0.79 36.89?1.34 59.82?2.58 57.03?4.67 76.68?1.64 87.38?0.66 87.28?1.26 61.62 UDGNN</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Chameleon</cell><cell>Cornell</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora</cell><cell>Average</cell></row><row><cell>GEOM-GCN ?</cell><cell>67.57</cell><cell>64.12</cell><cell>31.63</cell><cell>38.14</cell><cell>60.90</cell><cell>60.81</cell><cell>77.99</cell><cell>90.05</cell><cell>85.27</cell><cell>64.05</cell></row><row><cell>MLP ?</cell><cell>81.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*GCN 81.25?6.78 86.47?4.34 35.43?0.96 70.05?2.24 76.79?1.46 82.43?5.73 76.15?1.64 89.39?0.45 87.28?0.89 76.14 UDGNNGCN 84.60?5.32 87.64?3.74 36.13?1.21 68.13?2.59 74.53?1.19 84.32?7.29 76.05?1.83 89.85?0.35 86.97?1.21 76.47 6.2 Comparison with SOTA on Homophily &amp; Heterophily Datasets (RQ1) Table 1 and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on OGB Arxiv(Homophily) and Arxiv-year(Heterophily) over 10 runs. SAGE 72.23?0.17 51.86?0.18 UDGNN SAGE 72.34?0.16 52.38?0.14</figDesc><table><row><cell>Methods</cell><cell>Arxiv</cell><cell>Arxiv-year</cell></row><row><cell>MLP</cell><cell cols="2">55.50?0.23 36.70?0.21</cell></row><row><cell>APPNP</cell><cell cols="2">68.24?0.19 38.15?0.26</cell></row><row><cell>GPR-GNN</cell><cell cols="2">70.71?0.26 45.07?0.21</cell></row><row><cell>MixHop</cell><cell cols="2">72.68?0.16 51.81?0.17</cell></row><row><cell>DAGNN</cell><cell cols="2">72.09?0.25 42.76?0.26</cell></row><row><cell>DeeperGCN</cell><cell cols="2">71.92?0.16 43.45?0.25</cell></row><row><cell>JKNet</cell><cell cols="2">72.19?0.21 46.28?0.29</cell></row><row><cell>H2GCN</cell><cell cols="2">72.28?0.17 49.09?0.10</cell></row><row><cell>GCNII</cell><cell cols="2">72.74?0.16 47.21?0.28</cell></row><row><cell>GCN</cell><cell cols="2">71.74?0.29 46.02?0.26</cell></row><row><cell>UDGNN* GCN</cell><cell cols="2">72.82?0.20 52.74?0.23</cell></row><row><cell>UDGNN</cell><cell></cell><cell></cell></row></table><note>GCN 72.94?0.22 53.16?0.15 GraphSAGE 71.49?0.27 48.64?0.27 UDGNN*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of datasets. For the basic citation datasets, nodes correspond to papers; edges correspond to citation links, and the sparse bag-of-words are the feature representation of each node. Finally, the label of each node represents the topic of the paper. We use the fully supervised data split in<ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> -Arxiv<ref type="bibr" target="#b16">[17]</ref>: The Arxiv dataset is a large scale citation network collected from all Computer Science ARXIV papers. Each node is an ARXIV paper, and edges are citation relations between papers. The features are 128-dimensional averaged word embeddings of each paper, and labels are subject areas of papers.? Heterophily Datasets-Texas, Wisconsin, Cornell<ref type="bibr" target="#b32">[33]</ref>: Cornell, Texas, and Wisconsin are the web page networks captured from the computer science departments of these universities in the WebKB dataset. In these networks, nodes and edges represent the web pages and hyperlinks. Similar to the Citations networks, words in the web page represent the node features in the bag-of-word form. The web pages are labeled into five categories: student, project, course, staff, and faculty.</figDesc><table><row><cell>Dataset</cell><cell cols="7">#Classes #Nodes #Edges #Graph Type #Context #Features #Train/Val/Test</cell></row><row><cell>Texas</cell><cell>5</cell><cell>183</cell><cell>309</cell><cell cols="4">Heterophily Web pages 1,703 48%/32%/20%</cell></row><row><cell>Wisconsin</cell><cell>5</cell><cell>251</cell><cell>499</cell><cell cols="4">Heterophily Web pages 1,703 48%/32%/20%</cell></row><row><cell>Cornell</cell><cell>5</cell><cell>183</cell><cell>295</cell><cell cols="4">Heterophily Web pages 1,703 48%/32%/20%</cell></row><row><cell>Squirrel</cell><cell>5</cell><cell cols="6">5,201 217,073 Heterophily Wiki pages 2,089 48%/32%/20%</cell></row><row><cell>Chameleon</cell><cell>5</cell><cell>2277</cell><cell cols="5">36,101 Heterophily Wiki pages 2,325 48%/32%/20%</cell></row><row><cell>Actor</cell><cell>5</cell><cell>7,600</cell><cell cols="2">33,544 Heterophily</cell><cell>Movies</cell><cell>931</cell><cell>48%/32%/20%</cell></row><row><cell>Arxiv-year</cell><cell>5</cell><cell cols="3">169,343 1,166,243 Heterophily</cell><cell>OGB</cell><cell>128</cell><cell>50%25%25%</cell></row><row><cell>Cora-Random</cell><cell>7</cell><cell cols="4">2,708 7,333,264 Heterophily Citation</cell><cell>1,433</cell><cell>5%/19%/37%</cell></row><row><cell>CiteSeer</cell><cell>6</cell><cell>3,327</cell><cell>4,732</cell><cell>Homophily</cell><cell>Citation</cell><cell cols="2">3,703 48%/32%/20%</cell></row><row><cell>PubMed</cell><cell>3</cell><cell cols="2">19,717 44,338</cell><cell>Homophily</cell><cell>Citation</cell><cell>500</cell><cell>48%/32%/20%</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2,708</cell><cell>5,429</cell><cell>Homophily</cell><cell>Citation</cell><cell cols="2">1,433 48%/32%/20%</cell></row><row><cell>Arxiv</cell><cell>40</cell><cell cols="3">169,343 1,166,243 Homophily</cell><cell>OGB</cell><cell>128</cell><cell>48%/32%/20%</cell></row><row><cell cols="3">? Homophily Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">-Citeseer, Pubmed, Cora [20]:</cell><cell></cell><cell></cell><cell></cell></row></table><note>-Squirrel, Chameleon</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>izations, including BatchNorm, LayerNorm, NodeNorm, and PairNorm.H l+1 = Norm(H l + GraphConv(A, H l )).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(28)</cell></row><row><cell>Accuracy</cell><cell>75 80 85</cell><cell cols="3">Cora(Homo) SGC(Res)-BatchNorm SGC(DRIVE) SGC(Res) SGC(Res)-LayerNorm SGC(Res)-NodeNorm SGC(Res)-PairNorm</cell><cell></cell><cell>Accuracy</cell><cell>20 25 30 35</cell><cell></cell><cell cols="3">Actor(Hete) SGC(DRIVE) SGC(Res) SGC(Res)-BatchNorm SGC(Res)-LayerNorm SGC(Res)-NodeNorm SGC(Res)-PairNorm</cell><cell>Accuracy</cell><cell>30 40 50 60</cell><cell cols="2">Squirrel(Hete) SGC(DRIVE) SGC(Res) SGC(Res)-NodeNorm SGC(Res)-PairNorm SGC(Res)-LayerNorm SGC(Res)-BatchNorm</cell><cell></cell><cell>Accuracy</cell><cell>50 60 70 80</cell><cell></cell><cell>Wisconsin(Hete) SGC(DRIVE) SGC(Res) SGC(Res)-BatchNorm SGC(Res)-LayerNorm SGC(Res)-NodeNorm SGC(Res)-PairNorm</cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell cols="2">8 Layers 16</cell><cell>32</cell><cell>64</cell><cell>2</cell><cell>4</cell><cell>8 Layers 16</cell><cell>32</cell><cell>64</cell><cell></cell><cell>2</cell><cell>4</cell><cell>8 Layers 16</cell><cell>32</cell><cell>64</cell><cell>2</cell><cell>4</cell><cell>8 Layers 16</cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geometry of quantum states: An introduction to quantum entanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingemar</forename><surname>Bengtsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karol?yczkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A note on over-smoothing for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13318</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pageRank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On provable benefits of depth in training graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Ramezani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Henrickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph transformation policy network for chemical reaction prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330958</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="750" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno>PMLR. 2021</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="2793" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22092" to="22103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 2020</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tackling oversmoothing for general graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2008</biblScope>
		</imprint>
	</monogr>
	<note>In: arXiv e-prints (2020</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepGCNs: Making GCNs go as deep as CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><forename type="middle">Delgadillo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Kassem</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting path failure in time-evolving graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujia</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330847</idno>
		<idno>DOI: 10 . 1145 / 3292500 . 3330847</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large scale learning on non-Homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser Nam</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SkipNode: On alleviating over-smoothing for deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11628</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A universal approximation theorem of deep neural networks for expressing probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3094" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2020. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating node importance in knowledge graphs using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330855</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="596" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DropEdge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11418</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PairNorm: Tackling oversmoothing in gNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4917" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dirichlet energy constrained learning for deep graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Effective training strategies for deep graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuangqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11418abs/2006.07107</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">. In: preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">GCN(DRIVE) GCN(Res) GCN(Res)-BatchNorm GCN(Res)-LayerNorm GCN(Res)-NodeNorm GCN(Res)-PairNorm</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
