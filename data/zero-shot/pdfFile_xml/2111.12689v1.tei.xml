<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A stacked deep convolutional neural network to predict the remaining useful life of a turbofan engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sol?s-Mart?n</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Datrik Ingellicence S.A</orgName>
								<address>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="institution">Seville University</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Gal?n-P?ez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Datrik Ingellicence S.A</orgName>
								<address>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="institution">Seville University</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaqu?n</forename><surname>Borrego-D?az</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Artificial Intelligence</orgName>
								<orgName type="institution">Seville University</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A stacked deep convolutional neural network to predict the remaining useful life of a turbofan engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the data-driven techniques and methodologies used to predict the remaining useful life (RUL) of a fleet of aircraft engines that can suffer failures of diverse nature. The solution presented is based on two Deep Convolutional Neural Networks (DCNN) stacked in two levels. The first DCNN is used to extract a low-dimensional feature vector using the normalized raw data as input. The second DCNN ingests a list of vectors taken from the former DCNN and estimates the RUL. Model selection was carried out by means of Bayesian optimization using a repeated random subsampling validation approach. The proposed methodology was ranked in the third place of the 2021 PHM Conference Data Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Remaining useful life (RUL) is a cross-disciplinary field that belongs to the wider field of Prognostics and Health <ref type="bibr">Management (PHM)</ref>. This discipline studies the system behavior during its lifetime, that is, from the last check or maintenance performed on it until the system fails or the degradation of the system performance exceeds a certain threshold. Therefore, RUL enables to estimate the future reliability and scans the degradation of the system along the time <ref type="bibr" target="#b9">(Peng, Wang, &amp; Zi, 2018)</ref>. The final goal is to estimate the remaining time until such failure occurs or a degradation level is reached, given the system working condition at any point of the system's lifetime.</p><p>Two main methodologies for RUL estimation can be found in the literature: model-based methods and data-driven methods. Model-based methods establish a degradation model based on physics and statistics to predict the degradation David Sol?s-Mart?n et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. trend of the system. Model-based methods require a strong knowledge on the physical behavior of each specific component and failure type, thus it is difficult to develop models for the current increasingly complex systems (Z. <ref type="bibr" target="#b12">Zhao, Liang, Wang, &amp; Lu, 2017)</ref>. Moreover, some assumptions need to be taken on the models, thus those can be biased. For all these reasons, model-based methods have a limited prediction performance and have to be designed ad hoc for each different type of machinery.</p><p>On the other hand, data-driven methods use machine learning algorithms to learn the degradation trend of the system using historical condition monitoring data. In contrast to modelbased methods, prior expertise is not required. Their application can be straightforward when enough data of quality are available.</p><p>Among the existing data-driven methods, Deep Learning (DL) is one of the most popular and promising fields of study. During the last decade, the use of DL techniques has surged significantly. Especially in complex tasks with highdimensional nonlinear data. DL has had an enormous success in image processing, natural language processing, and signal processing. For this reason, it is not unexpected that DL based approaches are also widespread within the context of Prognostics and Health Management (PHM) research. One type of network that has been used to deal with sequence data is the deep convolutional neural network (DCNN) <ref type="bibr" target="#b5">(LeCun, Haffner, Bottou, &amp; Bengio, 1999</ref><ref type="bibr">). For example, in (B. Zhao, Lu, Chen, Liu, &amp; Wu, 2017</ref>) the authors applied DCNN to time series classification over a variety of datasets. Within of the context of PHM, <ref type="bibr" target="#b3">(Babu, Zhao, &amp; Li, 2016)</ref> and (X. <ref type="bibr" target="#b7">Li, Ding, &amp; Sun, 2018)</ref> authors have applied DCNN to RUL prediction of aircraft turbofan engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA AND PROBLEM DESCRIPTION</head><p>The Commercial Modular Aero-Propulsion System Simulation (CMAPSS) is a modeling software developed at NASA. It was used to create the previous CMAPSS dataset <ref type="bibr" target="#b10">(Saxena, Goebel, Simon, &amp; Eklund, 2008)</ref>. That first dataset was built by only taking samples from a system that is already degraded. "Therefore, the onset of the fault cannot be predicted; only the evolution of the fault can" (Arias <ref type="bibr" target="#b2">Chao, Kulkarni, Goebel, &amp; Fink, 2021)</ref>.</p><p>A new dataset, named N-CMAPSS, has been created providing the full history of the trajectories starting with a healthy condition until the failure occurs. A schematic of the turbofan model used in the simulations is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. All rotation components of the engine (fan, LPC, HPC, LPT, and HPT) can be affected by the degradation process. This model is defined as a nonlinear system denoted by:</p><formula xml:id="formula_0">x (t) s , x (t) v = F w (t) , ? (t)<label>(1)</label></formula><p>where x s are the physical properties, x v are the unobserved properties, w is the scenario-descriptor operating conditions and ? are the health model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>The dataset provided for the challenge consists of data from 90 simulated flights (extracted from the N-CMAPSS 1 (Arias <ref type="bibr" target="#b2">Chao et al., 2021)</ref>). Seven different failure modes, related to flow degradation or subcomponent efficiency that can be present in each flight have been defined. The flights are divided into three classes depending on the length of the flight. In class 1, fall flights with a duration from 1 to 3 hours, class 2 is composed of flights between 3 and 5 hours, and class 3 contains flights that take more than 5 hours. Each flight is divided into cycles, covering climb, cruise, and descend operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Problem definition</head><p>The problem revolves around the development of a model G capable of predicting the remaining useful life Y of the sys-1 The N-CMAPSS dataset can be downloaded from the NASA data repository tem, using the sensor outputs X s , the scenario descriptors W and auxiliary data A. The different variables available to estimate the RUL of the system are described in table 1. The former is an optimization problem that can be denoted as:</p><formula xml:id="formula_1">argmin S (y ??)<label>(2)</label></formula><p>where y and? = G(x s , w, a) are, respectively, the expected and estimated RUL. S is a scoring function defined as the average of the root-mean-square error (RMSE) and the NASA's scoring function (N s ) <ref type="bibr" target="#b10">(Saxena et al., 2008)</ref>:</p><formula xml:id="formula_2">S = 0.5 ? RM SE + O.5 ? N s (3) N s = 1 M exp(?|y ??|) ? 1<label>(4)</label></formula><p>being M the number of samples and ? equal to 1 13 in cas? Y &lt; Y and 1 10 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHODOLOGY</head><p>This section introduces the different phases or processes in which the proposed methodology can be decomposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data normalization</head><p>The 20 variables used to develop the different models (see <ref type="table" target="#tab_0">table 1</ref>) have different scales. For that reason, directly feeding the proposed networks with such data will slow down the learning and convergence of the models. Hence, a data normalization step, before the training stage, is required to ho- mogenize the variables into a common scale. More precisely, the standard normalization is used in this paper:</p><formula xml:id="formula_3">x f = x f ? ? f ? f (5)</formula><p>where x f is the data of a feature f , and ? f and ? f are its mean and standard deviation, respectively. Note that the mean and variance are computed for each training cross-validation set and are also used to normalize the validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Time window processing</head><p>After the data normalization, the inputs of the network are generated using a sliding time window through the normalized data. The size of the window, denoted as L w , is a parameter to be selected during the model selection stage. The inputs generated can be expressed as <ref type="figure" target="#fig_1">Figure 2</ref>) and the paired ground-RUL label is Y t . With this approach, for each unit, T k ? L w samples will be considered. Where T k is the total run time in seconds of each unit.</p><formula xml:id="formula_4">X k t = [ X k t end ?Lw , ..., X k t end ] (see</formula><p>It is common to set a constant RUL at the beginning of the experiment (H. <ref type="bibr" target="#b6">Li, Zhao, Zhang, &amp; Zio, 2020)</ref>, considering a healthy condition of the system during that early stage. In this work, this approach has not been followed since this kind of assumptions can introduce a bias in the model. Instead, the ground-RUL label has been set as a linear function of cycles from the RUL of each unit</p><formula xml:id="formula_5">Y k t = T U L k ? C k t ,</formula><p>where T U L k is the total useful life of the unit k in cycles and C k t is the number of past cycles from the beginning of the experiment at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Level 1 and level 2 model: Convolutional Neural Networks</head><p>The modeling phase consists of two levels (see <ref type="figure">Figure 3</ref>). In the first level, the goal is to find a good model to produce a time-windowed encoding of the raw input signals. This en- For both level models, Deep Convolutional Neural Networks (DCNN) have been selected. DCNN is a specialization of fully connected networks (FCN) that have been widely applied in image processing, natural language processing, and speech recognition with great success. The CNN uses parameter sharing and subsampling to extract feature maps with the most significant local features. The main operations in a DCNN are convolution and polling . The convolution operation implements parameter sharing and local receptive fields. The equation of the convolution is as follows:</p><formula xml:id="formula_6">S [i, j] = m n I [i + d * n, j + d * m] K [n, m] (6)</formula><p>where I is an input matrix, K is the kernel matrix (or convolution's parameters) of size n x m, d is the dilation rate, and S is the result of the convolution, called feature map. The kernel matrix is slipped across the input matrix looking for a pattern present in any place of it. Thereby, comparing with a FCN, the number of weights is reduced and, additionally, the overfitting chance. The pooling operation performs a downscaling by applying a statistic operation to each region of the input, which was previously divided into rectangular pool- ing regions. The pooling operation serves a few porpouses: reduces the computational requirements for the upper layer since the feature maps are downscaled, reduces the number of connections (parameters) for the upper fully connected layers, provides a form of spatial transformation invariance and helps mitigating the overfitting risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cross validation</head><p>Neural networks have a high number of parameters to be adjusted. Additionally, the size of the sliding window has to be optimized too. To deal with the overfitting problem during the model selection phase, a bunch of validation techniques can be found in the literature. The classical single hold-out strategy is discarded since it can easily overfit the hold-out set and lead to poor generalization results. The k-fold crossvalidation has the disadvantage that the size of the validation set decreases as k (the number of folds) is increased. There exist alternatives as the k-fold repeated random subsampling validation that overcome this issue (see <ref type="figure" target="#fig_3">Figure 4</ref>). The later strategy has been selected in this work, with k = 5 and a validation fold size of 30% of the training set, that is, 27 random units from a total of 90.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model Selection</head><p>The goal of the model selection phase is to obtain an optimal parameter configuration for each model. To this aim, Bayesian optimization has been selected as the optimization strategy. The models were trained using the RMSE as the loss function, since the NASA score is not differentiable. However, the loss function used in the Bayesian optimization to decide the next set of model parameters to be tested, is the score S, defined in equation <ref type="formula">(3)</ref>.</p><p>The well-known early stopping method, with a patience factor of 8 epochs, is used as a signal to finish the model training process. Hence, the training of the model will be stopped if the training loss on the validation fold has not been improved during the last 8 training epochs. Additionally, the learning rate is decreased by a factor of 0.1 when no improvement is seen on the validation loss in the last 3 training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Convolution Batch Normalization Activation function Pooling 2x2</head><p>Dropout ReLU output Fully connected layer <ref type="figure">Figure 5</ref>. Level 1 networks architectures tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head><p>This section describes the settings and parameters used to find the best models. Then the results of each level are compared and the final solution is described. Finally, this approach is applied to the hidden test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Level 1: encoding</head><p>The encoding model at level 1 of the stacking aims to carry out a dimension and noise reduction on the raw signals. To achieve this task, DCNN architecture was used. The classical DCNN architecture, which is shown in <ref type="figure">Figure 5</ref>, can be divided into two parts. The first one is a stacking of N b blocks composed of convolution and pooling layers. The goal of this first part is to extract characteristics potentially useful for the task. The second part consists of fully connected layers and in this case will perform the regression of the RUL.</p><p>To obtain an optimal hyperparameter configuration, Bayesian optimization was executed during 100 iterations. The first 10 iterations, consist in models with randomly selected hyperparameters. These first 10 random points of the model hyperparameter search space are used by the Bayesian process to create the initial estimation of the error surface. After these 10 iterations, the optimization process applies Bayesian rules to select the most promising point (model hyperparameter) to be tested. <ref type="table" target="#tab_1">Table 2</ref> summarizes the input parameter ranges and the best parameter configuration found for the L1 model. where ? p and ? p are the mean and variance, respectively, of the cross-validation predictions. As it can be observed in the charts, the models are more reliable when the failure occurs earlier. In the first cycles, the prediction seems to be almost constant, until the model is able to catch the descending trend. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Level 2: RUL estimation</head><p>The aim of the level 2 model of the stacking is to perform the final RUL predictions. The input of this model is the encoding of the raw signal generated by some of the level 1 models. Therefore, a first step is needed to generate the encoding for the train and test sets per fold. The encodings are extracted from the second full connected layer of 100 neurons.</p><p>Each sample input for each candidate level 2 model will be generated using a sliding window over the encodings. Since the encodings are a dimension reduced version of the raw signal, the level 2 model can expand the receptive field of the model and learn from the trend along the time. The parameter step defines the gap in seconds between encodings. <ref type="figure">Figure 6</ref> shows how the inputs for the models are obtained. The concept of image channel is used to compose the input. Each channel has 100 vector encodings. Therefore, the total number of encodings in the input is 100 ? chanells.</p><p>The level 2 model is also a DCNN. The cross-validation schema and parameter ranges to be optimized are the same considered in the level 1 model. Addicionaly, three more pa-step <ref type="figure">Figure 6</ref>. Sliding window for l2 models. Left: window over the raw signals to generate the encodings e i . Right: encoding input shape for the DCNN. rameters need to be considered in this L2 model optimization, namely f c 2 , step, and channels (see <ref type="table" target="#tab_1">Table 2</ref>). <ref type="table" target="#tab_1">Table 2</ref> summarizes the best parameter configuration found for the level 2 model, which is quite similar to that of level 1. This could be due to that the level 1 hyperparameters are provided to the optimization process as seed. There exist differences only in the training and regularization parameters. The rest of the architecture is identical. Regarding the additional parameters, it is interesting to note that the value selected for the parameter step is 989. Since the level 1 model provides a fairly good estimation of the RUL, the level 2 model only needs a set of sparse encodings to capture the trend and improve the RUL. The cross-validation score of this model is 3.44, while the ensemble score (that is, taking the average of predictions before computing the score), is 2.95. This gap between the cross-validation scores and ensemble predictions means that the predictions of the models have a good level of uncorrelation.</p><p>Figure 7 (bottom) shows the predictions and confidence intervals of the level 2 models. Comparing these with the level 1 models <ref type="figure" target="#fig_4">(Figure 7 top)</ref>, one can see that the predictions of the stacked models have been smoothed and improved. <ref type="figure" target="#fig_6">Figure  8</ref> presents the relation between the ground truth RUL and the score obtained by each stacked model. Finally, <ref type="figure" target="#fig_7">figure 9</ref> shows the performance of each model in each class. Note that the shorter are the flights, the better the model performs. In the same way, the improvement in the level 2 model performance is higher for shorter flights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">RUL prediction on test data</head><p>A common approach to obtain the final model consists in training it using all available data. In this approach, a number of training epochs must be selected, thus the validation data used during the model selection would also be part of the final training set. To select the number of epochs to train, the mean value among the best epoch obtained for each fold is considered. Usually, it would be necessary to save part of the training set as the final test set to validate the performance of the model. Hence, the number of samples in the training/validation set is reduced. A different approach has been used in the proposed methodology, which consists in using all models trained per fold as an ensemble. This approach has two main advantages. The first one is that it is possible to obtain a confidence metric of our final ensemble model performance by means of cross validation. The second one, related to the first, is that the predictions per sample could be used to create a confidence interval of the final mean prediction as it is shown in the <ref type="figure" target="#fig_4">Figure 7</ref> and 8.</p><p>The hidden validation set to be scored by the competition committed was composed of 38 units. The predictions generated by the final solution are: <ref type="bibr">22,</ref><ref type="bibr">21,</ref><ref type="bibr">19,</ref><ref type="bibr">24,</ref><ref type="bibr">13,</ref><ref type="bibr">18,</ref><ref type="bibr">12,</ref><ref type="bibr">22,</ref><ref type="bibr">11,</ref><ref type="bibr">6,</ref><ref type="bibr">22,</ref><ref type="bibr">24,</ref><ref type="bibr">19,</ref><ref type="bibr">10,</ref><ref type="bibr">19,</ref><ref type="bibr">21,</ref><ref type="bibr">20,</ref><ref type="bibr">17,</ref><ref type="bibr">20,</ref><ref type="bibr">26,</ref><ref type="bibr">19,</ref><ref type="bibr">12,</ref><ref type="bibr">11,</ref><ref type="bibr">13,</ref><ref type="bibr">9,</ref><ref type="bibr">37,</ref><ref type="bibr">25,</ref><ref type="bibr">4,</ref><ref type="bibr">18,</ref><ref type="bibr">25,</ref><ref type="bibr">18,</ref><ref type="bibr">14,</ref><ref type="bibr">11,</ref><ref type="bibr">12,</ref><ref type="bibr">22,</ref><ref type="bibr">10,</ref><ref type="bibr">19,</ref><ref type="bibr">19</ref>, which scored 3.651. This score is very close to the cross-validation score. An estimation of the score can be obtained by using the prediction/score distribution <ref type="figure" target="#fig_6">(Figure 8 bottom)</ref>. However, the ensemble score is 2.95, which is far from the real score obtained. This means that there is room for improvement by reducing this overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>This work has exposed a robust methodology to estimate RUL without the need of expert knowledge on the system studied. The success of this methodology has been demonstrated with the results obtained in the 2021 PHM Conference Data Challenge (ranked in the third place). The goal of the challenge was to predict RUL in turbogan aircraft engines, using data generated with a CMAPSS simulator. In this work, the process of obtaining the final solution is divided into two learning stages. In the first one, an encoding of the raw data is learned and used as input of the second learning stage to obtain the final model capable of estimating RUL. The finals results show that using one of the most basic architectures found in the literature is enough to achieve an excellent outcome.</p><p>A number of possible improvements of the current solution will be the target of future research: 1) Smoothing the RUL among cycles could smooth the error space, thus helping in the learning process. 2) In the proposed methodology, the inputs having a number of cycles lower than that required by the network, have been excluded from the training process.</p><p>In the case of the validation set, these inputs were filled with the earliest encoding. It is expected that training the networks applying the same filling will help reducing the overfitting. 3) Another possible improvement could be to balance the training folds so each failure type is properly represented in each validation set. 4) Train the level 2 model with a random gap between encodings. This allow to expand the training set, have an additional mechanism to compute the confidence intervals, and increase the number of predictions to calculate the final averaged RUL. 5) Finally, it would be interesting to study how predictive are the class and the health state features.</p><p>This work has been developed with a focus on reproducible research. To this aim, the parameters obtained and the parameter ranges used for model selection have been well described. In addition, the source code to train the models and validate the results can be found in the source code repository https://github.com/DatrikIntelligence/Stacked-DCNN-RUL-PHM21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work has been supported by Grant PID2019-109152GB-I00/AEI/10.13039/501100011033 (Agencia Estatal de Investigaci?n), Spain and by the Ministry of Science and Education of Spain through the national program "Ayudas para contratos para la formaci?n de investigadores en empresas</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Schematic of the model used in CMAPSS (Arias<ref type="bibr" target="#b2">Chao et al., 2021)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Sliding window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>coding is needed due to the high dimension of the input data. Another goal of this encoding step, besides the dimension reduction of the input, is to remove as much noise as possible. Such an encoding is used as the level 2 model input. The goal of this second level model is to provide an estimation of the RUL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>k-fold cross validation strategy (left), and k-fold random sampling strategy (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 (</head><label>7</label><figDesc>top) shows the predictions for 4 units made by the DCNN model. The confidence intervals have been computed as [? p ? 3? p , ? p + 3? p ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Prediction and confidence interval for 4 units the level 1 model (top) and the level 2 model (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>The chart shows the relation beetween RUL and the score (top), the prediction and the score (middle) and the prediction and RUL (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Performance of the models per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Variable description.</figDesc><table><row><cell cols="3">Symbol Set Description</cell><cell>Units</cell></row><row><cell>alt</cell><cell>W</cell><cell>Altitude</cell><cell>ft</cell></row><row><cell>Mach</cell><cell>W</cell><cell>Flight Mach number</cell><cell>-</cell></row><row><cell>TRA</cell><cell>W</cell><cell>Throttle-resolver angle</cell><cell>%</cell></row><row><cell>T2</cell><cell>W</cell><cell>Total temperature at fan inlet</cell><cell>?R</cell></row><row><cell>Wf</cell><cell cols="2">X s Fuel flow</cell><cell>pps</cell></row><row><cell>Nf</cell><cell cols="2">X s Physical fan speed</cell><cell>rpm</cell></row><row><cell>Nc</cell><cell cols="2">X s Physical core speed</cell><cell>rpm</cell></row><row><cell>T24</cell><cell cols="3">X s Total temperature at LPC outlet ?R</cell></row><row><cell>T30</cell><cell cols="3">X s Total temperature at HPC outlet ?R</cell></row><row><cell>T48</cell><cell cols="3">X s Total temperature at HPT outlet ?R</cell></row><row><cell>T50</cell><cell cols="3">X s Total temperature at LPT outlet ?R</cell></row><row><cell>P15</cell><cell cols="2">X s Total pressure in bypass-duct</cell><cell>psia</cell></row><row><cell>P2</cell><cell cols="2">X s Total pressure at fan inlet</cell><cell>psia</cell></row><row><cell>P21</cell><cell cols="2">X s Total pressure at fan outlet</cell><cell>psia</cell></row><row><cell>P24</cell><cell cols="2">X s Total pressure at LPC outlet</cell><cell>psia</cell></row><row><cell>Ps30</cell><cell cols="2">X s Static pressure at HPC outlet</cell><cell>psia</cell></row><row><cell>P40</cell><cell cols="2">X s Total pressure at burner outlet</cell><cell>psia</cell></row><row><cell>P50</cell><cell cols="2">X s Total pressure at LPT outlet</cell><cell>psia</cell></row><row><cell>Fc</cell><cell>A</cell><cell>Flight class</cell><cell>-</cell></row><row><cell>h s</cell><cell>A</cell><cell>Health state</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Parameter ranges used in the Bayesian optimization and values of the best models for level 1 (L1) and level (L2)</figDesc><table><row><cell>Patameter</cell><cell>Range or value</cell><cell>L1</cell><cell>L2</cell></row><row><cell>L w</cell><cell>[100, 500]</cell><cell>161</cell><cell>-</cell></row><row><cell>B s</cell><cell>32,64</cell><cell>116</cell><cell>31</cell></row><row><cell>C bs</cell><cell>[2,4]</cell><cell>4</cell><cell>4</cell></row><row><cell>N cb</cell><cell>[2,4]</cell><cell>4</cell><cell>4</cell></row><row><cell>l 1</cell><cell>[0, 1e-3]</cell><cell>7.23e-4</cell><cell>6.96e-4</cell></row><row><cell>l 2</cell><cell>[0, 1e-3]</cell><cell>0</cell><cell>1.73e-5</cell></row><row><cell>l r</cell><cell>[1e-5, 1e-3]</cell><cell>0.001</cell><cell>5.53e-4</cell></row><row><cell>dropout</cell><cell>[0, 0.9]</cell><cell>0.13</cell><cell>0.21</cell></row><row><cell>f c 1</cell><cell>[64, 256]</cell><cell>100</cell><cell>247</cell></row><row><cell>? conv</cell><cell>{tanh, ReLU,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>leakyReLU}</cell><cell>tanh</cell><cell>tanh</cell></row><row><cell>d rate</cell><cell>[1, 10]</cell><cell>2</cell><cell>2</cell></row><row><cell>K s</cell><cell>{(3,3), (10,1),</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(10,3)}</cell><cell>(10,1)</cell><cell>(10,1)</cell></row><row><cell>? f c</cell><cell>{tanh, ReLU,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>leakyReLU}</cell><cell cols="2">leakyReLU leakyReLU</cell></row><row><cell>? output</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell>f c 2</cell><cell>[100, 1000]</cell><cell>-</cell><cell>105</cell></row><row><cell>channels</cell><cell>[1, 3]</cell><cell>-</cell><cell>3</cell></row><row><cell>step</cell><cell>[64, 256]</cell><cell>-</cell><cell>989</cell></row><row><cell>#Net params</cell><cell></cell><cell>1,514,016</cell><cell>2,575,449</cell></row><row><cell>RMSE</cell><cell></cell><cell>10.46</cell><cell>6.24</cell></row><row><cell>MAE</cell><cell></cell><cell>7.689</cell><cell>4.27</cell></row><row><cell>NASA score</cell><cell></cell><cell>2.13</cell><cell>0.64</cell></row><row><cell>CV S score</cell><cell></cell><cell>6.30</cell><cell>3.44</cell></row><row><cell>std(S)</cell><cell></cell><cell>0.37</cell><cell>0.63</cell></row><row><cell>Ensemble</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S score</cell><cell></cell><cell>-</cell><cell>2.95</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The framework used in this work has been developed with Python 3.7. For network design and training, Tensorflow 2.0 <ref type="bibr" target="#b1">(Abadi et al., 2016)</ref> and Keras 2.3 <ref type="bibr" target="#b4">(Gulli &amp; Pal, 2017)</ref> have been used. As model optimization framework, Tune 1.6.0 <ref type="bibr" target="#b8">(Liaw et al., 2018)</ref> was selected.</p><p>The hardware used was a server with two GTX 1080Ti GPUs, 32GB of RAM memory, and 4 threads. The two GPUs were used to distribute different network training.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">of State Programme of Science Research and Innovations</title>
	</analytic>
	<monogr>
		<title level="m">DIN2019</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aircraft engine run-to-failure dataset under real flight conditions for prognostics and diagnostics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arias</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based regression approach for estimation of remaining useful life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on database systems for advanced applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep learning with keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object recognition with gradient-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape, contour and grouping in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Remaining useful life prediction using multi-scale deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">106113</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation in prognostics using deep convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05118</idno>
		<title level="m">Tune: A research platform for distributed model selection and training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Switching state-space degradation model with recursive filter/smoother for prognostics of remaining useful life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="822" to="832" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Damage propagation modeling for aircraft engine runto-failure simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eklund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 international conference on prognostics and health management</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Engineering and Electronics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Remaining useful life prediction of aircraft engine based on degradation pattern learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
