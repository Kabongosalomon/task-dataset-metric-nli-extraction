<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-aware Image Colorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Wei</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Virginia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-aware Image Colorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">InstColorization   Figure 1</ref><p>. Instance-aware colorization. We present an instance-aware colorization method that is capable of producing natural and colorful results on a wide range of scenes containing multiple objects with diverse context (e.g., vehicles, people, and man-made objects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Image colorization is inherently an ill-posed problem with multi-modal uncertainty. Previous methods leverage the deep neural network to map input grayscale images to plausible color outputs directly. Although these learningbased methods have shown impressive performance, they usually fail on the input images that contain multiple objects. The leading cause is that existing models perform learning and colorization on the entire image. In the absence of a clear figure-ground separation, these models cannot effectively locate and learn meaningful object-level semantics. In this paper, we propose a method for achieving instance-aware colorization. Our network architecture leverages an off-the-shelf object detector to obtain cropped object images and uses an instance colorization network to extract object-level features. We use a similar network to extract the full-image features and apply a fusion module to full object-level and image-level features to predict the final colors. Both colorization networks and fusion modules are learned from a large-scale dataset. Experimental results show that our work outperforms existing methods on different quality metrics and achieves state-of-the-art performance on image colorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatically converting a grayscale image to a plausible color image is an exciting research topic in computer vision and graphics, which has several practical applications such as legacy photos/video restoration or image compression. However, predicting two missing channels from a given single-channel grayscale image is inherently an illposed problem. Moreover, the colorization task could be multi-modal <ref type="bibr" target="#b2">[3]</ref> as there are multiple plausible choices to colorize an object (e.g., a vehicle can be white, black, red, etc.). Therefore, image colorization remains a challenging yet intriguing research problem awaiting exploration.</p><p>Traditional colorization methods rely on user intervention to provide some guidance such as color scribbles <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> or reference images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref> to obtain satisfactory results. With the advances of deep learning, an increasing amount of efforts has focused on leveraging deep neural network and large-scale dataset such as ImageNet <ref type="bibr" target="#b27">[28]</ref> or COCO-Stuff <ref type="bibr" target="#b1">[2]</ref> to learn colorization in an end-to-end fashion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1]</ref>. A variety of network architectures have been proposed to address image-level semantics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> at training or predict per-pixel color distributions to model multi-modality <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. Although these learning-based (a) Input (b) Deoldify <ref type="bibr" target="#b0">[1]</ref> (c) Zhang et al. <ref type="bibr" target="#b40">[41]</ref> (d) Ours <ref type="figure">Figure 2</ref>. Limitations of existing methods. Existing learning-based methods fail to predict plausible colors for multiple object instances such as skiers (top) and vehicles (bottom). The result of Deoldify <ref type="bibr" target="#b0">[1]</ref>(bottom) also suffers the context confusion (biasing to green color) due to the lack of clear figure-ground separation. methods have shown remarkable results on a wide variety of images, we observe that existing colorization models do not perform well on the images with multiple objects in a cluttered background (see <ref type="figure">Figure 2</ref>).</p><p>In this paper, we address the above issues and propose a novel deep learning framework to achieve instance-aware colorization. Our key insight is that a clear figure-ground separation can dramatically improve colorization performance. Performing colorization at the instance level is effective due to the following two reasons. First, unlike existing methods that learn to colorize the entire image, learning to colorize instances is a substantially easier task because it does not need to handle complex background clutter. Second, using localized objects (e.g., from an object detector) as inputs allows the instance colorization network to learn object-level representations for accurate colorization and avoiding color confusion with the background. Specifically, our network architecture consists of three parts: (i) an off-the-shelf pre-trained model to detect object instances and produce cropped object images; (ii) two backbone networks trained end-to-end for instance and full-image colorization, respectively; and (iii) a fusion module to selectively blend features extracted from layers of the two colorization networks. We adopt a three-step training that first trains instance network and full-image network separately, followed by training the fusion module with two backbones locked.</p><p>We validate our model on three public datasets (Ima-geNet <ref type="bibr" target="#b27">[28]</ref>, COCO-Stuff <ref type="bibr" target="#b1">[2]</ref>, and Places205 <ref type="bibr" target="#b42">[43]</ref>) using the network derived from Zhang et al. <ref type="bibr" target="#b40">[41]</ref> as the backbones. Experimental results show that our work outperforms existing colorization methods in terms of quality metrics across all datasets. <ref type="figure">Figure 1</ref> shows sample colorization results generated by our method.</p><p>Our contributions are as follows:</p><p>? A new learning-based method for fully automatic instance-aware image colorization. ? A novel network architecture that leverages off-theshelf models to detect the object and learn from largescale data to extract image features at the instance and full-image level, and to optimize the feature fusion to obtain the smooth colorization results. ? A comprehensive evaluation of our method on comparing with baselines and achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scribble-based colorization. Due to the multi-modal nature of image colorization problem, early attempts rely on additional high-level user scribbles (e.g., color points or strokes) to guide the colorization process <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. These methods, in general, formulate the colorization as a constrained optimization problem that propagates user-specified color scribbles based on some low-level similarity metrics. For instance, Levin et al. <ref type="bibr" target="#b19">[20]</ref> encourage assigning a similar color to adjacent pixels with similar luminance. Several follow-up approaches reduce color bleeding via edge detection <ref type="bibr" target="#b11">[12]</ref> or improve the efficiency of color propagation with texture similarity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref> or intrinsic distance <ref type="bibr" target="#b34">[35]</ref> . These methods can generate convincing results with detailed and careful guidance hints provided by the user. The process, however, is labor-intensive. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> partially alleviate the manual efforts by combining the color hints with a deep neural network.</p><p>Example-based colorization. To reduce intensive user efforts, several works colorize the input grayscale image with the color statistics transferred from a reference image specified by the user or searched from the Internet <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref>. These methods compute the correspondences between the reference and input image based on some low-level similarity metrics measured at pixel level <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21]</ref>, semantic segments level <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>, or super-pixel level <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>. The performance of these methods is highly dependent on how similar the reference image is to the input grayscale image. However, finding a suitable reference image is a non-trivial task even with the aid of automatic retrieval system <ref type="bibr" target="#b4">[5]</ref>. Consequently, such methods still rely on manual annotations of image regions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>To address these issues, recent advances include learning the mapping and colorization from large-scale dataset <ref type="bibr" target="#b10">[11]</ref> and the extension to video colorization <ref type="bibr" target="#b35">[36]</ref>.</p><p>Learning-based colorization Exploiting machine learning to automate the colorization process has received increasing attention in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b41">42]</ref>. Among existing works, the deep convolutional neural network has become the mainstream approach to learn color prediction from a large-scale dataset (e.g., ImageNet <ref type="bibr" target="#b27">[28]</ref>). Various network architectures have been proposed to address two key elements for convincing colorization: semantics and multi-modality <ref type="bibr" target="#b2">[3]</ref>. To model semantics, Iizuka et al. <ref type="bibr" target="#b12">[13]</ref> and Zhao et al. <ref type="bibr" target="#b41">[42]</ref> present a two-branch architecture that jointly learns and fuses local image features and global priors (e.g., semantic labels). Zhang et al. <ref type="bibr" target="#b37">[38]</ref> employ a cross-channel encoding scheme to provide semantic interpretability, which is also achieved by Larsson et al. <ref type="bibr" target="#b16">[17]</ref> that pre-trained their network for a classification task. To handle multimodality, some works proposed to predict per-pixel color distributions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> instead of a single color. These works have achieved impressive performance on images with moderate complexity but still suffer visual artifacts when processing complex images with multiple foreground objects as shown in <ref type="figure">Figure 2</ref>.</p><p>Our observation is that learning semantics at either image-level <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17]</ref> or pixel-level <ref type="bibr" target="#b41">[42]</ref> cannot sufficiently model the appearance variations of objects. Our work thus learns object-level semantics by training on the cropped object images and then fusing the learned objectlevel and full-image features to improve the performance of any off-the-shelf colorization networks.</p><p>Colorization for visual representation learning. Colorization has been used as a proxy task for learning visual representation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref> and visual tracking <ref type="bibr" target="#b31">[32]</ref>. The learned representation through colorization has been shown to transfer well to other downstream visual recognition tasks such as image classification, object detection, and segmentation. Our work is inspired by this line of research on self-supervised representation learning. Instead of aiming to learn a representation that generalizes well to object detection/segmentation, we focus on leveraging the off-the-shelf pre-trained object detector to improve image colorization.</p><p>Instance-aware image synthesis and manipulation. Instance-aware processing provides a clear figure-ground separation and facilitates synthesizing and manipulating visual appearance. Such approaches have been successfully applied to image generation <ref type="bibr" target="#b29">[30]</ref>, image-to-image translation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>, and semantic image synthesis <ref type="bibr" target="#b32">[33]</ref>. Our work leverages a similar high-level idea with these methods but differs in the following three aspects. First, unlike DA-GAN <ref type="bibr" target="#b22">[23]</ref> and FineGAN <ref type="bibr" target="#b29">[30]</ref> that focus only on one single instance, our method is capable of handling complex scenes with multiple instances via the proposed feature fusion module. Second, in contrast to InstaGAN [25] that processes non-overlapping instances sequentially, our method considers all potentially overlapping instances simultaneously and produces spatially coherent colorization. Third, compared with Pix2PixHD <ref type="bibr" target="#b32">[33]</ref> that uses instance boundary for improving synthesis quality, our work uses learned weight maps for blending features from multiple instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Our system takes a grayscale image X ? R H?W ?1 as input and predicts its two missing color channels Y ? R H?W ?2 in the CIE L * a * b * color space in an end-to-end fashion. Figure 3 illustrates our network architecture. First, we leverage an off-the-shelf pre-trained object detector to obtain multiple object bounding boxes {B i } N i=1 from the grayscale image, where N is the number of instances. We then generate a set of instance images {X i } N i=1 by resizing the images cropped from the grayscale image using the detected bounding boxes (Section 4.1). Next, we feed each instance image X i and input grayscale image X to the instance colorization network and full-image colorization network, respectively. The two networks share the same architecture (but different weights). We denote the extracted feature map of instance image X i and grayscale image X at the j-th network layer as f X i j and f X j (Section 4.2). Finally, we employ a fusion module that fuses all the instance features { f X i j } N i=1 with the full-image feature f X j at each layer. The fused full image feature at j-th layer, denoted as fX j , is then fed forward to j + 1-th layer. This step repeats until the last layer and obtains the predict color image Y (Section 4.3). We adopt a sequential approach that first trains the full-image network, followed by the instance network, and finally trains the feature fusion module by freezing the above two networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection</head><p>(Section 4.1)</p><formula xml:id="formula_0">{X i } N i=1 B i X i Input X Instance Colorization (Section 4.2) Fusion Module (Section 4.3) (Section 4.2) Full-image Colorization Y i Y GT i Y Y GT loss loss Figure 3. Method overview.</formula><p>Given a grayscale image X as input, our model starts with detecting the object bounding boxes (B i ) using an off-the-shelf object detection model. We then crop out every detected instance X i via B i and use instance colorization network to colorize X i . However, as the instances' colors may not be compatible with respect to the predicted background colors, we propose to fuse all the instances' feature maps in every layer with the extracted full-image feature map using the proposed fusion module. We can thus obtain globally consistent colorization results Y . Our training process sequentially trains our full-image colorization network, and the instance colorization network, and the proposed fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object detection</head><p>Our method leverages detected object instances for improving image colorization. To this end, we employ an offthe-shelf pre-trained network, Mask R-CNN <ref type="bibr" target="#b9">[10]</ref>, as our object detector. After detecting each object's bounding box B i , we crop out corresponding grayscale instance image X i and color instance image Y GT i from X and Y GT , and resize the cropped images to a resolution of 256 ? 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image colorization backbone</head><p>As shown in <ref type="figure">Figure 3</ref>, our network architecture contains two branches of colorization networks, one for colorizing the instance images and the other for colorizing the full image. We choose the architectures of these two networks so that they have the same number of layers to facilitate feature fusion (discussed in the next section). In this work, we adopt the main colorization network introduced in Zhang et al. <ref type="bibr" target="#b40">[41]</ref> as our backbones. Although these two colorization networks alone could predict the color instance images Y i and full image Y , we found that a na?ve blending of these results yield visible visual artifacts due to the inconsistency of the overlapping pixels. In the following section, we elaborate on how to fuse the intermediate feature maps from both instance and full-image networks to produce accurate and coherent colorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fusion module</head><p>Here, we discuss how to fuse the full-image feature with multiple instance features to achieve better colorization. <ref type="figure" target="#fig_0">Figure 4</ref> shows the architecture of our fusion module. Since the fusion takes place at multiple layers of the colorization networks, for the sake of simplicity, we only present the fusion module at the j-th layer. Apply the module to all the other layers is straightforward.</p><p>The fusion module takes inputs: (1) a full-image feature f X j ; (2) a bunch of instance features and corresponding object bounding boxes</p><formula xml:id="formula_1">{ f X i j , B i } N i=1 .</formula><p>For both kinds of features, we devise a small neural network with three convolutional layers to predict full-image weight map W F and perinstance weight map W i I . To fuse per-instance feature f X i j to the full-image feature f X j , we utilize the input bounding box B i , which defines the size and location of the instance. Specifically, we resize the instance feature f X i j as well as the weight map W i I to match the size of full-image and do zero padding on both of them. We denote resized the instance feature and weight map asf X i j andW i I . After that, we stack all the weight maps, apply softmax on each pixel, and obtain the fused feature using a weighted sum as follows:  where N is the number of instances.</p><formula xml:id="formula_2">fX j = f X j ?W F + N ? i=1f X i j ?W i I ,<label>(1)</label></formula><formula xml:id="formula_3">Full-image Feature ( f X j ) Instance Feature ({ f X i j } N i=1 ) Full-image</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss Function and Training</head><p>Following Zhang et al. <ref type="bibr" target="#b40">[41]</ref>, we adopt the smooth-1 loss with ? = 1 as follows:</p><formula xml:id="formula_4">? (x, y) = 1 2 (x ? y) 2 1l {|x?y|&lt;? } + ? (|x ? y| ? 1 2 ? )1l {|x?y| ? } (2)</formula><p>We train the whole network sequentially as follows. First, we train the full-image colorization and transfer the learned weights to initialize the instance colorization network. We then train the instance colorization network. Lastly, we freeze the weights in both the full-image model and instance model and move on training the fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present extensive experimental results to validate the proposed instance-aware colorization algorithm. We start by describing the datasets used in our experiments, performance evaluation metrics, and implementation details (Section 5.1). We then report the quantitative evaluation of three large-scale datasets and compare our results with the state-of-the-art colorization methods (Section 5.2). We show sample colorization results on several challenging images (Section 5.3). We carry out three ablation studies to validate our design choices (Section 5.4). Beyond standard performance benchmarking, we demonstrate the application of colorizing legacy black and white photographs (Section 5.6). We conclude the section with examples where our method fails (Section 5.7). Please refer to the project webpage for the dataset, source code, and additional visual comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setting</head><p>Datasets. We use three datasets for training and evaluation.</p><p>ImageNet <ref type="bibr" target="#b27">[28]</ref>: ImageNet dataset has been used by many existing colorization methods as a benchmark for performance evaluation. We use the original training split ( 1.3 million images) for training all the models and use the testing split (ctest10k) provided by <ref type="bibr" target="#b16">[17]</ref> with 10,000 images for evaluation.</p><p>COCO-Stuff <ref type="bibr" target="#b1">[2]</ref>: In contrast to the object-centric images in the ImageNet dataset, the COCO-Stuff dataset contains a wide variety of natural scenes with multiple objects present in the image. There are 118K images (each image is associated with a bounding box, instance segmentation, and semantic segmentation annotations). We use the 5,000 images in the original validation set for evaluation.</p><p>Places205 <ref type="bibr" target="#b42">[43]</ref>: To investigate how well a colorization method performs on images from a different dataset, we use the 20,500 testing images (from 205 categories) from the Places205 for evaluation. Note that we use the Place205 dataset only for evaluating the transferability. We do not use its training set and the scene category labels for training. Evaluation metrics. Following the experimental protocol by existing colorization methods, we report the PSNR and SSIM to quantify the colorization quality. To compute the SSIM on color images, we average the SSIM values computed from individual channels. We further use the recently proposed perceptual metric LPIPS by Zhang et al. <ref type="bibr" target="#b39">[40]</ref> (version 0.1; with VGG backbone). Training details. We adopt a three-step training process on the ImageNet dataset as follows.</p><p>(1) Full-image colorization network: We initialize the network with the pre-trained weight provided by <ref type="bibr" target="#b40">[41]</ref>. We train the network for two epochs with a learning rate of 1e-5. (2) Instance colorization network: We start with the In all the training processes, we use the ADAM optimizer <ref type="bibr" target="#b15">[16]</ref> with ? 1 = 0.99 and ? 2 = 0.999. For training, we resize all the images to a resolution of 256 ? 256. Training the model on the ImageNet takes about three days on a desktop machine with one single RTX 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative comparisons</head><p>Comparisons with the state-of-the-arts. We report the quantitative comparisons on three datasets in <ref type="table" target="#tab_1">Table 1</ref>. The first block of the results shows models trained on the Ima-geNet dataset. Our instance-aware model performs favorably against several recent methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref> on all three datasets, highlighting the effectiveness of our approach. Note that we adopted the automatic version of Zhang et al. <ref type="bibr" target="#b40">[41]</ref> (i.e., without using any color guidance) in all the experiments. In the second block, we show the results using our model finetuned on the COCO-Stuff training set (denoted by the "*"). As the COCO-Stuff dataset contains more diverse and challenging scenes, our results show that finetuning on the COCO-Stuff dataset further improves the performance on the other two datasets as well. To highlight the effectiveness of the proposed instance-aware colorization module, we also report the results of Zhang et al. <ref type="bibr" target="#b40">[41]</ref> finetuned on the same dataset as a strong baseline for a fair comparison. For evaluating the performance at the instance-level, we take the fullimage ground-truth/prediction and crop the instances us- ing the ground-truth bounding boxes to form instance-level ground-truth/predictions. <ref type="table" target="#tab_2">Table 2</ref> summarizes the performance computed by averaging over all the instances on the COCO-Stuff dataset. The results present a significant performance boost gained by our method in all metrics, which further highlights the contribution of instance-aware colorization to the improved performance.</p><p>User study. We conducted a user study to quantify the user-preference on the colorization results generated by our method and another two strong baselines, Zhang et al. <ref type="bibr" target="#b36">[37]</ref> (finetuned on the COCO-Stuff dataset) and a popular online colorization method DeOldify <ref type="bibr" target="#b0">[1]</ref>. We randomly select 100 images from the COCO-Stuff validation dataset. For each participant, we show him/her a pair of colorized results and ask for the preference (forced-choice comparison). In total, we have 24 participants casting 2400 votes in total.</p><p>The results show that on average our method is preferred when compared with Zhang et al. <ref type="bibr" target="#b36">[37]</ref> (61% v.s. 39%) and DeOldify <ref type="bibr" target="#b0">[1]</ref> (72% v.s. 28%). Interestingly, while DeOldify does not produce accurate colorization evaluated in the benchmark experiment, the saturated colorized results are (f) Ours <ref type="figure">Figure 5</ref>. Visual Comparisons with the state-of-the-arts. Our method predicts visually pleasing colors from complex scenes with multiple object instances. sometimes more preferred by the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Visual results</head><p>Comparisons with the state-of-the-art. <ref type="figure">Figure 5</ref> shows sample comparisons with other competing baseline methods on COCO-Stuff. In general, we observe a consistent improvement in visual quality, particularly for scenes with multiple instances. Visualizing the fusion network. <ref type="figure" target="#fig_2">Figure 6</ref> visualizes the learned masks for fusing instance-level and full-image level features at multiple levels. We show that the proposed instance-aware processing leads to improved visual quality for complex scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation study</head><p>Here, we conduct ablation study to validate several important design choices in our model in <ref type="table" target="#tab_3">Table 3</ref>. In all ablation study experiments, we use the COCO-Stuff validation dataset. First, we show that fusing features extracted from the instance network with the full-image network improve the performance. Fusing features for both encoder and decoder perform the best. Second, we explore different strate-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Layer3 Layer7 Layer10</p><p>Zhang et al. <ref type="bibr" target="#b40">[41]</ref> Our results gies of selecting object bounding boxes as inputs for our instance network. The results indicate that our default set- ting of choosing the top eight bounding boxes in terms of confidence score returned by object detector performs best and is slightly better than using the ground-truth bounding box. Third, we experiment with two alternative approaches (using the detected box as a mask or using the ground-truth instance mask provided in the COCO-Stuff dataset) for fusing features from multiple potentially overlapping object instances and the features from the full-image network. Using our fusion module obtains a notable performance boost than the other two options. This shows the capability of our fusion module to tackle more challenging scenarios with multiple overlapping objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Runtime analysis</head><p>Our colorization network involves two steps: (1) colorizing the individual instances and outputting the instance features; and (2) fusing the instance features into the full-image feature and producing a full-image colorization. Using a machine with Intel i9-7900X 3.30GHz CPU, 32GB memory, and NVIDIA RTX 2080ti GPU, our average inference time over all the experiments is 0.187s for an image of resolution 256 ? 256. Each of two steps takes approximately 50% of the running time, while the complexity of step 1 is proportional to the number of input instances and ranges from 0.013s (one instance) to 0.1s (eight instances).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Expert Our results </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Colorizing legacy black and white photos</head><p>We apply our colorization model to colorize legacy black and white photographs. <ref type="figure" target="#fig_3">Figure 7</ref> shows sample results along with manual colorization results by human expert 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Failure modes</head><p>We show 2 examples of failure cases in <ref type="figure">Figure 8</ref>. When the instances were not detected, our model reverts back to the full-image colorization network. As a result, our method may produce visible artifacts such as washed-out colors or bleeding across object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We present a novel instance-aware image colorization. By leveraging an off-the-shelf object detection model to crop out the images, our architecture extracts the feature from our instance branch and full images branch, then we fuse them with our newly proposed fusion module and obtain a better feature map to predict the better results. Through extensive experiments, we show that our work compares favorably against existing methods on three benchmark datasets.</p><p>In this supplementary document, we provide additional visual comparisons and quantitative evaluation to complement the main manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visualization of Fusion Module</head><p>We show two images where multiple instances have been detected by the object detection model. We visualize the weighted mask predicted by our fusion module at multiple layers (3rd, 7th, and 10th layers). Note that our fusion module learns to adaptively blend the features extracted from the instance colorization branch to enforce coherent colorization for the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Layer3</p><formula xml:id="formula_5">Layer7 Layer10 Layer3 Layer7 Layer10 Output Input Layer3 Layer7 Layer10 Layer3 Layer7</formula><p>Layer10 Output <ref type="figure">Figure 9</ref>. Visualizing the fusion network. The visualized weighted mask in layer3, layer7 and layer10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extended quantitative evaluation</head><p>In this section, we provide two additional quantitative evaluation.</p><p>Baselines using pre-trained weights. As we mentioned in our paper, we use Zhang et al. <ref type="bibr" target="#b40">[41]</ref> as our backbone colorization model. However, the model in <ref type="bibr" target="#b40">[41]</ref> is trained for guidance colorization with a resolution of 176?176. In our setting, we need a fully-automatic colorization model with resolution 256?256. In light of this, we retrain the model on the ImageNet dataset to fit our setting. Here we carry out an experiment on the COCOStuff dataset. We show a quantitative comparison and qualitative comparison between (1) the original pre-trained weight provided by the authors and (2) the weight after the fine-tuning step. In general, re-training the model under the resolution of 256?256 results in slight performance degradation under PSNR and similar performance in terms of LPIPS and SSIM with respect to the original pre-trained model.  <ref type="figure">Figure 11</ref>. Different model weight of <ref type="bibr" target="#b40">[41]</ref> We present some images that are inferencing from different weight of <ref type="bibr" target="#b40">[41]</ref>. The images above from left to right is Grayscale, Original weight, retrain on Imagenet, and fine-tuned on COCOStuff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. User Study setup</head><p>Here we describe the detailed procedure of our user study. We first ask the subjects to read a document, including the instruction of the webpage and selection basis (according to the color correctness and naturalness). The subjects then start a pair-wise forced-choice without any ground truth reference or grayscale reference (i.e., no-reference tests). We embed two redundant comparisons for sanity check (i.e the same comparisons appear in the user study twice). We reject the votes if the subjects do not answer consistently for both redundant comparisons. We summarize the results in the main paper. We provide all the images used in our user study as well as the user preference votes in the supplementary web-based viewer (click the 'User Study Result' button).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure cases</head><p>While we have shown significantly improved results, single image colorization remains a challenging problem. <ref type="figure">Figure 13</ref> shows two examples where our model is unable to predict the vibrant, bright colors from the given grayscale input image.  <ref type="figure">Figure 13</ref>. Failure cases. We present some images that are out of distribution and are capable of colorizing plausible color. The images above from left to right is (a) grayscale input, (b) ground truth color image and (c) our result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Feature fusion module. Given the full-image feature f X j and a bunch of instance features { f X i j } N i=1 from the j-th layer of the colorization network, we first predict the corresponding weight map W F and W i I through a small neural network with three convolutional layers. Both instance feature and weight map are resized, padded with zero to match the original size and local in the full image. The final fused feature fX j is thus computed using the weighted sum of retargeted features (seeEquation 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Iizuka et al.<ref type="bibr" target="#b12">[13]</ref> (c) Larrson et al.<ref type="bibr" target="#b17">[18]</ref> (d) Deoldify<ref type="bibr" target="#b0">[1]</ref> (e) Zhang et al.<ref type="bibr" target="#b40">[41]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Visualizing the fusion network. The visualized weighted mask in layer3, layer7 and layer10 show that our model learns to adaptively blend the features across different layers. Fusing instance-level features help improve colorization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Colorizing legacy photographs. The middle column shows the manually colorized results by the experts.(a) Missing detections (b) Superimposed detections Figure 8. Failure cases. (Left) our model reverts back to the fullimage colorization when a lot of vases are missing in the detection. (Right) the fusion module may get confused when there are many superimposed object bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Grayscale input (b) Ground truth color image (c) Our result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison at the full-image level. The methods in the first block are trained using the ImageNet dataset. The symbol * denotes the methods that are finetuned on the COCO-Stuff training set.PSNR ? SSIM ? LPIPS ? PSNR ? SSIM ? LPIPS ? PSNR ? SSIM ?</figDesc><table><row><cell>Method</cell><cell cols="3">Imagenet ctest10k</cell><cell cols="3">COCOStuff validation split</cell><cell cols="3">Places205 validation split</cell></row><row><cell cols="2">LPIPS ? lizuka et al. [13] 0.200</cell><cell>23.636</cell><cell>0.917</cell><cell>0.185</cell><cell>23.863</cell><cell>0.922</cell><cell>0.146</cell><cell>25.581</cell><cell>0.950</cell></row><row><cell>Larsson et al. [17]</cell><cell>0.188</cell><cell>25.107</cell><cell>0.927</cell><cell>0.183</cell><cell>25.061</cell><cell>0.930</cell><cell>0.161</cell><cell>25.722</cell><cell>0.951</cell></row><row><cell>Zhang et al. [38]</cell><cell>0.238</cell><cell>21.791</cell><cell>0.892</cell><cell>0.234</cell><cell>21.838</cell><cell>0.895</cell><cell>0.205</cell><cell>22.581</cell><cell>0.921</cell></row><row><cell>Zhang et al. [41]</cell><cell>0.145</cell><cell>26.166</cell><cell>0.932</cell><cell>0.138</cell><cell>26.823</cell><cell>0.937</cell><cell>0.149</cell><cell>25.823</cell><cell>0.948</cell></row><row><cell>Deoldify et al. [1]</cell><cell>0.187</cell><cell>23.537</cell><cell>0.914</cell><cell>0.180</cell><cell>23.692</cell><cell>0.920</cell><cell>0.161</cell><cell>23.983</cell><cell>0.939</cell></row><row><cell>Lei et al. [19]</cell><cell>0.202</cell><cell>24.522</cell><cell>0.917</cell><cell>0.191</cell><cell>24.588</cell><cell>0.922</cell><cell>0.175</cell><cell>25.072</cell><cell>0.942</cell></row><row><cell>Ours</cell><cell>0.134</cell><cell>26.980</cell><cell>0.933</cell><cell>0.125</cell><cell>27.777</cell><cell>0.940</cell><cell>0.130</cell><cell>27.167</cell><cell>0.954</cell></row><row><cell>Zhang et al. [41]*</cell><cell>0.140</cell><cell>26.482</cell><cell>0.932</cell><cell>0.128</cell><cell>27.251</cell><cell>0.938</cell><cell>0.153</cell><cell>25.720</cell><cell>0.947</cell></row><row><cell>Ours*</cell><cell>0.125</cell><cell>27.562</cell><cell>0.937</cell><cell>0.110</cell><cell>28.592</cell><cell>0.944</cell><cell>0.120</cell><cell>27.800</cell><cell>0.957</cell></row><row><cell cols="4">pre-trained weight from the trained full-image colorization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">network above and finetune the model for five epochs with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">a learning rate of 5e-5 on the extracted instances from the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">dataset. (3) Fusion module: Once both the full-image and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">instance network have been trained (i.e., warmed-up), we</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">integrate them with the proposed fusion module. We fine-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">tune all the trainable parameters for 2 epochs with a learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">rate of 2e-5. In our implementation, the numbers of chan-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">nels of full-image feature, instance feature and fused feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">in all 13 layers are 64, 128, 256, 512, 512, 512, 512, 256,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">256, 128, 128, 128 and 128.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison at the instance level. The methods in the first block are trained using the ImageNet dataset. The symbol * denotes the methods that are finetuned on the COCO-Stuff training set.</figDesc><table><row><cell>Method</cell><cell cols="3">COCOStuff validation split</cell></row><row><cell></cell><cell cols="3">LPIPS ? PSNR ? SSIM ?</cell></row><row><cell>lizuka et al. [13]</cell><cell>0.192</cell><cell>23.444</cell><cell>0.900</cell></row><row><cell>Larsson et al. [17]</cell><cell>0.179</cell><cell>25.249</cell><cell>0.914</cell></row><row><cell>Zhang et al. [38]</cell><cell>0.219</cell><cell>22.213</cell><cell>0.877</cell></row><row><cell>Zhang et al. [41]</cell><cell>0.154</cell><cell>26.447</cell><cell>0.918</cell></row><row><cell>Deoldify et al. [1]</cell><cell>0.174</cell><cell>23.923</cell><cell>0.904</cell></row><row><cell>Lei et al. [19]</cell><cell>0.177</cell><cell>24.914</cell><cell>0.908</cell></row><row><cell>Ours</cell><cell>0.115</cell><cell>28.339</cell><cell>0.929</cell></row><row><cell>Zhang et al. [41]*</cell><cell>0.149</cell><cell>26.675</cell><cell>0.919</cell></row><row><cell>Ours*</cell><cell>0.095</cell><cell>29.522</cell><cell>0.938</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablations. We validate our design choices by comparing with several alternative options.</figDesc><table><row><cell></cell><cell cols="3">(a) Different Fusion Part</cell><cell></cell><cell cols="4">(b) Different Bounding Box Selection</cell><cell cols="3">(c) Different Weighted Sum</cell><cell></cell></row><row><cell cols="2">Fusion Part</cell><cell cols="3">COCOStuff validation split</cell><cell>Box Selection</cell><cell cols="3">COCOStuff validation split</cell><cell>Weighted Sum</cell><cell cols="3">COCOStuff validation split</cell></row><row><cell cols="2">Encoder Decoder</cell><cell cols="3">LPIPS ? PSNR ? SSIM ?</cell><cell></cell><cell cols="3">LPIPS ? PSNR ? SSIM ?</cell><cell></cell><cell cols="3">LPIPS ? PSNR ? SSIM ?</cell></row><row><cell>? ?</cell><cell>? ?</cell><cell>0.128 0.120 0.117 0.110</cell><cell>27.251 28.146 27.959 28.592</cell><cell>0.938 0.942 0.941 0.944</cell><cell>Select top 8 Random select 8 Select by threshold G.T. bounding box</cell><cell>0.110 0.113 0.117 0.111</cell><cell>28.592 28.386 28.139 28.470</cell><cell>0.944 0.943 0.942 0.944</cell><cell>Box mask G.T. mask Fusion module</cell><cell>0.140 0.199 0.110</cell><cell>26.456 24.243 28.592</cell><cell>0.932 0.921 0.944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Quantitative Comparison with different models weight on COCOStuffWe test our colorization backbone model<ref type="bibr" target="#b40">[41]</ref> with different weight on COCOStuff.</figDesc><table><row><cell>Method</cell><cell cols="3">COCOStuff validation split</cell></row><row><cell></cell><cell cols="3">LPIPS ? PSNR ? SSIM ?</cell></row><row><cell>(a) Original model weight [41]</cell><cell>0.133</cell><cell>27.050</cell><cell>0.937</cell></row><row><cell>(b) Retrain on the ImageNet dataset</cell><cell>0.138</cell><cell>26.823</cell><cell>0.937</cell></row><row><cell>(c) Finetuned on the COCOStuff dataset</cell><cell>0.128</cell><cell>27.251</cell><cell>0.938</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">bit.ly/color_history_photos</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The project was funded in part by the Ministry of Science and Technology of Taiwan (108-2218-E-007 -050-and 107-2221-E-007-088-MY3).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">jantic/deoldify: A deep learning based project for colorizing and restoring old images (and video!)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Antic</surname></persName>
		</author>
		<ptr target="https://github.com/jantic/DeOldify,2019.Online" />
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">accessed</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic image colorization via multimodal predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic colorization with internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Yong-Sang</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Kumar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu-Yeung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno>156:1-156:8</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning diverse image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao-Chuang</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Jin</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning large-scale automatic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pixcolor: Pixel recursive colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ee Sin Ng, and Huang Zhiyong. Image colorization using similar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Kumar Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Yong-Sang</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep exemplar-based colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<idno>47:1-47:16</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An adaptive edge detection based colorization algorithm and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Shin</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ja-Ling</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Let there be color!: Joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>110:1-110:11</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Colorization by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revital</forename><surname>Irony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EGSR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully automatic video colorization with self-regularization and diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="689" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingge</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Sing</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<idno>152:1-152:9</idno>
		<title level="m">trinsic colorization. ACM TOG (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Qing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EGSR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Da-gan: Instance-level image translation by deep attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural consistency and controllability for diverse colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safa</forename><surname>Messaoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Instagan: Instance-aware image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingge</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<title level="m">Manga colorization. ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1214" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amelie</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards instance-level image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finegan: Unsupervised hierarchical disentanglement for fine-grained object generation and discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lazybrush: Flexible painting tool for hand-drawn cartoons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Skora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dingliana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGH</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transferring color to greyscale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomihisa</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast image and video colorization using chrominance blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep exemplarbased video colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Bermak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stage sketch colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Liu</surname></persName>
		</author>
		<idno>261:1-261:14</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Realtime user-guided image colorization with learned deep priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>119:1-119:11</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixellevel semantics guided image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
